Index: src/contrib/approximationscheduler/approximation.thrift
===================================================================
--- src/contrib/approximationscheduler/approximation.thrift	(revision 0)
+++ src/contrib/approximationscheduler/approximation.thrift	(working copy)
@@ -0,0 +1,37 @@
+#!/usr/local/bin/thrift -java
+
+namespace java org.apache.hadoop.mapred.approximation
+
+/**
+ * Defines a MapReduce job that can be approximated.
+ */
+struct Job {
+	1: string jobId,
+	2: i32 nmap,
+	3: i32 lmap,
+	4: i32 nred,
+	5: i32 lred,
+	
+	// Algorithm approximation
+	6:  bool approxAlgo,
+	7:  optional i32 lmapapprox,
+	8:  optional i32 lredapprox,
+	9:  optional double approxAlgoMax,
+	10: optional double approxAlgoVal,
+	
+	// Runtime drop
+	11: bool approxDrop,
+	12: optional double approxDropMax,
+	13: optional double approxDropVal,
+	
+	// Drop input data
+	14: bool approxInput,
+}
+
+/**
+ * Scheduler for approximations.
+ */
+service ApproximationScheduler {
+	list<Job> schedule(1:list<Job> jobs),
+}
+
Index: src/contrib/approximationscheduler/build.xml
===================================================================
--- src/contrib/approximationscheduler/build.xml	(revision 0)
+++ src/contrib/approximationscheduler/build.xml	(working copy)
@@ -0,0 +1,39 @@
+<?xml version="1.0"?>
+
+<!--
+   Licensed to the Apache Software Foundation (ASF) under one or more
+   contributor license agreements.  See the NOTICE file distributed with
+   this work for additional information regarding copyright ownership.
+   The ASF licenses this file to You under the Apache License, Version 2.0
+   (the "License"); you may not use this file except in compliance with
+   the License.  You may obtain a copy of the License at
+
+       http://www.apache.org/licenses/LICENSE-2.0
+
+   Unless required by applicable law or agreed to in writing, software
+   distributed under the License is distributed on an "AS IS" BASIS,
+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+   See the License for the specific language governing permissions and
+   limitations under the License.
+-->
+
+<!-- 
+Before you can run these subtargets directly, you need 
+to call at top-level: ant deploy-contrib compile-core-test
+-->
+<project name="approximationscheduler" default="jar">
+
+  <import file="../build-contrib.xml"/>
+
+  <!-- ====================================================== -->
+  <!-- Package a Hadoop contrib                               -->
+  <!-- ====================================================== -->
+  <target name="package" depends="jar, jar-examples" unless="skip.contrib">
+    <copy todir="${dist.dir}/lib" includeEmptyDirs="false" flatten="true">
+      <fileset dir="${build.dir}">
+        <include name="hadoop-${name}-${version}.jar" />
+      </fileset>
+    </copy>
+  </target>
+
+</project>
Index: src/contrib/approximationscheduler/compile
===================================================================
--- src/contrib/approximationscheduler/compile	(revision 0)
+++ src/contrib/approximationscheduler/compile	(working copy)
@@ -0,0 +1,7 @@
+#!/bin/bash
+
+thrift --gen java approximation.thrift
+#thrift --gen py approximation.thrift
+
+ant jar
+cp /home/goiri/hadoop-1.1.2-src/build/contrib/approximationscheduler/hadoop-approximationscheduler-1.1.3.jar /home/goiri/hadoop-1.1.2/lib/
\ No newline at end of file
Index: src/contrib/approximationscheduler/gen-java/compile
===================================================================
--- src/contrib/approximationscheduler/gen-java/compile	(revision 0)
+++ src/contrib/approximationscheduler/gen-java/compile	(working copy)
@@ -0,0 +1,2 @@
+mkdir classes
+javac -classpath "/home/goiri/.m2/repository/org/apache/thrift/libthrift/0.7.0/libthrift-0.7.0.jar" -d classes org/apache/hadoop/mapred/approximation/*.java
Index: src/contrib/approximationscheduler/gen-java/org/apache/hadoop/mapred/approximation/ApproximationScheduler.java
===================================================================
--- src/contrib/approximationscheduler/gen-java/org/apache/hadoop/mapred/approximation/ApproximationScheduler.java	(revision 0)
+++ src/contrib/approximationscheduler/gen-java/org/apache/hadoop/mapred/approximation/ApproximationScheduler.java	(working copy)
@@ -0,0 +1,989 @@
+/**
+ * Autogenerated by Thrift Compiler (0.8.0)
+ *
+ * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
+ *  @generated
+ */
+package org.apache.hadoop.mapred.approximation;
+
+import org.apache.thrift.scheme.IScheme;
+import org.apache.thrift.scheme.SchemeFactory;
+import org.apache.thrift.scheme.StandardScheme;
+
+import org.apache.thrift.scheme.TupleScheme;
+import org.apache.thrift.protocol.TTupleProtocol;
+import java.util.List;
+import java.util.ArrayList;
+import java.util.Map;
+import java.util.HashMap;
+import java.util.EnumMap;
+import java.util.Set;
+import java.util.HashSet;
+import java.util.EnumSet;
+import java.util.Collections;
+import java.util.BitSet;
+import java.nio.ByteBuffer;
+import java.util.Arrays;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+public class ApproximationScheduler {
+
+  /**
+   * Scheduler for approximations.
+   */
+  public interface Iface {
+
+    public List<Job> schedule(List<Job> jobs) throws org.apache.thrift.TException;
+
+  }
+
+  public interface AsyncIface {
+
+    public void schedule(List<Job> jobs, org.apache.thrift.async.AsyncMethodCallback<AsyncClient.schedule_call> resultHandler) throws org.apache.thrift.TException;
+
+  }
+
+  public static class Client extends org.apache.thrift.TServiceClient implements Iface {
+    public static class Factory implements org.apache.thrift.TServiceClientFactory<Client> {
+      public Factory() {}
+      public Client getClient(org.apache.thrift.protocol.TProtocol prot) {
+        return new Client(prot);
+      }
+      public Client getClient(org.apache.thrift.protocol.TProtocol iprot, org.apache.thrift.protocol.TProtocol oprot) {
+        return new Client(iprot, oprot);
+      }
+    }
+
+    public Client(org.apache.thrift.protocol.TProtocol prot)
+    {
+      super(prot, prot);
+    }
+
+    public Client(org.apache.thrift.protocol.TProtocol iprot, org.apache.thrift.protocol.TProtocol oprot) {
+      super(iprot, oprot);
+    }
+
+    public List<Job> schedule(List<Job> jobs) throws org.apache.thrift.TException
+    {
+      send_schedule(jobs);
+      return recv_schedule();
+    }
+
+    public void send_schedule(List<Job> jobs) throws org.apache.thrift.TException
+    {
+      schedule_args args = new schedule_args();
+      args.setJobs(jobs);
+      sendBase("schedule", args);
+    }
+
+    public List<Job> recv_schedule() throws org.apache.thrift.TException
+    {
+      schedule_result result = new schedule_result();
+      receiveBase(result, "schedule");
+      if (result.isSetSuccess()) {
+        return result.success;
+      }
+      throw new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.MISSING_RESULT, "schedule failed: unknown result");
+    }
+
+  }
+  public static class AsyncClient extends org.apache.thrift.async.TAsyncClient implements AsyncIface {
+    public static class Factory implements org.apache.thrift.async.TAsyncClientFactory<AsyncClient> {
+      private org.apache.thrift.async.TAsyncClientManager clientManager;
+      private org.apache.thrift.protocol.TProtocolFactory protocolFactory;
+      public Factory(org.apache.thrift.async.TAsyncClientManager clientManager, org.apache.thrift.protocol.TProtocolFactory protocolFactory) {
+        this.clientManager = clientManager;
+        this.protocolFactory = protocolFactory;
+      }
+      public AsyncClient getAsyncClient(org.apache.thrift.transport.TNonblockingTransport transport) {
+        return new AsyncClient(protocolFactory, clientManager, transport);
+      }
+    }
+
+    public AsyncClient(org.apache.thrift.protocol.TProtocolFactory protocolFactory, org.apache.thrift.async.TAsyncClientManager clientManager, org.apache.thrift.transport.TNonblockingTransport transport) {
+      super(protocolFactory, clientManager, transport);
+    }
+
+    public void schedule(List<Job> jobs, org.apache.thrift.async.AsyncMethodCallback<schedule_call> resultHandler) throws org.apache.thrift.TException {
+      checkReady();
+      schedule_call method_call = new schedule_call(jobs, resultHandler, this, ___protocolFactory, ___transport);
+      this.___currentMethod = method_call;
+      ___manager.call(method_call);
+    }
+
+    public static class schedule_call extends org.apache.thrift.async.TAsyncMethodCall {
+      private List<Job> jobs;
+      public schedule_call(List<Job> jobs, org.apache.thrift.async.AsyncMethodCallback<schedule_call> resultHandler, org.apache.thrift.async.TAsyncClient client, org.apache.thrift.protocol.TProtocolFactory protocolFactory, org.apache.thrift.transport.TNonblockingTransport transport) throws org.apache.thrift.TException {
+        super(client, protocolFactory, transport, resultHandler, false);
+        this.jobs = jobs;
+      }
+
+      public void write_args(org.apache.thrift.protocol.TProtocol prot) throws org.apache.thrift.TException {
+        prot.writeMessageBegin(new org.apache.thrift.protocol.TMessage("schedule", org.apache.thrift.protocol.TMessageType.CALL, 0));
+        schedule_args args = new schedule_args();
+        args.setJobs(jobs);
+        args.write(prot);
+        prot.writeMessageEnd();
+      }
+
+      public List<Job> getResult() throws org.apache.thrift.TException {
+        if (getState() != org.apache.thrift.async.TAsyncMethodCall.State.RESPONSE_READ) {
+          throw new IllegalStateException("Method call not finished!");
+        }
+        org.apache.thrift.transport.TMemoryInputTransport memoryTransport = new org.apache.thrift.transport.TMemoryInputTransport(getFrameBuffer().array());
+        org.apache.thrift.protocol.TProtocol prot = client.getProtocolFactory().getProtocol(memoryTransport);
+        return (new Client(prot)).recv_schedule();
+      }
+    }
+
+  }
+
+  public static class Processor<I extends Iface> extends org.apache.thrift.TBaseProcessor<I> implements org.apache.thrift.TProcessor {
+    private static final Logger LOGGER = LoggerFactory.getLogger(Processor.class.getName());
+    public Processor(I iface) {
+      super(iface, getProcessMap(new HashMap<String, org.apache.thrift.ProcessFunction<I, ? extends org.apache.thrift.TBase>>()));
+    }
+
+    protected Processor(I iface, Map<String,  org.apache.thrift.ProcessFunction<I, ? extends  org.apache.thrift.TBase>> processMap) {
+      super(iface, getProcessMap(processMap));
+    }
+
+    private static <I extends Iface> Map<String,  org.apache.thrift.ProcessFunction<I, ? extends  org.apache.thrift.TBase>> getProcessMap(Map<String,  org.apache.thrift.ProcessFunction<I, ? extends  org.apache.thrift.TBase>> processMap) {
+      processMap.put("schedule", new schedule());
+      return processMap;
+    }
+
+    private static class schedule<I extends Iface> extends org.apache.thrift.ProcessFunction<I, schedule_args> {
+      public schedule() {
+        super("schedule");
+      }
+
+      protected schedule_args getEmptyArgsInstance() {
+        return new schedule_args();
+      }
+
+      protected schedule_result getResult(I iface, schedule_args args) throws org.apache.thrift.TException {
+        schedule_result result = new schedule_result();
+        result.success = iface.schedule(args.jobs);
+        return result;
+      }
+    }
+
+  }
+
+  public static class schedule_args implements org.apache.thrift.TBase<schedule_args, schedule_args._Fields>, java.io.Serializable, Cloneable   {
+    private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("schedule_args");
+
+    private static final org.apache.thrift.protocol.TField JOBS_FIELD_DESC = new org.apache.thrift.protocol.TField("jobs", org.apache.thrift.protocol.TType.LIST, (short)1);
+
+    private static final Map<Class<? extends IScheme>, SchemeFactory> schemes = new HashMap<Class<? extends IScheme>, SchemeFactory>();
+    static {
+      schemes.put(StandardScheme.class, new schedule_argsStandardSchemeFactory());
+      schemes.put(TupleScheme.class, new schedule_argsTupleSchemeFactory());
+    }
+
+    public List<Job> jobs; // required
+
+    /** The set of fields this struct contains, along with convenience methods for finding and manipulating them. */
+    public enum _Fields implements org.apache.thrift.TFieldIdEnum {
+      JOBS((short)1, "jobs");
+
+      private static final Map<String, _Fields> byName = new HashMap<String, _Fields>();
+
+      static {
+        for (_Fields field : EnumSet.allOf(_Fields.class)) {
+          byName.put(field.getFieldName(), field);
+        }
+      }
+
+      /**
+       * Find the _Fields constant that matches fieldId, or null if its not found.
+       */
+      public static _Fields findByThriftId(int fieldId) {
+        switch(fieldId) {
+          case 1: // JOBS
+            return JOBS;
+          default:
+            return null;
+        }
+      }
+
+      /**
+       * Find the _Fields constant that matches fieldId, throwing an exception
+       * if it is not found.
+       */
+      public static _Fields findByThriftIdOrThrow(int fieldId) {
+        _Fields fields = findByThriftId(fieldId);
+        if (fields == null) throw new IllegalArgumentException("Field " + fieldId + " doesn't exist!");
+        return fields;
+      }
+
+      /**
+       * Find the _Fields constant that matches name, or null if its not found.
+       */
+      public static _Fields findByName(String name) {
+        return byName.get(name);
+      }
+
+      private final short _thriftId;
+      private final String _fieldName;
+
+      _Fields(short thriftId, String fieldName) {
+        _thriftId = thriftId;
+        _fieldName = fieldName;
+      }
+
+      public short getThriftFieldId() {
+        return _thriftId;
+      }
+
+      public String getFieldName() {
+        return _fieldName;
+      }
+    }
+
+    // isset id assignments
+    public static final Map<_Fields, org.apache.thrift.meta_data.FieldMetaData> metaDataMap;
+    static {
+      Map<_Fields, org.apache.thrift.meta_data.FieldMetaData> tmpMap = new EnumMap<_Fields, org.apache.thrift.meta_data.FieldMetaData>(_Fields.class);
+      tmpMap.put(_Fields.JOBS, new org.apache.thrift.meta_data.FieldMetaData("jobs", org.apache.thrift.TFieldRequirementType.DEFAULT, 
+          new org.apache.thrift.meta_data.ListMetaData(org.apache.thrift.protocol.TType.LIST, 
+              new org.apache.thrift.meta_data.StructMetaData(org.apache.thrift.protocol.TType.STRUCT, Job.class))));
+      metaDataMap = Collections.unmodifiableMap(tmpMap);
+      org.apache.thrift.meta_data.FieldMetaData.addStructMetaDataMap(schedule_args.class, metaDataMap);
+    }
+
+    public schedule_args() {
+    }
+
+    public schedule_args(
+      List<Job> jobs)
+    {
+      this();
+      this.jobs = jobs;
+    }
+
+    /**
+     * Performs a deep copy on <i>other</i>.
+     */
+    public schedule_args(schedule_args other) {
+      if (other.isSetJobs()) {
+        List<Job> __this__jobs = new ArrayList<Job>();
+        for (Job other_element : other.jobs) {
+          __this__jobs.add(new Job(other_element));
+        }
+        this.jobs = __this__jobs;
+      }
+    }
+
+    public schedule_args deepCopy() {
+      return new schedule_args(this);
+    }
+
+    @Override
+    public void clear() {
+      this.jobs = null;
+    }
+
+    public int getJobsSize() {
+      return (this.jobs == null) ? 0 : this.jobs.size();
+    }
+
+    public java.util.Iterator<Job> getJobsIterator() {
+      return (this.jobs == null) ? null : this.jobs.iterator();
+    }
+
+    public void addToJobs(Job elem) {
+      if (this.jobs == null) {
+        this.jobs = new ArrayList<Job>();
+      }
+      this.jobs.add(elem);
+    }
+
+    public List<Job> getJobs() {
+      return this.jobs;
+    }
+
+    public schedule_args setJobs(List<Job> jobs) {
+      this.jobs = jobs;
+      return this;
+    }
+
+    public void unsetJobs() {
+      this.jobs = null;
+    }
+
+    /** Returns true if field jobs is set (has been assigned a value) and false otherwise */
+    public boolean isSetJobs() {
+      return this.jobs != null;
+    }
+
+    public void setJobsIsSet(boolean value) {
+      if (!value) {
+        this.jobs = null;
+      }
+    }
+
+    public void setFieldValue(_Fields field, Object value) {
+      switch (field) {
+      case JOBS:
+        if (value == null) {
+          unsetJobs();
+        } else {
+          setJobs((List<Job>)value);
+        }
+        break;
+
+      }
+    }
+
+    public Object getFieldValue(_Fields field) {
+      switch (field) {
+      case JOBS:
+        return getJobs();
+
+      }
+      throw new IllegalStateException();
+    }
+
+    /** Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise */
+    public boolean isSet(_Fields field) {
+      if (field == null) {
+        throw new IllegalArgumentException();
+      }
+
+      switch (field) {
+      case JOBS:
+        return isSetJobs();
+      }
+      throw new IllegalStateException();
+    }
+
+    @Override
+    public boolean equals(Object that) {
+      if (that == null)
+        return false;
+      if (that instanceof schedule_args)
+        return this.equals((schedule_args)that);
+      return false;
+    }
+
+    public boolean equals(schedule_args that) {
+      if (that == null)
+        return false;
+
+      boolean this_present_jobs = true && this.isSetJobs();
+      boolean that_present_jobs = true && that.isSetJobs();
+      if (this_present_jobs || that_present_jobs) {
+        if (!(this_present_jobs && that_present_jobs))
+          return false;
+        if (!this.jobs.equals(that.jobs))
+          return false;
+      }
+
+      return true;
+    }
+
+    @Override
+    public int hashCode() {
+      return 0;
+    }
+
+    public int compareTo(schedule_args other) {
+      if (!getClass().equals(other.getClass())) {
+        return getClass().getName().compareTo(other.getClass().getName());
+      }
+
+      int lastComparison = 0;
+      schedule_args typedOther = (schedule_args)other;
+
+      lastComparison = Boolean.valueOf(isSetJobs()).compareTo(typedOther.isSetJobs());
+      if (lastComparison != 0) {
+        return lastComparison;
+      }
+      if (isSetJobs()) {
+        lastComparison = org.apache.thrift.TBaseHelper.compareTo(this.jobs, typedOther.jobs);
+        if (lastComparison != 0) {
+          return lastComparison;
+        }
+      }
+      return 0;
+    }
+
+    public _Fields fieldForId(int fieldId) {
+      return _Fields.findByThriftId(fieldId);
+    }
+
+    public void read(org.apache.thrift.protocol.TProtocol iprot) throws org.apache.thrift.TException {
+      schemes.get(iprot.getScheme()).getScheme().read(iprot, this);
+    }
+
+    public void write(org.apache.thrift.protocol.TProtocol oprot) throws org.apache.thrift.TException {
+      schemes.get(oprot.getScheme()).getScheme().write(oprot, this);
+    }
+
+    @Override
+    public String toString() {
+      StringBuilder sb = new StringBuilder("schedule_args(");
+      boolean first = true;
+
+      sb.append("jobs:");
+      if (this.jobs == null) {
+        sb.append("null");
+      } else {
+        sb.append(this.jobs);
+      }
+      first = false;
+      sb.append(")");
+      return sb.toString();
+    }
+
+    public void validate() throws org.apache.thrift.TException {
+      // check for required fields
+    }
+
+    private void writeObject(java.io.ObjectOutputStream out) throws java.io.IOException {
+      try {
+        write(new org.apache.thrift.protocol.TCompactProtocol(new org.apache.thrift.transport.TIOStreamTransport(out)));
+      } catch (org.apache.thrift.TException te) {
+        throw new java.io.IOException(te);
+      }
+    }
+
+    private void readObject(java.io.ObjectInputStream in) throws java.io.IOException, ClassNotFoundException {
+      try {
+        read(new org.apache.thrift.protocol.TCompactProtocol(new org.apache.thrift.transport.TIOStreamTransport(in)));
+      } catch (org.apache.thrift.TException te) {
+        throw new java.io.IOException(te);
+      }
+    }
+
+    private static class schedule_argsStandardSchemeFactory implements SchemeFactory {
+      public schedule_argsStandardScheme getScheme() {
+        return new schedule_argsStandardScheme();
+      }
+    }
+
+    private static class schedule_argsStandardScheme extends StandardScheme<schedule_args> {
+
+      public void read(org.apache.thrift.protocol.TProtocol iprot, schedule_args struct) throws org.apache.thrift.TException {
+        org.apache.thrift.protocol.TField schemeField;
+        iprot.readStructBegin();
+        while (true)
+        {
+          schemeField = iprot.readFieldBegin();
+          if (schemeField.type == org.apache.thrift.protocol.TType.STOP) { 
+            break;
+          }
+          switch (schemeField.id) {
+            case 1: // JOBS
+              if (schemeField.type == org.apache.thrift.protocol.TType.LIST) {
+                {
+                  org.apache.thrift.protocol.TList _list0 = iprot.readListBegin();
+                  struct.jobs = new ArrayList<Job>(_list0.size);
+                  for (int _i1 = 0; _i1 < _list0.size; ++_i1)
+                  {
+                    Job _elem2; // required
+                    _elem2 = new Job();
+                    _elem2.read(iprot);
+                    struct.jobs.add(_elem2);
+                  }
+                  iprot.readListEnd();
+                }
+                struct.setJobsIsSet(true);
+              } else { 
+                org.apache.thrift.protocol.TProtocolUtil.skip(iprot, schemeField.type);
+              }
+              break;
+            default:
+              org.apache.thrift.protocol.TProtocolUtil.skip(iprot, schemeField.type);
+          }
+          iprot.readFieldEnd();
+        }
+        iprot.readStructEnd();
+
+        // check for required fields of primitive type, which can't be checked in the validate method
+        struct.validate();
+      }
+
+      public void write(org.apache.thrift.protocol.TProtocol oprot, schedule_args struct) throws org.apache.thrift.TException {
+        struct.validate();
+
+        oprot.writeStructBegin(STRUCT_DESC);
+        if (struct.jobs != null) {
+          oprot.writeFieldBegin(JOBS_FIELD_DESC);
+          {
+            oprot.writeListBegin(new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRUCT, struct.jobs.size()));
+            for (Job _iter3 : struct.jobs)
+            {
+              _iter3.write(oprot);
+            }
+            oprot.writeListEnd();
+          }
+          oprot.writeFieldEnd();
+        }
+        oprot.writeFieldStop();
+        oprot.writeStructEnd();
+      }
+
+    }
+
+    private static class schedule_argsTupleSchemeFactory implements SchemeFactory {
+      public schedule_argsTupleScheme getScheme() {
+        return new schedule_argsTupleScheme();
+      }
+    }
+
+    private static class schedule_argsTupleScheme extends TupleScheme<schedule_args> {
+
+      @Override
+      public void write(org.apache.thrift.protocol.TProtocol prot, schedule_args struct) throws org.apache.thrift.TException {
+        TTupleProtocol oprot = (TTupleProtocol) prot;
+        BitSet optionals = new BitSet();
+        if (struct.isSetJobs()) {
+          optionals.set(0);
+        }
+        oprot.writeBitSet(optionals, 1);
+        if (struct.isSetJobs()) {
+          {
+            oprot.writeI32(struct.jobs.size());
+            for (Job _iter4 : struct.jobs)
+            {
+              _iter4.write(oprot);
+            }
+          }
+        }
+      }
+
+      @Override
+      public void read(org.apache.thrift.protocol.TProtocol prot, schedule_args struct) throws org.apache.thrift.TException {
+        TTupleProtocol iprot = (TTupleProtocol) prot;
+        BitSet incoming = iprot.readBitSet(1);
+        if (incoming.get(0)) {
+          {
+            org.apache.thrift.protocol.TList _list5 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRUCT, iprot.readI32());
+            struct.jobs = new ArrayList<Job>(_list5.size);
+            for (int _i6 = 0; _i6 < _list5.size; ++_i6)
+            {
+              Job _elem7; // required
+              _elem7 = new Job();
+              _elem7.read(iprot);
+              struct.jobs.add(_elem7);
+            }
+          }
+          struct.setJobsIsSet(true);
+        }
+      }
+    }
+
+  }
+
+  public static class schedule_result implements org.apache.thrift.TBase<schedule_result, schedule_result._Fields>, java.io.Serializable, Cloneable   {
+    private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("schedule_result");
+
+    private static final org.apache.thrift.protocol.TField SUCCESS_FIELD_DESC = new org.apache.thrift.protocol.TField("success", org.apache.thrift.protocol.TType.LIST, (short)0);
+
+    private static final Map<Class<? extends IScheme>, SchemeFactory> schemes = new HashMap<Class<? extends IScheme>, SchemeFactory>();
+    static {
+      schemes.put(StandardScheme.class, new schedule_resultStandardSchemeFactory());
+      schemes.put(TupleScheme.class, new schedule_resultTupleSchemeFactory());
+    }
+
+    public List<Job> success; // required
+
+    /** The set of fields this struct contains, along with convenience methods for finding and manipulating them. */
+    public enum _Fields implements org.apache.thrift.TFieldIdEnum {
+      SUCCESS((short)0, "success");
+
+      private static final Map<String, _Fields> byName = new HashMap<String, _Fields>();
+
+      static {
+        for (_Fields field : EnumSet.allOf(_Fields.class)) {
+          byName.put(field.getFieldName(), field);
+        }
+      }
+
+      /**
+       * Find the _Fields constant that matches fieldId, or null if its not found.
+       */
+      public static _Fields findByThriftId(int fieldId) {
+        switch(fieldId) {
+          case 0: // SUCCESS
+            return SUCCESS;
+          default:
+            return null;
+        }
+      }
+
+      /**
+       * Find the _Fields constant that matches fieldId, throwing an exception
+       * if it is not found.
+       */
+      public static _Fields findByThriftIdOrThrow(int fieldId) {
+        _Fields fields = findByThriftId(fieldId);
+        if (fields == null) throw new IllegalArgumentException("Field " + fieldId + " doesn't exist!");
+        return fields;
+      }
+
+      /**
+       * Find the _Fields constant that matches name, or null if its not found.
+       */
+      public static _Fields findByName(String name) {
+        return byName.get(name);
+      }
+
+      private final short _thriftId;
+      private final String _fieldName;
+
+      _Fields(short thriftId, String fieldName) {
+        _thriftId = thriftId;
+        _fieldName = fieldName;
+      }
+
+      public short getThriftFieldId() {
+        return _thriftId;
+      }
+
+      public String getFieldName() {
+        return _fieldName;
+      }
+    }
+
+    // isset id assignments
+    public static final Map<_Fields, org.apache.thrift.meta_data.FieldMetaData> metaDataMap;
+    static {
+      Map<_Fields, org.apache.thrift.meta_data.FieldMetaData> tmpMap = new EnumMap<_Fields, org.apache.thrift.meta_data.FieldMetaData>(_Fields.class);
+      tmpMap.put(_Fields.SUCCESS, new org.apache.thrift.meta_data.FieldMetaData("success", org.apache.thrift.TFieldRequirementType.DEFAULT, 
+          new org.apache.thrift.meta_data.ListMetaData(org.apache.thrift.protocol.TType.LIST, 
+              new org.apache.thrift.meta_data.StructMetaData(org.apache.thrift.protocol.TType.STRUCT, Job.class))));
+      metaDataMap = Collections.unmodifiableMap(tmpMap);
+      org.apache.thrift.meta_data.FieldMetaData.addStructMetaDataMap(schedule_result.class, metaDataMap);
+    }
+
+    public schedule_result() {
+    }
+
+    public schedule_result(
+      List<Job> success)
+    {
+      this();
+      this.success = success;
+    }
+
+    /**
+     * Performs a deep copy on <i>other</i>.
+     */
+    public schedule_result(schedule_result other) {
+      if (other.isSetSuccess()) {
+        List<Job> __this__success = new ArrayList<Job>();
+        for (Job other_element : other.success) {
+          __this__success.add(new Job(other_element));
+        }
+        this.success = __this__success;
+      }
+    }
+
+    public schedule_result deepCopy() {
+      return new schedule_result(this);
+    }
+
+    @Override
+    public void clear() {
+      this.success = null;
+    }
+
+    public int getSuccessSize() {
+      return (this.success == null) ? 0 : this.success.size();
+    }
+
+    public java.util.Iterator<Job> getSuccessIterator() {
+      return (this.success == null) ? null : this.success.iterator();
+    }
+
+    public void addToSuccess(Job elem) {
+      if (this.success == null) {
+        this.success = new ArrayList<Job>();
+      }
+      this.success.add(elem);
+    }
+
+    public List<Job> getSuccess() {
+      return this.success;
+    }
+
+    public schedule_result setSuccess(List<Job> success) {
+      this.success = success;
+      return this;
+    }
+
+    public void unsetSuccess() {
+      this.success = null;
+    }
+
+    /** Returns true if field success is set (has been assigned a value) and false otherwise */
+    public boolean isSetSuccess() {
+      return this.success != null;
+    }
+
+    public void setSuccessIsSet(boolean value) {
+      if (!value) {
+        this.success = null;
+      }
+    }
+
+    public void setFieldValue(_Fields field, Object value) {
+      switch (field) {
+      case SUCCESS:
+        if (value == null) {
+          unsetSuccess();
+        } else {
+          setSuccess((List<Job>)value);
+        }
+        break;
+
+      }
+    }
+
+    public Object getFieldValue(_Fields field) {
+      switch (field) {
+      case SUCCESS:
+        return getSuccess();
+
+      }
+      throw new IllegalStateException();
+    }
+
+    /** Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise */
+    public boolean isSet(_Fields field) {
+      if (field == null) {
+        throw new IllegalArgumentException();
+      }
+
+      switch (field) {
+      case SUCCESS:
+        return isSetSuccess();
+      }
+      throw new IllegalStateException();
+    }
+
+    @Override
+    public boolean equals(Object that) {
+      if (that == null)
+        return false;
+      if (that instanceof schedule_result)
+        return this.equals((schedule_result)that);
+      return false;
+    }
+
+    public boolean equals(schedule_result that) {
+      if (that == null)
+        return false;
+
+      boolean this_present_success = true && this.isSetSuccess();
+      boolean that_present_success = true && that.isSetSuccess();
+      if (this_present_success || that_present_success) {
+        if (!(this_present_success && that_present_success))
+          return false;
+        if (!this.success.equals(that.success))
+          return false;
+      }
+
+      return true;
+    }
+
+    @Override
+    public int hashCode() {
+      return 0;
+    }
+
+    public int compareTo(schedule_result other) {
+      if (!getClass().equals(other.getClass())) {
+        return getClass().getName().compareTo(other.getClass().getName());
+      }
+
+      int lastComparison = 0;
+      schedule_result typedOther = (schedule_result)other;
+
+      lastComparison = Boolean.valueOf(isSetSuccess()).compareTo(typedOther.isSetSuccess());
+      if (lastComparison != 0) {
+        return lastComparison;
+      }
+      if (isSetSuccess()) {
+        lastComparison = org.apache.thrift.TBaseHelper.compareTo(this.success, typedOther.success);
+        if (lastComparison != 0) {
+          return lastComparison;
+        }
+      }
+      return 0;
+    }
+
+    public _Fields fieldForId(int fieldId) {
+      return _Fields.findByThriftId(fieldId);
+    }
+
+    public void read(org.apache.thrift.protocol.TProtocol iprot) throws org.apache.thrift.TException {
+      schemes.get(iprot.getScheme()).getScheme().read(iprot, this);
+    }
+
+    public void write(org.apache.thrift.protocol.TProtocol oprot) throws org.apache.thrift.TException {
+      schemes.get(oprot.getScheme()).getScheme().write(oprot, this);
+      }
+
+    @Override
+    public String toString() {
+      StringBuilder sb = new StringBuilder("schedule_result(");
+      boolean first = true;
+
+      sb.append("success:");
+      if (this.success == null) {
+        sb.append("null");
+      } else {
+        sb.append(this.success);
+      }
+      first = false;
+      sb.append(")");
+      return sb.toString();
+    }
+
+    public void validate() throws org.apache.thrift.TException {
+      // check for required fields
+    }
+
+    private void writeObject(java.io.ObjectOutputStream out) throws java.io.IOException {
+      try {
+        write(new org.apache.thrift.protocol.TCompactProtocol(new org.apache.thrift.transport.TIOStreamTransport(out)));
+      } catch (org.apache.thrift.TException te) {
+        throw new java.io.IOException(te);
+      }
+    }
+
+    private void readObject(java.io.ObjectInputStream in) throws java.io.IOException, ClassNotFoundException {
+      try {
+        read(new org.apache.thrift.protocol.TCompactProtocol(new org.apache.thrift.transport.TIOStreamTransport(in)));
+      } catch (org.apache.thrift.TException te) {
+        throw new java.io.IOException(te);
+      }
+    }
+
+    private static class schedule_resultStandardSchemeFactory implements SchemeFactory {
+      public schedule_resultStandardScheme getScheme() {
+        return new schedule_resultStandardScheme();
+      }
+    }
+
+    private static class schedule_resultStandardScheme extends StandardScheme<schedule_result> {
+
+      public void read(org.apache.thrift.protocol.TProtocol iprot, schedule_result struct) throws org.apache.thrift.TException {
+        org.apache.thrift.protocol.TField schemeField;
+        iprot.readStructBegin();
+        while (true)
+        {
+          schemeField = iprot.readFieldBegin();
+          if (schemeField.type == org.apache.thrift.protocol.TType.STOP) { 
+            break;
+          }
+          switch (schemeField.id) {
+            case 0: // SUCCESS
+              if (schemeField.type == org.apache.thrift.protocol.TType.LIST) {
+                {
+                  org.apache.thrift.protocol.TList _list8 = iprot.readListBegin();
+                  struct.success = new ArrayList<Job>(_list8.size);
+                  for (int _i9 = 0; _i9 < _list8.size; ++_i9)
+                  {
+                    Job _elem10; // required
+                    _elem10 = new Job();
+                    _elem10.read(iprot);
+                    struct.success.add(_elem10);
+                  }
+                  iprot.readListEnd();
+                }
+                struct.setSuccessIsSet(true);
+              } else { 
+                org.apache.thrift.protocol.TProtocolUtil.skip(iprot, schemeField.type);
+              }
+              break;
+            default:
+              org.apache.thrift.protocol.TProtocolUtil.skip(iprot, schemeField.type);
+          }
+          iprot.readFieldEnd();
+        }
+        iprot.readStructEnd();
+
+        // check for required fields of primitive type, which can't be checked in the validate method
+        struct.validate();
+      }
+
+      public void write(org.apache.thrift.protocol.TProtocol oprot, schedule_result struct) throws org.apache.thrift.TException {
+        struct.validate();
+
+        oprot.writeStructBegin(STRUCT_DESC);
+        if (struct.success != null) {
+          oprot.writeFieldBegin(SUCCESS_FIELD_DESC);
+          {
+            oprot.writeListBegin(new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRUCT, struct.success.size()));
+            for (Job _iter11 : struct.success)
+            {
+              _iter11.write(oprot);
+            }
+            oprot.writeListEnd();
+          }
+          oprot.writeFieldEnd();
+        }
+        oprot.writeFieldStop();
+        oprot.writeStructEnd();
+      }
+
+    }
+
+    private static class schedule_resultTupleSchemeFactory implements SchemeFactory {
+      public schedule_resultTupleScheme getScheme() {
+        return new schedule_resultTupleScheme();
+      }
+    }
+
+    private static class schedule_resultTupleScheme extends TupleScheme<schedule_result> {
+
+      @Override
+      public void write(org.apache.thrift.protocol.TProtocol prot, schedule_result struct) throws org.apache.thrift.TException {
+        TTupleProtocol oprot = (TTupleProtocol) prot;
+        BitSet optionals = new BitSet();
+        if (struct.isSetSuccess()) {
+          optionals.set(0);
+        }
+        oprot.writeBitSet(optionals, 1);
+        if (struct.isSetSuccess()) {
+          {
+            oprot.writeI32(struct.success.size());
+            for (Job _iter12 : struct.success)
+            {
+              _iter12.write(oprot);
+            }
+          }
+        }
+      }
+
+      @Override
+      public void read(org.apache.thrift.protocol.TProtocol prot, schedule_result struct) throws org.apache.thrift.TException {
+        TTupleProtocol iprot = (TTupleProtocol) prot;
+        BitSet incoming = iprot.readBitSet(1);
+        if (incoming.get(0)) {
+          {
+            org.apache.thrift.protocol.TList _list13 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRUCT, iprot.readI32());
+            struct.success = new ArrayList<Job>(_list13.size);
+            for (int _i14 = 0; _i14 < _list13.size; ++_i14)
+            {
+              Job _elem15; // required
+              _elem15 = new Job();
+              _elem15.read(iprot);
+              struct.success.add(_elem15);
+            }
+          }
+          struct.setSuccessIsSet(true);
+        }
+      }
+    }
+
+  }
+
+}
Index: src/contrib/approximationscheduler/gen-java/org/apache/hadoop/mapred/approximation/Job.java
===================================================================
--- src/contrib/approximationscheduler/gen-java/org/apache/hadoop/mapred/approximation/Job.java	(revision 0)
+++ src/contrib/approximationscheduler/gen-java/org/apache/hadoop/mapred/approximation/Job.java	(working copy)
@@ -0,0 +1,1619 @@
+/**
+ * Autogenerated by Thrift Compiler (0.8.0)
+ *
+ * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
+ *  @generated
+ */
+package org.apache.hadoop.mapred.approximation;
+
+import org.apache.thrift.scheme.IScheme;
+import org.apache.thrift.scheme.SchemeFactory;
+import org.apache.thrift.scheme.StandardScheme;
+
+import org.apache.thrift.scheme.TupleScheme;
+import org.apache.thrift.protocol.TTupleProtocol;
+import java.util.List;
+import java.util.ArrayList;
+import java.util.Map;
+import java.util.HashMap;
+import java.util.EnumMap;
+import java.util.Set;
+import java.util.HashSet;
+import java.util.EnumSet;
+import java.util.Collections;
+import java.util.BitSet;
+import java.nio.ByteBuffer;
+import java.util.Arrays;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+/**
+ * Defines a MapReduce job that can be approximated.
+ */
+public class Job implements org.apache.thrift.TBase<Job, Job._Fields>, java.io.Serializable, Cloneable {
+  private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("Job");
+
+  private static final org.apache.thrift.protocol.TField JOB_ID_FIELD_DESC = new org.apache.thrift.protocol.TField("jobId", org.apache.thrift.protocol.TType.STRING, (short)1);
+  private static final org.apache.thrift.protocol.TField NMAP_FIELD_DESC = new org.apache.thrift.protocol.TField("nmap", org.apache.thrift.protocol.TType.I32, (short)2);
+  private static final org.apache.thrift.protocol.TField LMAP_FIELD_DESC = new org.apache.thrift.protocol.TField("lmap", org.apache.thrift.protocol.TType.I32, (short)3);
+  private static final org.apache.thrift.protocol.TField NRED_FIELD_DESC = new org.apache.thrift.protocol.TField("nred", org.apache.thrift.protocol.TType.I32, (short)4);
+  private static final org.apache.thrift.protocol.TField LRED_FIELD_DESC = new org.apache.thrift.protocol.TField("lred", org.apache.thrift.protocol.TType.I32, (short)5);
+  private static final org.apache.thrift.protocol.TField APPROX_ALGO_FIELD_DESC = new org.apache.thrift.protocol.TField("approxAlgo", org.apache.thrift.protocol.TType.BOOL, (short)6);
+  private static final org.apache.thrift.protocol.TField LMAPAPPROX_FIELD_DESC = new org.apache.thrift.protocol.TField("lmapapprox", org.apache.thrift.protocol.TType.I32, (short)7);
+  private static final org.apache.thrift.protocol.TField LREDAPPROX_FIELD_DESC = new org.apache.thrift.protocol.TField("lredapprox", org.apache.thrift.protocol.TType.I32, (short)8);
+  private static final org.apache.thrift.protocol.TField APPROX_ALGO_MAX_FIELD_DESC = new org.apache.thrift.protocol.TField("approxAlgoMax", org.apache.thrift.protocol.TType.DOUBLE, (short)9);
+  private static final org.apache.thrift.protocol.TField APPROX_ALGO_VAL_FIELD_DESC = new org.apache.thrift.protocol.TField("approxAlgoVal", org.apache.thrift.protocol.TType.DOUBLE, (short)10);
+  private static final org.apache.thrift.protocol.TField APPROX_DROP_FIELD_DESC = new org.apache.thrift.protocol.TField("approxDrop", org.apache.thrift.protocol.TType.BOOL, (short)11);
+  private static final org.apache.thrift.protocol.TField APPROX_DROP_MAX_FIELD_DESC = new org.apache.thrift.protocol.TField("approxDropMax", org.apache.thrift.protocol.TType.DOUBLE, (short)12);
+  private static final org.apache.thrift.protocol.TField APPROX_DROP_VAL_FIELD_DESC = new org.apache.thrift.protocol.TField("approxDropVal", org.apache.thrift.protocol.TType.DOUBLE, (short)13);
+  private static final org.apache.thrift.protocol.TField APPROX_INPUT_FIELD_DESC = new org.apache.thrift.protocol.TField("approxInput", org.apache.thrift.protocol.TType.BOOL, (short)14);
+
+  private static final Map<Class<? extends IScheme>, SchemeFactory> schemes = new HashMap<Class<? extends IScheme>, SchemeFactory>();
+  static {
+    schemes.put(StandardScheme.class, new JobStandardSchemeFactory());
+    schemes.put(TupleScheme.class, new JobTupleSchemeFactory());
+  }
+
+  public String jobId; // required
+  public int nmap; // required
+  public int lmap; // required
+  public int nred; // required
+  public int lred; // required
+  public boolean approxAlgo; // required
+  public int lmapapprox; // optional
+  public int lredapprox; // optional
+  public double approxAlgoMax; // optional
+  public double approxAlgoVal; // optional
+  public boolean approxDrop; // required
+  public double approxDropMax; // optional
+  public double approxDropVal; // optional
+  public boolean approxInput; // required
+
+  /** The set of fields this struct contains, along with convenience methods for finding and manipulating them. */
+  public enum _Fields implements org.apache.thrift.TFieldIdEnum {
+    JOB_ID((short)1, "jobId"),
+    NMAP((short)2, "nmap"),
+    LMAP((short)3, "lmap"),
+    NRED((short)4, "nred"),
+    LRED((short)5, "lred"),
+    APPROX_ALGO((short)6, "approxAlgo"),
+    LMAPAPPROX((short)7, "lmapapprox"),
+    LREDAPPROX((short)8, "lredapprox"),
+    APPROX_ALGO_MAX((short)9, "approxAlgoMax"),
+    APPROX_ALGO_VAL((short)10, "approxAlgoVal"),
+    APPROX_DROP((short)11, "approxDrop"),
+    APPROX_DROP_MAX((short)12, "approxDropMax"),
+    APPROX_DROP_VAL((short)13, "approxDropVal"),
+    APPROX_INPUT((short)14, "approxInput");
+
+    private static final Map<String, _Fields> byName = new HashMap<String, _Fields>();
+
+    static {
+      for (_Fields field : EnumSet.allOf(_Fields.class)) {
+        byName.put(field.getFieldName(), field);
+      }
+    }
+
+    /**
+     * Find the _Fields constant that matches fieldId, or null if its not found.
+     */
+    public static _Fields findByThriftId(int fieldId) {
+      switch(fieldId) {
+        case 1: // JOB_ID
+          return JOB_ID;
+        case 2: // NMAP
+          return NMAP;
+        case 3: // LMAP
+          return LMAP;
+        case 4: // NRED
+          return NRED;
+        case 5: // LRED
+          return LRED;
+        case 6: // APPROX_ALGO
+          return APPROX_ALGO;
+        case 7: // LMAPAPPROX
+          return LMAPAPPROX;
+        case 8: // LREDAPPROX
+          return LREDAPPROX;
+        case 9: // APPROX_ALGO_MAX
+          return APPROX_ALGO_MAX;
+        case 10: // APPROX_ALGO_VAL
+          return APPROX_ALGO_VAL;
+        case 11: // APPROX_DROP
+          return APPROX_DROP;
+        case 12: // APPROX_DROP_MAX
+          return APPROX_DROP_MAX;
+        case 13: // APPROX_DROP_VAL
+          return APPROX_DROP_VAL;
+        case 14: // APPROX_INPUT
+          return APPROX_INPUT;
+        default:
+          return null;
+      }
+    }
+
+    /**
+     * Find the _Fields constant that matches fieldId, throwing an exception
+     * if it is not found.
+     */
+    public static _Fields findByThriftIdOrThrow(int fieldId) {
+      _Fields fields = findByThriftId(fieldId);
+      if (fields == null) throw new IllegalArgumentException("Field " + fieldId + " doesn't exist!");
+      return fields;
+    }
+
+    /**
+     * Find the _Fields constant that matches name, or null if its not found.
+     */
+    public static _Fields findByName(String name) {
+      return byName.get(name);
+    }
+
+    private final short _thriftId;
+    private final String _fieldName;
+
+    _Fields(short thriftId, String fieldName) {
+      _thriftId = thriftId;
+      _fieldName = fieldName;
+    }
+
+    public short getThriftFieldId() {
+      return _thriftId;
+    }
+
+    public String getFieldName() {
+      return _fieldName;
+    }
+  }
+
+  // isset id assignments
+  private static final int __NMAP_ISSET_ID = 0;
+  private static final int __LMAP_ISSET_ID = 1;
+  private static final int __NRED_ISSET_ID = 2;
+  private static final int __LRED_ISSET_ID = 3;
+  private static final int __APPROXALGO_ISSET_ID = 4;
+  private static final int __LMAPAPPROX_ISSET_ID = 5;
+  private static final int __LREDAPPROX_ISSET_ID = 6;
+  private static final int __APPROXALGOMAX_ISSET_ID = 7;
+  private static final int __APPROXALGOVAL_ISSET_ID = 8;
+  private static final int __APPROXDROP_ISSET_ID = 9;
+  private static final int __APPROXDROPMAX_ISSET_ID = 10;
+  private static final int __APPROXDROPVAL_ISSET_ID = 11;
+  private static final int __APPROXINPUT_ISSET_ID = 12;
+  private BitSet __isset_bit_vector = new BitSet(13);
+  private _Fields optionals[] = {_Fields.LMAPAPPROX,_Fields.LREDAPPROX,_Fields.APPROX_ALGO_MAX,_Fields.APPROX_ALGO_VAL,_Fields.APPROX_DROP_MAX,_Fields.APPROX_DROP_VAL};
+  public static final Map<_Fields, org.apache.thrift.meta_data.FieldMetaData> metaDataMap;
+  static {
+    Map<_Fields, org.apache.thrift.meta_data.FieldMetaData> tmpMap = new EnumMap<_Fields, org.apache.thrift.meta_data.FieldMetaData>(_Fields.class);
+    tmpMap.put(_Fields.JOB_ID, new org.apache.thrift.meta_data.FieldMetaData("jobId", org.apache.thrift.TFieldRequirementType.DEFAULT, 
+        new org.apache.thrift.meta_data.FieldValueMetaData(org.apache.thrift.protocol.TType.STRING)));
+    tmpMap.put(_Fields.NMAP, new org.apache.thrift.meta_data.FieldMetaData("nmap", org.apache.thrift.TFieldRequirementType.DEFAULT, 
+        new org.apache.thrift.meta_data.FieldValueMetaData(org.apache.thrift.protocol.TType.I32)));
+    tmpMap.put(_Fields.LMAP, new org.apache.thrift.meta_data.FieldMetaData("lmap", org.apache.thrift.TFieldRequirementType.DEFAULT, 
+        new org.apache.thrift.meta_data.FieldValueMetaData(org.apache.thrift.protocol.TType.I32)));
+    tmpMap.put(_Fields.NRED, new org.apache.thrift.meta_data.FieldMetaData("nred", org.apache.thrift.TFieldRequirementType.DEFAULT, 
+        new org.apache.thrift.meta_data.FieldValueMetaData(org.apache.thrift.protocol.TType.I32)));
+    tmpMap.put(_Fields.LRED, new org.apache.thrift.meta_data.FieldMetaData("lred", org.apache.thrift.TFieldRequirementType.DEFAULT, 
+        new org.apache.thrift.meta_data.FieldValueMetaData(org.apache.thrift.protocol.TType.I32)));
+    tmpMap.put(_Fields.APPROX_ALGO, new org.apache.thrift.meta_data.FieldMetaData("approxAlgo", org.apache.thrift.TFieldRequirementType.DEFAULT, 
+        new org.apache.thrift.meta_data.FieldValueMetaData(org.apache.thrift.protocol.TType.BOOL)));
+    tmpMap.put(_Fields.LMAPAPPROX, new org.apache.thrift.meta_data.FieldMetaData("lmapapprox", org.apache.thrift.TFieldRequirementType.OPTIONAL, 
+        new org.apache.thrift.meta_data.FieldValueMetaData(org.apache.thrift.protocol.TType.I32)));
+    tmpMap.put(_Fields.LREDAPPROX, new org.apache.thrift.meta_data.FieldMetaData("lredapprox", org.apache.thrift.TFieldRequirementType.OPTIONAL, 
+        new org.apache.thrift.meta_data.FieldValueMetaData(org.apache.thrift.protocol.TType.I32)));
+    tmpMap.put(_Fields.APPROX_ALGO_MAX, new org.apache.thrift.meta_data.FieldMetaData("approxAlgoMax", org.apache.thrift.TFieldRequirementType.OPTIONAL, 
+        new org.apache.thrift.meta_data.FieldValueMetaData(org.apache.thrift.protocol.TType.DOUBLE)));
+    tmpMap.put(_Fields.APPROX_ALGO_VAL, new org.apache.thrift.meta_data.FieldMetaData("approxAlgoVal", org.apache.thrift.TFieldRequirementType.OPTIONAL, 
+        new org.apache.thrift.meta_data.FieldValueMetaData(org.apache.thrift.protocol.TType.DOUBLE)));
+    tmpMap.put(_Fields.APPROX_DROP, new org.apache.thrift.meta_data.FieldMetaData("approxDrop", org.apache.thrift.TFieldRequirementType.DEFAULT, 
+        new org.apache.thrift.meta_data.FieldValueMetaData(org.apache.thrift.protocol.TType.BOOL)));
+    tmpMap.put(_Fields.APPROX_DROP_MAX, new org.apache.thrift.meta_data.FieldMetaData("approxDropMax", org.apache.thrift.TFieldRequirementType.OPTIONAL, 
+        new org.apache.thrift.meta_data.FieldValueMetaData(org.apache.thrift.protocol.TType.DOUBLE)));
+    tmpMap.put(_Fields.APPROX_DROP_VAL, new org.apache.thrift.meta_data.FieldMetaData("approxDropVal", org.apache.thrift.TFieldRequirementType.OPTIONAL, 
+        new org.apache.thrift.meta_data.FieldValueMetaData(org.apache.thrift.protocol.TType.DOUBLE)));
+    tmpMap.put(_Fields.APPROX_INPUT, new org.apache.thrift.meta_data.FieldMetaData("approxInput", org.apache.thrift.TFieldRequirementType.DEFAULT, 
+        new org.apache.thrift.meta_data.FieldValueMetaData(org.apache.thrift.protocol.TType.BOOL)));
+    metaDataMap = Collections.unmodifiableMap(tmpMap);
+    org.apache.thrift.meta_data.FieldMetaData.addStructMetaDataMap(Job.class, metaDataMap);
+  }
+
+  public Job() {
+  }
+
+  public Job(
+    String jobId,
+    int nmap,
+    int lmap,
+    int nred,
+    int lred,
+    boolean approxAlgo,
+    boolean approxDrop,
+    boolean approxInput)
+  {
+    this();
+    this.jobId = jobId;
+    this.nmap = nmap;
+    setNmapIsSet(true);
+    this.lmap = lmap;
+    setLmapIsSet(true);
+    this.nred = nred;
+    setNredIsSet(true);
+    this.lred = lred;
+    setLredIsSet(true);
+    this.approxAlgo = approxAlgo;
+    setApproxAlgoIsSet(true);
+    this.approxDrop = approxDrop;
+    setApproxDropIsSet(true);
+    this.approxInput = approxInput;
+    setApproxInputIsSet(true);
+  }
+
+  /**
+   * Performs a deep copy on <i>other</i>.
+   */
+  public Job(Job other) {
+    __isset_bit_vector.clear();
+    __isset_bit_vector.or(other.__isset_bit_vector);
+    if (other.isSetJobId()) {
+      this.jobId = other.jobId;
+    }
+    this.nmap = other.nmap;
+    this.lmap = other.lmap;
+    this.nred = other.nred;
+    this.lred = other.lred;
+    this.approxAlgo = other.approxAlgo;
+    this.lmapapprox = other.lmapapprox;
+    this.lredapprox = other.lredapprox;
+    this.approxAlgoMax = other.approxAlgoMax;
+    this.approxAlgoVal = other.approxAlgoVal;
+    this.approxDrop = other.approxDrop;
+    this.approxDropMax = other.approxDropMax;
+    this.approxDropVal = other.approxDropVal;
+    this.approxInput = other.approxInput;
+  }
+
+  public Job deepCopy() {
+    return new Job(this);
+  }
+
+  @Override
+  public void clear() {
+    this.jobId = null;
+    setNmapIsSet(false);
+    this.nmap = 0;
+    setLmapIsSet(false);
+    this.lmap = 0;
+    setNredIsSet(false);
+    this.nred = 0;
+    setLredIsSet(false);
+    this.lred = 0;
+    setApproxAlgoIsSet(false);
+    this.approxAlgo = false;
+    setLmapapproxIsSet(false);
+    this.lmapapprox = 0;
+    setLredapproxIsSet(false);
+    this.lredapprox = 0;
+    setApproxAlgoMaxIsSet(false);
+    this.approxAlgoMax = 0.0;
+    setApproxAlgoValIsSet(false);
+    this.approxAlgoVal = 0.0;
+    setApproxDropIsSet(false);
+    this.approxDrop = false;
+    setApproxDropMaxIsSet(false);
+    this.approxDropMax = 0.0;
+    setApproxDropValIsSet(false);
+    this.approxDropVal = 0.0;
+    setApproxInputIsSet(false);
+    this.approxInput = false;
+  }
+
+  public String getJobId() {
+    return this.jobId;
+  }
+
+  public Job setJobId(String jobId) {
+    this.jobId = jobId;
+    return this;
+  }
+
+  public void unsetJobId() {
+    this.jobId = null;
+  }
+
+  /** Returns true if field jobId is set (has been assigned a value) and false otherwise */
+  public boolean isSetJobId() {
+    return this.jobId != null;
+  }
+
+  public void setJobIdIsSet(boolean value) {
+    if (!value) {
+      this.jobId = null;
+    }
+  }
+
+  public int getNmap() {
+    return this.nmap;
+  }
+
+  public Job setNmap(int nmap) {
+    this.nmap = nmap;
+    setNmapIsSet(true);
+    return this;
+  }
+
+  public void unsetNmap() {
+    __isset_bit_vector.clear(__NMAP_ISSET_ID);
+  }
+
+  /** Returns true if field nmap is set (has been assigned a value) and false otherwise */
+  public boolean isSetNmap() {
+    return __isset_bit_vector.get(__NMAP_ISSET_ID);
+  }
+
+  public void setNmapIsSet(boolean value) {
+    __isset_bit_vector.set(__NMAP_ISSET_ID, value);
+  }
+
+  public int getLmap() {
+    return this.lmap;
+  }
+
+  public Job setLmap(int lmap) {
+    this.lmap = lmap;
+    setLmapIsSet(true);
+    return this;
+  }
+
+  public void unsetLmap() {
+    __isset_bit_vector.clear(__LMAP_ISSET_ID);
+  }
+
+  /** Returns true if field lmap is set (has been assigned a value) and false otherwise */
+  public boolean isSetLmap() {
+    return __isset_bit_vector.get(__LMAP_ISSET_ID);
+  }
+
+  public void setLmapIsSet(boolean value) {
+    __isset_bit_vector.set(__LMAP_ISSET_ID, value);
+  }
+
+  public int getNred() {
+    return this.nred;
+  }
+
+  public Job setNred(int nred) {
+    this.nred = nred;
+    setNredIsSet(true);
+    return this;
+  }
+
+  public void unsetNred() {
+    __isset_bit_vector.clear(__NRED_ISSET_ID);
+  }
+
+  /** Returns true if field nred is set (has been assigned a value) and false otherwise */
+  public boolean isSetNred() {
+    return __isset_bit_vector.get(__NRED_ISSET_ID);
+  }
+
+  public void setNredIsSet(boolean value) {
+    __isset_bit_vector.set(__NRED_ISSET_ID, value);
+  }
+
+  public int getLred() {
+    return this.lred;
+  }
+
+  public Job setLred(int lred) {
+    this.lred = lred;
+    setLredIsSet(true);
+    return this;
+  }
+
+  public void unsetLred() {
+    __isset_bit_vector.clear(__LRED_ISSET_ID);
+  }
+
+  /** Returns true if field lred is set (has been assigned a value) and false otherwise */
+  public boolean isSetLred() {
+    return __isset_bit_vector.get(__LRED_ISSET_ID);
+  }
+
+  public void setLredIsSet(boolean value) {
+    __isset_bit_vector.set(__LRED_ISSET_ID, value);
+  }
+
+  public boolean isApproxAlgo() {
+    return this.approxAlgo;
+  }
+
+  public Job setApproxAlgo(boolean approxAlgo) {
+    this.approxAlgo = approxAlgo;
+    setApproxAlgoIsSet(true);
+    return this;
+  }
+
+  public void unsetApproxAlgo() {
+    __isset_bit_vector.clear(__APPROXALGO_ISSET_ID);
+  }
+
+  /** Returns true if field approxAlgo is set (has been assigned a value) and false otherwise */
+  public boolean isSetApproxAlgo() {
+    return __isset_bit_vector.get(__APPROXALGO_ISSET_ID);
+  }
+
+  public void setApproxAlgoIsSet(boolean value) {
+    __isset_bit_vector.set(__APPROXALGO_ISSET_ID, value);
+  }
+
+  public int getLmapapprox() {
+    return this.lmapapprox;
+  }
+
+  public Job setLmapapprox(int lmapapprox) {
+    this.lmapapprox = lmapapprox;
+    setLmapapproxIsSet(true);
+    return this;
+  }
+
+  public void unsetLmapapprox() {
+    __isset_bit_vector.clear(__LMAPAPPROX_ISSET_ID);
+  }
+
+  /** Returns true if field lmapapprox is set (has been assigned a value) and false otherwise */
+  public boolean isSetLmapapprox() {
+    return __isset_bit_vector.get(__LMAPAPPROX_ISSET_ID);
+  }
+
+  public void setLmapapproxIsSet(boolean value) {
+    __isset_bit_vector.set(__LMAPAPPROX_ISSET_ID, value);
+  }
+
+  public int getLredapprox() {
+    return this.lredapprox;
+  }
+
+  public Job setLredapprox(int lredapprox) {
+    this.lredapprox = lredapprox;
+    setLredapproxIsSet(true);
+    return this;
+  }
+
+  public void unsetLredapprox() {
+    __isset_bit_vector.clear(__LREDAPPROX_ISSET_ID);
+  }
+
+  /** Returns true if field lredapprox is set (has been assigned a value) and false otherwise */
+  public boolean isSetLredapprox() {
+    return __isset_bit_vector.get(__LREDAPPROX_ISSET_ID);
+  }
+
+  public void setLredapproxIsSet(boolean value) {
+    __isset_bit_vector.set(__LREDAPPROX_ISSET_ID, value);
+  }
+
+  public double getApproxAlgoMax() {
+    return this.approxAlgoMax;
+  }
+
+  public Job setApproxAlgoMax(double approxAlgoMax) {
+    this.approxAlgoMax = approxAlgoMax;
+    setApproxAlgoMaxIsSet(true);
+    return this;
+  }
+
+  public void unsetApproxAlgoMax() {
+    __isset_bit_vector.clear(__APPROXALGOMAX_ISSET_ID);
+  }
+
+  /** Returns true if field approxAlgoMax is set (has been assigned a value) and false otherwise */
+  public boolean isSetApproxAlgoMax() {
+    return __isset_bit_vector.get(__APPROXALGOMAX_ISSET_ID);
+  }
+
+  public void setApproxAlgoMaxIsSet(boolean value) {
+    __isset_bit_vector.set(__APPROXALGOMAX_ISSET_ID, value);
+  }
+
+  public double getApproxAlgoVal() {
+    return this.approxAlgoVal;
+  }
+
+  public Job setApproxAlgoVal(double approxAlgoVal) {
+    this.approxAlgoVal = approxAlgoVal;
+    setApproxAlgoValIsSet(true);
+    return this;
+  }
+
+  public void unsetApproxAlgoVal() {
+    __isset_bit_vector.clear(__APPROXALGOVAL_ISSET_ID);
+  }
+
+  /** Returns true if field approxAlgoVal is set (has been assigned a value) and false otherwise */
+  public boolean isSetApproxAlgoVal() {
+    return __isset_bit_vector.get(__APPROXALGOVAL_ISSET_ID);
+  }
+
+  public void setApproxAlgoValIsSet(boolean value) {
+    __isset_bit_vector.set(__APPROXALGOVAL_ISSET_ID, value);
+  }
+
+  public boolean isApproxDrop() {
+    return this.approxDrop;
+  }
+
+  public Job setApproxDrop(boolean approxDrop) {
+    this.approxDrop = approxDrop;
+    setApproxDropIsSet(true);
+    return this;
+  }
+
+  public void unsetApproxDrop() {
+    __isset_bit_vector.clear(__APPROXDROP_ISSET_ID);
+  }
+
+  /** Returns true if field approxDrop is set (has been assigned a value) and false otherwise */
+  public boolean isSetApproxDrop() {
+    return __isset_bit_vector.get(__APPROXDROP_ISSET_ID);
+  }
+
+  public void setApproxDropIsSet(boolean value) {
+    __isset_bit_vector.set(__APPROXDROP_ISSET_ID, value);
+  }
+
+  public double getApproxDropMax() {
+    return this.approxDropMax;
+  }
+
+  public Job setApproxDropMax(double approxDropMax) {
+    this.approxDropMax = approxDropMax;
+    setApproxDropMaxIsSet(true);
+    return this;
+  }
+
+  public void unsetApproxDropMax() {
+    __isset_bit_vector.clear(__APPROXDROPMAX_ISSET_ID);
+  }
+
+  /** Returns true if field approxDropMax is set (has been assigned a value) and false otherwise */
+  public boolean isSetApproxDropMax() {
+    return __isset_bit_vector.get(__APPROXDROPMAX_ISSET_ID);
+  }
+
+  public void setApproxDropMaxIsSet(boolean value) {
+    __isset_bit_vector.set(__APPROXDROPMAX_ISSET_ID, value);
+  }
+
+  public double getApproxDropVal() {
+    return this.approxDropVal;
+  }
+
+  public Job setApproxDropVal(double approxDropVal) {
+    this.approxDropVal = approxDropVal;
+    setApproxDropValIsSet(true);
+    return this;
+  }
+
+  public void unsetApproxDropVal() {
+    __isset_bit_vector.clear(__APPROXDROPVAL_ISSET_ID);
+  }
+
+  /** Returns true if field approxDropVal is set (has been assigned a value) and false otherwise */
+  public boolean isSetApproxDropVal() {
+    return __isset_bit_vector.get(__APPROXDROPVAL_ISSET_ID);
+  }
+
+  public void setApproxDropValIsSet(boolean value) {
+    __isset_bit_vector.set(__APPROXDROPVAL_ISSET_ID, value);
+  }
+
+  public boolean isApproxInput() {
+    return this.approxInput;
+  }
+
+  public Job setApproxInput(boolean approxInput) {
+    this.approxInput = approxInput;
+    setApproxInputIsSet(true);
+    return this;
+  }
+
+  public void unsetApproxInput() {
+    __isset_bit_vector.clear(__APPROXINPUT_ISSET_ID);
+  }
+
+  /** Returns true if field approxInput is set (has been assigned a value) and false otherwise */
+  public boolean isSetApproxInput() {
+    return __isset_bit_vector.get(__APPROXINPUT_ISSET_ID);
+  }
+
+  public void setApproxInputIsSet(boolean value) {
+    __isset_bit_vector.set(__APPROXINPUT_ISSET_ID, value);
+  }
+
+  public void setFieldValue(_Fields field, Object value) {
+    switch (field) {
+    case JOB_ID:
+      if (value == null) {
+        unsetJobId();
+      } else {
+        setJobId((String)value);
+      }
+      break;
+
+    case NMAP:
+      if (value == null) {
+        unsetNmap();
+      } else {
+        setNmap((Integer)value);
+      }
+      break;
+
+    case LMAP:
+      if (value == null) {
+        unsetLmap();
+      } else {
+        setLmap((Integer)value);
+      }
+      break;
+
+    case NRED:
+      if (value == null) {
+        unsetNred();
+      } else {
+        setNred((Integer)value);
+      }
+      break;
+
+    case LRED:
+      if (value == null) {
+        unsetLred();
+      } else {
+        setLred((Integer)value);
+      }
+      break;
+
+    case APPROX_ALGO:
+      if (value == null) {
+        unsetApproxAlgo();
+      } else {
+        setApproxAlgo((Boolean)value);
+      }
+      break;
+
+    case LMAPAPPROX:
+      if (value == null) {
+        unsetLmapapprox();
+      } else {
+        setLmapapprox((Integer)value);
+      }
+      break;
+
+    case LREDAPPROX:
+      if (value == null) {
+        unsetLredapprox();
+      } else {
+        setLredapprox((Integer)value);
+      }
+      break;
+
+    case APPROX_ALGO_MAX:
+      if (value == null) {
+        unsetApproxAlgoMax();
+      } else {
+        setApproxAlgoMax((Double)value);
+      }
+      break;
+
+    case APPROX_ALGO_VAL:
+      if (value == null) {
+        unsetApproxAlgoVal();
+      } else {
+        setApproxAlgoVal((Double)value);
+      }
+      break;
+
+    case APPROX_DROP:
+      if (value == null) {
+        unsetApproxDrop();
+      } else {
+        setApproxDrop((Boolean)value);
+      }
+      break;
+
+    case APPROX_DROP_MAX:
+      if (value == null) {
+        unsetApproxDropMax();
+      } else {
+        setApproxDropMax((Double)value);
+      }
+      break;
+
+    case APPROX_DROP_VAL:
+      if (value == null) {
+        unsetApproxDropVal();
+      } else {
+        setApproxDropVal((Double)value);
+      }
+      break;
+
+    case APPROX_INPUT:
+      if (value == null) {
+        unsetApproxInput();
+      } else {
+        setApproxInput((Boolean)value);
+      }
+      break;
+
+    }
+  }
+
+  public Object getFieldValue(_Fields field) {
+    switch (field) {
+    case JOB_ID:
+      return getJobId();
+
+    case NMAP:
+      return Integer.valueOf(getNmap());
+
+    case LMAP:
+      return Integer.valueOf(getLmap());
+
+    case NRED:
+      return Integer.valueOf(getNred());
+
+    case LRED:
+      return Integer.valueOf(getLred());
+
+    case APPROX_ALGO:
+      return Boolean.valueOf(isApproxAlgo());
+
+    case LMAPAPPROX:
+      return Integer.valueOf(getLmapapprox());
+
+    case LREDAPPROX:
+      return Integer.valueOf(getLredapprox());
+
+    case APPROX_ALGO_MAX:
+      return Double.valueOf(getApproxAlgoMax());
+
+    case APPROX_ALGO_VAL:
+      return Double.valueOf(getApproxAlgoVal());
+
+    case APPROX_DROP:
+      return Boolean.valueOf(isApproxDrop());
+
+    case APPROX_DROP_MAX:
+      return Double.valueOf(getApproxDropMax());
+
+    case APPROX_DROP_VAL:
+      return Double.valueOf(getApproxDropVal());
+
+    case APPROX_INPUT:
+      return Boolean.valueOf(isApproxInput());
+
+    }
+    throw new IllegalStateException();
+  }
+
+  /** Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise */
+  public boolean isSet(_Fields field) {
+    if (field == null) {
+      throw new IllegalArgumentException();
+    }
+
+    switch (field) {
+    case JOB_ID:
+      return isSetJobId();
+    case NMAP:
+      return isSetNmap();
+    case LMAP:
+      return isSetLmap();
+    case NRED:
+      return isSetNred();
+    case LRED:
+      return isSetLred();
+    case APPROX_ALGO:
+      return isSetApproxAlgo();
+    case LMAPAPPROX:
+      return isSetLmapapprox();
+    case LREDAPPROX:
+      return isSetLredapprox();
+    case APPROX_ALGO_MAX:
+      return isSetApproxAlgoMax();
+    case APPROX_ALGO_VAL:
+      return isSetApproxAlgoVal();
+    case APPROX_DROP:
+      return isSetApproxDrop();
+    case APPROX_DROP_MAX:
+      return isSetApproxDropMax();
+    case APPROX_DROP_VAL:
+      return isSetApproxDropVal();
+    case APPROX_INPUT:
+      return isSetApproxInput();
+    }
+    throw new IllegalStateException();
+  }
+
+  @Override
+  public boolean equals(Object that) {
+    if (that == null)
+      return false;
+    if (that instanceof Job)
+      return this.equals((Job)that);
+    return false;
+  }
+
+  public boolean equals(Job that) {
+    if (that == null)
+      return false;
+
+    boolean this_present_jobId = true && this.isSetJobId();
+    boolean that_present_jobId = true && that.isSetJobId();
+    if (this_present_jobId || that_present_jobId) {
+      if (!(this_present_jobId && that_present_jobId))
+        return false;
+      if (!this.jobId.equals(that.jobId))
+        return false;
+    }
+
+    boolean this_present_nmap = true;
+    boolean that_present_nmap = true;
+    if (this_present_nmap || that_present_nmap) {
+      if (!(this_present_nmap && that_present_nmap))
+        return false;
+      if (this.nmap != that.nmap)
+        return false;
+    }
+
+    boolean this_present_lmap = true;
+    boolean that_present_lmap = true;
+    if (this_present_lmap || that_present_lmap) {
+      if (!(this_present_lmap && that_present_lmap))
+        return false;
+      if (this.lmap != that.lmap)
+        return false;
+    }
+
+    boolean this_present_nred = true;
+    boolean that_present_nred = true;
+    if (this_present_nred || that_present_nred) {
+      if (!(this_present_nred && that_present_nred))
+        return false;
+      if (this.nred != that.nred)
+        return false;
+    }
+
+    boolean this_present_lred = true;
+    boolean that_present_lred = true;
+    if (this_present_lred || that_present_lred) {
+      if (!(this_present_lred && that_present_lred))
+        return false;
+      if (this.lred != that.lred)
+        return false;
+    }
+
+    boolean this_present_approxAlgo = true;
+    boolean that_present_approxAlgo = true;
+    if (this_present_approxAlgo || that_present_approxAlgo) {
+      if (!(this_present_approxAlgo && that_present_approxAlgo))
+        return false;
+      if (this.approxAlgo != that.approxAlgo)
+        return false;
+    }
+
+    boolean this_present_lmapapprox = true && this.isSetLmapapprox();
+    boolean that_present_lmapapprox = true && that.isSetLmapapprox();
+    if (this_present_lmapapprox || that_present_lmapapprox) {
+      if (!(this_present_lmapapprox && that_present_lmapapprox))
+        return false;
+      if (this.lmapapprox != that.lmapapprox)
+        return false;
+    }
+
+    boolean this_present_lredapprox = true && this.isSetLredapprox();
+    boolean that_present_lredapprox = true && that.isSetLredapprox();
+    if (this_present_lredapprox || that_present_lredapprox) {
+      if (!(this_present_lredapprox && that_present_lredapprox))
+        return false;
+      if (this.lredapprox != that.lredapprox)
+        return false;
+    }
+
+    boolean this_present_approxAlgoMax = true && this.isSetApproxAlgoMax();
+    boolean that_present_approxAlgoMax = true && that.isSetApproxAlgoMax();
+    if (this_present_approxAlgoMax || that_present_approxAlgoMax) {
+      if (!(this_present_approxAlgoMax && that_present_approxAlgoMax))
+        return false;
+      if (this.approxAlgoMax != that.approxAlgoMax)
+        return false;
+    }
+
+    boolean this_present_approxAlgoVal = true && this.isSetApproxAlgoVal();
+    boolean that_present_approxAlgoVal = true && that.isSetApproxAlgoVal();
+    if (this_present_approxAlgoVal || that_present_approxAlgoVal) {
+      if (!(this_present_approxAlgoVal && that_present_approxAlgoVal))
+        return false;
+      if (this.approxAlgoVal != that.approxAlgoVal)
+        return false;
+    }
+
+    boolean this_present_approxDrop = true;
+    boolean that_present_approxDrop = true;
+    if (this_present_approxDrop || that_present_approxDrop) {
+      if (!(this_present_approxDrop && that_present_approxDrop))
+        return false;
+      if (this.approxDrop != that.approxDrop)
+        return false;
+    }
+
+    boolean this_present_approxDropMax = true && this.isSetApproxDropMax();
+    boolean that_present_approxDropMax = true && that.isSetApproxDropMax();
+    if (this_present_approxDropMax || that_present_approxDropMax) {
+      if (!(this_present_approxDropMax && that_present_approxDropMax))
+        return false;
+      if (this.approxDropMax != that.approxDropMax)
+        return false;
+    }
+
+    boolean this_present_approxDropVal = true && this.isSetApproxDropVal();
+    boolean that_present_approxDropVal = true && that.isSetApproxDropVal();
+    if (this_present_approxDropVal || that_present_approxDropVal) {
+      if (!(this_present_approxDropVal && that_present_approxDropVal))
+        return false;
+      if (this.approxDropVal != that.approxDropVal)
+        return false;
+    }
+
+    boolean this_present_approxInput = true;
+    boolean that_present_approxInput = true;
+    if (this_present_approxInput || that_present_approxInput) {
+      if (!(this_present_approxInput && that_present_approxInput))
+        return false;
+      if (this.approxInput != that.approxInput)
+        return false;
+    }
+
+    return true;
+  }
+
+  @Override
+  public int hashCode() {
+    return 0;
+  }
+
+  public int compareTo(Job other) {
+    if (!getClass().equals(other.getClass())) {
+      return getClass().getName().compareTo(other.getClass().getName());
+    }
+
+    int lastComparison = 0;
+    Job typedOther = (Job)other;
+
+    lastComparison = Boolean.valueOf(isSetJobId()).compareTo(typedOther.isSetJobId());
+    if (lastComparison != 0) {
+      return lastComparison;
+    }
+    if (isSetJobId()) {
+      lastComparison = org.apache.thrift.TBaseHelper.compareTo(this.jobId, typedOther.jobId);
+      if (lastComparison != 0) {
+        return lastComparison;
+      }
+    }
+    lastComparison = Boolean.valueOf(isSetNmap()).compareTo(typedOther.isSetNmap());
+    if (lastComparison != 0) {
+      return lastComparison;
+    }
+    if (isSetNmap()) {
+      lastComparison = org.apache.thrift.TBaseHelper.compareTo(this.nmap, typedOther.nmap);
+      if (lastComparison != 0) {
+        return lastComparison;
+      }
+    }
+    lastComparison = Boolean.valueOf(isSetLmap()).compareTo(typedOther.isSetLmap());
+    if (lastComparison != 0) {
+      return lastComparison;
+    }
+    if (isSetLmap()) {
+      lastComparison = org.apache.thrift.TBaseHelper.compareTo(this.lmap, typedOther.lmap);
+      if (lastComparison != 0) {
+        return lastComparison;
+      }
+    }
+    lastComparison = Boolean.valueOf(isSetNred()).compareTo(typedOther.isSetNred());
+    if (lastComparison != 0) {
+      return lastComparison;
+    }
+    if (isSetNred()) {
+      lastComparison = org.apache.thrift.TBaseHelper.compareTo(this.nred, typedOther.nred);
+      if (lastComparison != 0) {
+        return lastComparison;
+      }
+    }
+    lastComparison = Boolean.valueOf(isSetLred()).compareTo(typedOther.isSetLred());
+    if (lastComparison != 0) {
+      return lastComparison;
+    }
+    if (isSetLred()) {
+      lastComparison = org.apache.thrift.TBaseHelper.compareTo(this.lred, typedOther.lred);
+      if (lastComparison != 0) {
+        return lastComparison;
+      }
+    }
+    lastComparison = Boolean.valueOf(isSetApproxAlgo()).compareTo(typedOther.isSetApproxAlgo());
+    if (lastComparison != 0) {
+      return lastComparison;
+    }
+    if (isSetApproxAlgo()) {
+      lastComparison = org.apache.thrift.TBaseHelper.compareTo(this.approxAlgo, typedOther.approxAlgo);
+      if (lastComparison != 0) {
+        return lastComparison;
+      }
+    }
+    lastComparison = Boolean.valueOf(isSetLmapapprox()).compareTo(typedOther.isSetLmapapprox());
+    if (lastComparison != 0) {
+      return lastComparison;
+    }
+    if (isSetLmapapprox()) {
+      lastComparison = org.apache.thrift.TBaseHelper.compareTo(this.lmapapprox, typedOther.lmapapprox);
+      if (lastComparison != 0) {
+        return lastComparison;
+      }
+    }
+    lastComparison = Boolean.valueOf(isSetLredapprox()).compareTo(typedOther.isSetLredapprox());
+    if (lastComparison != 0) {
+      return lastComparison;
+    }
+    if (isSetLredapprox()) {
+      lastComparison = org.apache.thrift.TBaseHelper.compareTo(this.lredapprox, typedOther.lredapprox);
+      if (lastComparison != 0) {
+        return lastComparison;
+      }
+    }
+    lastComparison = Boolean.valueOf(isSetApproxAlgoMax()).compareTo(typedOther.isSetApproxAlgoMax());
+    if (lastComparison != 0) {
+      return lastComparison;
+    }
+    if (isSetApproxAlgoMax()) {
+      lastComparison = org.apache.thrift.TBaseHelper.compareTo(this.approxAlgoMax, typedOther.approxAlgoMax);
+      if (lastComparison != 0) {
+        return lastComparison;
+      }
+    }
+    lastComparison = Boolean.valueOf(isSetApproxAlgoVal()).compareTo(typedOther.isSetApproxAlgoVal());
+    if (lastComparison != 0) {
+      return lastComparison;
+    }
+    if (isSetApproxAlgoVal()) {
+      lastComparison = org.apache.thrift.TBaseHelper.compareTo(this.approxAlgoVal, typedOther.approxAlgoVal);
+      if (lastComparison != 0) {
+        return lastComparison;
+      }
+    }
+    lastComparison = Boolean.valueOf(isSetApproxDrop()).compareTo(typedOther.isSetApproxDrop());
+    if (lastComparison != 0) {
+      return lastComparison;
+    }
+    if (isSetApproxDrop()) {
+      lastComparison = org.apache.thrift.TBaseHelper.compareTo(this.approxDrop, typedOther.approxDrop);
+      if (lastComparison != 0) {
+        return lastComparison;
+      }
+    }
+    lastComparison = Boolean.valueOf(isSetApproxDropMax()).compareTo(typedOther.isSetApproxDropMax());
+    if (lastComparison != 0) {
+      return lastComparison;
+    }
+    if (isSetApproxDropMax()) {
+      lastComparison = org.apache.thrift.TBaseHelper.compareTo(this.approxDropMax, typedOther.approxDropMax);
+      if (lastComparison != 0) {
+        return lastComparison;
+      }
+    }
+    lastComparison = Boolean.valueOf(isSetApproxDropVal()).compareTo(typedOther.isSetApproxDropVal());
+    if (lastComparison != 0) {
+      return lastComparison;
+    }
+    if (isSetApproxDropVal()) {
+      lastComparison = org.apache.thrift.TBaseHelper.compareTo(this.approxDropVal, typedOther.approxDropVal);
+      if (lastComparison != 0) {
+        return lastComparison;
+      }
+    }
+    lastComparison = Boolean.valueOf(isSetApproxInput()).compareTo(typedOther.isSetApproxInput());
+    if (lastComparison != 0) {
+      return lastComparison;
+    }
+    if (isSetApproxInput()) {
+      lastComparison = org.apache.thrift.TBaseHelper.compareTo(this.approxInput, typedOther.approxInput);
+      if (lastComparison != 0) {
+        return lastComparison;
+      }
+    }
+    return 0;
+  }
+
+  public _Fields fieldForId(int fieldId) {
+    return _Fields.findByThriftId(fieldId);
+  }
+
+  public void read(org.apache.thrift.protocol.TProtocol iprot) throws org.apache.thrift.TException {
+    schemes.get(iprot.getScheme()).getScheme().read(iprot, this);
+  }
+
+  public void write(org.apache.thrift.protocol.TProtocol oprot) throws org.apache.thrift.TException {
+    schemes.get(oprot.getScheme()).getScheme().write(oprot, this);
+  }
+
+  @Override
+  public String toString() {
+    StringBuilder sb = new StringBuilder("Job(");
+    boolean first = true;
+
+    sb.append("jobId:");
+    if (this.jobId == null) {
+      sb.append("null");
+    } else {
+      sb.append(this.jobId);
+    }
+    first = false;
+    if (!first) sb.append(", ");
+    sb.append("nmap:");
+    sb.append(this.nmap);
+    first = false;
+    if (!first) sb.append(", ");
+    sb.append("lmap:");
+    sb.append(this.lmap);
+    first = false;
+    if (!first) sb.append(", ");
+    sb.append("nred:");
+    sb.append(this.nred);
+    first = false;
+    if (!first) sb.append(", ");
+    sb.append("lred:");
+    sb.append(this.lred);
+    first = false;
+    if (!first) sb.append(", ");
+    sb.append("approxAlgo:");
+    sb.append(this.approxAlgo);
+    first = false;
+    if (isSetLmapapprox()) {
+      if (!first) sb.append(", ");
+      sb.append("lmapapprox:");
+      sb.append(this.lmapapprox);
+      first = false;
+    }
+    if (isSetLredapprox()) {
+      if (!first) sb.append(", ");
+      sb.append("lredapprox:");
+      sb.append(this.lredapprox);
+      first = false;
+    }
+    if (isSetApproxAlgoMax()) {
+      if (!first) sb.append(", ");
+      sb.append("approxAlgoMax:");
+      sb.append(this.approxAlgoMax);
+      first = false;
+    }
+    if (isSetApproxAlgoVal()) {
+      if (!first) sb.append(", ");
+      sb.append("approxAlgoVal:");
+      sb.append(this.approxAlgoVal);
+      first = false;
+    }
+    if (!first) sb.append(", ");
+    sb.append("approxDrop:");
+    sb.append(this.approxDrop);
+    first = false;
+    if (isSetApproxDropMax()) {
+      if (!first) sb.append(", ");
+      sb.append("approxDropMax:");
+      sb.append(this.approxDropMax);
+      first = false;
+    }
+    if (isSetApproxDropVal()) {
+      if (!first) sb.append(", ");
+      sb.append("approxDropVal:");
+      sb.append(this.approxDropVal);
+      first = false;
+    }
+    if (!first) sb.append(", ");
+    sb.append("approxInput:");
+    sb.append(this.approxInput);
+    first = false;
+    sb.append(")");
+    return sb.toString();
+  }
+
+  public void validate() throws org.apache.thrift.TException {
+    // check for required fields
+  }
+
+  private void writeObject(java.io.ObjectOutputStream out) throws java.io.IOException {
+    try {
+      write(new org.apache.thrift.protocol.TCompactProtocol(new org.apache.thrift.transport.TIOStreamTransport(out)));
+    } catch (org.apache.thrift.TException te) {
+      throw new java.io.IOException(te);
+    }
+  }
+
+  private void readObject(java.io.ObjectInputStream in) throws java.io.IOException, ClassNotFoundException {
+    try {
+      // it doesn't seem like you should have to do this, but java serialization is wacky, and doesn't call the default constructor.
+      __isset_bit_vector = new BitSet(1);
+      read(new org.apache.thrift.protocol.TCompactProtocol(new org.apache.thrift.transport.TIOStreamTransport(in)));
+    } catch (org.apache.thrift.TException te) {
+      throw new java.io.IOException(te);
+    }
+  }
+
+  private static class JobStandardSchemeFactory implements SchemeFactory {
+    public JobStandardScheme getScheme() {
+      return new JobStandardScheme();
+    }
+  }
+
+  private static class JobStandardScheme extends StandardScheme<Job> {
+
+    public void read(org.apache.thrift.protocol.TProtocol iprot, Job struct) throws org.apache.thrift.TException {
+      org.apache.thrift.protocol.TField schemeField;
+      iprot.readStructBegin();
+      while (true)
+      {
+        schemeField = iprot.readFieldBegin();
+        if (schemeField.type == org.apache.thrift.protocol.TType.STOP) { 
+          break;
+        }
+        switch (schemeField.id) {
+          case 1: // JOB_ID
+            if (schemeField.type == org.apache.thrift.protocol.TType.STRING) {
+              struct.jobId = iprot.readString();
+              struct.setJobIdIsSet(true);
+            } else { 
+              org.apache.thrift.protocol.TProtocolUtil.skip(iprot, schemeField.type);
+            }
+            break;
+          case 2: // NMAP
+            if (schemeField.type == org.apache.thrift.protocol.TType.I32) {
+              struct.nmap = iprot.readI32();
+              struct.setNmapIsSet(true);
+            } else { 
+              org.apache.thrift.protocol.TProtocolUtil.skip(iprot, schemeField.type);
+            }
+            break;
+          case 3: // LMAP
+            if (schemeField.type == org.apache.thrift.protocol.TType.I32) {
+              struct.lmap = iprot.readI32();
+              struct.setLmapIsSet(true);
+            } else { 
+              org.apache.thrift.protocol.TProtocolUtil.skip(iprot, schemeField.type);
+            }
+            break;
+          case 4: // NRED
+            if (schemeField.type == org.apache.thrift.protocol.TType.I32) {
+              struct.nred = iprot.readI32();
+              struct.setNredIsSet(true);
+            } else { 
+              org.apache.thrift.protocol.TProtocolUtil.skip(iprot, schemeField.type);
+            }
+            break;
+          case 5: // LRED
+            if (schemeField.type == org.apache.thrift.protocol.TType.I32) {
+              struct.lred = iprot.readI32();
+              struct.setLredIsSet(true);
+            } else { 
+              org.apache.thrift.protocol.TProtocolUtil.skip(iprot, schemeField.type);
+            }
+            break;
+          case 6: // APPROX_ALGO
+            if (schemeField.type == org.apache.thrift.protocol.TType.BOOL) {
+              struct.approxAlgo = iprot.readBool();
+              struct.setApproxAlgoIsSet(true);
+            } else { 
+              org.apache.thrift.protocol.TProtocolUtil.skip(iprot, schemeField.type);
+            }
+            break;
+          case 7: // LMAPAPPROX
+            if (schemeField.type == org.apache.thrift.protocol.TType.I32) {
+              struct.lmapapprox = iprot.readI32();
+              struct.setLmapapproxIsSet(true);
+            } else { 
+              org.apache.thrift.protocol.TProtocolUtil.skip(iprot, schemeField.type);
+            }
+            break;
+          case 8: // LREDAPPROX
+            if (schemeField.type == org.apache.thrift.protocol.TType.I32) {
+              struct.lredapprox = iprot.readI32();
+              struct.setLredapproxIsSet(true);
+            } else { 
+              org.apache.thrift.protocol.TProtocolUtil.skip(iprot, schemeField.type);
+            }
+            break;
+          case 9: // APPROX_ALGO_MAX
+            if (schemeField.type == org.apache.thrift.protocol.TType.DOUBLE) {
+              struct.approxAlgoMax = iprot.readDouble();
+              struct.setApproxAlgoMaxIsSet(true);
+            } else { 
+              org.apache.thrift.protocol.TProtocolUtil.skip(iprot, schemeField.type);
+            }
+            break;
+          case 10: // APPROX_ALGO_VAL
+            if (schemeField.type == org.apache.thrift.protocol.TType.DOUBLE) {
+              struct.approxAlgoVal = iprot.readDouble();
+              struct.setApproxAlgoValIsSet(true);
+            } else { 
+              org.apache.thrift.protocol.TProtocolUtil.skip(iprot, schemeField.type);
+            }
+            break;
+          case 11: // APPROX_DROP
+            if (schemeField.type == org.apache.thrift.protocol.TType.BOOL) {
+              struct.approxDrop = iprot.readBool();
+              struct.setApproxDropIsSet(true);
+            } else { 
+              org.apache.thrift.protocol.TProtocolUtil.skip(iprot, schemeField.type);
+            }
+            break;
+          case 12: // APPROX_DROP_MAX
+            if (schemeField.type == org.apache.thrift.protocol.TType.DOUBLE) {
+              struct.approxDropMax = iprot.readDouble();
+              struct.setApproxDropMaxIsSet(true);
+            } else { 
+              org.apache.thrift.protocol.TProtocolUtil.skip(iprot, schemeField.type);
+            }
+            break;
+          case 13: // APPROX_DROP_VAL
+            if (schemeField.type == org.apache.thrift.protocol.TType.DOUBLE) {
+              struct.approxDropVal = iprot.readDouble();
+              struct.setApproxDropValIsSet(true);
+            } else { 
+              org.apache.thrift.protocol.TProtocolUtil.skip(iprot, schemeField.type);
+            }
+            break;
+          case 14: // APPROX_INPUT
+            if (schemeField.type == org.apache.thrift.protocol.TType.BOOL) {
+              struct.approxInput = iprot.readBool();
+              struct.setApproxInputIsSet(true);
+            } else { 
+              org.apache.thrift.protocol.TProtocolUtil.skip(iprot, schemeField.type);
+            }
+            break;
+          default:
+            org.apache.thrift.protocol.TProtocolUtil.skip(iprot, schemeField.type);
+        }
+        iprot.readFieldEnd();
+      }
+      iprot.readStructEnd();
+
+      // check for required fields of primitive type, which can't be checked in the validate method
+      struct.validate();
+    }
+
+    public void write(org.apache.thrift.protocol.TProtocol oprot, Job struct) throws org.apache.thrift.TException {
+      struct.validate();
+
+      oprot.writeStructBegin(STRUCT_DESC);
+      if (struct.jobId != null) {
+        oprot.writeFieldBegin(JOB_ID_FIELD_DESC);
+        oprot.writeString(struct.jobId);
+        oprot.writeFieldEnd();
+      }
+      oprot.writeFieldBegin(NMAP_FIELD_DESC);
+      oprot.writeI32(struct.nmap);
+      oprot.writeFieldEnd();
+      oprot.writeFieldBegin(LMAP_FIELD_DESC);
+      oprot.writeI32(struct.lmap);
+      oprot.writeFieldEnd();
+      oprot.writeFieldBegin(NRED_FIELD_DESC);
+      oprot.writeI32(struct.nred);
+      oprot.writeFieldEnd();
+      oprot.writeFieldBegin(LRED_FIELD_DESC);
+      oprot.writeI32(struct.lred);
+      oprot.writeFieldEnd();
+      oprot.writeFieldBegin(APPROX_ALGO_FIELD_DESC);
+      oprot.writeBool(struct.approxAlgo);
+      oprot.writeFieldEnd();
+      if (struct.isSetLmapapprox()) {
+        oprot.writeFieldBegin(LMAPAPPROX_FIELD_DESC);
+        oprot.writeI32(struct.lmapapprox);
+        oprot.writeFieldEnd();
+      }
+      if (struct.isSetLredapprox()) {
+        oprot.writeFieldBegin(LREDAPPROX_FIELD_DESC);
+        oprot.writeI32(struct.lredapprox);
+        oprot.writeFieldEnd();
+      }
+      if (struct.isSetApproxAlgoMax()) {
+        oprot.writeFieldBegin(APPROX_ALGO_MAX_FIELD_DESC);
+        oprot.writeDouble(struct.approxAlgoMax);
+        oprot.writeFieldEnd();
+      }
+      if (struct.isSetApproxAlgoVal()) {
+        oprot.writeFieldBegin(APPROX_ALGO_VAL_FIELD_DESC);
+        oprot.writeDouble(struct.approxAlgoVal);
+        oprot.writeFieldEnd();
+      }
+      oprot.writeFieldBegin(APPROX_DROP_FIELD_DESC);
+      oprot.writeBool(struct.approxDrop);
+      oprot.writeFieldEnd();
+      if (struct.isSetApproxDropMax()) {
+        oprot.writeFieldBegin(APPROX_DROP_MAX_FIELD_DESC);
+        oprot.writeDouble(struct.approxDropMax);
+        oprot.writeFieldEnd();
+      }
+      if (struct.isSetApproxDropVal()) {
+        oprot.writeFieldBegin(APPROX_DROP_VAL_FIELD_DESC);
+        oprot.writeDouble(struct.approxDropVal);
+        oprot.writeFieldEnd();
+      }
+      oprot.writeFieldBegin(APPROX_INPUT_FIELD_DESC);
+      oprot.writeBool(struct.approxInput);
+      oprot.writeFieldEnd();
+      oprot.writeFieldStop();
+      oprot.writeStructEnd();
+    }
+
+  }
+
+  private static class JobTupleSchemeFactory implements SchemeFactory {
+    public JobTupleScheme getScheme() {
+      return new JobTupleScheme();
+    }
+  }
+
+  private static class JobTupleScheme extends TupleScheme<Job> {
+
+    @Override
+    public void write(org.apache.thrift.protocol.TProtocol prot, Job struct) throws org.apache.thrift.TException {
+      TTupleProtocol oprot = (TTupleProtocol) prot;
+      BitSet optionals = new BitSet();
+      if (struct.isSetJobId()) {
+        optionals.set(0);
+      }
+      if (struct.isSetNmap()) {
+        optionals.set(1);
+      }
+      if (struct.isSetLmap()) {
+        optionals.set(2);
+      }
+      if (struct.isSetNred()) {
+        optionals.set(3);
+      }
+      if (struct.isSetLred()) {
+        optionals.set(4);
+      }
+      if (struct.isSetApproxAlgo()) {
+        optionals.set(5);
+      }
+      if (struct.isSetLmapapprox()) {
+        optionals.set(6);
+      }
+      if (struct.isSetLredapprox()) {
+        optionals.set(7);
+      }
+      if (struct.isSetApproxAlgoMax()) {
+        optionals.set(8);
+      }
+      if (struct.isSetApproxAlgoVal()) {
+        optionals.set(9);
+      }
+      if (struct.isSetApproxDrop()) {
+        optionals.set(10);
+      }
+      if (struct.isSetApproxDropMax()) {
+        optionals.set(11);
+      }
+      if (struct.isSetApproxDropVal()) {
+        optionals.set(12);
+      }
+      if (struct.isSetApproxInput()) {
+        optionals.set(13);
+      }
+      oprot.writeBitSet(optionals, 14);
+      if (struct.isSetJobId()) {
+        oprot.writeString(struct.jobId);
+      }
+      if (struct.isSetNmap()) {
+        oprot.writeI32(struct.nmap);
+      }
+      if (struct.isSetLmap()) {
+        oprot.writeI32(struct.lmap);
+      }
+      if (struct.isSetNred()) {
+        oprot.writeI32(struct.nred);
+      }
+      if (struct.isSetLred()) {
+        oprot.writeI32(struct.lred);
+      }
+      if (struct.isSetApproxAlgo()) {
+        oprot.writeBool(struct.approxAlgo);
+      }
+      if (struct.isSetLmapapprox()) {
+        oprot.writeI32(struct.lmapapprox);
+      }
+      if (struct.isSetLredapprox()) {
+        oprot.writeI32(struct.lredapprox);
+      }
+      if (struct.isSetApproxAlgoMax()) {
+        oprot.writeDouble(struct.approxAlgoMax);
+      }
+      if (struct.isSetApproxAlgoVal()) {
+        oprot.writeDouble(struct.approxAlgoVal);
+      }
+      if (struct.isSetApproxDrop()) {
+        oprot.writeBool(struct.approxDrop);
+      }
+      if (struct.isSetApproxDropMax()) {
+        oprot.writeDouble(struct.approxDropMax);
+      }
+      if (struct.isSetApproxDropVal()) {
+        oprot.writeDouble(struct.approxDropVal);
+      }
+      if (struct.isSetApproxInput()) {
+        oprot.writeBool(struct.approxInput);
+      }
+    }
+
+    @Override
+    public void read(org.apache.thrift.protocol.TProtocol prot, Job struct) throws org.apache.thrift.TException {
+      TTupleProtocol iprot = (TTupleProtocol) prot;
+      BitSet incoming = iprot.readBitSet(14);
+      if (incoming.get(0)) {
+        struct.jobId = iprot.readString();
+        struct.setJobIdIsSet(true);
+      }
+      if (incoming.get(1)) {
+        struct.nmap = iprot.readI32();
+        struct.setNmapIsSet(true);
+      }
+      if (incoming.get(2)) {
+        struct.lmap = iprot.readI32();
+        struct.setLmapIsSet(true);
+      }
+      if (incoming.get(3)) {
+        struct.nred = iprot.readI32();
+        struct.setNredIsSet(true);
+      }
+      if (incoming.get(4)) {
+        struct.lred = iprot.readI32();
+        struct.setLredIsSet(true);
+      }
+      if (incoming.get(5)) {
+        struct.approxAlgo = iprot.readBool();
+        struct.setApproxAlgoIsSet(true);
+      }
+      if (incoming.get(6)) {
+        struct.lmapapprox = iprot.readI32();
+        struct.setLmapapproxIsSet(true);
+      }
+      if (incoming.get(7)) {
+        struct.lredapprox = iprot.readI32();
+        struct.setLredapproxIsSet(true);
+      }
+      if (incoming.get(8)) {
+        struct.approxAlgoMax = iprot.readDouble();
+        struct.setApproxAlgoMaxIsSet(true);
+      }
+      if (incoming.get(9)) {
+        struct.approxAlgoVal = iprot.readDouble();
+        struct.setApproxAlgoValIsSet(true);
+      }
+      if (incoming.get(10)) {
+        struct.approxDrop = iprot.readBool();
+        struct.setApproxDropIsSet(true);
+      }
+      if (incoming.get(11)) {
+        struct.approxDropMax = iprot.readDouble();
+        struct.setApproxDropMaxIsSet(true);
+      }
+      if (incoming.get(12)) {
+        struct.approxDropVal = iprot.readDouble();
+        struct.setApproxDropValIsSet(true);
+      }
+      if (incoming.get(13)) {
+        struct.approxInput = iprot.readBool();
+        struct.setApproxInputIsSet(true);
+      }
+    }
+  }
+
+}
+
Index: src/contrib/approximationscheduler/gen-py/__init__.py
===================================================================
Index: src/contrib/approximationscheduler/gen-py/approximation/ApproximationScheduler-remote
===================================================================
--- src/contrib/approximationscheduler/gen-py/approximation/ApproximationScheduler-remote	(revision 0)
+++ src/contrib/approximationscheduler/gen-py/approximation/ApproximationScheduler-remote	(working copy)
@@ -0,0 +1,88 @@
+#!/usr/bin/env python
+#
+# Autogenerated by Thrift Compiler (0.8.0)
+#
+# DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
+#
+#  options string: py
+#
+
+import sys
+import pprint
+from urlparse import urlparse
+from thrift.transport import TTransport
+from thrift.transport import TSocket
+from thrift.transport import THttpClient
+from thrift.protocol import TBinaryProtocol
+
+import ApproximationScheduler
+from ttypes import *
+
+if len(sys.argv) <= 1 or sys.argv[1] == '--help':
+  print ''
+  print 'Usage: ' + sys.argv[0] + ' [-h host[:port]] [-u url] [-f[ramed]] function [arg1 [arg2...]]'
+  print ''
+  print 'Functions:'
+  print '  bool schedule()'
+  print ''
+  sys.exit(0)
+
+pp = pprint.PrettyPrinter(indent = 2)
+host = 'localhost'
+port = 9090
+uri = ''
+framed = False
+http = False
+argi = 1
+
+if sys.argv[argi] == '-h':
+  parts = sys.argv[argi+1].split(':')
+  host = parts[0]
+  if len(parts) > 1:
+    port = int(parts[1])
+  argi += 2
+
+if sys.argv[argi] == '-u':
+  url = urlparse(sys.argv[argi+1])
+  parts = url[1].split(':')
+  host = parts[0]
+  if len(parts) > 1:
+    port = int(parts[1])
+  else:
+    port = 80
+  uri = url[2]
+  if url[4]:
+    uri += '?%s' % url[4]
+  http = True
+  argi += 2
+
+if sys.argv[argi] == '-f' or sys.argv[argi] == '-framed':
+  framed = True
+  argi += 1
+
+cmd = sys.argv[argi]
+args = sys.argv[argi+1:]
+
+if http:
+  transport = THttpClient.THttpClient(host, port, uri)
+else:
+  socket = TSocket.TSocket(host, port)
+  if framed:
+    transport = TTransport.TFramedTransport(socket)
+  else:
+    transport = TTransport.TBufferedTransport(socket)
+protocol = TBinaryProtocol.TBinaryProtocol(transport)
+client = ApproximationScheduler.Client(protocol)
+transport.open()
+
+if cmd == 'schedule':
+  if len(args) != 0:
+    print 'schedule requires 0 args'
+    sys.exit(1)
+  pp.pprint(client.schedule())
+
+else:
+  print 'Unrecognized method %s' % cmd
+  sys.exit(1)
+
+transport.close()

Property changes on: src/contrib/approximationscheduler/gen-py/approximation/ApproximationScheduler-remote
___________________________________________________________________
Added: svn:executable
## -0,0 +1 ##
+*
\ No newline at end of property
Index: src/contrib/approximationscheduler/gen-py/approximation/ApproximationScheduler.py
===================================================================
--- src/contrib/approximationscheduler/gen-py/approximation/ApproximationScheduler.py	(revision 0)
+++ src/contrib/approximationscheduler/gen-py/approximation/ApproximationScheduler.py	(working copy)
@@ -0,0 +1,192 @@
+#
+# Autogenerated by Thrift Compiler (0.8.0)
+#
+# DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
+#
+#  options string: py
+#
+
+from thrift.Thrift import TType, TMessageType, TException
+from ttypes import *
+from thrift.Thrift import TProcessor
+from thrift.transport import TTransport
+from thrift.protocol import TBinaryProtocol, TProtocol
+try:
+  from thrift.protocol import fastbinary
+except:
+  fastbinary = None
+
+
+class Iface:
+  def schedule(self, ):
+    pass
+
+
+class Client(Iface):
+  def __init__(self, iprot, oprot=None):
+    self._iprot = self._oprot = iprot
+    if oprot is not None:
+      self._oprot = oprot
+    self._seqid = 0
+
+  def schedule(self, ):
+    self.send_schedule()
+    return self.recv_schedule()
+
+  def send_schedule(self, ):
+    self._oprot.writeMessageBegin('schedule', TMessageType.CALL, self._seqid)
+    args = schedule_args()
+    args.write(self._oprot)
+    self._oprot.writeMessageEnd()
+    self._oprot.trans.flush()
+
+  def recv_schedule(self, ):
+    (fname, mtype, rseqid) = self._iprot.readMessageBegin()
+    if mtype == TMessageType.EXCEPTION:
+      x = TApplicationException()
+      x.read(self._iprot)
+      self._iprot.readMessageEnd()
+      raise x
+    result = schedule_result()
+    result.read(self._iprot)
+    self._iprot.readMessageEnd()
+    if result.success is not None:
+      return result.success
+    raise TApplicationException(TApplicationException.MISSING_RESULT, "schedule failed: unknown result");
+
+
+class Processor(Iface, TProcessor):
+  def __init__(self, handler):
+    self._handler = handler
+    self._processMap = {}
+    self._processMap["schedule"] = Processor.process_schedule
+
+  def process(self, iprot, oprot):
+    (name, type, seqid) = iprot.readMessageBegin()
+    if name not in self._processMap:
+      iprot.skip(TType.STRUCT)
+      iprot.readMessageEnd()
+      x = TApplicationException(TApplicationException.UNKNOWN_METHOD, 'Unknown function %s' % (name))
+      oprot.writeMessageBegin(name, TMessageType.EXCEPTION, seqid)
+      x.write(oprot)
+      oprot.writeMessageEnd()
+      oprot.trans.flush()
+      return
+    else:
+      self._processMap[name](self, seqid, iprot, oprot)
+    return True
+
+  def process_schedule(self, seqid, iprot, oprot):
+    args = schedule_args()
+    args.read(iprot)
+    iprot.readMessageEnd()
+    result = schedule_result()
+    result.success = self._handler.schedule()
+    oprot.writeMessageBegin("schedule", TMessageType.REPLY, seqid)
+    result.write(oprot)
+    oprot.writeMessageEnd()
+    oprot.trans.flush()
+
+
+# HELPER FUNCTIONS AND STRUCTURES
+
+class schedule_args:
+
+  thrift_spec = (
+  )
+
+  def read(self, iprot):
+    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
+      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
+      return
+    iprot.readStructBegin()
+    while True:
+      (fname, ftype, fid) = iprot.readFieldBegin()
+      if ftype == TType.STOP:
+        break
+      else:
+        iprot.skip(ftype)
+      iprot.readFieldEnd()
+    iprot.readStructEnd()
+
+  def write(self, oprot):
+    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
+      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
+      return
+    oprot.writeStructBegin('schedule_args')
+    oprot.writeFieldStop()
+    oprot.writeStructEnd()
+
+  def validate(self):
+    return
+
+
+  def __repr__(self):
+    L = ['%s=%r' % (key, value)
+      for key, value in self.__dict__.iteritems()]
+    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))
+
+  def __eq__(self, other):
+    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__
+
+  def __ne__(self, other):
+    return not (self == other)
+
+class schedule_result:
+  """
+  Attributes:
+   - success
+  """
+
+  thrift_spec = (
+    (0, TType.BOOL, 'success', None, None, ), # 0
+  )
+
+  def __init__(self, success=None,):
+    self.success = success
+
+  def read(self, iprot):
+    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
+      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
+      return
+    iprot.readStructBegin()
+    while True:
+      (fname, ftype, fid) = iprot.readFieldBegin()
+      if ftype == TType.STOP:
+        break
+      if fid == 0:
+        if ftype == TType.BOOL:
+          self.success = iprot.readBool();
+        else:
+          iprot.skip(ftype)
+      else:
+        iprot.skip(ftype)
+      iprot.readFieldEnd()
+    iprot.readStructEnd()
+
+  def write(self, oprot):
+    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
+      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
+      return
+    oprot.writeStructBegin('schedule_result')
+    if self.success is not None:
+      oprot.writeFieldBegin('success', TType.BOOL, 0)
+      oprot.writeBool(self.success)
+      oprot.writeFieldEnd()
+    oprot.writeFieldStop()
+    oprot.writeStructEnd()
+
+  def validate(self):
+    return
+
+
+  def __repr__(self):
+    L = ['%s=%r' % (key, value)
+      for key, value in self.__dict__.iteritems()]
+    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))
+
+  def __eq__(self, other):
+    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__
+
+  def __ne__(self, other):
+    return not (self == other)
Index: src/contrib/approximationscheduler/gen-py/approximation/__init__.py
===================================================================
--- src/contrib/approximationscheduler/gen-py/approximation/__init__.py	(revision 0)
+++ src/contrib/approximationscheduler/gen-py/approximation/__init__.py	(working copy)
@@ -0,0 +1 @@
+__all__ = ['ttypes', 'constants', 'ApproximationScheduler']
Index: src/contrib/approximationscheduler/gen-py/approximation/constants.py
===================================================================
--- src/contrib/approximationscheduler/gen-py/approximation/constants.py	(revision 0)
+++ src/contrib/approximationscheduler/gen-py/approximation/constants.py	(working copy)
@@ -0,0 +1,11 @@
+#
+# Autogenerated by Thrift Compiler (0.8.0)
+#
+# DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
+#
+#  options string: py
+#
+
+from thrift.Thrift import TType, TMessageType, TException
+from ttypes import *
+
Index: src/contrib/approximationscheduler/gen-py/approximation/ttypes.py
===================================================================
--- src/contrib/approximationscheduler/gen-py/approximation/ttypes.py	(revision 0)
+++ src/contrib/approximationscheduler/gen-py/approximation/ttypes.py	(working copy)
@@ -0,0 +1,18 @@
+#
+# Autogenerated by Thrift Compiler (0.8.0)
+#
+# DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
+#
+#  options string: py
+#
+
+from thrift.Thrift import TType, TMessageType, TException
+
+from thrift.transport import TTransport
+from thrift.protocol import TBinaryProtocol, TProtocol
+try:
+  from thrift.protocol import fastbinary
+except:
+  fastbinary = None
+
+
Index: src/contrib/approximationscheduler/ivy/libraries.properties
===================================================================
--- src/contrib/approximationscheduler/ivy/libraries.properties	(revision 0)
+++ src/contrib/approximationscheduler/ivy/libraries.properties	(working copy)
@@ -0,0 +1,5 @@
+#This properties file lists the versions of the various artifacts used by streaming.
+#It drives ivy and the generation of a maven POM
+
+#Please list the dependencies name with version if they are different from the ones 
+#listed in the global libraries.properties file (in alphabetical order)
Index: src/contrib/approximationscheduler/ivy.xml
===================================================================
--- src/contrib/approximationscheduler/ivy.xml	(revision 0)
+++ src/contrib/approximationscheduler/ivy.xml	(working copy)
@@ -0,0 +1,70 @@
+<?xml version="1.0" ?>
+<ivy-module version="1.0">
+  <info organisation="org.apache.hadoop" module="${ant.project.name}">
+    <license name="Apache 2.0"/>
+    <ivyauthor name="Apache Hadoop Team" url="http://hadoop.apache.org"/>
+    <description>
+        Apache Hadoop contrib
+    </description>
+  </info>
+  <configurations defaultconfmapping="default">
+    <!--these match the Maven configurations-->
+    <conf name="default" extends="master,runtime"/>
+    <conf name="master" description="contains the artifact but no dependencies"/>
+    <conf name="runtime" description="runtime but not the artifact" />
+
+    <conf name="common" visibility="private" 
+      description="artifacts needed to compile/test the application"/>
+  </configurations>
+
+  <publications>
+    <!--get the artifact from our module name-->
+    <artifact conf="master"/>
+  </publications>
+  <dependencies>
+    <dependency org="commons-logging"
+      name="commons-logging"
+      rev="${commons-logging.version}"
+      conf="common->default"/>
+    <dependency org="commons-collections"
+      name="commons-collections"
+      rev="${commons-collections.version}"
+      conf="common->default"/>
+    <dependency org="log4j"
+      name="log4j"
+      rev="${log4j.version}"
+      conf="common->master"/>
+   <dependency org="junit"
+      name="junit"
+      rev="${junit.version}"
+      conf="common->default"/>
+    <dependency org="org.mortbay.jetty"
+      name="jetty-util"
+      rev="${jetty-util.version}"
+      conf="common->master"/>
+    <dependency org="org.mortbay.jetty"
+      name="jetty"
+      rev="${jetty.version}"
+      conf="common->default"/>
+    <dependency org="org.mortbay.jetty"
+      name="jsp-api-2.1"
+      rev="${jsp-api-2.1.version}"
+      conf="common->master"/>
+    <dependency org="commons-httpclient"
+      name="commons-httpclient"
+      rev="${commons-httpclient.version}"
+      conf="common->master"/> 
+    <dependency org="commons-configuration"
+      name="commons-configuration"
+      rev="${commons-configuration.version}"
+      conf="common->master"/>
+    <dependency org="org.apache.commons"
+      name="commons-math"
+      rev="${commons-math.version}"
+      conf="common->master"/>
+    <dependency org="commons-lang"
+      name="commons-lang"
+      rev="${commons-lang.version}"
+      conf="common->master"/>
+  </dependencies>
+</ivy-module>
Index: src/contrib/approximationscheduler/lib/libthrift-0.8.0.jar
===================================================================
Cannot display: file marked as a binary type.
svn:mime-type = application/octet-stream
Index: src/contrib/approximationscheduler/lib/libthrift-0.8.0.jar
===================================================================
--- src/contrib/approximationscheduler/lib/libthrift-0.8.0.jar	(revision 0)
+++ src/contrib/approximationscheduler/lib/libthrift-0.8.0.jar	(working copy)

Property changes on: src/contrib/approximationscheduler/lib/libthrift-0.8.0.jar
___________________________________________________________________
Added: svn:mime-type
## -0,0 +1 ##
+application/octet-stream
\ No newline at end of property
Index: src/contrib/approximationscheduler/lib/slf4j-api-1.7.5.jar
===================================================================
Cannot display: file marked as a binary type.
svn:mime-type = application/octet-stream
Index: src/contrib/approximationscheduler/lib/slf4j-api-1.7.5.jar
===================================================================
--- src/contrib/approximationscheduler/lib/slf4j-api-1.7.5.jar	(revision 0)
+++ src/contrib/approximationscheduler/lib/slf4j-api-1.7.5.jar	(working copy)

Property changes on: src/contrib/approximationscheduler/lib/slf4j-api-1.7.5.jar
___________________________________________________________________
Added: svn:mime-type
## -0,0 +1 ##
+application/octet-stream
\ No newline at end of property
Index: src/contrib/approximationscheduler/lib/slf4j-log4j12-1.7.5.jar
===================================================================
Cannot display: file marked as a binary type.
svn:mime-type = application/octet-stream
Index: src/contrib/approximationscheduler/lib/slf4j-log4j12-1.7.5.jar
===================================================================
--- src/contrib/approximationscheduler/lib/slf4j-log4j12-1.7.5.jar	(revision 0)
+++ src/contrib/approximationscheduler/lib/slf4j-log4j12-1.7.5.jar	(working copy)

Property changes on: src/contrib/approximationscheduler/lib/slf4j-log4j12-1.7.5.jar
___________________________________________________________________
Added: svn:mime-type
## -0,0 +1 ##
+application/octet-stream
\ No newline at end of property
Index: src/contrib/approximationscheduler/src/java/org/apache/hadoop/mapred/ApproximationTaskScheduler.java
===================================================================
--- src/contrib/approximationscheduler/src/java/org/apache/hadoop/mapred/ApproximationTaskScheduler.java	(revision 0)
+++ src/contrib/approximationscheduler/src/java/org/apache/hadoop/mapred/ApproximationTaskScheduler.java	(working copy)
@@ -0,0 +1,120 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.mapred;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+
+import java.io.IOException;
+import java.util.Collection;
+import java.util.List;
+import java.util.LinkedList;
+
+import org.apache.thrift.TException;
+import org.apache.thrift.protocol.TProtocol;
+import org.apache.thrift.protocol.TBinaryProtocol;
+import org.apache.thrift.transport.TSocket;
+import org.apache.thrift.transport.TTransport;
+import org.apache.thrift.transport.TTransportException;
+
+import org.apache.hadoop.mapred.approximation.ApproximationScheduler;
+import org.apache.hadoop.mapred.approximation.Job;
+
+import org.apache.hadoop.mapreduce.server.jobtracker.TaskTracker;
+
+/**
+ * Scheduler that connects to an external scheduler to decide the approximation level.
+ */
+public class ApproximationTaskScheduler extends JobQueueTaskScheduler {
+	public static final Log LOG = LogFactory.getLog(ApproximationTaskScheduler.class);
+	
+	private ApproximationScheduler.Client client = null;
+	
+	// Sleep/wake nodes according to utilization
+	private NodeManager nodeManager;
+	
+	public ApproximationTaskScheduler() {
+		super();
+		
+		this.nodeManager = new NodeManager(this);
+	}
+	
+	@Override
+	public synchronized void start() throws IOException {
+		super.start();
+		this.nodeManager.start();
+	}
+	
+	/**
+	 * Get a client to connect to the scheduler.
+	 */
+	public ApproximationScheduler.Client getClient() throws Exception {
+		if (this.client == null) {
+			TTransport transport = new TSocket("localhost", 9090);
+			transport.open();
+			
+			TProtocol protocol = new  TBinaryProtocol(transport);
+			
+			this.client = new ApproximationScheduler.Client(protocol);
+		}
+		return this.client;
+	}
+	/**
+	 * Decides where to assign a task based on the external scheduler.
+	 */
+	@Override
+	public synchronized List<Task> assignTasks(TaskTracker taskTracker) throws IOException {
+		TaskTrackerStatus taskTrackerStatus = taskTracker.getStatus();
+		
+		// Connect to the approximation server
+		try {
+			// Get the information of the jobs
+			List<Job> inJobs = this.getJobList();
+			if (inJobs.size() > 0) {
+				// Ask the external scheduler
+				List<Job> outJobs = getClient().schedule(inJobs);
+				// Result
+				for (Job job : outJobs) {
+					//LOG.info(new JobExtended(job));
+				}
+			}
+		} catch (Exception e) {
+			LOG.error("Cannot connect to server");
+			this.client = null;
+			// By default, we use the FIFO scheduler
+			return super.assignTasks(taskTracker);
+		}
+		
+		// By default, we use the FIFO scheduler
+		return super.assignTasks(taskTracker);
+	}
+	
+	/**
+	 * Get the list of jobs from the job queue.
+	 */
+	private List<Job> getJobList() {
+		Collection<JobInProgress> jobQueue = jobQueueJobInProgressListener.getJobQueue();
+	
+		List<Job> inJobs = new LinkedList<Job>();
+		for (JobInProgress job : jobQueue) {
+			inJobs.add(new JobExtended(job));
+		}
+		return inJobs;
+	}
+}
+// 
\ No newline at end of file
Index: src/contrib/approximationscheduler/src/java/org/apache/hadoop/mapred/JobExtended.java
===================================================================
--- src/contrib/approximationscheduler/src/java/org/apache/hadoop/mapred/JobExtended.java	(revision 0)
+++ src/contrib/approximationscheduler/src/java/org/apache/hadoop/mapred/JobExtended.java	(working copy)
@@ -0,0 +1,88 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.mapred;
+
+import org.apache.hadoop.mapred.approximation.Job;
+
+/**
+ * Extends the job data structure generated by thrift.
+ */
+public class JobExtended extends Job {
+	public JobExtended(Job job) {
+		super(job);
+	}
+
+	public JobExtended(JobInProgress job) {
+		super();
+		
+		JobConf jobConf = job.getJobConf();
+		
+		super.setJobId(job.getJobID().toString());
+		// Maps
+		super.setNmap(job.desiredMaps());
+		//super.setNmap(jobConf.getNumReduceTasks());
+		// Reduces
+		super.setNred(job.desiredReduces());
+		//super.setNmap(jobConf.getNumMapTasks());
+		
+		// Get job characteristics from default types
+		String jobName = jobConf.get("mapred.job.name");
+		// Sleep job
+		if (jobName.equalsIgnoreCase("SleepJob")) {
+			super.setLmap(jobConf.getInt("sleep.job.map.sleep.count", -1)/1000);
+			super.setLred(jobConf.getInt("sleep.job.reduce.sleep.count", -1)/1000);
+		// DC placement
+		} else if (jobName.startsWith("streamjob") && "dcplacement.sh".equalsIgnoreCase(jobConf.get("stream.map.streamprocessor"))) {
+			super.setLmap(100);
+			super.setLred(10);
+			super.setLmapapprox(60);
+			super.setLredapprox(10);
+		}
+		// Get job characteristics provided by the user
+		if (jobConf.getInt("job.description.map.length", -1) != -1)
+			super.setLmap(jobConf.getInt("job.description.map.length", -1));
+		if (jobConf.getInt("job.description.reduce.length", -1) != -1)
+			super.setLred(jobConf.getInt("job.description.reduce.length", -1));
+		if (jobConf.getInt("job.description.map.approximation.length", -1) != -1)
+			super.setLmapapprox(jobConf.getInt("job.description.map.approximation.length", -1));
+		if (jobConf.getInt("job.description.reduce.approximation.length", -1) != -1)
+			super.setLredapprox(jobConf.getInt("job.description.reduce.approximation.length", -1));
+		
+		// Algorithm approximation
+		float approxAlgoMax = jobConf.getFloat("mapred.map.approximate", 0);
+		if (approxAlgoMax > 0) {
+			super.setApproxAlgo(true);
+			super.setApproxAlgoMax(approxAlgoMax);
+		}
+		
+		// Drop appropximation
+		float approxDropMax = jobConf.getFloat("mapred.map.approximate.drop.percentage", 1.0f);
+		if (approxDropMax < 1) {
+			super.setApproxDrop(true);
+			super.setApproxDropMax(1.0-approxDropMax);
+		}
+	}
+	
+	/**
+	 * String representing the job.
+	 */
+	public String toString() {
+		//return super.toString();
+		return super.getJobId() + " M:"+super.getNmap()+" R:"+super.getNred()+" "+(super.isApproxAlgo() ? "A" : " ") +" "+(super.isApproxDrop() ? "D" : " ") +" "+(super.isApproxInput() ? "I" : " ");
+	}
+}
Index: src/contrib/approximationscheduler/src/java/org/apache/hadoop/mapred/NodeFileManager.java
===================================================================
--- src/contrib/approximationscheduler/src/java/org/apache/hadoop/mapred/NodeFileManager.java	(revision 0)
+++ src/contrib/approximationscheduler/src/java/org/apache/hadoop/mapred/NodeFileManager.java	(working copy)
@@ -0,0 +1,108 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.mapred;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+
+import java.util.Collection;
+import java.util.List;
+import java.util.LinkedList;
+
+import java.io.FileInputStream;
+import java.io.DataInputStream;
+import java.io.InputStreamReader;
+import java.io.BufferedReader;
+import java.io.FileWriter;
+import java.io.BufferedWriter;
+
+/**
+ * Manage a file containing a list of nodes.
+ */
+public class NodeFileManager {
+	public static final Log LOG = LogFactory.getLog(NodeFileManager.class);
+	
+	/**
+	 * Add a host to a file.
+	 */
+	public static boolean addHost(String hostname, String filename) {
+		Collection<String> hosts = readHostFile(filename);
+		if (!hosts.contains(hostname)) {
+			hosts.add(hostname);
+			writeHostFile(filename, hosts);
+			return true;
+		} else {
+			return false;
+		}
+	}
+	
+	/**
+	 * Remove a host from a file.
+	 */
+	public static boolean removeHost(String hostname, String filename) {
+		Collection<String> hosts = readHostFile(filename);
+		if (hosts.contains(hostname)) {
+			hosts.remove(hostname);
+			writeHostFile(filename, hosts);
+			return true;
+		} else {
+			return false;
+		}
+	}
+	
+	/**
+	 * Read host files.
+	 */
+	public static Collection<String> readHostFile(String filename) {
+		List<String> ret = new LinkedList<String>();
+		try{
+			FileInputStream fstream = new FileInputStream(filename);
+			DataInputStream in = new DataInputStream(fstream);
+			BufferedReader br = new BufferedReader(new InputStreamReader(in));
+			String strLine = null;
+			while ((strLine = br.readLine()) != null)   {
+				// Clean line
+				strLine = strLine.replaceAll("\n", "");
+				strLine = strLine.trim();
+				if (!strLine.equals("")) {
+					ret.add(strLine.trim());
+				}
+			}
+			in.close();
+		} catch (Exception e) {
+			LOG.error("Cannot read " + filename + ": " + e.getMessage());
+		}
+		return ret;
+	}
+	
+	/**
+	 * Write a host file.
+	 */
+	public static void writeHostFile(String filename, Collection<String> hosts) {
+		try{
+			FileWriter fstream = new FileWriter(filename);
+			BufferedWriter out = new BufferedWriter(fstream);
+			for (String host : hosts) {
+				out.write(host+"\n");
+			}
+			out.close();
+		} catch (Exception e) {
+			LOG.error("Error writing " + filename + ": " + e.getMessage());
+		}
+	}
+}
Index: src/contrib/approximationscheduler/src/java/org/apache/hadoop/mapred/NodeManager.java
===================================================================
--- src/contrib/approximationscheduler/src/java/org/apache/hadoop/mapred/NodeManager.java	(revision 0)
+++ src/contrib/approximationscheduler/src/java/org/apache/hadoop/mapred/NodeManager.java	(working copy)
@@ -0,0 +1,379 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.mapred;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+
+import org.apache.commons.lang.ArrayUtils;
+
+import java.util.Collection;
+import java.util.Arrays;
+import java.util.Iterator;
+import java.util.List;
+import java.util.LinkedList;
+import java.util.Map;
+import java.util.HashMap;
+import java.util.Set;
+import java.util.HashSet;
+
+import org.apache.hadoop.mapreduce.TaskType;
+
+// import org.apache.hadoop.mapred.TaskScheduler;
+
+
+
+/**
+ * Represents an action over a node
+ */
+class NodeAction {
+	public long t;
+	public String action;
+	public String host;
+	public Process process;
+	
+	public NodeAction(String action, String host, Process process) {
+		this.action =  action;
+		this.host =    host;
+		this.process = process;
+		this.t = System.currentTimeMillis();
+	}
+	
+	public String getId() {
+		return this.action + "-" + this.host;
+	}
+	
+	/**
+	 * Check if we can remove the action
+	 */
+	public boolean isDone() {
+		try {
+			process.exitValue();
+			return true;
+		} catch (Exception e) {
+			//LOG.error("Action still running...");
+			return false;
+		}
+	}
+	
+	/**
+	 * Check if the process is older than 10 seconds.
+	 */
+	public boolean isOld() {
+		if (System.currentTimeMillis() - this.t > 10*1000) {
+			return true;
+		} else {
+			return false;
+		}
+	}
+}
+
+
+/**
+ * Manages the nodes. Sleeps and wakes servers.
+ */
+public class NodeManager extends Thread {
+	public static final Log LOG = LogFactory.getLog(NodeManager.class);
+
+	private static final int SLEEP_PERIOD = 1*1000; // 1 second
+	
+	//private static String sleepNodesFiles;
+	private static String SLEEP_NODES_FILE =    "conf/sleep_nodes";
+	private static String ALL_NODES_FILE =      "conf/slaves";
+	private static String COVERING_NODES_FILE = "conf/slavescovering";
+	
+	private static TaskScheduler scheduler;
+	
+	// Actions -> Process
+	private List<NodeAction> listActions;
+	
+	// If the daemon keeps running
+	private boolean keepRunning = true;
+	
+	// The maximum number of nodes
+	private int maxNumNodes = -1;
+	
+	/**
+	 * Initialize.
+	 */
+	public NodeManager(TaskScheduler scheduler) {
+		this.scheduler = scheduler;
+		
+		this.listActions = new LinkedList<NodeAction>();
+		
+		// Get the file to define which nodes are sleeping
+		//sleepNodesFiles = scheduler.getConf().get("topology.sleep.file.name", "conf/sleep_nodes");
+	}
+	
+	/**
+	 * Set the maximum number of nodes.
+	 */
+	public void setMaxNumNodes(int maxNumNodes) {
+		this.maxNumNodes = maxNumNodes;
+	}
+	
+	/**
+	 * Get maximum number of nodes.
+	 */
+	public int getMaxNumNodes() {
+		return this.maxNumNodes;
+	}
+	
+	/**
+	 * Stop.
+	 */
+	public void terminate() {
+		this.keepRunning = false;
+	}
+	
+	/**
+	 * Periodically checks if the nodes can be turned off and proceeds.
+	 */
+	public void run() {
+		while (this.keepRunning) {
+			try {
+				Thread.sleep(SLEEP_PERIOD);
+				
+				// Remove no needed actions
+				purgeActions();
+				
+				// Check which nodes are needed
+				int pendingMaps = 0;
+				int pendingReduces = 0;
+				for (JobInProgress job : scheduler.getJobs(null)) {
+					pendingMaps +=    job.pendingMaps();
+					pendingReduces += job.pendingReduces();
+				}
+				
+				int numNodes = 0;
+				int availableMaps = 0;
+				int availableReduces = 0;
+				int sleepingMaps = 0;
+				int sleepingReduces = 0;
+				for (TaskTrackerStatus taskTracker : scheduler.taskTrackerManager.taskTrackers()) {
+					String host = taskTracker.getHost();
+					if (taskTracker.isSleep()) {
+						sleepingMaps +=     taskTracker.getAvailableMapSlots();
+						sleepingReduces +=  taskTracker.getAvailableReduceSlots();
+					} else {
+						numNodes++;
+						availableMaps +=    taskTracker.getAvailableMapSlots();
+						availableReduces += taskTracker.getAvailableReduceSlots();
+					}
+				}
+				
+				/*
+				// Jobs -> Nodes
+				LOG.info("Jobs:");
+				for (JobInProgress job : scheduler.getJobs(null)) {
+					LOG.info("  " + job.getJobID() + " -> " + getNodesJob(job));
+				}
+				
+				// Nodes
+				LOG.info("Nodes:");
+				for (TaskTrackerStatus taskTracker : scheduler.taskTrackerManager.taskTrackers()) {
+					String host = taskTracker.getHost();
+					LOG.info("  " + host + " " + (taskTracker.isSleep() ? "Z" : " "));
+				}
+				
+				LOG.info("Required:     " + getRequiredNodes());
+				LOG.info("Not-required: " + getNonRequiredNodes());
+				*/
+				
+				// Do we need more slots?
+				boolean needSlots = (pendingMaps > availableMaps || pendingReduces > availableReduces);
+				if (maxNumNodes>0 && numNodes>maxNumNodes) {
+					needSlots = false; // Node maximum limit
+				}
+				// Check if we need to change the status of a node
+				Collection<String> noReqNodes = getNonRequiredNodes();
+				for (TaskTrackerStatus taskTracker : scheduler.taskTrackerManager.taskTrackers()) {
+					String host = taskTracker.getHost();
+					// Send to sleep a non-required nodes
+					if (!needSlots && noReqNodes.contains(host) && !taskTracker.isSleep()) {
+						// Check if the action is not already going on
+						if (!isActionRunning("sleep", host)) {
+							Process p = sleep(host);
+							listActions.add(new NodeAction("sleep", host, p));
+						}
+						// Update hosts stats
+						numNodes--;
+						availableMaps    -= taskTracker.getAvailableMapSlots();
+						availableReduces -= taskTracker.getAvailableReduceSlots();
+						availableMaps    += taskTracker.getAvailableMapSlots();
+						availableReduces += taskTracker.getAvailableReduceSlots();
+					}
+					// Wake a node
+					if (needSlots && taskTracker.isSleep()) {
+						// Check if the action is not already going on
+						if (!isActionRunning("wake", host)) {
+							Process p = wake(host);
+							listActions.add(new NodeAction("wake", host, p));
+						}
+						// Update hosts stats
+						numNodes++;
+						sleepingMaps     -= taskTracker.getAvailableMapSlots();
+						sleepingReduces  -= taskTracker.getAvailableReduceSlots();
+						availableMaps    += taskTracker.getAvailableMapSlots();
+						availableReduces += taskTracker.getAvailableReduceSlots();
+					}
+					// Recalculate if we need more slots
+					needSlots = (pendingMaps > availableMaps || pendingReduces > availableReduces);
+					if (maxNumNodes>0 && numNodes>maxNumNodes) {
+						needSlots = false; // Node maximum
+					}
+				}
+			} catch (Exception e) {
+				LOG.error("Error checking the nodes: " + e);
+				LOG.error(e.getMessage());
+			}
+		}
+	}
+	
+	/**
+	 * Get the nodes required by a job.
+	 */
+	public static Set<String> getNodesJob(JobInProgress job) {
+		Set<String> ret = new HashSet<String>();
+	
+		// Get all the tasks
+		Set<TaskInProgress> tasks = new HashSet<TaskInProgress>();
+		tasks.addAll(Arrays.asList(job.getTasks(TaskType.JOB_SETUP)));
+		tasks.addAll(Arrays.asList(job.getTasks(TaskType.MAP)));
+		tasks.addAll(Arrays.asList(job.getTasks(TaskType.REDUCE)));
+		tasks.addAll(Arrays.asList(job.getTasks(TaskType.JOB_CLEANUP)));
+		
+		// Add the hosts for each task
+		for (TaskInProgress task : tasks) {
+			for (TaskStatus attempt : task.getTaskStatuses()) {
+				String hostname = JobInProgress.convertTrackerNameToHostName(attempt.getTaskTracker());
+				ret.add(hostname);
+			}
+		}
+		
+		return ret;
+	}
+	
+	/**
+	 * Get the list of all the available nodes.
+	 */
+	private Set<String> getAllNodes() {
+		Set<String> ret = new HashSet<String>();
+		for (TaskTrackerStatus taskTracker : scheduler.taskTrackerManager.taskTrackers()) {
+			ret.add(taskTracker.getHost());
+		}
+		return ret;
+	}
+	
+	/**
+	 * Get the list of nodes that need to be running.
+	 */
+	private Set<String> getRequiredNodes() {
+		Set<String> ret = new HashSet<String>();
+		// Add nodes required by jobs
+		for (JobInProgress job : scheduler.getJobs(null)) {
+			ret.addAll(getNodesJob(job));
+		}
+		// Add covering subset
+		Collection<String> coveringNodes = NodeFileManager.readHostFile(COVERING_NODES_FILE);
+		for (String hostname : getAllNodes()) {
+			if (coveringNodes.contains(hostname)) {
+				ret.add(hostname);
+			}
+		}
+		return ret;
+	}
+	
+	/**
+	 * Get the list of nodes that can be turned off.
+	 */
+	private Set<String> getNonRequiredNodes() {
+		Set<String> ret = getAllNodes();
+		ret.removeAll(getRequiredNodes());
+		return ret;
+	}
+	
+	/**
+	 * Clean finished and old actions.
+	 */
+	private void purgeActions() {
+		// Check list of actions
+		for (Iterator<NodeAction> it = this.listActions.iterator(); it.hasNext(); ) {
+			NodeAction action = it.next();
+			if (action.isDone()) {
+				// The node has waken up
+				if (action.action.equals("wake")) {
+					NodeFileManager.removeHost(action.host, SLEEP_NODES_FILE);
+				}
+				it.remove();
+			} else if(action.isOld()) {
+				// Kill old action
+				action.process.destroy();
+				it.remove();
+			}
+		}
+	}
+	
+	/**
+	 * Check if the action is running.
+	 */
+	public boolean isActionRunning(String action, String host) {
+		for (NodeAction aux : this.listActions) {
+			if (aux.getId().equals(action+"-"+host)) {
+				return true;
+			}
+		}
+		return false;
+	}
+	
+	/**
+	 * Send node to sleep.
+	 */
+	public static Process sleep(String hostname) {
+		LOG.info("Sending " + hostname + " to sleep...");
+		Process child = null;
+		try {
+			child = Runtime.getRuntime().exec("/usr/bin/sudo /bin/parasol s3 " + hostname);
+			//child.waitFor();
+			
+			// Add to the sleeping nodes
+			NodeFileManager.addHost(hostname, SLEEP_NODES_FILE);
+		} catch (Exception e) {
+			LOG.error("Error sleeping " + hostname);
+		}
+		return child;
+	}
+	
+	/**
+	 * Wake a node up.
+	 */
+	public static Process wake(String hostname) {
+		LOG.info("Waking " + hostname + "...");
+		Process child = null;
+		try {
+			child = Runtime.getRuntime().exec("/usr/bin/sudo /bin/parasol wake " + hostname);
+			//child.waitFor();
+			
+			// It should be removed once is up and running
+			//NodeFileManager.removeHost(hostname, SLEEP_NODES_FILE);
+		} catch (Exception e) {
+			LOG.error("Error waking up " + hostname);
+		}
+		return child;
+	}
+}
Index: src/contrib/approximationscheduler/src/java/org/apache/hadoop/mapred/approximation/ApproximationScheduler.java
===================================================================
--- src/contrib/approximationscheduler/src/java/org/apache/hadoop/mapred/approximation/ApproximationScheduler.java	(revision 0)
+++ src/contrib/approximationscheduler/src/java/org/apache/hadoop/mapred/approximation/ApproximationScheduler.java	(working copy)
@@ -0,0 +1 @@
+link ../../../../../../../gen-java/org/apache/hadoop/mapred/approximation/ApproximationScheduler.java
\ No newline at end of file

Property changes on: src/contrib/approximationscheduler/src/java/org/apache/hadoop/mapred/approximation/ApproximationScheduler.java
___________________________________________________________________
Added: svn:special
## -0,0 +1 ##
+*
\ No newline at end of property
Index: src/contrib/approximationscheduler/src/java/org/apache/hadoop/mapred/approximation/Job.java
===================================================================
--- src/contrib/approximationscheduler/src/java/org/apache/hadoop/mapred/approximation/Job.java	(revision 0)
+++ src/contrib/approximationscheduler/src/java/org/apache/hadoop/mapred/approximation/Job.java	(working copy)
@@ -0,0 +1 @@
+link ../../../../../../../gen-java/org/apache/hadoop/mapred/approximation/Job.java
\ No newline at end of file

Property changes on: src/contrib/approximationscheduler/src/java/org/apache/hadoop/mapred/approximation/Job.java
___________________________________________________________________
Added: svn:special
## -0,0 +1 ##
+*
\ No newline at end of property
Index: src/contrib/approximationscheduler/src/test/org/apache/hadoop/mapred/FakeSchedulable.java
===================================================================
--- src/contrib/approximationscheduler/src/test/org/apache/hadoop/mapred/FakeSchedulable.java	(revision 0)
+++ src/contrib/approximationscheduler/src/test/org/apache/hadoop/mapred/FakeSchedulable.java	(working copy)
@@ -0,0 +1,124 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.mapred;
+
+import java.io.IOException;
+import java.util.Collection;
+
+import org.apache.hadoop.mapreduce.TaskType;
+
+/**
+ * Dummy implementation of Schedulable for unit testing.
+ */
+public class FakeSchedulable extends Schedulable {
+  private int demand;
+  private int runningTasks;
+  private int minShare;
+  private double weight;
+  private JobPriority priority;
+  private long startTime;
+  
+  public FakeSchedulable() {
+    this(0, 0, 1, 0, 0, JobPriority.NORMAL, 0);
+  }
+  
+  public FakeSchedulable(int demand) {
+    this(demand, 0, 1, 0, 0, JobPriority.NORMAL, 0);
+  }
+  
+  public FakeSchedulable(int demand, int minShare) {
+    this(demand, minShare, 1, 0, 0, JobPriority.NORMAL, 0);
+  }
+  
+  public FakeSchedulable(int demand, int minShare, double weight) {
+    this(demand, minShare, weight, 0, 0, JobPriority.NORMAL, 0);
+  }
+  
+  public FakeSchedulable(int demand, int minShare, double weight, int fairShare,
+      int runningTasks, JobPriority priority, long startTime) {
+    this.demand = demand;
+    this.minShare = minShare;
+    this.weight = weight;
+    setFairShare(fairShare);
+    this.runningTasks = runningTasks;
+    this.priority = priority;
+    this.startTime = startTime;
+  }
+  
+  @Override
+  public Task assignTask(TaskTrackerStatus tts, long currentTime,
+      Collection<JobInProgress> visited) throws IOException {
+    return null;
+  }
+
+  @Override
+  public int getDemand() {
+    return demand;
+  }
+
+  @Override
+  public String getName() {
+    return "FakeSchedulable" + this.hashCode();
+  }
+
+  @Override
+  public JobPriority getPriority() {
+    return priority;
+  }
+
+  @Override
+  public int getRunningTasks() {
+    return runningTasks;
+  }
+
+  @Override
+  public long getStartTime() {
+    return startTime;
+  }
+  
+  @Override
+  public double getWeight() {
+    return weight;
+  }
+  
+  @Override
+  public int getMinShare() {
+    return minShare;
+  }
+
+  @Override
+  public void redistributeShare() {}
+
+  @Override
+  public void updateDemand() {}
+
+  @Override
+  public TaskType getTaskType() {
+    return TaskType.MAP;
+  }
+
+  @Override
+  protected String getMetricsContextName() {
+    return "fake";
+  }
+
+  @Override
+  void updateMetrics() {
+  }
+}
Index: src/contrib/approximationscheduler/src/test/org/apache/hadoop/mapred/TestCapBasedLoadManager.java
===================================================================
--- src/contrib/approximationscheduler/src/test/org/apache/hadoop/mapred/TestCapBasedLoadManager.java	(revision 0)
+++ src/contrib/approximationscheduler/src/test/org/apache/hadoop/mapred/TestCapBasedLoadManager.java	(working copy)
@@ -0,0 +1,160 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.mapred;
+
+import java.util.ArrayList;
+import java.util.List;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.mapred.TaskStatus.State;
+
+import junit.framework.TestCase;
+
+/**
+ * Exercise the canAssignMap and canAssignReduce methods in 
+ * CapBasedLoadManager.
+ */
+public class TestCapBasedLoadManager extends TestCase {
+  
+  /**
+   * Returns a running MapTaskStatus.
+   */
+  private TaskStatus getRunningMapTaskStatus() {
+    TaskStatus ts = new MapTaskStatus();
+    ts.setRunState(State.RUNNING);
+    return ts;
+  }
+
+  /**
+   * Returns a running ReduceTaskStatus.
+   */
+  private TaskStatus getRunningReduceTaskStatus() {
+    TaskStatus ts = new ReduceTaskStatus();
+    ts.setRunState(State.RUNNING);
+    return ts;
+  }
+  
+  /**
+   * Returns a TaskTrackerStatus with the specified statistics. 
+   * @param mapCap        The capacity of map tasks 
+   * @param reduceCap     The capacity of reduce tasks
+   * @param runningMap    The number of running map tasks
+   * @param runningReduce The number of running reduce tasks
+   */
+  private TaskTrackerStatus getTaskTrackerStatus(int mapCap, int reduceCap, 
+      int runningMap, int runningReduce) {
+    List<TaskStatus> ts = new ArrayList<TaskStatus>();
+    for (int i = 0; i < runningMap; i++) {
+      ts.add(getRunningMapTaskStatus());
+    }
+    for (int i = 0; i < runningReduce; i++) {
+      ts.add(getRunningReduceTaskStatus());
+    }
+    TaskTrackerStatus tracker = new TaskTrackerStatus("tracker", 
+        "tracker_host", 1234, ts, 0, 0, mapCap, reduceCap);
+    return tracker;
+  }
+
+  /**
+   * A single test of canAssignMap.
+   */
+  private void oneTestCanAssignMap(float maxDiff, int mapCap, int runningMap,
+      int totalMapSlots, int totalRunnableMap, int expectedAssigned) {
+    
+    CapBasedLoadManager manager = new CapBasedLoadManager();
+    Configuration conf = new Configuration();
+    conf.setFloat("mapred.fairscheduler.load.max.diff", maxDiff);
+    manager.setConf(conf);
+    
+    TaskTrackerStatus ts = getTaskTrackerStatus(mapCap, 1, runningMap, 1);
+    
+    int numAssigned = 0;
+    while (manager.canAssignMap(ts, totalRunnableMap, totalMapSlots, numAssigned)) {
+      numAssigned++;
+    }
+      
+    assertEquals( "When maxDiff=" + maxDiff + ", with totalRunnableMap=" 
+        + totalRunnableMap + " and totalMapSlots=" + totalMapSlots
+        + ", a tracker with runningMap=" + runningMap + " and mapCap="
+        + mapCap + " should be able to assign " + expectedAssigned + " maps",
+        expectedAssigned, numAssigned);
+  }
+  
+  
+  /** 
+   * Test canAssignMap method.
+   */
+  public void testCanAssignMap() {
+    oneTestCanAssignMap(0.0f, 5, 0, 50, 1, 1);
+    oneTestCanAssignMap(0.0f, 5, 1, 50, 10, 0);
+    // 20% load + 20% diff = 40% of available slots, but rounds
+    // up with floating point error: so we get 3/5 slots on TT.
+    // 1 already taken, so assigns 2 more
+    oneTestCanAssignMap(0.2f, 5, 1, 50, 10, 2);
+    oneTestCanAssignMap(0.0f, 5, 1, 50, 11, 1);
+    oneTestCanAssignMap(0.0f, 5, 2, 50, 11, 0);
+    oneTestCanAssignMap(0.3f, 5, 2, 50, 6, 1);
+    oneTestCanAssignMap(1.0f, 5, 5, 50, 50, 0);
+  }
+  
+  
+  /**
+   * A single test of canAssignReduce.
+   */
+  private void oneTestCanAssignReduce(float maxDiff, int reduceCap,
+      int runningReduce, int totalReduceSlots, int totalRunnableReduce,
+      int expectedAssigned) {
+    
+    CapBasedLoadManager manager = new CapBasedLoadManager();
+    Configuration conf = new Configuration();
+    conf.setFloat("mapred.fairscheduler.load.max.diff", maxDiff);
+    manager.setConf(conf);
+    
+    TaskTrackerStatus ts = getTaskTrackerStatus(1, reduceCap, 1,
+        runningReduce);
+    
+    int numAssigned = 0;
+    while (manager.canAssignReduce(ts, totalRunnableReduce, totalReduceSlots, numAssigned)) {
+      numAssigned++;
+    }
+      
+    assertEquals( "When maxDiff=" + maxDiff + ", with totalRunnableReduce=" 
+        + totalRunnableReduce + " and totalReduceSlots=" + totalReduceSlots
+        + ", a tracker with runningReduce=" + runningReduce + " and reduceCap="
+        + reduceCap + " should be able to assign " + expectedAssigned + " reduces",
+        expectedAssigned, numAssigned);
+  }
+    
+  /** 
+   * Test canAssignReduce method.
+   */
+  public void testCanAssignReduce() {
+    oneTestCanAssignReduce(0.0f, 5, 0, 50, 1, 1);
+    oneTestCanAssignReduce(0.0f, 5, 1, 50, 10, 0);
+    // 20% load + 20% diff = 40% of available slots, but rounds
+    // up with floating point error: so we get 3/5 slots on TT.
+    // 1 already taken, so assigns 2 more
+    oneTestCanAssignReduce(0.2f, 5, 1, 50, 10, 2);
+    oneTestCanAssignReduce(0.0f, 5, 1, 50, 11, 1);
+    oneTestCanAssignReduce(0.0f, 5, 2, 50, 11, 0);
+    oneTestCanAssignReduce(0.3f, 5, 2, 50, 6, 1);
+    oneTestCanAssignReduce(1.0f, 5, 5, 50, 50, 0);
+  }
+  
+}
Index: src/contrib/approximationscheduler/src/test/org/apache/hadoop/mapred/TestComputeFairShares.java
===================================================================
--- src/contrib/approximationscheduler/src/test/org/apache/hadoop/mapred/TestComputeFairShares.java	(revision 0)
+++ src/contrib/approximationscheduler/src/test/org/apache/hadoop/mapred/TestComputeFairShares.java	(working copy)
@@ -0,0 +1,184 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.mapred;
+
+import java.util.ArrayList;
+import java.util.List;
+
+import junit.framework.TestCase;
+
+/**
+ * Exercise the computeFairShares method in SchedulingAlgorithms.
+ */
+public class TestComputeFairShares extends TestCase {
+  private List<Schedulable> scheds;
+  
+  @Override
+  protected void setUp() throws Exception {
+    scheds = new ArrayList<Schedulable>();
+  }
+  
+  /** 
+   * Basic test - pools with different demands that are all higher than their
+   * fair share (of 10 slots) should each get their fair share.
+   */
+  public void testEqualSharing() {
+    scheds.add(new FakeSchedulable(100));
+    scheds.add(new FakeSchedulable(50));
+    scheds.add(new FakeSchedulable(30));
+    scheds.add(new FakeSchedulable(20));
+    SchedulingAlgorithms.computeFairShares(scheds, 40);
+    verifyShares(10, 10, 10, 10);
+  }
+  
+  /**
+   * In this test, pool 4 has a smaller demand than the 40 / 4 = 10 slots that
+   * it would be assigned with equal sharing. It should only get the 3 slots
+   * it demands. The other pools must then split the remaining 37 slots, but
+   * pool 3, with 11 slots demanded, is now below its share of 37/3 ~= 12.3,
+   * so it only gets 11 slots. Pools 1 and 2 split the rest and get 13 each. 
+   */
+  public void testLowDemands() {
+    scheds.add(new FakeSchedulable(100));
+    scheds.add(new FakeSchedulable(50));
+    scheds.add(new FakeSchedulable(11));
+    scheds.add(new FakeSchedulable(3));
+    SchedulingAlgorithms.computeFairShares(scheds, 40);
+    verifyShares(13, 13, 11, 3);
+  }
+  
+  /**
+   * In this test, some pools have minimum shares set. Pool 1 has a min share
+   * of 20 so it gets 20 slots. Pool 2 also has a min share of 20, but its
+   * demand is only 10 so it can only get 10 slots. The remaining pools have
+   * 10 slots to split between them. Pool 4 gets 3 slots because its demand is
+   * only 3, and pool 3 gets the remaining 7 slots. Pool 4 also had a min share
+   * of 2 slots but this should not affect the outcome.
+   */
+  public void testMinShares() {
+    scheds.add(new FakeSchedulable(100, 20));
+    scheds.add(new FakeSchedulable(10, 20));
+    scheds.add(new FakeSchedulable(10, 0));
+    scheds.add(new FakeSchedulable(3, 2));
+    SchedulingAlgorithms.computeFairShares(scheds, 40);
+    verifyShares(20, 10, 7, 3);
+  }
+  
+  /**
+   * Basic test for weighted shares with no minimum shares and no low demands.
+   * Each pool should get slots in proportion to its weight.
+   */
+  public void testWeightedSharing() {
+    scheds.add(new FakeSchedulable(100, 0, 2.0));
+    scheds.add(new FakeSchedulable(50,  0, 1.0));
+    scheds.add(new FakeSchedulable(30,  0, 1.0));
+    scheds.add(new FakeSchedulable(20,  0, 0.5));
+    SchedulingAlgorithms.computeFairShares(scheds, 45);
+    verifyShares(20, 10, 10, 5);
+  }
+
+  /**
+   * Weighted sharing test where pools 1 and 2 are now given lower demands than
+   * above. Pool 1 stops at 10 slots, leaving 35. If the remaining pools split
+   * this into a 1:1:0.5 ratio, they would get 14:14:7 slots respectively, but
+   * pool 2's demand is only 11, so it only gets 11. The remaining 2 pools split
+   * the 24 slots left into a 1:0.5 ratio, getting 16 and 8 slots respectively.
+   */
+  public void testWeightedSharingWithLowDemands() {
+    scheds.add(new FakeSchedulable(10, 0, 2.0));
+    scheds.add(new FakeSchedulable(11, 0, 1.0));
+    scheds.add(new FakeSchedulable(30, 0, 1.0));
+    scheds.add(new FakeSchedulable(20, 0, 0.5));
+    SchedulingAlgorithms.computeFairShares(scheds, 45);
+    verifyShares(10, 11, 16, 8);
+  }
+
+  /**
+   * Weighted fair sharing test with min shares. As in the min share test above,
+   * pool 1 has a min share greater than its demand so it only gets its demand.
+   * Pool 3 has a min share of 15 even though its weight is very small, so it
+   * gets 15 slots. The remaining pools share the remaining 20 slots equally,
+   * getting 10 each. Pool 3's min share of 5 slots doesn't affect this.
+   */
+  public void testWeightedSharingWithMinShares() {
+    scheds.add(new FakeSchedulable(10, 20, 2.0));
+    scheds.add(new FakeSchedulable(11, 0, 1.0));
+    scheds.add(new FakeSchedulable(30, 5, 1.0));
+    scheds.add(new FakeSchedulable(20, 15, 0.5));
+    SchedulingAlgorithms.computeFairShares(scheds, 45);
+    verifyShares(10, 10, 10, 15);
+  }
+
+  /**
+   * Test that shares are computed accurately even when there are many more
+   * frameworks than available slots.
+   */
+  public void testSmallShares() {
+    scheds.add(new FakeSchedulable(10));
+    scheds.add(new FakeSchedulable(5));
+    scheds.add(new FakeSchedulable(3));
+    scheds.add(new FakeSchedulable(2));
+    SchedulingAlgorithms.computeFairShares(scheds, 1);
+    verifyShares(0.25, 0.25, 0.25, 0.25);
+  }
+
+  /**
+   * Test that shares are computed accurately even when the number of slots is
+   * very large.
+   */  
+  public void testLargeShares() {
+    int million = 1000 * 1000;
+    scheds.add(new FakeSchedulable(100 * million));
+    scheds.add(new FakeSchedulable(50 * million));
+    scheds.add(new FakeSchedulable(30 * million));
+    scheds.add(new FakeSchedulable(20 * million));
+    SchedulingAlgorithms.computeFairShares(scheds, 40 * million);
+    verifyShares(10 * million, 10 * million, 10 * million, 10 * million);
+  }
+
+  /**
+   * Test that having a pool with 0 demand doesn't confuse the algorithm.
+   */
+  public void testZeroDemand() {
+    scheds.add(new FakeSchedulable(100));
+    scheds.add(new FakeSchedulable(50));
+    scheds.add(new FakeSchedulable(30));
+    scheds.add(new FakeSchedulable(0));
+    SchedulingAlgorithms.computeFairShares(scheds, 30);
+    verifyShares(10, 10, 10, 0);
+  }
+  
+  /**
+   * Test that being called on an empty list doesn't confuse the algorithm.
+   */
+  public void testEmptyList() {
+    SchedulingAlgorithms.computeFairShares(scheds, 40);
+    verifyShares();
+  }
+  
+  /**
+   * Check that a given list of shares have been assigned to this.scheds.
+   */
+  private void verifyShares(double... shares) {
+    assertEquals(scheds.size(), shares.length);
+    for (int i = 0; i < shares.length; i++) {
+      assertEquals(shares[i], scheds.get(i).getFairShare(), 0.01);
+    }
+  }
+}
Index: src/contrib/approximationscheduler/src/test/org/apache/hadoop/mapred/TestFairScheduler.java
===================================================================
--- src/contrib/approximationscheduler/src/test/org/apache/hadoop/mapred/TestFairScheduler.java	(revision 0)
+++ src/contrib/approximationscheduler/src/test/org/apache/hadoop/mapred/TestFairScheduler.java	(working copy)
@@ -0,0 +1,3062 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.mapred;
+
+import java.io.File;
+import java.io.FileWriter;
+import java.io.IOException;
+import java.io.PrintWriter;
+import java.util.ArrayList;
+import java.util.Collection;
+import java.util.HashMap;
+import java.util.IdentityHashMap;
+import java.util.LinkedHashSet;
+import java.util.LinkedList;
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
+import java.util.TreeMap;
+
+import junit.framework.TestCase;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.io.BytesWritable;
+import org.apache.hadoop.mapred.FairScheduler.JobInfo;
+import org.apache.hadoop.mapred.MRConstants;
+import org.apache.hadoop.mapreduce.Job;
+import org.apache.hadoop.mapred.JobInProgress.KillInterruptedException;
+import org.apache.hadoop.mapred.UtilsForTests.FakeClock;
+import org.apache.hadoop.mapreduce.TaskType;
+import org.apache.hadoop.mapreduce.server.jobtracker.TaskTracker;
+import org.apache.hadoop.mapreduce.split.JobSplit;
+import org.apache.hadoop.metrics.ContextFactory;
+import org.apache.hadoop.metrics.MetricsContext;
+import org.apache.hadoop.metrics.MetricsUtil;
+import org.apache.hadoop.metrics.spi.NoEmitMetricsContext;
+import org.apache.hadoop.metrics.spi.OutputRecord;
+import org.apache.hadoop.net.Node;
+import org.mortbay.log.Log;
+
+public class TestFairScheduler extends TestCase {
+  final static String TEST_DIR = new File(System.getProperty("test.build.data",
+      "build/contrib/streaming/test/data")).getAbsolutePath();
+  final static String ALLOC_FILE = new File(TEST_DIR, 
+      "test-pools").getAbsolutePath();
+  
+  private static final String POOL_PROPERTY = "pool";
+  private static final String EXPLICIT_POOL_PROPERTY = "mapred.fairscheduler.pool";
+  
+  private static int jobCounter;
+  
+  class FakeJobInProgress extends JobInProgress {
+    
+    private FakeTaskTrackerManager taskTrackerManager;
+    private int mapCounter = 0;
+    private int reduceCounter = 0;
+    private final String[][] mapInputLocations; // Array of hosts for each map
+    private boolean initialized;
+    
+    public FakeJobInProgress(JobConf jobConf,
+        FakeTaskTrackerManager taskTrackerManager, 
+        String[][] mapInputLocations, JobTracker jt) throws IOException {
+      super(new JobID("test", ++jobCounter), jobConf, jt);
+      this.taskTrackerManager = taskTrackerManager;
+      this.mapInputLocations = mapInputLocations;
+      this.startTime = System.currentTimeMillis();
+      this.status = new JobStatus();
+      this.status.setRunState(JobStatus.PREP);
+      this.nonLocalRunningMaps = new LinkedHashSet<TaskInProgress>();
+      this.runningMapCache = new IdentityHashMap<Node, Set<TaskInProgress>>();
+      this.nonRunningReduces = new LinkedHashSet<TaskInProgress>();   
+      this.runningReduces = new LinkedHashSet<TaskInProgress>();
+      this.initialized = false;
+    }
+    
+    @Override
+    public synchronized void initTasks() throws IOException {
+      // initTasks is needed to create non-empty cleanup and setup TIP
+      // arrays, otherwise calls such as job.getTaskInProgress will fail
+      JobID jobId = getJobID();
+      JobConf conf = getJobConf();
+      String jobFile = "";
+      // create two cleanup tips, one map and one reduce.
+      cleanup = new TaskInProgress[2];
+      // cleanup map tip.
+      cleanup[0] = new TaskInProgress(jobId, jobFile, null, 
+              jobtracker, conf, this, numMapTasks, 1);
+      cleanup[0].setJobCleanupTask();
+      // cleanup reduce tip.
+      cleanup[1] = new TaskInProgress(jobId, jobFile, numMapTasks,
+                         numReduceTasks, jobtracker, conf, this, 1);
+      cleanup[1].setJobCleanupTask();
+      // create two setup tips, one map and one reduce.
+      setup = new TaskInProgress[2];
+      // setup map tip.
+      setup[0] = new TaskInProgress(jobId, jobFile, null, 
+              jobtracker, conf, this, numMapTasks + 1, 1);
+      setup[0].setJobSetupTask();
+      // setup reduce tip.
+      setup[1] = new TaskInProgress(jobId, jobFile, numMapTasks,
+                         numReduceTasks + 1, jobtracker, conf, this, 1);
+      setup[1].setJobSetupTask();
+      // create maps
+      numMapTasks = conf.getNumMapTasks();
+      maps = new TaskInProgress[numMapTasks];
+      // empty format
+      JobSplit.TaskSplitMetaInfo split = JobSplit.EMPTY_TASK_SPLIT;
+      for (int i = 0; i < numMapTasks; i++) {
+        String[] inputLocations = null;
+        if (mapInputLocations != null)
+          inputLocations = mapInputLocations[i];
+        maps[i] = new FakeTaskInProgress(getJobID(), i,
+            getJobConf(), this, inputLocations, split, jobtracker);
+        if (mapInputLocations == null) // Job has no locality info
+          nonLocalMaps.add(maps[i]);
+      }
+      // create reduces
+      numReduceTasks = conf.getNumReduceTasks();
+      reduces = new TaskInProgress[numReduceTasks];
+      for (int i = 0; i < numReduceTasks; i++) {
+        reduces[i] = new FakeTaskInProgress(getJobID(), i,
+            getJobConf(), this, jobtracker);
+      }
+      
+      initialized = true;
+    }
+    
+    @Override
+    public boolean inited() {
+      return initialized;
+    }
+    
+    @Override
+    public Task obtainNewMapTask(final TaskTrackerStatus tts, int clusterSize,
+        int numUniqueHosts) throws IOException {
+      return obtainNewMapTask(tts, clusterSize, numUniqueHosts, Integer.MAX_VALUE);
+    }
+    
+    @Override
+    public Task obtainNewNodeLocalMapTask(final TaskTrackerStatus tts, int clusterSize,
+        int numUniqueHosts) throws IOException {
+      return obtainNewMapTask(tts, clusterSize, numUniqueHosts, 1);
+    }
+    
+    @Override
+    public Task obtainNewNodeOrRackLocalMapTask(final TaskTrackerStatus tts, int clusterSize,
+        int numUniqueHosts) throws IOException {
+      return obtainNewMapTask(tts, clusterSize, numUniqueHosts, 2);
+    }
+
+    public Task obtainNewMapTask(final TaskTrackerStatus tts, int clusterSize,
+        int numUniqueHosts, int localityLevel) throws IOException {
+      for (int map = 0; map < maps.length; map++) {
+        FakeTaskInProgress tip = (FakeTaskInProgress) maps[map];
+        if (!tip.isRunning() && !tip.isComplete() &&
+            getLocalityLevel(tip, tts) < localityLevel) {
+          TaskAttemptID attemptId = getTaskAttemptID(tip);
+          JobSplit.TaskSplitMetaInfo split = JobSplit.EMPTY_TASK_SPLIT;
+          Task task = new MapTask("", attemptId, 0, split.getSplitIndex(), 1) {
+            @Override
+            public String toString() {
+              return String.format("%s on %s", getTaskID(), tts.getTrackerName());
+            }
+          };
+          runningMapTasks++;
+          tip.createTaskAttempt(task, tts.getTrackerName());
+          nonLocalRunningMaps.add(tip);
+          taskTrackerManager.startTask(tts.getTrackerName(), task, tip);
+          return task;
+        }
+      }
+      return null;
+    }
+    
+    @Override
+    public Task obtainNewReduceTask(final TaskTrackerStatus tts,
+        int clusterSize, int ignored) throws IOException {
+      for (int reduce = 0; reduce < reduces.length; reduce++) {
+        FakeTaskInProgress tip = 
+          (FakeTaskInProgress) reduces[reduce];
+        if (!tip.isRunning() && !tip.isComplete()) {
+          TaskAttemptID attemptId = getTaskAttemptID(tip);
+          Task task = new ReduceTask("", attemptId, 0, maps.length, 1) {
+            @Override
+            public String toString() {
+              return String.format("%s on %s", getTaskID(), tts.getTrackerName());
+            }
+          };
+          runningReduceTasks++;
+          tip.createTaskAttempt(task, tts.getTrackerName());
+          runningReduces.add(tip);
+          taskTrackerManager.startTask(tts.getTrackerName(), task, tip);
+          return task;
+        }
+      }
+      return null;
+    }
+    
+    public void mapTaskFinished(TaskInProgress tip) {
+      runningMapTasks--;
+      finishedMapTasks++;
+      nonLocalRunningMaps.remove(tip);
+    }
+    
+    public void reduceTaskFinished(TaskInProgress tip) {
+      runningReduceTasks--;
+      finishedReduceTasks++;
+      runningReduces.remove(tip);
+    }
+    
+    private TaskAttemptID getTaskAttemptID(TaskInProgress tip) {
+      JobID jobId = getJobID();
+      return new TaskAttemptID(jobId.getJtIdentifier(),
+          jobId.getId(), tip.isMapTask(), tip.getIdWithinJob(), tip.nextTaskId++);
+    }
+    
+    @Override
+    int getLocalityLevel(TaskInProgress tip, TaskTrackerStatus tts) {
+      FakeTaskInProgress ftip = (FakeTaskInProgress) tip;
+      if (ftip.inputLocations != null) {
+        // Check whether we're on the same host as an input split
+        for (String location: ftip.inputLocations) {
+          if (location.equals(tts.host)) {
+            return 0;
+          }
+        }
+        // Check whether we're on the same rack as an input split
+        for (String location: ftip.inputLocations) {
+          if (getRack(location).equals(getRack(tts.host))) {
+            return 1;
+          }
+        }
+        // Not on same rack or host
+        return 2;
+      } else {
+        // Job has no locality info  
+        return -1;
+      }
+    }
+  }
+  
+  class FakeTaskInProgress extends TaskInProgress {
+    private boolean isMap;
+    private FakeJobInProgress fakeJob;
+    private TreeMap<TaskAttemptID, String> activeTasks;
+    private TaskStatus taskStatus;
+    private boolean isComplete = false;
+    private String[] inputLocations;
+    
+    // Constructor for map
+    FakeTaskInProgress(JobID jId, int id, JobConf jobConf,
+        FakeJobInProgress job, String[] inputLocations, 
+        JobSplit.TaskSplitMetaInfo split, JobTracker jt) {
+      super(jId, "", split, jt, jobConf, job, id, 1);
+      this.isMap = true;
+      this.fakeJob = job;
+      this.inputLocations = inputLocations;
+      activeTasks = new TreeMap<TaskAttemptID, String>();
+      taskStatus = TaskStatus.createTaskStatus(isMap);
+      taskStatus.setRunState(TaskStatus.State.UNASSIGNED);
+    }
+
+    // Constructor for reduce
+    FakeTaskInProgress(JobID jId, int id, JobConf jobConf,
+        FakeJobInProgress job, JobTracker jt) {
+      super(jId, "", jobConf.getNumMapTasks(), id, jt, jobConf, job, 1);
+      this.isMap = false;
+      this.fakeJob = job;
+      activeTasks = new TreeMap<TaskAttemptID, String>();
+      taskStatus = TaskStatus.createTaskStatus(isMap);
+      taskStatus.setRunState(TaskStatus.State.UNASSIGNED);
+    }
+    
+    private void createTaskAttempt(Task task, String taskTracker) {
+      activeTasks.put(task.getTaskID(), taskTracker);
+      taskStatus = TaskStatus.createTaskStatus(isMap, task.getTaskID(),
+          0.5f, 1, TaskStatus.State.RUNNING, "", "", "", 
+          TaskStatus.Phase.STARTING, new Counters());
+      taskStatus.setStartTime(clock.getTime());
+    }
+    
+    @Override
+    TreeMap<TaskAttemptID, String> getActiveTasks() {
+      return activeTasks;
+    }
+    
+    public synchronized boolean isComplete() {
+      return isComplete;
+    }
+    
+    public boolean isRunning() {
+      return activeTasks.size() > 0;
+    }
+    
+    @Override
+    public TaskStatus getTaskStatus(TaskAttemptID taskid) {
+      return taskStatus;
+    }
+    
+    void killAttempt() {
+      if (isMap) {
+        fakeJob.mapTaskFinished(this);
+      }
+      else {
+        fakeJob.reduceTaskFinished(this);
+      }
+      activeTasks.clear();
+      taskStatus.setRunState(TaskStatus.State.UNASSIGNED);
+    }
+    
+    void finishAttempt() {
+      isComplete = true;
+      if (isMap) {
+        fakeJob.mapTaskFinished(this);
+      }
+      else {
+        fakeJob.reduceTaskFinished(this);
+      }
+      activeTasks.clear();
+      taskStatus.setRunState(TaskStatus.State.UNASSIGNED);
+    }
+  }
+  
+  static class FakeQueueManager extends QueueManager {
+    private Set<String> queues = null;
+    FakeQueueManager() {
+      super(new Configuration());
+    }
+    void setQueues(Set<String> queues) {
+      this.queues = queues;
+    }
+    public synchronized Set<String> getLeafQueueNames() {
+      return queues;
+    }
+  }
+  
+  static class FakeTaskTrackerManager implements TaskTrackerManager {
+    int maps = 0;
+    int reduces = 0;
+    int maxMapTasksPerTracker = 2;
+    int maxReduceTasksPerTracker = 2;
+    long ttExpiryInterval = 10 * 60 * 1000L; // default interval
+    List<JobInProgressListener> listeners =
+      new ArrayList<JobInProgressListener>();
+    Map<JobID, JobInProgress> jobs = new HashMap<JobID, JobInProgress>();
+    
+    private Map<String, TaskTracker> trackers =
+      new HashMap<String, TaskTracker>();
+    private Map<String, TaskStatus> statuses = 
+      new HashMap<String, TaskStatus>();
+    private Map<String, FakeTaskInProgress> tips = 
+      new HashMap<String, FakeTaskInProgress>();
+    private Map<String, TaskTrackerStatus> trackerForTip =
+      new HashMap<String, TaskTrackerStatus>();
+    
+    public FakeTaskTrackerManager(int numRacks, int numTrackersPerRack) {
+      int nextTrackerId = 1;
+      for (int rack = 1; rack <= numRacks; rack++) {
+        for (int node = 1; node <= numTrackersPerRack; node++) {
+          int id = nextTrackerId++;
+          String host = "rack" + rack + ".node" + node;
+          System.out.println("Creating TaskTracker tt" + id + " on " + host);
+          TaskTracker tt = new TaskTracker("tt" + id);
+          tt.setStatus(new TaskTrackerStatus("tt" + id, host, 0,
+              new ArrayList<TaskStatus>(), 0, 0,
+              maxMapTasksPerTracker, maxReduceTasksPerTracker));
+          trackers.put("tt" + id, tt);
+        }
+      }
+    }
+    
+    @Override
+    public ClusterStatus getClusterStatus() {
+      int numTrackers = trackers.size();
+
+      return new ClusterStatus(numTrackers, 0, 0,
+          ttExpiryInterval, maps, reduces,
+          numTrackers * maxMapTasksPerTracker,
+          numTrackers * maxReduceTasksPerTracker,
+          JobTracker.State.RUNNING);
+    }
+
+    @Override
+    public QueueManager getQueueManager() {
+      return null;
+    }
+    
+    @Override
+    public int getNumberOfUniqueHosts() {
+      return trackers.size();
+    }
+
+    @Override
+    public Collection<TaskTrackerStatus> taskTrackers() {
+      List<TaskTrackerStatus> statuses = new ArrayList<TaskTrackerStatus>();
+      for (TaskTracker tt : trackers.values()) {
+        statuses.add(tt.getStatus());
+      }
+      return statuses;
+    }
+
+
+    @Override
+    public void addJobInProgressListener(JobInProgressListener listener) {
+      listeners.add(listener);
+    }
+
+    @Override
+    public void removeJobInProgressListener(JobInProgressListener listener) {
+      listeners.remove(listener);
+    }
+    
+    @Override
+    public int getNextHeartbeatInterval() {
+      return MRConstants.HEARTBEAT_INTERVAL_MIN;
+    }
+
+    @Override
+    public void killJob(JobID jobid) {
+      return;
+    }
+
+    @Override
+    public JobInProgress getJob(JobID jobid) {
+      return jobs.get(jobid);
+    }
+
+    public void initJob (JobInProgress job) {
+      try {
+        job.initTasks();
+      } catch (KillInterruptedException e) {
+      } catch (IOException e) {
+      }
+    }
+    
+    public void failJob (JobInProgress job) {
+      // do nothing
+    }
+    
+    // Test methods
+    
+    public void submitJob(JobInProgress job) throws IOException {
+      jobs.put(job.getJobID(), job);
+      for (JobInProgressListener listener : listeners) {
+        listener.jobAdded(job);
+      }
+    }
+    
+    public TaskTracker getTaskTracker(String trackerID) {
+      return trackers.get(trackerID);
+    }
+    
+    public void startTask(String trackerName, Task t, FakeTaskInProgress tip) {
+      final boolean isMap = t.isMapTask();
+      if (isMap) {
+        maps++;
+      } else {
+        reduces++;
+      }
+      String attemptId = t.getTaskID().toString();
+      TaskStatus status = tip.getTaskStatus(t.getTaskID());
+      TaskTrackerStatus trackerStatus = trackers.get(trackerName).getStatus();
+      tips.put(attemptId, tip);
+      statuses.put(attemptId, status);
+      trackerForTip.put(attemptId, trackerStatus);
+      status.setRunState(TaskStatus.State.RUNNING);
+    }
+    
+    public void reportTaskOnTracker(String trackerName, Task t) {
+      FakeTaskInProgress tip = tips.get(t.getTaskID().toString());
+      TaskTrackerStatus trackerStatus = trackers.get(trackerName).getStatus();
+      trackerStatus.getTaskReports().add(tip.getTaskStatus(t.getTaskID()));
+    }
+    
+    public void finishTask(String taskTrackerName, String attemptId) {
+      FakeTaskInProgress tip = tips.get(attemptId);
+      if (tip.isMapTask()) {
+        maps--;
+      } else {
+        reduces--;
+      }
+      tip.finishAttempt();
+      TaskStatus status = statuses.get(attemptId);
+      trackers.get(taskTrackerName).getStatus().getTaskReports().remove(status);
+    }
+
+    @Override
+    public boolean killTask(TaskAttemptID attemptId, boolean shouldFail) {
+      String attemptIdStr = attemptId.toString();
+      FakeTaskInProgress tip = tips.get(attemptIdStr);
+      if (tip.isMapTask()) {
+        maps--;
+      } else {
+        reduces--;
+      }
+      tip.killAttempt();
+      TaskStatus status = statuses.get(attemptIdStr);
+      trackerForTip.get(attemptIdStr).getTaskReports().remove(status);
+      return true;
+    }
+
+    @Override
+    public boolean isInSafeMode() {
+      // TODO Auto-generated method stub
+      return false;
+    }
+  }
+  
+  protected JobConf conf;
+  protected FairScheduler scheduler;
+  private FakeTaskTrackerManager taskTrackerManager;
+  private FakeClock clock;
+  private JobTracker jobTracker;
+
+  @Override
+  protected void setUp() throws Exception {
+    jobCounter = 0;
+    new File(TEST_DIR).mkdirs(); // Make sure data directory exists
+    // Create an empty pools file (so we can add/remove pools later)
+    FileWriter fileWriter = new FileWriter(ALLOC_FILE);
+    fileWriter.write("<?xml version=\"1.0\"?>\n");
+    fileWriter.write("<allocations />\n");
+    fileWriter.close();
+    setUpCluster(1, 2, false);
+  }
+
+  public String getRack(String hostname) {
+    // Host names are of the form rackN.nodeM, so split at the dot.
+    return hostname.split("\\.")[0];
+  }
+
+  private void setUpCluster(int numRacks, int numNodesPerRack,
+      boolean assignMultiple) throws IOException {
+    
+    resetMetrics();
+    
+    conf = new JobConf();
+    conf.set("mapred.fairscheduler.allocation.file", ALLOC_FILE);
+    conf.set("mapred.fairscheduler.poolnameproperty", POOL_PROPERTY);
+    conf.setBoolean("mapred.fairscheduler.assignmultiple", assignMultiple);
+    // Manually set locality delay because we aren't using a JobTracker so
+    // we can't auto-compute it from the heartbeat interval.
+    conf.setLong("mapred.fairscheduler.locality.delay.node", 5000);
+    conf.setLong("mapred.fairscheduler.locality.delay.rack", 10000);
+    conf.set("mapred.job.tracker", "localhost:0");
+    conf.set("mapred.job.tracker.http.address", "0.0.0.0:0");
+    taskTrackerManager = new FakeTaskTrackerManager(numRacks, numNodesPerRack);
+    clock = new FakeClock();
+    try {
+      jobTracker = new JobTracker(conf, clock);
+      jobTracker.setSafeModeInternal(JobTracker.SafeModeAction.SAFEMODE_ENTER);
+      jobTracker.initializeFilesystem();
+      jobTracker.setSafeModeInternal(JobTracker.SafeModeAction.SAFEMODE_LEAVE);
+      jobTracker.initialize();
+    } catch (Exception e) {
+      throw new RuntimeException("Could not start JT", e);
+    }
+    scheduler = new FairScheduler(clock, true);
+    scheduler.waitForMapsBeforeLaunchingReduces = false;
+    scheduler.setConf(conf);
+    scheduler.setTaskTrackerManager(taskTrackerManager);
+    scheduler.start();
+    // TaskStatus complains if a task's start time is 0, so advance it a bit
+    advanceTime(100);
+  }
+  
+  /**
+   * Set up a metrics context that doesn't emit anywhere but stores the data
+   * so we can verify it. Also clears it of any data so that different test
+   * cases don't pollute each other.
+   */
+  private void resetMetrics() throws IOException {
+    ContextFactory factory = ContextFactory.getFactory();
+    factory.setAttribute("fairscheduler.class",
+        NoEmitMetricsContext.class.getName());
+    
+    MetricsUtil.getContext("fairscheduler").createRecord("jobs").remove();
+    MetricsUtil.getContext("fairscheduler").createRecord("pools").remove();
+  }
+
+  @Override
+  protected void tearDown() throws Exception {
+    if (scheduler != null) {
+      scheduler.terminate();
+    }
+  }
+  
+  private JobInProgress submitJobNotInitialized(int state, int maps, int reduces)
+	    throws IOException {
+    return submitJob(state, maps, reduces, null, null, false);
+  }
+
+  private JobInProgress submitJob(int state, int maps, int reduces)
+      throws IOException {
+    return submitJob(state, maps, reduces, null, null, true);
+  }
+  
+  private JobInProgress submitJob(int state, int maps, int reduces, String pool)
+      throws IOException {
+    return submitJob(state, maps, reduces, pool, null, true);
+  }
+  
+  private JobInProgress submitJob(int state, int maps, int reduces, String pool,
+      String[][] mapInputLocations, boolean initializeJob) throws IOException {
+    JobConf jobConf = new JobConf(conf);
+    jobConf.setNumMapTasks(maps);
+    jobConf.setNumReduceTasks(reduces);
+    if (pool != null)
+      jobConf.set(POOL_PROPERTY, pool);
+    JobInProgress job = new FakeJobInProgress(jobConf, taskTrackerManager,
+        mapInputLocations, jobTracker);
+    if (initializeJob) {
+      taskTrackerManager.initJob(job);
+    }
+    job.getStatus().setRunState(state);
+    taskTrackerManager.submitJob(job);
+    job.startTime = clock.time;
+    return job;
+  }
+  
+  protected void submitJobs(int number, int state, int maps, int reduces)
+    throws IOException {
+    for (int i = 0; i < number; i++) {
+      submitJob(state, maps, reduces);
+    }
+  }
+
+  public void testAllocationFileParsing() throws Exception {
+    PrintWriter out = new PrintWriter(new FileWriter(ALLOC_FILE));
+    out.println("<?xml version=\"1.0\"?>");
+    out.println("<allocations>"); 
+    // Give pool A a minimum of 1 map, 2 reduces
+    out.println("<pool name=\"poolA\">");
+    out.println("<minMaps>1</minMaps>");
+    out.println("<minReduces>2</minReduces>");
+    out.println("</pool>");
+    // Give pool B a minimum of 2 maps, 1 reduce
+    out.println("<pool name=\"poolB\">");
+    out.println("<minMaps>2</minMaps>");
+    out.println("<minReduces>1</minReduces>");
+    out.println("</pool>");
+    // Give pool C min maps but no min reduces
+    out.println("<pool name=\"poolC\">");
+    out.println("<minMaps>2</minMaps>");
+    out.println("</pool>");
+    // Give pool D a limit of 3 running jobs
+    out.println("<pool name=\"poolD\">");
+    out.println("<maxRunningJobs>3</maxRunningJobs>");
+    out.println("</pool>");
+    // Give pool E a preemption timeout of one minute
+    out.println("<pool name=\"poolE\">");
+    out.println("<minSharePreemptionTimeout>60</minSharePreemptionTimeout>");
+    out.println("</pool>");
+    // Set default limit of jobs per pool to 15
+    out.println("<poolMaxJobsDefault>15</poolMaxJobsDefault>");
+    // Set default limit of jobs per user to 5
+    out.println("<userMaxJobsDefault>5</userMaxJobsDefault>");
+    // Give user1 a limit of 10 jobs
+    out.println("<user name=\"user1\">");
+    out.println("<maxRunningJobs>10</maxRunningJobs>");
+    out.println("</user>");
+    // Set default min share preemption timeout to 2 minutes
+    out.println("<defaultMinSharePreemptionTimeout>120" 
+        + "</defaultMinSharePreemptionTimeout>"); 
+    // Set fair share preemption timeout to 5 minutes
+    out.println("<fairSharePreemptionTimeout>300</fairSharePreemptionTimeout>"); 
+    out.println("</allocations>"); 
+    out.close();
+    
+    PoolManager poolManager = scheduler.getPoolManager();
+    poolManager.reloadAllocs();
+    
+    assertEquals(6, poolManager.getPools().size()); // 5 in file + default pool
+    assertEquals(0, poolManager.getAllocation(Pool.DEFAULT_POOL_NAME,
+        TaskType.MAP));
+    assertEquals(0, poolManager.getAllocation(Pool.DEFAULT_POOL_NAME,
+        TaskType.REDUCE));
+    assertEquals(1, poolManager.getAllocation("poolA", TaskType.MAP));
+    assertEquals(2, poolManager.getAllocation("poolA", TaskType.REDUCE));
+    assertEquals(2, poolManager.getAllocation("poolB", TaskType.MAP));
+    assertEquals(1, poolManager.getAllocation("poolB", TaskType.REDUCE));
+    assertEquals(2, poolManager.getAllocation("poolC", TaskType.MAP));
+    assertEquals(0, poolManager.getAllocation("poolC", TaskType.REDUCE));
+    assertEquals(0, poolManager.getAllocation("poolD", TaskType.MAP));
+    assertEquals(0, poolManager.getAllocation("poolD", TaskType.REDUCE));
+    assertEquals(0, poolManager.getAllocation("poolE", TaskType.MAP));
+    assertEquals(0, poolManager.getAllocation("poolE", TaskType.REDUCE));
+    assertEquals(15, poolManager.getPoolMaxJobs(Pool.DEFAULT_POOL_NAME));
+    assertEquals(15, poolManager.getPoolMaxJobs("poolA"));
+    assertEquals(15, poolManager.getPoolMaxJobs("poolB"));
+    assertEquals(15, poolManager.getPoolMaxJobs("poolC"));
+    assertEquals(3, poolManager.getPoolMaxJobs("poolD"));
+    assertEquals(15, poolManager.getPoolMaxJobs("poolE"));
+    assertEquals(10, poolManager.getUserMaxJobs("user1"));
+    assertEquals(5, poolManager.getUserMaxJobs("user2"));
+    assertEquals(120000, poolManager.getMinSharePreemptionTimeout(
+        Pool.DEFAULT_POOL_NAME));
+    assertEquals(120000, poolManager.getMinSharePreemptionTimeout("poolA"));
+    assertEquals(120000, poolManager.getMinSharePreemptionTimeout("poolB"));
+    assertEquals(120000, poolManager.getMinSharePreemptionTimeout("poolC"));
+    assertEquals(120000, poolManager.getMinSharePreemptionTimeout("poolD"));
+    assertEquals(120000, poolManager.getMinSharePreemptionTimeout("poolA"));
+    assertEquals(60000, poolManager.getMinSharePreemptionTimeout("poolE"));
+    assertEquals(300000, poolManager.getFairSharePreemptionTimeout());
+  }
+  
+  public void testTaskNotAssignedWhenNoJobsArePresent() throws IOException {
+    assertNull(scheduler.assignTasks(tracker("tt1")));
+  }
+
+  public void testNonRunningJobsAreIgnored() throws IOException {
+    submitJobs(1, JobStatus.SUCCEEDED, 10, 10);
+    submitJobs(1, JobStatus.FAILED, 10, 10);
+    submitJobs(1, JobStatus.KILLED, 10, 10);
+    assertNull(scheduler.assignTasks(tracker("tt1")));
+    advanceTime(100); // Check that we still don't assign jobs after an update
+    assertNull(scheduler.assignTasks(tracker("tt1")));
+  }
+
+  /**
+   * This test contains two jobs with fewer required tasks than there are slots.
+   * We check that all tasks are assigned, but job 1 gets them first because it
+   * was submitted earlier.
+   */
+  public void testSmallJobs() throws IOException {
+    JobInProgress job1 = submitJob(JobStatus.RUNNING, 2, 1);
+    JobInfo info1 = scheduler.infos.get(job1);
+    
+    // Check scheduler variables
+    assertEquals(0,    info1.mapSchedulable.getRunningTasks());
+    assertEquals(0,    info1.reduceSchedulable.getRunningTasks());
+    assertEquals(2,    info1.mapSchedulable.getDemand());
+    assertEquals(1,    info1.reduceSchedulable.getDemand());
+    assertEquals(2.0,  info1.mapSchedulable.getFairShare());
+    assertEquals(1.0,  info1.reduceSchedulable.getFairShare());
+    verifyMetrics();
+
+    // Advance time before submitting another job j2, to make j1 run before j2
+    // deterministically.
+    advanceTime(100);
+    JobInProgress job2 = submitJob(JobStatus.RUNNING, 1, 2);
+    JobInfo info2 = scheduler.infos.get(job2);
+    
+    // Check scheduler variables
+    assertEquals(0,    info1.mapSchedulable.getRunningTasks());
+    assertEquals(0,    info1.reduceSchedulable.getRunningTasks());
+    assertEquals(2,    info1.mapSchedulable.getDemand());
+    assertEquals(1,    info1.reduceSchedulable.getDemand());
+    assertEquals(2.0,  info1.mapSchedulable.getFairShare());
+    assertEquals(1.0,  info1.reduceSchedulable.getFairShare());
+    assertEquals(0,    info2.mapSchedulable.getRunningTasks());
+    assertEquals(0,    info2.reduceSchedulable.getRunningTasks());
+    assertEquals(1,    info2.mapSchedulable.getDemand());
+    assertEquals(2,    info2.reduceSchedulable.getDemand());
+    assertEquals(1.0,  info2.mapSchedulable.getFairShare());
+    assertEquals(2.0,  info2.reduceSchedulable.getFairShare());
+    verifyMetrics();
+    
+    // Assign tasks and check that jobs alternate in filling slots
+    checkAssignment("tt1", "attempt_test_0001_m_000000_0 on tt1");
+    checkAssignment("tt1", "attempt_test_0001_r_000000_0 on tt1");
+    checkAssignment("tt1", "attempt_test_0002_m_000000_0 on tt1");
+    checkAssignment("tt1", "attempt_test_0002_r_000000_0 on tt1");
+    checkAssignment("tt2", "attempt_test_0001_m_000001_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0002_r_000001_0 on tt2");
+    assertNull(scheduler.assignTasks(tracker("tt2")));
+    
+    // Check that the scheduler has started counting the tasks as running
+    // as soon as it launched them.
+    assertEquals(2,  info1.mapSchedulable.getRunningTasks());
+    assertEquals(1,  info1.reduceSchedulable.getRunningTasks());
+    assertEquals(2,  info1.mapSchedulable.getDemand());
+    assertEquals(1,  info1.reduceSchedulable.getDemand());
+    assertEquals(1,  info2.mapSchedulable.getRunningTasks());
+    assertEquals(2,  info2.reduceSchedulable.getRunningTasks());
+    assertEquals(1, info2.mapSchedulable.getDemand());
+    assertEquals(2, info2.reduceSchedulable.getDemand());
+    verifyMetrics();
+  }
+  /**
+   * This test is identical to testSmallJobs but sets assignMultiple to
+   * true so that multiple tasks can be assigned per heartbeat.
+   */
+  public void testSmallJobsWithAssignMultiple() throws IOException {
+    setUpCluster(1, 2, true);
+    
+    JobInProgress job1 = submitJob(JobStatus.RUNNING, 2, 1);
+    JobInfo info1 = scheduler.infos.get(job1);
+    
+    // Check scheduler variables
+    assertEquals(0,    info1.mapSchedulable.getRunningTasks());
+    assertEquals(0,    info1.reduceSchedulable.getRunningTasks());
+    assertEquals(2,    info1.mapSchedulable.getDemand());
+    assertEquals(1,    info1.reduceSchedulable.getDemand());
+    assertEquals(2.0,  info1.mapSchedulable.getFairShare());
+    assertEquals(1.0,  info1.reduceSchedulable.getFairShare());
+    verifyMetrics();
+    
+    // Advance time before submitting another job j2, to make j1 run before j2
+    // deterministically.
+    advanceTime(100);
+    JobInProgress job2 = submitJob(JobStatus.RUNNING, 1, 2);
+    JobInfo info2 = scheduler.infos.get(job2);
+    
+    // Check scheduler variables
+    assertEquals(0,    info1.mapSchedulable.getRunningTasks());
+    assertEquals(0,    info1.reduceSchedulable.getRunningTasks());
+    assertEquals(2,    info1.mapSchedulable.getDemand());
+    assertEquals(1,    info1.reduceSchedulable.getDemand());
+    assertEquals(2.0,  info1.mapSchedulable.getFairShare());
+    assertEquals(1.0,  info1.reduceSchedulable.getFairShare());
+    assertEquals(0,    info2.mapSchedulable.getRunningTasks());
+    assertEquals(0,    info2.reduceSchedulable.getRunningTasks());
+    assertEquals(1,    info2.mapSchedulable.getDemand());
+    assertEquals(2,    info2.reduceSchedulable.getDemand());
+    assertEquals(1.0,  info2.mapSchedulable.getFairShare());
+    assertEquals(2.0,  info2.reduceSchedulable.getFairShare());
+    verifyMetrics();
+    
+    // Assign tasks and check that jobs alternate in filling slots
+    checkAssignment("tt1", "attempt_test_0001_m_000000_0 on tt1",
+                           "attempt_test_0001_r_000000_0 on tt1",
+                           "attempt_test_0002_m_000000_0 on tt1",
+                           "attempt_test_0002_r_000000_0 on tt1");
+    checkAssignment("tt2", "attempt_test_0001_m_000001_0 on tt2",
+                           "attempt_test_0002_r_000001_0 on tt2");
+    assertNull(scheduler.assignTasks(tracker("tt2")));
+    
+    // Check that the scheduler has started counting the tasks as running
+    // as soon as it launched them.
+    assertEquals(2,  info1.mapSchedulable.getRunningTasks());
+    assertEquals(1,  info1.reduceSchedulable.getRunningTasks());
+    assertEquals(2,  info1.mapSchedulable.getDemand());
+    assertEquals(1,  info1.reduceSchedulable.getDemand());
+    assertEquals(1,  info2.mapSchedulable.getRunningTasks());
+    assertEquals(2,  info2.reduceSchedulable.getRunningTasks());
+    assertEquals(1, info2.mapSchedulable.getDemand());
+    assertEquals(2, info2.reduceSchedulable.getDemand());
+    verifyMetrics();
+  }
+  
+  /**
+   * This test begins by submitting two jobs with 10 maps and reduces each.
+   * The first job is submitted 100ms after the second, to make it get slots
+   * first deterministically. We then assign a wave of tasks and check that
+   * they are given alternately to job1, job2, job1, job2, etc. We finish
+   * these tasks and assign a second wave, which should continue to be
+   * allocated in this manner.
+   */
+  public void testLargeJobs() throws IOException {
+    JobInProgress job1 = submitJob(JobStatus.RUNNING, 10, 10);
+    JobInfo info1 = scheduler.infos.get(job1);
+    
+    // Check scheduler variables
+    assertEquals(0,    info1.mapSchedulable.getRunningTasks());
+    assertEquals(0,    info1.reduceSchedulable.getRunningTasks());
+    assertEquals(10,   info1.mapSchedulable.getDemand());
+    assertEquals(10,   info1.reduceSchedulable.getDemand());
+    assertEquals(4.0,  info1.mapSchedulable.getFairShare());
+    assertEquals(4.0,  info1.reduceSchedulable.getFairShare());
+    
+    // Advance time before submitting another job j2, to make j1 run before j2
+    // deterministically.
+    advanceTime(100);
+    JobInProgress job2 = submitJob(JobStatus.RUNNING, 10, 10);
+    JobInfo info2 = scheduler.infos.get(job2);
+    
+    // Check scheduler variables
+    assertEquals(0,    info1.mapSchedulable.getRunningTasks());
+    assertEquals(0,    info1.reduceSchedulable.getRunningTasks());
+    assertEquals(10,   info1.mapSchedulable.getDemand());
+    assertEquals(10,   info1.reduceSchedulable.getDemand());
+    assertEquals(2.0,  info1.mapSchedulable.getFairShare());
+    assertEquals(2.0,  info1.reduceSchedulable.getFairShare());
+    assertEquals(0,    info2.mapSchedulable.getRunningTasks());
+    assertEquals(0,    info2.reduceSchedulable.getRunningTasks());
+    assertEquals(10,   info2.mapSchedulable.getDemand());
+    assertEquals(10,   info2.reduceSchedulable.getDemand());
+    assertEquals(2.0,  info2.mapSchedulable.getFairShare());
+    assertEquals(2.0,  info2.reduceSchedulable.getFairShare());
+    
+    // Check that tasks are filled alternately by the jobs
+    checkAssignment("tt1", "attempt_test_0001_m_000000_0 on tt1");
+    checkAssignment("tt1", "attempt_test_0001_r_000000_0 on tt1");
+    checkAssignment("tt1", "attempt_test_0002_m_000000_0 on tt1");
+    checkAssignment("tt1", "attempt_test_0002_r_000000_0 on tt1");
+    checkAssignment("tt2", "attempt_test_0001_m_000001_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0001_r_000001_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0002_m_000001_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0002_r_000001_0 on tt2");
+    
+    // Check that no new tasks can be launched once the tasktrackers are full
+    assertNull(scheduler.assignTasks(tracker("tt1")));
+    assertNull(scheduler.assignTasks(tracker("tt2")));
+    
+    // Check that the scheduler has started counting the tasks as running
+    // as soon as it launched them.
+    assertEquals(2,  info1.mapSchedulable.getRunningTasks());
+    assertEquals(2,  info1.reduceSchedulable.getRunningTasks());
+    assertEquals(10,  info1.mapSchedulable.getDemand());
+    assertEquals(10,  info1.reduceSchedulable.getDemand());
+    assertEquals(2,  info2.mapSchedulable.getRunningTasks());
+    assertEquals(2,  info2.reduceSchedulable.getRunningTasks());
+    assertEquals(10, info2.mapSchedulable.getDemand());
+    assertEquals(10, info2.reduceSchedulable.getDemand());
+    
+    // Finish up the tasks and advance time again. Note that we must finish
+    // the task since FakeJobInProgress does not properly maintain running
+    // tasks, so the scheduler will always get an empty task list from
+    // the JobInProgress's getTasks(TaskType.MAP)/getTasks(TaskType.REDUCE) and 
+    // think they finished.
+    taskTrackerManager.finishTask("tt1", "attempt_test_0001_m_000000_0");
+    taskTrackerManager.finishTask("tt1", "attempt_test_0002_m_000000_0");
+    taskTrackerManager.finishTask("tt1", "attempt_test_0001_r_000000_0");
+    taskTrackerManager.finishTask("tt1", "attempt_test_0002_r_000000_0");
+    taskTrackerManager.finishTask("tt2", "attempt_test_0001_m_000001_0");
+    taskTrackerManager.finishTask("tt2", "attempt_test_0002_m_000001_0");
+    taskTrackerManager.finishTask("tt2", "attempt_test_0001_r_000001_0");
+    taskTrackerManager.finishTask("tt2", "attempt_test_0002_r_000001_0");
+    advanceTime(200);
+    assertEquals(0,   info1.mapSchedulable.getRunningTasks());
+    assertEquals(0,   info1.reduceSchedulable.getRunningTasks());
+    assertEquals(0,   info2.mapSchedulable.getRunningTasks());
+    assertEquals(0,   info2.reduceSchedulable.getRunningTasks());
+
+    // Check that tasks are filled alternately by the jobs
+    checkAssignment("tt1", "attempt_test_0001_m_000002_0 on tt1");
+    checkAssignment("tt1", "attempt_test_0001_r_000002_0 on tt1");
+    checkAssignment("tt1", "attempt_test_0002_m_000002_0 on tt1");
+    checkAssignment("tt1", "attempt_test_0002_r_000002_0 on tt1");
+    checkAssignment("tt2", "attempt_test_0001_m_000003_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0001_r_000003_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0002_m_000003_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0002_r_000003_0 on tt2");
+    
+    // Check scheduler variables; the demands should now be 8 because 2 tasks
+    // of each type have finished in each job
+    assertEquals(2,    info1.mapSchedulable.getRunningTasks());
+    assertEquals(2,    info1.reduceSchedulable.getRunningTasks());
+    assertEquals(8,   info1.mapSchedulable.getDemand());
+    assertEquals(8,   info1.reduceSchedulable.getDemand());
+    assertEquals(2.0,  info1.mapSchedulable.getFairShare());
+    assertEquals(2.0,  info1.reduceSchedulable.getFairShare());
+    assertEquals(2,    info2.mapSchedulable.getRunningTasks());
+    assertEquals(2,    info2.reduceSchedulable.getRunningTasks());
+    assertEquals(8,   info2.mapSchedulable.getDemand());
+    assertEquals(8,   info2.reduceSchedulable.getDemand());
+    assertEquals(2.0,  info2.mapSchedulable.getFairShare());
+    assertEquals(2.0,  info2.reduceSchedulable.getFairShare());
+  }
+  
+  /**
+   * A copy of testLargeJobs that enables the assignMultiple feature to launch
+   * multiple tasks per heartbeat. Results should be the same as testLargeJobs.
+   */
+  public void testLargeJobsWithAssignMultiple() throws IOException {
+    setUpCluster(1, 2, true);
+    
+    JobInProgress job1 = submitJob(JobStatus.RUNNING, 10, 10);
+    JobInfo info1 = scheduler.infos.get(job1);
+    
+    // Check scheduler variables
+    assertEquals(0,    info1.mapSchedulable.getRunningTasks());
+    assertEquals(0,    info1.reduceSchedulable.getRunningTasks());
+    assertEquals(10,   info1.mapSchedulable.getDemand());
+    assertEquals(10,   info1.reduceSchedulable.getDemand());
+    assertEquals(4.0,  info1.mapSchedulable.getFairShare());
+    assertEquals(4.0,  info1.reduceSchedulable.getFairShare());
+    
+    // Advance time before submitting another job j2, to make j1 run before j2
+    // deterministically.
+    advanceTime(100);
+    JobInProgress job2 = submitJob(JobStatus.RUNNING, 10, 10);
+    JobInfo info2 = scheduler.infos.get(job2);
+    
+    // Check scheduler variables; the fair shares should now have been allocated
+    // equally between j1 and j2, but j1 should have (4 slots)*(100 ms) deficit
+    assertEquals(0,    info1.mapSchedulable.getRunningTasks());
+    assertEquals(0,    info1.reduceSchedulable.getRunningTasks());
+    assertEquals(10,   info1.mapSchedulable.getDemand());
+    assertEquals(10,   info1.reduceSchedulable.getDemand());
+    assertEquals(2.0,  info1.mapSchedulable.getFairShare());
+    assertEquals(2.0,  info1.reduceSchedulable.getFairShare());
+    assertEquals(0,    info2.mapSchedulable.getRunningTasks());
+    assertEquals(0,    info2.reduceSchedulable.getRunningTasks());
+    assertEquals(10,   info2.mapSchedulable.getDemand());
+    assertEquals(10,   info2.reduceSchedulable.getDemand());
+    assertEquals(2.0,  info2.mapSchedulable.getFairShare());
+    assertEquals(2.0,  info2.reduceSchedulable.getFairShare());
+    
+    // Check that tasks are filled alternately by the jobs
+    checkAssignment("tt1", "attempt_test_0001_m_000000_0 on tt1",
+                           "attempt_test_0001_r_000000_0 on tt1",
+                           "attempt_test_0002_m_000000_0 on tt1",
+                           "attempt_test_0002_r_000000_0 on tt1");
+    checkAssignment("tt2", "attempt_test_0001_m_000001_0 on tt2",
+                           "attempt_test_0001_r_000001_0 on tt2",
+                           "attempt_test_0002_m_000001_0 on tt2",
+                           "attempt_test_0002_r_000001_0 on tt2");
+    
+    // Check that no new tasks can be launched once the tasktrackers are full
+    assertNull(scheduler.assignTasks(tracker("tt1")));
+    assertNull(scheduler.assignTasks(tracker("tt2")));
+    
+    // Check that the scheduler has started counting the tasks as running
+    // as soon as it launched them.
+    assertEquals(2,  info1.mapSchedulable.getRunningTasks());
+    assertEquals(2,  info1.reduceSchedulable.getRunningTasks());
+    assertEquals(10,  info1.mapSchedulable.getDemand());
+    assertEquals(10,  info1.reduceSchedulable.getDemand());
+    assertEquals(2,  info2.mapSchedulable.getRunningTasks());
+    assertEquals(2,  info2.reduceSchedulable.getRunningTasks());
+    assertEquals(10, info2.mapSchedulable.getDemand());
+    assertEquals(10, info2.reduceSchedulable.getDemand());
+    
+    // Finish up the tasks and advance time again. Note that we must finish
+    // the task since FakeJobInProgress does not properly maintain running
+    // tasks, so the scheduler will always get an empty task list from
+    // the JobInProgress's getTasks(TaskType.MAP)/getTasks(TaskType.REDUCE) and
+    // think they finished.
+    taskTrackerManager.finishTask("tt1", "attempt_test_0001_m_000000_0");
+    taskTrackerManager.finishTask("tt1", "attempt_test_0002_m_000000_0");
+    taskTrackerManager.finishTask("tt1", "attempt_test_0001_r_000000_0");
+    taskTrackerManager.finishTask("tt1", "attempt_test_0002_r_000000_0");
+    taskTrackerManager.finishTask("tt2", "attempt_test_0001_m_000001_0");
+    taskTrackerManager.finishTask("tt2", "attempt_test_0002_m_000001_0");
+    taskTrackerManager.finishTask("tt2", "attempt_test_0001_r_000001_0");
+    taskTrackerManager.finishTask("tt2", "attempt_test_0002_r_000001_0");
+    advanceTime(200);
+    assertEquals(0,   info1.mapSchedulable.getRunningTasks());
+    assertEquals(0,   info1.reduceSchedulable.getRunningTasks());
+    assertEquals(0,   info2.mapSchedulable.getRunningTasks());
+    assertEquals(0,   info2.reduceSchedulable.getRunningTasks());
+
+    // Check that tasks are filled alternately by the jobs
+    checkAssignment("tt1", "attempt_test_0001_m_000002_0 on tt1",
+                           "attempt_test_0001_r_000002_0 on tt1",
+                           "attempt_test_0002_m_000002_0 on tt1",
+                           "attempt_test_0002_r_000002_0 on tt1");
+    checkAssignment("tt2", "attempt_test_0001_m_000003_0 on tt2",
+                           "attempt_test_0001_r_000003_0 on tt2",
+                           "attempt_test_0002_m_000003_0 on tt2",
+                           "attempt_test_0002_r_000003_0 on tt2");
+    
+    // Check scheduler variables; the demands should now be 8 because 2 tasks
+    // of each type have finished in each job
+    assertEquals(2,    info1.mapSchedulable.getRunningTasks());
+    assertEquals(2,    info1.reduceSchedulable.getRunningTasks());
+    assertEquals(8,   info1.mapSchedulable.getDemand());
+    assertEquals(8,   info1.reduceSchedulable.getDemand());
+    assertEquals(2.0,  info1.mapSchedulable.getFairShare());
+    assertEquals(2.0,  info1.reduceSchedulable.getFairShare());
+    assertEquals(2,    info2.mapSchedulable.getRunningTasks());
+    assertEquals(2,    info2.reduceSchedulable.getRunningTasks());
+    assertEquals(8,   info2.mapSchedulable.getDemand());
+    assertEquals(8,   info2.reduceSchedulable.getDemand());
+    assertEquals(2.0,  info2.mapSchedulable.getFairShare());
+    assertEquals(2.0,  info2.reduceSchedulable.getFairShare());
+  }
+
+  /**
+   * We submit two jobs such that one has 2x the priority of the other to 
+   * a cluster of 3 nodes, wait for 100 ms, and check that the weights/shares 
+   * the high-priority job gets 4 tasks while the normal-priority job gets 2.
+   */
+  public void testJobsWithPriorities() throws IOException {
+    setUpCluster(1, 3, false);
+    
+    JobInProgress job1 = submitJob(JobStatus.RUNNING, 10, 10);
+    JobInfo info1 = scheduler.infos.get(job1);
+    JobInProgress job2 = submitJob(JobStatus.RUNNING, 10, 10);
+    JobInfo info2 = scheduler.infos.get(job2);
+    job2.setPriority(JobPriority.HIGH);
+    scheduler.update();
+    
+    // Check scheduler variables
+    assertEquals(0,   info1.mapSchedulable.getRunningTasks());
+    assertEquals(0,   info1.reduceSchedulable.getRunningTasks());
+    assertEquals(10,  info1.mapSchedulable.getDemand());
+    assertEquals(10,  info1.reduceSchedulable.getDemand());
+    assertEquals(2.0, info1.mapSchedulable.getFairShare(), 0.1);
+    assertEquals(2.0, info1.reduceSchedulable.getFairShare(), 0.1);
+    assertEquals(0,   info2.mapSchedulable.getRunningTasks());
+    assertEquals(0,   info2.reduceSchedulable.getRunningTasks());
+    assertEquals(10,  info2.mapSchedulable.getDemand());
+    assertEquals(10,  info2.reduceSchedulable.getDemand());
+    assertEquals(4.0, info2.mapSchedulable.getFairShare(), 0.1);
+    assertEquals(4.0, info2.reduceSchedulable.getFairShare(), 0.1);
+    
+    // Advance time
+    advanceTime(100);
+    
+    // Assign tasks and check that j2 gets 2x more tasks than j1. In addition,
+    // whenever the jobs' runningTasks/weight ratios are tied, j1 should get
+    // the new task first because it started first; thus the tasks of each
+    // type should be handed out alternately to 1, 2, 2, 1, 2, 2, etc.
+    System.out.println("HEREEEE");
+    checkAssignment("tt1", "attempt_test_0001_m_000000_0 on tt1");
+    checkAssignment("tt1", "attempt_test_0001_r_000000_0 on tt1");
+    checkAssignment("tt1", "attempt_test_0002_m_000000_0 on tt1");
+    checkAssignment("tt1", "attempt_test_0002_r_000000_0 on tt1");
+    checkAssignment("tt2", "attempt_test_0002_m_000001_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0002_r_000001_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0001_m_000001_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0001_r_000001_0 on tt2");
+    checkAssignment("tt3", "attempt_test_0002_m_000002_0 on tt3");
+    checkAssignment("tt3", "attempt_test_0002_r_000002_0 on tt3");
+    checkAssignment("tt3", "attempt_test_0002_m_000003_0 on tt3");
+    checkAssignment("tt3", "attempt_test_0002_r_000003_0 on tt3");
+  }
+  
+  /**
+   * This test starts by submitting three large jobs:
+   * - job1 in the default pool, at time 0
+   * - job2 in poolA, with an allocation of 1 map / 2 reduces, at time 200
+   * - job3 in poolB, with an allocation of 2 maps / 1 reduce, at time 300
+   * 
+   * We then assign tasks to all slots. The maps should be assigned in the
+   * order job2, job3, job 3, job1 because jobs 3 and 2 have guaranteed slots
+   * (1 and 2 respectively). Job2 comes before job3 when they are both at 0
+   * slots because it has an earlier start time. In a similar manner,
+   * reduces should be assigned as job2, job3, job2, job1.
+   */
+  public void testLargeJobsWithPools() throws Exception {
+    // Set up pools file
+    PrintWriter out = new PrintWriter(new FileWriter(ALLOC_FILE));
+    out.println("<?xml version=\"1.0\"?>");
+    out.println("<allocations>");
+    // Give pool A a minimum of 1 map, 2 reduces
+    out.println("<pool name=\"poolA\">");
+    out.println("<minMaps>1</minMaps>");
+    out.println("<minReduces>2</minReduces>");
+    out.println("</pool>");
+    // Give pool B a minimum of 2 maps, 1 reduce
+    out.println("<pool name=\"poolB\">");
+    out.println("<minMaps>2</minMaps>");
+    out.println("<minReduces>1</minReduces>");
+    out.println("</pool>");
+    out.println("</allocations>");
+    out.close();
+    scheduler.getPoolManager().reloadAllocs();
+    Pool defaultPool = scheduler.getPoolManager().getPool("default");
+    Pool poolA = scheduler.getPoolManager().getPool("poolA");
+    Pool poolB = scheduler.getPoolManager().getPool("poolB");
+    
+    JobInProgress job1 = submitJob(JobStatus.RUNNING, 10, 10);
+    JobInfo info1 = scheduler.infos.get(job1);
+    
+    // Check scheduler variables
+    assertEquals(0,    info1.mapSchedulable.getRunningTasks());
+    assertEquals(0,    info1.reduceSchedulable.getRunningTasks());
+    assertEquals(10,   info1.mapSchedulable.getDemand());
+    assertEquals(10,   info1.reduceSchedulable.getDemand());
+    assertEquals(4.0,  info1.mapSchedulable.getFairShare());
+    assertEquals(4.0,  info1.reduceSchedulable.getFairShare());
+    
+    // Advance time 200ms and submit jobs 2 and 3
+    advanceTime(200);
+    JobInProgress job2 = submitJob(JobStatus.RUNNING, 10, 10, "poolA");
+    JobInfo info2 = scheduler.infos.get(job2);
+    advanceTime(100);
+    JobInProgress job3 = submitJob(JobStatus.RUNNING, 10, 10, "poolB");
+    JobInfo info3 = scheduler.infos.get(job3);
+    
+    // Check that minimum and fair shares have been allocated
+    assertEquals(0,    defaultPool.getMapSchedulable().getMinShare());
+    assertEquals(0,    defaultPool.getReduceSchedulable().getMinShare());
+    assertEquals(1.0,  info1.mapSchedulable.getFairShare());
+    assertEquals(1.0,  info1.reduceSchedulable.getFairShare());
+    assertEquals(1,    poolA.getMapSchedulable().getMinShare());
+    assertEquals(2,    poolA.getReduceSchedulable().getMinShare());
+    assertEquals(1.0,  info2.mapSchedulable.getFairShare());
+    assertEquals(2.0,  info2.reduceSchedulable.getFairShare());
+    assertEquals(2,    poolB.getMapSchedulable().getMinShare());
+    assertEquals(1,    poolB.getReduceSchedulable().getMinShare());
+    assertEquals(2.0,  info3.mapSchedulable.getFairShare());
+    assertEquals(1.0,  info3.reduceSchedulable.getFairShare());
+    
+    // Advance time 100ms
+    advanceTime(100);
+    
+    // Assign tasks and check that slots are first given to needy jobs
+    checkAssignment("tt1", "attempt_test_0002_m_000000_0 on tt1");
+    checkAssignment("tt1", "attempt_test_0002_r_000000_0 on tt1");
+    checkAssignment("tt1", "attempt_test_0003_m_000000_0 on tt1");
+    checkAssignment("tt1", "attempt_test_0003_r_000000_0 on tt1");
+    checkAssignment("tt2", "attempt_test_0003_m_000001_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0002_r_000001_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0001_m_000000_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0001_r_000000_0 on tt2");
+  }
+
+  /**
+   * This test starts by submitting three large jobs:
+   * - job1 in the default pool, at time 0
+   * - job2 in poolA, with an allocation of 2 maps / 2 reduces, at time 200
+   * - job3 in poolA, with an allocation of 2 maps / 2 reduces, at time 300
+   * 
+   * After this, we start assigning tasks. The first two tasks of each type
+   * should be assigned to job2 and job3 since they are in a pool with an
+   * allocation guarantee, but the next two slots should be assigned to job 3
+   * because the pool will no longer be needy.
+   */
+  public void testLargeJobsWithExcessCapacity() throws Exception {
+    // Set up pools file
+    PrintWriter out = new PrintWriter(new FileWriter(ALLOC_FILE));
+    out.println("<?xml version=\"1.0\"?>");
+    out.println("<allocations>");
+    // Give pool A a minimum of 2 maps, 2 reduces
+    out.println("<pool name=\"poolA\">");
+    out.println("<minMaps>2</minMaps>");
+    out.println("<minReduces>2</minReduces>");
+    out.println("</pool>");
+    out.println("</allocations>");
+    out.close();
+    scheduler.getPoolManager().reloadAllocs();
+    Pool poolA = scheduler.getPoolManager().getPool("poolA");
+    
+    JobInProgress job1 = submitJob(JobStatus.RUNNING, 10, 10);
+    JobInfo info1 = scheduler.infos.get(job1);
+    
+    // Check scheduler variables
+    assertEquals(0,    info1.mapSchedulable.getRunningTasks());
+    assertEquals(0,    info1.reduceSchedulable.getRunningTasks());
+    assertEquals(10,   info1.mapSchedulable.getDemand());
+    assertEquals(10,   info1.reduceSchedulable.getDemand());
+    assertEquals(4.0,  info1.mapSchedulable.getFairShare());
+    assertEquals(4.0,  info1.reduceSchedulable.getFairShare());
+    
+    // Advance time 200ms and submit job 2
+    advanceTime(200);
+    JobInProgress job2 = submitJob(JobStatus.RUNNING, 10, 10, "poolA");
+    JobInfo info2 = scheduler.infos.get(job2);
+    
+    // Check that minimum and fair shares have been allocated
+    assertEquals(2,    poolA.getMapSchedulable().getMinShare());
+    assertEquals(2,    poolA.getReduceSchedulable().getMinShare());
+    assertEquals(2.0,  info1.mapSchedulable.getFairShare());
+    assertEquals(2.0,  info1.reduceSchedulable.getFairShare());
+    assertEquals(2.0,  info2.mapSchedulable.getFairShare());
+    assertEquals(2.0,  info2.reduceSchedulable.getFairShare());
+    
+    // Advance time 100ms and submit job 3
+    advanceTime(100);
+    JobInProgress job3 = submitJob(JobStatus.RUNNING, 10, 10, "poolA");
+    JobInfo info3 = scheduler.infos.get(job3);
+    
+    // Check that minimum and fair shares have been allocated
+    assertEquals(2,    poolA.getMapSchedulable().getMinShare());
+    assertEquals(2,    poolA.getReduceSchedulable().getMinShare());
+    assertEquals(2.0,  info1.mapSchedulable.getFairShare());
+    assertEquals(2.0,  info1.reduceSchedulable.getFairShare());
+    assertEquals(1.0,  info2.mapSchedulable.getFairShare());
+    assertEquals(1.0,  info2.reduceSchedulable.getFairShare());
+    assertEquals(1.0,  info3.mapSchedulable.getFairShare());
+    assertEquals(1.0,  info3.reduceSchedulable.getFairShare());
+    
+    // Advance time
+    advanceTime(100);
+    
+    // Assign tasks and check that slots are first given to needy jobs, but
+    // that job 1 gets two tasks after due to having a larger share.
+    checkAssignment("tt1", "attempt_test_0002_m_000000_0 on tt1");
+    checkAssignment("tt1", "attempt_test_0002_r_000000_0 on tt1");
+    checkAssignment("tt1", "attempt_test_0003_m_000000_0 on tt1");
+    checkAssignment("tt1", "attempt_test_0003_r_000000_0 on tt1");
+    checkAssignment("tt2", "attempt_test_0001_m_000000_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0001_r_000000_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0001_m_000001_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0001_r_000001_0 on tt2");
+  }
+  
+  /**
+   * A copy of testLargeJobsWithExcessCapacity that enables assigning multiple
+   * tasks per heartbeat. Results should match testLargeJobsWithExcessCapacity.
+   */
+  public void testLargeJobsWithExcessCapacityAndAssignMultiple() 
+      throws Exception {
+    setUpCluster(1, 2, true);
+    
+    // Set up pools file
+    PrintWriter out = new PrintWriter(new FileWriter(ALLOC_FILE));
+    out.println("<?xml version=\"1.0\"?>");
+    out.println("<allocations>");
+    // Give pool A a minimum of 2 maps, 2 reduces
+    out.println("<pool name=\"poolA\">");
+    out.println("<minMaps>2</minMaps>");
+    out.println("<minReduces>2</minReduces>");
+    out.println("</pool>");
+    out.println("</allocations>");
+    out.close();
+    scheduler.getPoolManager().reloadAllocs();
+    Pool poolA = scheduler.getPoolManager().getPool("poolA");
+    
+    JobInProgress job1 = submitJob(JobStatus.RUNNING, 10, 10);
+    JobInfo info1 = scheduler.infos.get(job1);
+    
+    // Check scheduler variables
+    assertEquals(0,    info1.mapSchedulable.getRunningTasks());
+    assertEquals(0,    info1.reduceSchedulable.getRunningTasks());
+    assertEquals(10,   info1.mapSchedulable.getDemand());
+    assertEquals(10,   info1.reduceSchedulable.getDemand());
+    assertEquals(4.0,  info1.mapSchedulable.getFairShare());
+    assertEquals(4.0,  info1.reduceSchedulable.getFairShare());
+    
+    // Advance time 200ms and submit job 2
+    advanceTime(200);
+    JobInProgress job2 = submitJob(JobStatus.RUNNING, 10, 10, "poolA");
+    JobInfo info2 = scheduler.infos.get(job2);
+    
+    // Check that minimum and fair shares have been allocated
+    assertEquals(2,    poolA.getMapSchedulable().getMinShare());
+    assertEquals(2,    poolA.getReduceSchedulable().getMinShare());
+    assertEquals(2.0,  info1.mapSchedulable.getFairShare());
+    assertEquals(2.0,  info1.reduceSchedulable.getFairShare());
+    assertEquals(2.0,  info2.mapSchedulable.getFairShare());
+    assertEquals(2.0,  info2.reduceSchedulable.getFairShare());
+    
+    // Advance time 100ms and submit job 3
+    advanceTime(100);
+    JobInProgress job3 = submitJob(JobStatus.RUNNING, 10, 10, "poolA");
+    JobInfo info3 = scheduler.infos.get(job3);
+    
+    // Check that minimum and fair shares have been allocated
+    assertEquals(2,    poolA.getMapSchedulable().getMinShare());
+    assertEquals(2,    poolA.getReduceSchedulable().getMinShare());
+    assertEquals(2.0,  info1.mapSchedulable.getFairShare());
+    assertEquals(2.0,  info1.reduceSchedulable.getFairShare());
+    assertEquals(1.0,  info2.mapSchedulable.getFairShare());
+    assertEquals(1.0,  info2.reduceSchedulable.getFairShare());
+    assertEquals(1.0,  info3.mapSchedulable.getFairShare());
+    assertEquals(1.0,  info3.reduceSchedulable.getFairShare());
+    
+    // Advance time
+    advanceTime(100);
+    
+    // Assign tasks and check that slots are first given to needy jobs, but
+    // that job 1 gets two tasks after due to having a larger share.
+    checkAssignment("tt1", "attempt_test_0002_m_000000_0 on tt1",
+                           "attempt_test_0002_r_000000_0 on tt1",
+                           "attempt_test_0003_m_000000_0 on tt1",
+                           "attempt_test_0003_r_000000_0 on tt1");
+    checkAssignment("tt2", "attempt_test_0001_m_000000_0 on tt2",
+                           "attempt_test_0001_r_000000_0 on tt2",
+                           "attempt_test_0001_m_000001_0 on tt2",
+                           "attempt_test_0001_r_000001_0 on tt2");
+  }
+  
+  /**
+   * This test starts by submitting two jobs at time 0:
+   * - job1 in the default pool
+   * - job2, with 1 map and 1 reduce, in poolA, which has an alloc of 4
+   *   maps and 4 reduces
+   * 
+   * When we assign the slots, job2 should only get 1 of each type of task.
+   */
+  public void testSmallJobInLargePool() throws Exception {
+    // Set up pools file
+    PrintWriter out = new PrintWriter(new FileWriter(ALLOC_FILE));
+    out.println("<?xml version=\"1.0\"?>");
+    out.println("<allocations>");
+    // Give pool A a minimum of 4 maps, 4 reduces
+    out.println("<pool name=\"poolA\">");
+    out.println("<minMaps>4</minMaps>");
+    out.println("<minReduces>4</minReduces>");
+    out.println("</pool>");
+    out.println("</allocations>");
+    out.close();
+    scheduler.getPoolManager().reloadAllocs();
+    
+    JobInProgress job1 = submitJob(JobStatus.RUNNING, 10, 10);
+    JobInfo info1 = scheduler.infos.get(job1);
+    JobInProgress job2 = submitJob(JobStatus.RUNNING, 1, 1, "poolA");
+    JobInfo info2 = scheduler.infos.get(job2);
+    
+    // Check scheduler variables
+    assertEquals(0,    info1.mapSchedulable.getRunningTasks());
+    assertEquals(0,    info1.reduceSchedulable.getRunningTasks());
+    assertEquals(10,   info1.mapSchedulable.getDemand());
+    assertEquals(10,   info1.reduceSchedulable.getDemand());
+    assertEquals(3.0,  info1.mapSchedulable.getFairShare());
+    assertEquals(3.0,  info1.reduceSchedulable.getFairShare());
+    assertEquals(0,    info2.mapSchedulable.getRunningTasks());
+    assertEquals(0,    info2.reduceSchedulable.getRunningTasks());
+    assertEquals(1,    info2.mapSchedulable.getDemand());
+    assertEquals(1,    info2.reduceSchedulable.getDemand());
+    assertEquals(1.0,  info2.mapSchedulable.getFairShare());
+    assertEquals(1.0,  info2.reduceSchedulable.getFairShare());
+    
+    // Assign tasks and check that slots are first given to needy jobs
+    checkAssignment("tt1", "attempt_test_0002_m_000000_0 on tt1");
+    checkAssignment("tt1", "attempt_test_0002_r_000000_0 on tt1");
+    checkAssignment("tt1", "attempt_test_0001_m_000000_0 on tt1");
+    checkAssignment("tt1", "attempt_test_0001_r_000000_0 on tt1");
+    checkAssignment("tt2", "attempt_test_0001_m_000001_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0001_r_000001_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0001_m_000002_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0001_r_000002_0 on tt2");
+  }
+  
+  /**
+   * This test starts by submitting four jobs in the default pool. However, the
+   * maxRunningJobs limit for this pool has been set to two. We should see only
+   * the first two jobs get scheduled, each with half the total slots.
+   */
+  public void testPoolMaxJobs() throws Exception {
+    // Set up pools file
+    PrintWriter out = new PrintWriter(new FileWriter(ALLOC_FILE));
+    out.println("<?xml version=\"1.0\"?>");
+    out.println("<allocations>");
+    out.println("<pool name=\"default\">");
+    out.println("<maxRunningJobs>2</maxRunningJobs>");
+    out.println("</pool>");
+    out.println("</allocations>");
+    out.close();
+    scheduler.getPoolManager().reloadAllocs();
+    
+    // Submit jobs, advancing time in-between to make sure that they are
+    // all submitted at distinct times.
+    JobInProgress job1 = submitJobNotInitialized(JobStatus.PREP, 10, 10);
+    assertTrue(((FakeJobInProgress)job1).inited());
+    job1.getStatus().setRunState(JobStatus.RUNNING);
+    JobInfo info1 = scheduler.infos.get(job1);
+    advanceTime(10);
+    JobInProgress job2 = submitJobNotInitialized(JobStatus.PREP, 10, 10);
+    assertTrue(((FakeJobInProgress)job2).inited());
+    job2.getStatus().setRunState(JobStatus.RUNNING);
+    JobInfo info2 = scheduler.infos.get(job2);
+    advanceTime(10);
+    JobInProgress job3 = submitJobNotInitialized(JobStatus.PREP, 10, 10);
+    JobInfo info3 = scheduler.infos.get(job3);
+    advanceTime(10);
+    JobInProgress job4 = submitJobNotInitialized(JobStatus.PREP, 10, 10);
+    JobInfo info4 = scheduler.infos.get(job4);
+    
+    // Only two of the jobs should be initialized.
+    assertTrue(((FakeJobInProgress)job1).inited());
+    assertTrue(((FakeJobInProgress)job2).inited());
+    assertFalse(((FakeJobInProgress)job3).inited());
+    assertFalse(((FakeJobInProgress)job4).inited());
+    
+    // Check scheduler variables
+    assertEquals(2.0,  info1.mapSchedulable.getFairShare());
+    assertEquals(2.0,  info1.reduceSchedulable.getFairShare());
+    assertEquals(2.0,  info2.mapSchedulable.getFairShare());
+    assertEquals(2.0,  info2.reduceSchedulable.getFairShare());
+    assertEquals(0.0,  info3.mapSchedulable.getFairShare());
+    assertEquals(0.0,  info3.reduceSchedulable.getFairShare());
+    assertEquals(0.0,  info4.mapSchedulable.getFairShare());
+    assertEquals(0.0,  info4.reduceSchedulable.getFairShare());
+    
+    // Assign tasks and check that only jobs 1 and 2 get them
+    checkAssignment("tt1", "attempt_test_0001_m_000000_0 on tt1");
+    checkAssignment("tt1", "attempt_test_0001_r_000000_0 on tt1");
+    checkAssignment("tt1", "attempt_test_0002_m_000000_0 on tt1");
+    checkAssignment("tt1", "attempt_test_0002_r_000000_0 on tt1");
+    advanceTime(100);
+    checkAssignment("tt2", "attempt_test_0001_m_000001_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0001_r_000001_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0002_m_000001_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0002_r_000001_0 on tt2");
+  }
+
+  /**
+   * This test starts by submitting two jobs by user "user1" to the default
+   * pool, and two jobs by "user2". We set user1's job limit to 1. We should
+   * see one job from user1 and two from user2. 
+   */
+  public void testUserMaxJobs() throws Exception {
+    // Set up pools file
+    PrintWriter out = new PrintWriter(new FileWriter(ALLOC_FILE));
+    out.println("<?xml version=\"1.0\"?>");
+    out.println("<allocations>");
+    out.println("<user name=\"user1\">");
+    out.println("<maxRunningJobs>1</maxRunningJobs>");
+    out.println("</user>");
+    out.println("</allocations>");
+    out.close();
+    scheduler.getPoolManager().reloadAllocs();
+    
+    // Submit jobs, advancing time in-between to make sure that they are
+    // all submitted at distinct times.
+    JobInProgress job1 = submitJob(JobStatus.RUNNING, 10, 10);
+    job1.getJobConf().set("user.name", "user1");
+    JobInfo info1 = scheduler.infos.get(job1);
+    advanceTime(10);
+    JobInProgress job2 = submitJob(JobStatus.RUNNING, 10, 10);
+    job2.getJobConf().set("user.name", "user1");
+    JobInfo info2 = scheduler.infos.get(job2);
+    advanceTime(10);
+    JobInProgress job3 = submitJob(JobStatus.RUNNING, 10, 10);
+    job3.getJobConf().set("user.name", "user2");
+    JobInfo info3 = scheduler.infos.get(job3);
+    advanceTime(10);
+    JobInProgress job4 = submitJob(JobStatus.RUNNING, 10, 10);
+    job4.getJobConf().set("user.name", "user2");
+    JobInfo info4 = scheduler.infos.get(job4);
+    
+    // Check scheduler variables
+    assertEquals(1.33,  info1.mapSchedulable.getFairShare(), 0.1);
+    assertEquals(1.33,  info1.reduceSchedulable.getFairShare(), 0.1);
+    assertEquals(0.0,   info2.mapSchedulable.getFairShare());
+    assertEquals(0.0,   info2.reduceSchedulable.getFairShare());
+    assertEquals(1.33,  info3.mapSchedulable.getFairShare(), 0.1);
+    assertEquals(1.33,  info3.reduceSchedulable.getFairShare(), 0.1);
+    assertEquals(1.33,  info4.mapSchedulable.getFairShare(), 0.1);
+    assertEquals(1.33,  info4.reduceSchedulable.getFairShare(), 0.1);
+    
+    // Assign tasks and check that slots are given only to jobs 1, 3 and 4
+    checkAssignment("tt1", "attempt_test_0001_m_000000_0 on tt1");
+    checkAssignment("tt1", "attempt_test_0001_r_000000_0 on tt1");
+    checkAssignment("tt1", "attempt_test_0003_m_000000_0 on tt1");
+    checkAssignment("tt1", "attempt_test_0003_r_000000_0 on tt1");
+    advanceTime(100);
+    checkAssignment("tt2", "attempt_test_0004_m_000000_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0004_r_000000_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0001_m_000001_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0001_r_000001_0 on tt2");
+  }
+  
+  /**
+   * Test a combination of pool job limits and user job limits, the latter
+   * specified through both the userMaxJobsDefaults (for some users) and
+   * user-specific &lt;user&gt; elements in the allocations file. 
+   */
+  public void testComplexJobLimits() throws Exception {
+    // Set up pools file
+    PrintWriter out = new PrintWriter(new FileWriter(ALLOC_FILE));
+    out.println("<?xml version=\"1.0\"?>");
+    out.println("<allocations>");
+    out.println("<pool name=\"poolA\">");
+    out.println("<maxRunningJobs>1</maxRunningJobs>");
+    out.println("</pool>");
+    out.println("<user name=\"user1\">");
+    out.println("<maxRunningJobs>1</maxRunningJobs>");
+    out.println("</user>");
+    out.println("<user name=\"user2\">");
+    out.println("<maxRunningJobs>10</maxRunningJobs>");
+    out.println("</user>");
+    out.println("<userMaxJobsDefault>2</userMaxJobsDefault>");
+    out.println("</allocations>");
+    out.close();
+    scheduler.getPoolManager().reloadAllocs();
+    
+    // Submit jobs, advancing time in-between to make sure that they are
+    // all submitted at distinct times.
+    
+    // Two jobs for user1; only one should get to run
+    JobInProgress job1 = submitJob(JobStatus.RUNNING, 10, 10);
+    job1.getJobConf().set("user.name", "user1");
+    JobInfo info1 = scheduler.infos.get(job1);
+    advanceTime(10);
+    JobInProgress job2 = submitJob(JobStatus.RUNNING, 10, 10);
+    job2.getJobConf().set("user.name", "user1");
+    JobInfo info2 = scheduler.infos.get(job2);
+    advanceTime(10);
+    
+    // Three jobs for user2; all should get to run
+    JobInProgress job3 = submitJob(JobStatus.RUNNING, 10, 10);
+    job3.getJobConf().set("user.name", "user2");
+    JobInfo info3 = scheduler.infos.get(job3);
+    advanceTime(10);
+    JobInProgress job4 = submitJob(JobStatus.RUNNING, 10, 10);
+    job4.getJobConf().set("user.name", "user2");
+    JobInfo info4 = scheduler.infos.get(job4);
+    advanceTime(10);
+    JobInProgress job5 = submitJob(JobStatus.RUNNING, 10, 10);
+    job5.getJobConf().set("user.name", "user2");
+    JobInfo info5 = scheduler.infos.get(job5);
+    advanceTime(10);
+    
+    // Three jobs for user3; only two should get to run
+    JobInProgress job6 = submitJob(JobStatus.RUNNING, 10, 10);
+    job6.getJobConf().set("user.name", "user3");
+    JobInfo info6 = scheduler.infos.get(job6);
+    advanceTime(10);
+    JobInProgress job7 = submitJob(JobStatus.RUNNING, 10, 10);
+    job7.getJobConf().set("user.name", "user3");
+    JobInfo info7 = scheduler.infos.get(job7);
+    advanceTime(10);
+    JobInProgress job8 = submitJob(JobStatus.RUNNING, 10, 10);
+    job8.getJobConf().set("user.name", "user3");
+    JobInfo info8 = scheduler.infos.get(job8);
+    advanceTime(10);
+    
+    // Two jobs for user4, in poolA; only one should get to run
+    JobInProgress job9 = submitJob(JobStatus.RUNNING, 10, 10, "poolA");
+    job9.getJobConf().set("user.name", "user4");
+    JobInfo info9 = scheduler.infos.get(job9);
+    advanceTime(10);
+    JobInProgress job10 = submitJob(JobStatus.RUNNING, 10, 10, "poolA");
+    job10.getJobConf().set("user.name", "user4");
+    JobInfo info10 = scheduler.infos.get(job10);
+    advanceTime(10);
+    
+    // Check scheduler variables. The jobs in poolA should get half
+    // the total share, while those in the default pool should get
+    // the other half. This works out to 2 slots each for the jobs
+    // in poolA and 1/3 each for the jobs in the default pool because
+    // there are 2 runnable jobs in poolA and 6 jobs in the default pool.
+    assertEquals(0.33,   info1.mapSchedulable.getFairShare(), 0.1);
+    assertEquals(0.33,   info1.reduceSchedulable.getFairShare(), 0.1);
+    assertEquals(0.0,    info2.mapSchedulable.getFairShare());
+    assertEquals(0.0,    info2.reduceSchedulable.getFairShare());
+    assertEquals(0.33,   info3.mapSchedulable.getFairShare(), 0.1);
+    assertEquals(0.33,   info3.reduceSchedulable.getFairShare(), 0.1);
+    assertEquals(0.33,   info4.mapSchedulable.getFairShare(), 0.1);
+    assertEquals(0.33,   info4.reduceSchedulable.getFairShare(), 0.1);
+    assertEquals(0.33,   info5.mapSchedulable.getFairShare(), 0.1);
+    assertEquals(0.33,   info5.reduceSchedulable.getFairShare(), 0.1);
+    assertEquals(0.33,   info6.mapSchedulable.getFairShare(), 0.1);
+    assertEquals(0.33,   info6.reduceSchedulable.getFairShare(), 0.1);
+    assertEquals(0.33,   info7.mapSchedulable.getFairShare(), 0.1);
+    assertEquals(0.33,   info7.reduceSchedulable.getFairShare(), 0.1);
+    assertEquals(0.0,    info8.mapSchedulable.getFairShare());
+    assertEquals(0.0,    info8.reduceSchedulable.getFairShare());
+    assertEquals(2.0,    info9.mapSchedulable.getFairShare(), 0.1);
+    assertEquals(2.0,    info9.reduceSchedulable.getFairShare(), 0.1);
+    assertEquals(0.0,    info10.mapSchedulable.getFairShare());
+    assertEquals(0.0,    info10.reduceSchedulable.getFairShare());
+  }
+  
+  public void testSizeBasedWeight() throws Exception {
+    scheduler.sizeBasedWeight = true;
+    JobInProgress job1 = submitJob(JobStatus.RUNNING, 2, 10);
+    JobInProgress job2 = submitJob(JobStatus.RUNNING, 20, 1);
+    assertTrue(scheduler.infos.get(job2).mapSchedulable.getFairShare() >
+               scheduler.infos.get(job1).mapSchedulable.getFairShare());
+    assertTrue(scheduler.infos.get(job1).reduceSchedulable.getFairShare() >
+               scheduler.infos.get(job2).reduceSchedulable.getFairShare());
+  }
+
+  /**
+   * This test submits jobs in three pools: poolA, which has a weight
+   * of 2.0; poolB, which has a weight of 0.5; and the default pool, which
+   * should have a weight of 1.0. It then checks that the map and reduce
+   * fair shares are given out accordingly. We then submit a second job to
+   * pool B and check that each gets half of the pool (weight of 0.25).
+   */
+  public void testPoolWeights() throws Exception {
+    // Set up pools file
+    PrintWriter out = new PrintWriter(new FileWriter(ALLOC_FILE));
+    out.println("<?xml version=\"1.0\"?>");
+    out.println("<allocations>");
+    out.println("<pool name=\"poolA\">");
+    out.println("<weight>2.0</weight>");
+    out.println("</pool>");
+    out.println("<pool name=\"poolB\">");
+    out.println("<weight>0.5</weight>");
+    out.println("</pool>");
+    out.println("</allocations>");
+    out.close();
+    scheduler.getPoolManager().reloadAllocs();
+    
+    // Submit jobs, advancing time in-between to make sure that they are
+    // all submitted at distinct times.
+    JobInProgress job1 = submitJob(JobStatus.RUNNING, 10, 10);
+    JobInfo info1 = scheduler.infos.get(job1);
+    JobInProgress job2 = submitJob(JobStatus.RUNNING, 10, 10, "poolA");
+    JobInfo info2 = scheduler.infos.get(job2);
+    JobInProgress job3 = submitJob(JobStatus.RUNNING, 10, 10, "poolB");
+    JobInfo info3 = scheduler.infos.get(job3);
+    advanceTime(10);
+    
+    assertEquals(1.14,  info1.mapSchedulable.getFairShare(), 0.01);
+    assertEquals(1.14,  info1.reduceSchedulable.getFairShare(), 0.01);
+    assertEquals(2.28,  info2.mapSchedulable.getFairShare(), 0.01);
+    assertEquals(2.28,  info2.reduceSchedulable.getFairShare(), 0.01);
+    assertEquals(0.57,  info3.mapSchedulable.getFairShare(), 0.01);
+    assertEquals(0.57,  info3.reduceSchedulable.getFairShare(), 0.01);
+    
+    JobInProgress job4 = submitJob(JobStatus.RUNNING, 10, 10, "poolB");
+    JobInfo info4 = scheduler.infos.get(job4);
+    advanceTime(10);
+    
+    assertEquals(1.14,  info1.mapSchedulable.getFairShare(), 0.01);
+    assertEquals(1.14,  info1.reduceSchedulable.getFairShare(), 0.01);
+    assertEquals(2.28,  info2.mapSchedulable.getFairShare(), 0.01);
+    assertEquals(2.28,  info2.reduceSchedulable.getFairShare(), 0.01);
+    assertEquals(0.28,  info3.mapSchedulable.getFairShare(), 0.01);
+    assertEquals(0.28,  info3.reduceSchedulable.getFairShare(), 0.01);
+    assertEquals(0.28,  info4.mapSchedulable.getFairShare(), 0.01);
+    assertEquals(0.28,  info4.reduceSchedulable.getFairShare(), 0.01);
+    verifyMetrics();    
+  }
+
+  /**
+   * This test submits jobs in two pools, poolA and poolB. None of the
+   * jobs in poolA have maps, but this should not affect their reduce
+   * share.
+   */
+  public void testPoolWeightsWhenNoMaps() throws Exception {
+    // Set up pools file
+    PrintWriter out = new PrintWriter(new FileWriter(ALLOC_FILE));
+    out.println("<?xml version=\"1.0\"?>");
+    out.println("<allocations>");
+    out.println("<pool name=\"poolA\">");
+    out.println("<weight>2.0</weight>");
+    out.println("</pool>");
+    out.println("<pool name=\"poolB\">");
+    out.println("<weight>1.0</weight>");
+    out.println("</pool>");
+    out.println("</allocations>");
+    out.close();
+    scheduler.getPoolManager().reloadAllocs();
+    
+    // Submit jobs, advancing time in-between to make sure that they are
+    // all submitted at distinct times.
+    JobInProgress job1 = submitJob(JobStatus.RUNNING, 0, 10, "poolA");
+    JobInfo info1 = scheduler.infos.get(job1);
+    JobInProgress job2 = submitJob(JobStatus.RUNNING, 0, 10, "poolA");
+    JobInfo info2 = scheduler.infos.get(job2);
+    JobInProgress job3 = submitJob(JobStatus.RUNNING, 10, 10, "poolB");
+    JobInfo info3 = scheduler.infos.get(job3);
+    advanceTime(10);
+    
+    /*
+    assertEquals(0,     info1.mapWeight, 0.01);
+    assertEquals(1.0,   info1.reduceWeight, 0.01);
+    assertEquals(0,     info2.mapWeight, 0.01);
+    assertEquals(1.0,   info2.reduceWeight, 0.01);
+    assertEquals(1.0,   info3.mapWeight, 0.01);
+    assertEquals(1.0,   info3.reduceWeight, 0.01);
+    */
+    
+    assertEquals(0,     info1.mapSchedulable.getFairShare(), 0.01);
+    assertEquals(1.33,  info1.reduceSchedulable.getFairShare(), 0.01);
+    assertEquals(0,     info2.mapSchedulable.getFairShare(), 0.01);
+    assertEquals(1.33,  info2.reduceSchedulable.getFairShare(), 0.01);
+    assertEquals(4,     info3.mapSchedulable.getFairShare(), 0.01);
+    assertEquals(1.33,  info3.reduceSchedulable.getFairShare(), 0.01);
+  }
+
+  public void testPoolMaxMapsReduces() throws Exception {
+    PrintWriter out = new PrintWriter(new FileWriter(ALLOC_FILE));
+    out.println("<?xml version=\"1.0\"?>");
+    out.println("<allocations>");
+    // Pool with upper bound
+    out.println("<pool name=\"poolLimited\">");
+    out.println("<weight>1.0</weight>");
+    out.println("<maxMaps>2</maxMaps>");
+    out.println("<maxReduces>1</maxReduces>");
+    out.println("</pool>");
+    out.println("</allocations>");
+    out.close();
+    scheduler.getPoolManager().reloadAllocs();
+    // Create two jobs with ten maps
+    JobInProgress job1 = submitJob(JobStatus.RUNNING, 10, 5, "poolLimited");
+    advanceTime(10);
+    JobInProgress job2 = submitJob(JobStatus.RUNNING, 10, 5);
+    checkAssignment("tt1", "attempt_test_0002_m_000000_0 on tt1");
+    checkAssignment("tt1", "attempt_test_0002_r_000000_0 on tt1");
+    checkAssignment("tt1", "attempt_test_0001_m_000000_0 on tt1");
+    checkAssignment("tt1", "attempt_test_0001_r_000000_0 on tt1");
+    checkAssignment("tt2", "attempt_test_0002_m_000001_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0002_r_000001_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0001_m_000001_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0002_r_000002_0 on tt2");
+
+    Pool limited = scheduler.getPoolManager().getPool("poolLimited");
+    assertEquals(2, limited.getSchedulable(TaskType.MAP).getRunningTasks());
+    assertEquals(1, limited.getSchedulable(TaskType.REDUCE).getRunningTasks());
+    Pool defaultPool = scheduler.getPoolManager().getPool("default");
+    assertEquals(2, defaultPool.getSchedulable(TaskType.MAP).getRunningTasks());
+    assertEquals(3, defaultPool.getSchedulable(TaskType.REDUCE)
+        .getRunningTasks());
+    assertEquals(2, job1.runningMapTasks);
+    assertEquals(1, job1.runningReduceTasks);
+    assertEquals(2, job2.runningMapTasks);
+    assertEquals(3, job2.runningReduceTasks);
+  }
+
+  /**
+   * Tests that max-running-tasks per node are set by assigning load
+   * equally accross the cluster in CapBasedLoadManager.
+   */
+  public void testCapBasedLoadManager() {
+    CapBasedLoadManager loadMgr = new CapBasedLoadManager();
+    // Arguments to getCap: totalRunnableTasks, nodeCap, totalSlots
+    // Desired behavior: return ceil(nodeCap * min(1, runnableTasks/totalSlots))
+    assertEquals(1, loadMgr.getCap(1, 1, 100));
+    assertEquals(1, loadMgr.getCap(1, 2, 100));
+    assertEquals(1, loadMgr.getCap(1, 10, 100));
+    assertEquals(1, loadMgr.getCap(200, 1, 100));
+    assertEquals(1, loadMgr.getCap(1, 5, 100));
+    assertEquals(3, loadMgr.getCap(50, 5, 100));
+    assertEquals(5, loadMgr.getCap(100, 5, 100));
+    assertEquals(5, loadMgr.getCap(200, 5, 100));
+  }
+
+  /**
+   * This test starts by launching a job in the default pool that takes
+   * all the slots in the cluster. We then submit a job in a pool with
+   * min share of 2 maps and 1 reduce task. After the min share preemption
+   * timeout, this pool should be allowed to preempt tasks. 
+   */
+  public void testMinSharePreemption() throws Exception {
+    // Enable preemption in scheduler
+    scheduler.preemptionEnabled = true;
+    // Set up pools file
+    PrintWriter out = new PrintWriter(new FileWriter(ALLOC_FILE));
+    out.println("<?xml version=\"1.0\"?>");
+    out.println("<allocations>");
+    // Give pool A a min share of 2 maps and 1 reduce, and a preemption
+    // timeout of 1 minute
+    out.println("<pool name=\"poolA\">");
+    out.println("<minMaps>2</minMaps>");
+    out.println("<minReduces>1</minReduces>");
+    out.println("<minSharePreemptionTimeout>60</minSharePreemptionTimeout>");
+    out.println("</pool>");
+    out.println("</allocations>");
+    out.close();
+    scheduler.getPoolManager().reloadAllocs();
+    Pool poolA = scheduler.getPoolManager().getPool("poolA");
+
+    // Submit job 1 and assign all slots to it. Sleep a bit before assigning
+    // tasks on tt1 and tt2 to ensure that the ones on tt2 get preempted first.
+    JobInProgress job1 = submitJob(JobStatus.RUNNING, 10, 10);
+    checkAssignment("tt1", "attempt_test_0001_m_000000_0 on tt1");
+    checkAssignment("tt1", "attempt_test_0001_r_000000_0 on tt1");
+    checkAssignment("tt1", "attempt_test_0001_m_000001_0 on tt1");
+    checkAssignment("tt1", "attempt_test_0001_r_000001_0 on tt1");
+    advanceTime(100);
+    checkAssignment("tt2", "attempt_test_0001_m_000002_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0001_r_000002_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0001_m_000003_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0001_r_000003_0 on tt2");
+    
+    // Ten seconds later, submit job 2.
+    advanceTime(10000);
+    JobInProgress job2 = submitJob(JobStatus.RUNNING, 10, 10, "poolA");
+    
+    // Ten seconds later, check that job 2 is not able to preempt tasks.
+    advanceTime(10000);
+    assertEquals(0, scheduler.tasksToPreempt(poolA.getMapSchedulable(),
+        clock.getTime()));
+    assertEquals(0, scheduler.tasksToPreempt(poolA.getReduceSchedulable(),
+        clock.getTime()));
+    
+    // Advance time by 49 more seconds, putting us at 59s after the
+    // submission of job 2. It should still not be able to preempt.
+    advanceTime(49000);
+    assertEquals(0, scheduler.tasksToPreempt(poolA.getMapSchedulable(),
+        clock.getTime()));
+    assertEquals(0, scheduler.tasksToPreempt(poolA.getReduceSchedulable(),
+        clock.getTime()));
+    
+    // Advance time by 2 seconds, putting us at 61s after the submission
+    // of job 2. It should now be able to preempt 2 maps and 1 reduce.
+    advanceTime(2000);
+    assertEquals(2, scheduler.tasksToPreempt(poolA.getMapSchedulable(),
+        clock.getTime()));
+    assertEquals(1, scheduler.tasksToPreempt(poolA.getReduceSchedulable(),
+        clock.getTime()));
+    
+    // Test that the tasks actually get preempted and we can assign new ones
+    scheduler.preemptTasksIfNecessary();
+    scheduler.update();
+    assertEquals(2, job1.runningMaps());
+    assertEquals(3, job1.runningReduces());
+    checkAssignment("tt2", "attempt_test_0002_m_000000_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0002_m_000001_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0002_r_000000_0 on tt2");
+    assertNull(scheduler.assignTasks(tracker("tt1")));
+    assertNull(scheduler.assignTasks(tracker("tt2")));
+  }
+
+  /**
+   * This test starts by launching a job in the default pool that takes
+   * all the slots in the cluster. We then submit a job in a pool with
+   * min share of 3 maps and 3 reduce tasks, but which only actually
+   * needs 1 map and 2 reduces. We check that this pool does not prempt
+   * more than this many tasks despite its min share being higher. 
+   */
+  public void testMinSharePreemptionWithSmallJob() throws Exception {
+    // Enable preemption in scheduler
+    scheduler.preemptionEnabled = true;
+    // Set up pools file
+    PrintWriter out = new PrintWriter(new FileWriter(ALLOC_FILE));
+    out.println("<?xml version=\"1.0\"?>");
+    out.println("<allocations>");
+    // Give pool A a min share of 2 maps and 1 reduce, and a preemption
+    // timeout of 1 minute
+    out.println("<pool name=\"poolA\">");
+    out.println("<minMaps>3</minMaps>");
+    out.println("<minReduces>3</minReduces>");
+    out.println("<minSharePreemptionTimeout>60</minSharePreemptionTimeout>");
+    out.println("</pool>");
+    out.println("</allocations>");
+    out.close();
+    scheduler.getPoolManager().reloadAllocs();
+    Pool poolA = scheduler.getPoolManager().getPool("poolA");
+
+    // Submit job 1 and assign all slots to it. Sleep a bit before assigning
+    // tasks on tt1 and tt2 to ensure that the ones on tt2 get preempted first.
+    JobInProgress job1 = submitJob(JobStatus.RUNNING, 10, 10);
+    checkAssignment("tt1", "attempt_test_0001_m_000000_0 on tt1");
+    checkAssignment("tt1", "attempt_test_0001_r_000000_0 on tt1");
+    checkAssignment("tt1", "attempt_test_0001_m_000001_0 on tt1");
+    checkAssignment("tt1", "attempt_test_0001_r_000001_0 on tt1");
+    advanceTime(100);
+    checkAssignment("tt2", "attempt_test_0001_m_000002_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0001_r_000002_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0001_m_000003_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0001_r_000003_0 on tt2");
+    
+    // Ten seconds later, submit job 2.
+    advanceTime(10000);
+    JobInProgress job2 = submitJob(JobStatus.RUNNING, 1, 2, "poolA");
+    
+    // Advance time by 59 seconds and check that no preemption occurs.
+    advanceTime(59000);
+    assertEquals(0, scheduler.tasksToPreempt(poolA.getMapSchedulable(),
+        clock.getTime()));
+    assertEquals(0, scheduler.tasksToPreempt(poolA.getReduceSchedulable(),
+        clock.getTime()));
+    
+    // Advance time by 2 seconds, putting us at 61s after the submission
+    // of job 2. Job 2 should now preempt 1 map and 2 reduces.
+    advanceTime(2000);
+    assertEquals(1, scheduler.tasksToPreempt(poolA.getMapSchedulable(),
+        clock.getTime()));
+    assertEquals(2, scheduler.tasksToPreempt(poolA.getReduceSchedulable(),
+        clock.getTime()));
+
+    // Test that the tasks actually get preempted and we can assign new ones
+    scheduler.preemptTasksIfNecessary();
+    scheduler.update();
+    assertEquals(3, job1.runningMaps());
+    assertEquals(2, job1.runningReduces());
+    checkAssignment("tt2", "attempt_test_0002_r_000000_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0002_m_000000_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0002_r_000001_0 on tt2");
+    assertNull(scheduler.assignTasks(tracker("tt1")));
+    assertNull(scheduler.assignTasks(tracker("tt2")));
+  }
+
+  /**
+   * This test runs on a 4-node (8-slot) cluster to allow 3 pools with fair
+   * shares greater than 2 slots to coexist (which makes the half-fair-share 
+   * of each pool more than 1 so that fair share preemption can kick in). 
+   * 
+   * The test first starts job 1, which takes 6 map slots and 6 reduce slots,
+   * in pool 1.  We then submit job 2 in pool 2, which takes 2 slots of each
+   * type. Finally, we submit a third job, job 3 in pool3, which gets no slots. 
+   * At this point the fair share of each pool will be 8/3 ~= 2.7 slots. 
+   * Pool 1 will be above its fair share, pool 2 will be below it but at half
+   * fair share, and pool 3 will be below half fair share. Therefore pool 3 
+   * should preempt a task (after a timeout) but pools 1 and 2 shouldn't. 
+   */
+  public void testFairSharePreemption() throws Exception {
+    // Create a bigger cluster than normal (4 tasktrackers instead of 2)
+    setUpCluster(1, 4, false);
+    // Enable preemption in scheduler
+    scheduler.preemptionEnabled = true;
+    // Set up pools file with a fair share preemtion timeout of 1 minute
+    PrintWriter out = new PrintWriter(new FileWriter(ALLOC_FILE));
+    out.println("<?xml version=\"1.0\"?>");
+    out.println("<allocations>");
+    out.println("<fairSharePreemptionTimeout>60</fairSharePreemptionTimeout>");
+    out.println("</allocations>");
+    out.close();
+    scheduler.getPoolManager().reloadAllocs();
+    
+    // Grab pools (they'll be created even though they're not in the alloc file)
+    Pool pool1 = scheduler.getPoolManager().getPool("pool1");
+    Pool pool2 = scheduler.getPoolManager().getPool("pool2");
+    Pool pool3 = scheduler.getPoolManager().getPool("pool3");
+
+    // Submit job 1. We advance time by 100 between each task tracker
+    // assignment stage to ensure that the tasks from job1 on tt3 are the ones
+    // that are deterministically preempted first (being the latest launched
+    // tasks in an over-allocated job).
+    JobInProgress job1 = submitJob(JobStatus.RUNNING, 6, 6, "pool1");
+    advanceTime(100);
+    checkAssignment("tt1", "attempt_test_0001_m_000000_0 on tt1");
+    checkAssignment("tt1", "attempt_test_0001_r_000000_0 on tt1");
+    checkAssignment("tt1", "attempt_test_0001_m_000001_0 on tt1");
+    checkAssignment("tt1", "attempt_test_0001_r_000001_0 on tt1");
+    advanceTime(100);
+    checkAssignment("tt2", "attempt_test_0001_m_000002_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0001_r_000002_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0001_m_000003_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0001_r_000003_0 on tt2");
+    advanceTime(100);
+    checkAssignment("tt3", "attempt_test_0001_m_000004_0 on tt3");
+    checkAssignment("tt3", "attempt_test_0001_r_000004_0 on tt3");
+    checkAssignment("tt3", "attempt_test_0001_m_000005_0 on tt3");
+    checkAssignment("tt3", "attempt_test_0001_r_000005_0 on tt3");
+    advanceTime(100);
+    
+    // Submit job 2. It should get the last 2 slots.
+    JobInProgress job2 = submitJob(JobStatus.RUNNING, 10, 10, "pool2");
+    advanceTime(100);
+    checkAssignment("tt4", "attempt_test_0002_m_000000_0 on tt4");
+    checkAssignment("tt4", "attempt_test_0002_r_000000_0 on tt4");
+    checkAssignment("tt4", "attempt_test_0002_m_000001_0 on tt4");
+    checkAssignment("tt4", "attempt_test_0002_r_000001_0 on tt4");
+    
+    // Submit job 3.
+    JobInProgress job3 = submitJob(JobStatus.RUNNING, 10, 10, "pool3");
+    
+    // Check that after 59 seconds, neither pool can preempt
+    advanceTime(59000);
+    assertEquals(0, scheduler.tasksToPreempt(pool2.getMapSchedulable(),
+        clock.getTime()));
+    assertEquals(0, scheduler.tasksToPreempt(pool2.getReduceSchedulable(),
+        clock.getTime()));
+    assertEquals(0, scheduler.tasksToPreempt(pool3.getMapSchedulable(),
+        clock.getTime()));
+    assertEquals(0, scheduler.tasksToPreempt(pool3.getReduceSchedulable(),
+        clock.getTime()));
+    
+    // Wait 2 more seconds, so that job 3 has now been in the system for 61s.
+    // Now pool 3 should be able to preempt 2 tasks (its share of 2.7 rounded
+    // down to its floor), but pool 2 shouldn't.
+    advanceTime(2000);
+    assertEquals(0, scheduler.tasksToPreempt(pool2.getMapSchedulable(),
+        clock.getTime()));
+    assertEquals(0, scheduler.tasksToPreempt(pool2.getReduceSchedulable(),
+        clock.getTime()));
+    assertEquals(2, scheduler.tasksToPreempt(pool3.getMapSchedulable(),
+        clock.getTime()));
+    assertEquals(2, scheduler.tasksToPreempt(pool3.getReduceSchedulable(),
+        clock.getTime()));
+    
+    // Test that the tasks actually get preempted and we can assign new ones
+    scheduler.preemptTasksIfNecessary();
+    scheduler.update();
+    assertEquals(4, job1.runningMaps());
+    assertEquals(4, job1.runningReduces());
+    checkAssignment("tt3", "attempt_test_0003_m_000000_0 on tt3");
+    checkAssignment("tt3", "attempt_test_0003_r_000000_0 on tt3");
+    checkAssignment("tt3", "attempt_test_0003_m_000001_0 on tt3");
+    checkAssignment("tt3", "attempt_test_0003_r_000001_0 on tt3");
+    assertNull(scheduler.assignTasks(tracker("tt1")));
+    assertNull(scheduler.assignTasks(tracker("tt2")));
+    assertNull(scheduler.assignTasks(tracker("tt3")));
+    assertNull(scheduler.assignTasks(tracker("tt4")));
+  }
+  
+  /**
+   * This test runs on a 3-node (6-slot) cluster to allow 3 pools with fair
+   * shares equal 2 slots to coexist (which makes the half-fair-share 
+   * of each pool equal to 1 so that fair share preemption can kick in). 
+   * 
+   * The test first starts job 1, which takes 3 map slots and 0 reduce slots,
+   * in pool 1.  We then submit job 2 in pool 2, which takes 3 map slots and zero
+   * reduce slots. Finally, we submit a third job, job 3 in pool3, which gets no slots. 
+   * At this point the fair share of each pool will be 6/3 = 2 slots. 
+   * Pool 1 and 2 will be above their fair share and pool 3 will be below half fair share. 
+   * Therefore pool 3 should preempt tasks from both pool 1 & 2 (after a timeout) but 
+   * pools 1 and 2 shouldn't. 
+   */
+  public void testFairSharePreemptionFromMultiplePools() throws Exception {
+	// Create a bigger cluster than normal (3 tasktrackers instead of 2)
+	setUpCluster(1, 3, false);
+	// Enable preemption in scheduler
+	scheduler.preemptionEnabled = true;
+	// Set up pools file with a fair share preemtion timeout of 1 minute
+	PrintWriter out = new PrintWriter(new FileWriter(ALLOC_FILE));
+	out.println("<?xml version=\"1.0\"?>");
+	out.println("<allocations>");
+	out.println("<fairSharePreemptionTimeout>60</fairSharePreemptionTimeout>");
+	out.println("</allocations>");
+	out.close();
+	scheduler.getPoolManager().reloadAllocs();
+	 
+	// Grab pools (they'll be created even though they're not in the alloc file)
+	Pool pool1 = scheduler.getPoolManager().getPool("pool1");
+	Pool pool2 = scheduler.getPoolManager().getPool("pool2");
+	Pool pool3 = scheduler.getPoolManager().getPool("pool3");
+
+	// Submit job 1. We advance time by 100 between each task tracker
+	// assignment stage to ensure that the tasks from job1 on tt3 are the ones
+	// that are deterministically preempted first (being the latest launched
+	// tasks in an over-allocated job).
+	JobInProgress job1 = submitJob(JobStatus.RUNNING, 12, 0, "pool1");
+	advanceTime(100);
+	checkAssignment("tt1", "attempt_test_0001_m_000000_0 on tt1");
+	checkAssignment("tt1", "attempt_test_0001_m_000001_0 on tt1");
+	advanceTime(100);
+	checkAssignment("tt2", "attempt_test_0001_m_000002_0 on tt2");
+	advanceTime(100);
+	    
+	// Submit job 2. It should get the last 3 slots.
+	JobInProgress job2 = submitJob(JobStatus.RUNNING, 10, 0, "pool2");
+	advanceTime(100);
+	checkAssignment("tt2", "attempt_test_0002_m_000000_0 on tt2");
+	checkAssignment("tt3", "attempt_test_0002_m_000001_0 on tt3");
+	advanceTime(100);
+	checkAssignment("tt3", "attempt_test_0002_m_000002_0 on tt3");
+
+	advanceTime(100);
+	    
+	// Submit job 3.
+	JobInProgress job3 = submitJob(JobStatus.RUNNING, 10, 0, "pool3");
+	    
+	// Check that after 59 seconds, neither pool can preempt
+	advanceTime(59000);
+	assertEquals(0, scheduler.tasksToPreempt(pool2.getMapSchedulable(),
+			clock.getTime()));
+	assertEquals(0, scheduler.tasksToPreempt(pool2.getReduceSchedulable(),
+	        clock.getTime()));
+	assertEquals(0, scheduler.tasksToPreempt(pool3.getMapSchedulable(),
+	        clock.getTime()));
+	assertEquals(0, scheduler.tasksToPreempt(pool3.getReduceSchedulable(),
+	        clock.getTime()));
+	    
+	// Wait 2 more seconds, so that job 3 has now been in the system for 61s.
+	// Now pool 3 should be able to preempt 2 tasks (its share of 2 rounded
+	// down to its floor), but pool 1 & 2 shouldn't.
+	advanceTime(2000);
+	assertEquals(0, scheduler.tasksToPreempt(pool2.getMapSchedulable(),
+	        clock.getTime()));
+	assertEquals(0, scheduler.tasksToPreempt(pool2.getReduceSchedulable(),
+	        clock.getTime()));
+	assertEquals(2, scheduler.tasksToPreempt(pool3.getMapSchedulable(),
+	        clock.getTime()));
+	assertEquals(0, scheduler.tasksToPreempt(pool3.getReduceSchedulable(),
+	        clock.getTime()));
+	    
+	// Test that the tasks actually get preempted and we can assign new ones.
+	// This should preempt one task each from pool1 and pool2
+	scheduler.preemptTasksIfNecessary();
+	scheduler.update();
+	assertEquals(2, job2.runningMaps());  
+	assertEquals(2, job1.runningMaps());  
+	checkAssignment("tt2", "attempt_test_0003_m_000000_0 on tt2");
+	checkAssignment("tt3", "attempt_test_0003_m_000001_0 on tt3");
+	assertNull(scheduler.assignTasks(tracker("tt1")));
+	assertNull(scheduler.assignTasks(tracker("tt2")));
+	assertNull(scheduler.assignTasks(tracker("tt3")));
+  }
+  
+  
+  /**
+   * This test submits a job that takes all 4 slots, and then a second job in
+   * a pool that has both a min share of 2 slots with a 60s timeout and a
+   * fair share timeout of 60s. After 60 seconds, this pool will be starved
+   * of both min share (2 slots of each type) and fair share (2 slots of each
+   * type), and we test that it does not kill more than 2 tasks of each type
+   * in total.
+   */
+  public void testMinAndFairSharePreemption() throws Exception {
+    // Enable preemption in scheduler
+    scheduler.preemptionEnabled = true;
+    // Set up pools file
+    PrintWriter out = new PrintWriter(new FileWriter(ALLOC_FILE));
+    out.println("<?xml version=\"1.0\"?>");
+    out.println("<allocations>");
+    // Give pool A a min share of 2 maps and 1 reduce, and a preemption
+    // timeout of 1 minute
+    out.println("<pool name=\"poolA\">");
+    out.println("<minMaps>2</minMaps>");
+    out.println("<minReduces>2</minReduces>");
+    out.println("<minSharePreemptionTimeout>60</minSharePreemptionTimeout>");
+    out.println("</pool>");
+    out.println("<fairSharePreemptionTimeout>60</fairSharePreemptionTimeout>");
+    out.println("</allocations>");
+    out.close();
+    scheduler.getPoolManager().reloadAllocs();
+    Pool poolA = scheduler.getPoolManager().getPool("poolA");
+
+    // Submit job 1 and assign all slots to it. Sleep a bit before assigning
+    // tasks on tt1 and tt2 to ensure that the ones on tt2 get preempted first.
+    JobInProgress job1 = submitJob(JobStatus.RUNNING, 10, 10);
+    checkAssignment("tt1", "attempt_test_0001_m_000000_0 on tt1");
+    checkAssignment("tt1", "attempt_test_0001_r_000000_0 on tt1");
+    checkAssignment("tt1", "attempt_test_0001_m_000001_0 on tt1");
+    checkAssignment("tt1", "attempt_test_0001_r_000001_0 on tt1");
+    advanceTime(100);
+    checkAssignment("tt2", "attempt_test_0001_m_000002_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0001_r_000002_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0001_m_000003_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0001_r_000003_0 on tt2");
+    
+    // Ten seconds later, submit job 2.
+    advanceTime(10000);
+    JobInProgress job2 = submitJob(JobStatus.RUNNING, 10, 10, "poolA");
+    
+    // Ten seconds later, check that job 2 is not able to preempt tasks.
+    advanceTime(10000);
+    assertEquals(0, scheduler.tasksToPreempt(poolA.getMapSchedulable(),
+        clock.getTime()));
+    assertEquals(0, scheduler.tasksToPreempt(poolA.getReduceSchedulable(),
+        clock.getTime()));
+    
+    // Advance time by 49 more seconds, putting us at 59s after the
+    // submission of job 2. It should still not be able to preempt.
+    advanceTime(49000);
+    assertEquals(0, scheduler.tasksToPreempt(poolA.getMapSchedulable(),
+        clock.getTime()));
+    assertEquals(0, scheduler.tasksToPreempt(poolA.getReduceSchedulable(),
+        clock.getTime()));
+    
+    // Advance time by 2 seconds, putting us at 61s after the submission
+    // of job 2. It should now be able to preempt 2 maps and 1 reduce.
+    advanceTime(2000);
+    assertEquals(2, scheduler.tasksToPreempt(poolA.getMapSchedulable(),
+        clock.getTime()));
+    assertEquals(2, scheduler.tasksToPreempt(poolA.getReduceSchedulable(),
+        clock.getTime()));
+
+    // Test that the tasks actually get preempted and we can assign new ones
+    scheduler.preemptTasksIfNecessary();
+    scheduler.update();
+    assertEquals(2, job1.runningMaps());
+    assertEquals(2, job1.runningReduces());
+    checkAssignment("tt2", "attempt_test_0002_m_000000_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0002_r_000000_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0002_m_000001_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0002_r_000001_0 on tt2");
+    assertNull(scheduler.assignTasks(tracker("tt1")));
+    assertNull(scheduler.assignTasks(tracker("tt2")));
+  }
+  
+  /**
+   * This is a copy of testMinAndFairSharePreemption that turns preemption
+   * off and verifies that no tasks get killed.
+   */
+  public void testNoPreemptionIfDisabled() throws Exception {
+    // Set up pools file
+    PrintWriter out = new PrintWriter(new FileWriter(ALLOC_FILE));
+    out.println("<?xml version=\"1.0\"?>");
+    out.println("<allocations>");
+    // Give pool A a min share of 2 maps and 1 reduce, and a preemption
+    // timeout of 1 minute
+    out.println("<pool name=\"poolA\">");
+    out.println("<minMaps>2</minMaps>");
+    out.println("<minReduces>2</minReduces>");
+    out.println("<minSharePreemptionTimeout>60</minSharePreemptionTimeout>");
+    out.println("</pool>");
+    out.println("<fairSharePreemptionTimeout>60</fairSharePreemptionTimeout>");
+    out.println("</allocations>");
+    out.close();
+    scheduler.getPoolManager().reloadAllocs();
+
+    // Submit job 1 and assign all slots to it. Sleep a bit before assigning
+    // tasks on tt1 and tt2 to ensure that the ones on tt2 get preempted first.
+    JobInProgress job1 = submitJob(JobStatus.RUNNING, 10, 10);
+    checkAssignment("tt1", "attempt_test_0001_m_000000_0 on tt1");
+    checkAssignment("tt1", "attempt_test_0001_r_000000_0 on tt1");
+    checkAssignment("tt1", "attempt_test_0001_m_000001_0 on tt1");
+    checkAssignment("tt1", "attempt_test_0001_r_000001_0 on tt1");
+    advanceTime(100);
+    checkAssignment("tt2", "attempt_test_0001_m_000002_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0001_r_000002_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0001_m_000003_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0001_r_000003_0 on tt2");
+    
+    // Ten seconds later, submit job 2.
+    advanceTime(10000);
+    JobInProgress job2 = submitJob(JobStatus.RUNNING, 10, 10, "poolA");
+    
+    // Advance time by 61s, putting us past the preemption timeout,
+    // and check that no tasks get preempted.
+    advanceTime(61000);
+    scheduler.preemptTasksIfNecessary();
+    scheduler.update();
+    assertEquals(4, job1.runningMaps());
+    assertEquals(4, job1.runningReduces());
+    assertNull(scheduler.assignTasks(tracker("tt1")));
+    assertNull(scheduler.assignTasks(tracker("tt2")));
+  }
+
+  /**
+   * This is a copy of testMinAndFairSharePreemption that turns preemption
+   * on but also turns on mapred.fairscheduler.preemption.only.log (the
+   * "dry run" parameter for testing out preemption) and verifies that no
+   * tasks get killed.
+   */
+  public void testNoPreemptionIfOnlyLogging() throws Exception {
+    // Turn on preemption, but for logging only
+    scheduler.preemptionEnabled = true;
+    scheduler.onlyLogPreemption = true;
+    // Set up pools file
+    PrintWriter out = new PrintWriter(new FileWriter(ALLOC_FILE));
+    out.println("<?xml version=\"1.0\"?>");
+    out.println("<allocations>");
+    // Give pool A a min share of 2 maps and 1 reduce, and a preemption
+    // timeout of 1 minute
+    out.println("<pool name=\"poolA\">");
+    out.println("<minMaps>2</minMaps>");
+    out.println("<minReduces>2</minReduces>");
+    out.println("<minSharePreemptionTimeout>60</minSharePreemptionTimeout>");
+    out.println("</pool>");
+    out.println("<fairSharePreemptionTimeout>60</fairSharePreemptionTimeout>");
+    out.println("</allocations>");
+    out.close();
+    scheduler.getPoolManager().reloadAllocs();
+
+    // Submit job 1 and assign all slots to it. Sleep a bit before assigning
+    // tasks on tt1 and tt2 to ensure that the ones on tt2 get preempted first.
+    JobInProgress job1 = submitJob(JobStatus.RUNNING, 10, 10);
+    checkAssignment("tt1", "attempt_test_0001_m_000000_0 on tt1");
+    checkAssignment("tt1", "attempt_test_0001_r_000000_0 on tt1");
+    checkAssignment("tt1", "attempt_test_0001_m_000001_0 on tt1");
+    checkAssignment("tt1", "attempt_test_0001_r_000001_0 on tt1");
+    advanceTime(100);
+    checkAssignment("tt2", "attempt_test_0001_m_000002_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0001_r_000002_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0001_m_000003_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0001_r_000003_0 on tt2");
+    
+    // Ten seconds later, submit job 2.
+    advanceTime(10000);
+    JobInProgress job2 = submitJob(JobStatus.RUNNING, 10, 10, "poolA");
+    
+    // Advance time by 61s, putting us past the preemption timeout,
+    // and check that no tasks get preempted.
+    advanceTime(61000);
+    scheduler.preemptTasksIfNecessary();
+    scheduler.update();
+    assertEquals(4, job1.runningMaps());
+    assertEquals(4, job1.runningReduces());
+    assertNull(scheduler.assignTasks(tracker("tt1")));
+    assertNull(scheduler.assignTasks(tracker("tt2")));
+  }
+
+  /**
+   * This test exercises delay scheduling at the node level. We submit a job
+   * with data on rack1.node2 and check that it doesn't get assigned on earlier
+   * nodes. A second job with no locality info should get assigned instead.
+   * 
+   * TaskTracker names in this test map to nodes as follows:
+   * - tt1 = rack1.node1
+   * - tt2 = rack1.node2
+   * - tt3 = rack2.node1
+   * - tt4 = rack2.node2
+   */
+  public void testDelaySchedulingAtNodeLevel() throws IOException {
+    setUpCluster(2, 2, true);
+    scheduler.assignMultiple = true;
+    
+    JobInProgress job1 = submitJob(JobStatus.RUNNING, 1, 0, "pool1",
+        new String[][] {
+          {"rack2.node2"}
+        }, true);
+    JobInfo info1 = scheduler.infos.get(job1);
+    
+    // Advance time before submitting another job j2, to make j1 be ahead
+    // of j2 in the queue deterministically.
+    advanceTime(100);
+    JobInProgress job2 = submitJob(JobStatus.RUNNING, 10, 0);
+    
+    // Assign tasks on nodes 1-3 and check that j2 gets them
+    checkAssignment("tt1", "attempt_test_0002_m_000000_0 on tt1", 
+                           "attempt_test_0002_m_000001_0 on tt1");
+    checkAssignment("tt2", "attempt_test_0002_m_000002_0 on tt2",
+                           "attempt_test_0002_m_000003_0 on tt2");
+    checkAssignment("tt3", "attempt_test_0002_m_000004_0 on tt3",
+                           "attempt_test_0002_m_000005_0 on tt3");
+    
+    // Assign a task on node 4 now and check that j1 gets it. The other slot
+    // on the node should be given to j2 because j1 will be out of tasks.
+    checkAssignment("tt4", "attempt_test_0001_m_000000_0 on tt4",
+                           "attempt_test_0002_m_000006_0 on tt4");
+    
+    // Check that delay scheduling info is properly set
+    assertEquals(info1.lastMapLocalityLevel, LocalityLevel.NODE);
+    assertEquals(info1.timeWaitedForLocalMap, 0);
+    assertEquals(info1.skippedAtLastHeartbeat, false);
+  }
+  
+  /**
+   * This test submits a job and causes it to exceed its node-level delay,
+   * and thus to go on to launch a rack-local task. We submit one job with data
+   * on rack2.node4 and check that it does not get assigned on any of the other
+   * nodes until 10 seconds (the delay configured in setUpCluster) pass.
+   * Finally, after some delay, we let the job assign local tasks and check
+   * that it has returned to waiting for node locality.
+   * 
+   * TaskTracker names in this test map to nodes as follows:
+   * - tt1 = rack1.node1
+   * - tt2 = rack1.node2
+   * - tt3 = rack2.node1
+   * - tt4 = rack2.node2
+   */
+  public void testDelaySchedulingAtRackLevel() throws IOException {
+    setUpCluster(2, 2, true);
+    scheduler.assignMultiple = true;
+    
+    JobInProgress job1 = submitJob(JobStatus.RUNNING, 4, 0, "pool1",
+        new String[][] {
+          {"rack2.node2"}, {"rack2.node2"}, {"rack2.node2"}, {"rack2.node2"}
+        }, true);
+    JobInfo info1 = scheduler.infos.get(job1);
+    
+    // Advance time before submitting another job j2, to make j1 be ahead
+    // of j2 in the queue deterministically.
+    advanceTime(100);
+    JobInProgress job2 = submitJob(JobStatus.RUNNING, 20, 0);
+    
+    // Assign tasks on nodes 1-3 and check that j2 gets them
+    checkAssignment("tt1", "attempt_test_0002_m_000000_0 on tt1", 
+                           "attempt_test_0002_m_000001_0 on tt1");
+    checkAssignment("tt2", "attempt_test_0002_m_000002_0 on tt2",
+                           "attempt_test_0002_m_000003_0 on tt2");
+    checkAssignment("tt3", "attempt_test_0002_m_000004_0 on tt3",
+                           "attempt_test_0002_m_000005_0 on tt3");
+    
+    // Advance time by 6 seconds to put us past the 5-second node locality delay
+    advanceTime(6000);
+    
+    // Finish some tasks on each node
+    taskTrackerManager.finishTask("tt1", "attempt_test_0002_m_000000_0");
+    taskTrackerManager.finishTask("tt2", "attempt_test_0002_m_000002_0");
+    taskTrackerManager.finishTask("tt3", "attempt_test_0002_m_000004_0");
+    advanceTime(100);
+    
+    // Check that job 1 is only assigned on node 3 (which is rack-local)
+    checkAssignment("tt1", "attempt_test_0002_m_000006_0 on tt1");
+    checkAssignment("tt2", "attempt_test_0002_m_000007_0 on tt2");
+    checkAssignment("tt3", "attempt_test_0001_m_000000_0 on tt3");
+    
+    // Check that delay scheduling info is properly set
+    assertEquals(info1.lastMapLocalityLevel, LocalityLevel.RACK);
+    assertEquals(info1.timeWaitedForLocalMap, 0);
+    assertEquals(info1.skippedAtLastHeartbeat, false);
+    
+    // Also give job 1 some tasks on node 4. Its lastMapLocalityLevel
+    // should go back to 0 after it gets assigned these.
+    checkAssignment("tt4", "attempt_test_0001_m_000001_0 on tt4",
+                           "attempt_test_0001_m_000002_0 on tt4");
+    
+    // Check that delay scheduling info is properly set
+    assertEquals(info1.lastMapLocalityLevel, LocalityLevel.NODE);
+    assertEquals(info1.timeWaitedForLocalMap, 0);
+    assertEquals(info1.skippedAtLastHeartbeat, false);
+    
+    // Check that job 1 no longer assigns tasks in the same rack now
+    // that it has obtained a node-local task
+    taskTrackerManager.finishTask("tt1", "attempt_test_0002_m_000001_0");
+    taskTrackerManager.finishTask("tt2", "attempt_test_0002_m_000003_0");
+    taskTrackerManager.finishTask("tt3", "attempt_test_0002_m_000005_0");
+    advanceTime(100);
+    checkAssignment("tt1", "attempt_test_0002_m_000008_0 on tt1");
+    checkAssignment("tt2", "attempt_test_0002_m_000009_0 on tt2");
+    checkAssignment("tt3", "attempt_test_0002_m_000010_0 on tt3");
+    advanceTime(100);
+    
+    // However, job 1 should still be able to launch tasks on node 4
+    taskTrackerManager.finishTask("tt4", "attempt_test_0001_m_000001_0");
+    advanceTime(100);
+    checkAssignment("tt4", "attempt_test_0001_m_000003_0 on tt4");
+  }
+  
+  /**
+   * This test submits a job and causes it to exceed its node-level delay,
+   * then its rack-level delay. It should then launch tasks off-rack.
+   * However, once the job gets a rack-local slot it should stay in-rack,
+   * and once it gets a node-local slot it should stay in-node.
+   * For simplicity, we don't submit a second job in this test.
+   * 
+   * TaskTracker names in this test map to nodes as follows:
+   * - tt1 = rack1.node1
+   * - tt2 = rack1.node2
+   * - tt3 = rack2.node1
+   * - tt4 = rack2.node2
+   */
+  public void testDelaySchedulingOffRack() throws IOException {
+    setUpCluster(2, 2, true);
+    scheduler.assignMultiple = true;
+    
+    JobInProgress job1 = submitJob(JobStatus.RUNNING, 8, 0, "pool1",
+        new String[][] {
+          {"rack2.node2"}, {"rack2.node2"}, {"rack2.node2"}, {"rack2.node2"},
+          {"rack2.node2"}, {"rack2.node2"}, {"rack2.node2"}, {"rack2.node2"},
+        }, true);
+    JobInfo info1 = scheduler.infos.get(job1);
+    advanceTime(100);
+    
+    // Check that nothing is assigned on trackers 1-3
+    assertNull(scheduler.assignTasks(tracker("tt1")));
+    assertNull(scheduler.assignTasks(tracker("tt2")));
+    assertNull(scheduler.assignTasks(tracker("tt3")));
+    
+    // Advance time by 6 seconds to put us past the 5-sec node locality delay
+    advanceTime(6000);
+
+    // Check that nothing is assigned on trackers 1-2; the job would assign
+    // a task on tracker 3 (rack1.node2) so we skip that one 
+    assertNull(scheduler.assignTasks(tracker("tt1")));
+    assertNull(scheduler.assignTasks(tracker("tt2")));
+    
+    // Repeat to see that receiving multiple heartbeats works
+    advanceTime(100);
+    assertNull(scheduler.assignTasks(tracker("tt1")));
+    assertNull(scheduler.assignTasks(tracker("tt2")));
+    advanceTime(100);
+    assertNull(scheduler.assignTasks(tracker("tt1")));
+    assertNull(scheduler.assignTasks(tracker("tt2")));
+
+    // Check that delay scheduling info is properly set
+    assertEquals(info1.lastMapLocalityLevel, LocalityLevel.NODE);
+    assertEquals(info1.timeWaitedForLocalMap, 6200);
+    assertEquals(info1.skippedAtLastHeartbeat, true);
+    
+    // Advance time by 11 seconds to put us past the 10-sec rack locality delay
+    advanceTime(11000);
+    
+    // Now the job should be able to assign tasks on tt1 and tt2
+    checkAssignment("tt1", "attempt_test_0001_m_000000_0 on tt1",
+                           "attempt_test_0001_m_000001_0 on tt1");
+    checkAssignment("tt2", "attempt_test_0001_m_000002_0 on tt2",
+                           "attempt_test_0001_m_000003_0 on tt2");
+
+    // Check that delay scheduling info is properly set
+    assertEquals(info1.lastMapLocalityLevel, LocalityLevel.ANY);
+    assertEquals(info1.timeWaitedForLocalMap, 0);
+    assertEquals(info1.skippedAtLastHeartbeat, false);
+    
+    // Now assign a task on tt3. This should make the job stop assigning
+    // on tt1 and tt2 (checked after we finish some tasks there)
+    checkAssignment("tt3", "attempt_test_0001_m_000004_0 on tt3",
+                           "attempt_test_0001_m_000005_0 on tt3");
+
+    // Check that delay scheduling info is properly set
+    assertEquals(info1.lastMapLocalityLevel, LocalityLevel.RACK);
+    assertEquals(info1.timeWaitedForLocalMap, 0);
+    assertEquals(info1.skippedAtLastHeartbeat, false);
+    
+    // Check that j1 no longer assigns tasks on rack 1 now
+    taskTrackerManager.finishTask("tt1", "attempt_test_0001_m_000001_0");
+    taskTrackerManager.finishTask("tt2", "attempt_test_0001_m_000003_0");
+    advanceTime(100);
+    assertNull(scheduler.assignTasks(tracker("tt1")));
+    assertNull(scheduler.assignTasks(tracker("tt2")));
+    
+    // However, tasks on rack 2 should still be assigned
+    taskTrackerManager.finishTask("tt3", "attempt_test_0001_m_000004_0");
+    advanceTime(100);
+    checkAssignment("tt3", "attempt_test_0001_m_000006_0 on tt3");
+    
+    // Now assign a task on node 4
+    checkAssignment("tt4", "attempt_test_0001_m_000007_0 on tt4");
+
+    // Check that delay scheduling info is set so we are looking for node-local
+    // tasks at this point
+    assertEquals(info1.lastMapLocalityLevel, LocalityLevel.NODE);
+    assertEquals(info1.timeWaitedForLocalMap, 0);
+    assertEquals(info1.skippedAtLastHeartbeat, false);
+  }
+  
+  /**
+   * This test submits two jobs with 4 maps and 3 reduces in total to a
+   * 4-node cluster. Although the cluster has 2 map slots and 2 reduce
+   * slots per node, it should only launch one map and one reduce on each
+   * node to balance the load. We check that this happens even if
+   * assignMultiple is set to true so the scheduler has the opportunity
+   * to launch multiple tasks per heartbeat.
+   */
+  public void testAssignMultipleWithUnderloadedCluster() throws IOException {
+    setUpCluster(1, 4, true);
+    JobInProgress job1 = submitJob(JobStatus.RUNNING, 2, 2);
+    
+    // Advance to make j1 be scheduled before j2 deterministically.
+    advanceTime(100);
+    JobInProgress job2 = submitJob(JobStatus.RUNNING, 2, 1);
+    
+    // Assign tasks and check that at most one map and one reduce slot is used
+    // on each node, and that no tasks are assigned on subsequent heartbeats
+    checkAssignment("tt1", "attempt_test_0001_m_000000_0 on tt1",
+                           "attempt_test_0001_r_000000_0 on tt1");
+    assertNull(scheduler.assignTasks(tracker("tt1")));
+    checkAssignment("tt2", "attempt_test_0002_m_000000_0 on tt2",
+                           "attempt_test_0002_r_000000_0 on tt2");
+    assertNull(scheduler.assignTasks(tracker("tt2")));
+    checkAssignment("tt3", "attempt_test_0001_m_000001_0 on tt3",
+                           "attempt_test_0001_r_000001_0 on tt3");
+    assertNull(scheduler.assignTasks(tracker("tt3")));
+    checkAssignment("tt4", "attempt_test_0002_m_000001_0 on tt4");
+    assertNull(scheduler.assignTasks(tracker("tt4")));
+  }
+  
+  /**
+   * This test submits four jobs in the default pool, which is set to FIFO mode:
+   * - job1, with 1 map and 1 reduce
+   * - job2, with 3 maps and 3 reduces
+   * - job3, with 1 map, 1 reduce, and priority set to HIGH
+   * - job4, with 3 maps and 3 reduces
+   * 
+   * We check that the scheduler assigns tasks first to job3 (because it is
+   * high priority), then to job1, then to job2.
+   */
+  public void testFifoPool() throws Exception {
+    // Set up pools file
+    PrintWriter out = new PrintWriter(new FileWriter(ALLOC_FILE));
+    out.println("<?xml version=\"1.0\"?>");
+    out.println("<allocations>");
+    out.println("<pool name=\"default\">");
+    out.println("<schedulingMode>fifo</schedulingMode>");
+    out.println("</pool>");
+    out.println("</allocations>");
+    out.close();
+    scheduler.getPoolManager().reloadAllocs();
+    
+    // Submit jobs, advancing time in-between to make sure that they are
+    // all submitted at distinct times.
+    JobInProgress job1 = submitJob(JobStatus.RUNNING, 1, 1);
+    advanceTime(10);
+    JobInProgress job2 = submitJob(JobStatus.RUNNING, 3, 3);
+    advanceTime(10);
+    JobInProgress job3 = submitJob(JobStatus.RUNNING, 1, 1);
+    job3.setPriority(JobPriority.HIGH);
+    advanceTime(10);
+    JobInProgress job4 = submitJob(JobStatus.RUNNING, 3, 3);
+    
+    // Assign tasks and check that they're given first to job3 (because it is
+    // high priority), then to job1, then to job2.
+    checkAssignment("tt1", "attempt_test_0003_m_000000_0 on tt1");
+    checkAssignment("tt1", "attempt_test_0003_r_000000_0 on tt1");
+    checkAssignment("tt1", "attempt_test_0001_m_000000_0 on tt1");
+    checkAssignment("tt1", "attempt_test_0001_r_000000_0 on tt1");
+    checkAssignment("tt2", "attempt_test_0002_m_000000_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0002_r_000000_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0002_m_000001_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0002_r_000001_0 on tt2");
+  }
+  
+  /**
+   * This test submits 2 large jobs each to 2 pools, which are both set to FIFO
+   * mode through the global defaultPoolSchedulingMode setting. We check that
+   * the scheduler assigns tasks only to the first job within each pool, but
+   * alternates between the pools to give each pool a fair share.
+   */
+  public void testMultipleFifoPools() throws Exception {
+    // Set up pools file
+    PrintWriter out = new PrintWriter(new FileWriter(ALLOC_FILE));
+    out.println("<?xml version=\"1.0\"?>");
+    out.println("<allocations>");
+    out.println("<defaultPoolSchedulingMode>fifo</defaultPoolSchedulingMode>");
+    out.println("</allocations>");
+    out.close();
+    scheduler.getPoolManager().reloadAllocs();
+    
+    // Submit jobs, advancing time in-between to make sure that they are
+    // all submitted at distinct times.
+    JobInProgress job1 = submitJob(JobStatus.RUNNING, 10, 10, "poolA");
+    advanceTime(10);
+    JobInProgress job2 = submitJob(JobStatus.RUNNING, 10, 10, "poolA");
+    advanceTime(10);
+    JobInProgress job3 = submitJob(JobStatus.RUNNING, 10, 10, "poolB");
+    advanceTime(10);
+    JobInProgress job4 = submitJob(JobStatus.RUNNING, 10, 10, "poolB");
+    
+    // Assign tasks and check that they alternate between jobs 1 and 3, the
+    // head-of-line jobs in their respective pools.
+    checkAssignment("tt1", "attempt_test_0001_m_000000_0 on tt1");
+    checkAssignment("tt1", "attempt_test_0001_r_000000_0 on tt1");
+    checkAssignment("tt1", "attempt_test_0003_m_000000_0 on tt1");
+    checkAssignment("tt1", "attempt_test_0003_r_000000_0 on tt1");
+    checkAssignment("tt2", "attempt_test_0001_m_000001_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0001_r_000001_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0003_m_000001_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0003_r_000001_0 on tt2");
+  }
+  
+  /**
+   * This test submits 2 large jobs each to 2 pools, one of which is set to FIFO
+   * mode through the global defaultPoolSchedulingMode setting, and one of which
+   * is set to fair mode. We check that the scheduler assigns tasks only to the
+   * first job in the FIFO pool but to both jobs in the fair sharing pool.
+   */
+  public void testFifoAndFairPools() throws Exception {
+    // Set up pools file
+    PrintWriter out = new PrintWriter(new FileWriter(ALLOC_FILE));
+    out.println("<?xml version=\"1.0\"?>");
+    out.println("<allocations>");
+    out.println("<defaultPoolSchedulingMode>fifo</defaultPoolSchedulingMode>");
+    out.println("<pool name=\"poolB\">");
+    out.println("<schedulingMode>fair</schedulingMode>");
+    out.println("</pool>");
+    out.println("</allocations>");
+    out.close();
+    scheduler.getPoolManager().reloadAllocs();
+    
+    // Submit jobs, advancing time in-between to make sure that they are
+    // all submitted at distinct times.
+    JobInProgress job1 = submitJob(JobStatus.RUNNING, 10, 10, "poolA");
+    advanceTime(10);
+    JobInProgress job2 = submitJob(JobStatus.RUNNING, 10, 10, "poolA");
+    advanceTime(10);
+    JobInProgress job3 = submitJob(JobStatus.RUNNING, 10, 10, "poolB");
+    advanceTime(10);
+    JobInProgress job4 = submitJob(JobStatus.RUNNING, 10, 10, "poolB");
+    
+    // Assign tasks and check that only job 1 gets tasks in pool A, but
+    // jobs 3 and 4 both get tasks in pool B.
+    checkAssignment("tt1", "attempt_test_0001_m_000000_0 on tt1");
+    checkAssignment("tt1", "attempt_test_0001_r_000000_0 on tt1");
+    checkAssignment("tt1", "attempt_test_0003_m_000000_0 on tt1");
+    checkAssignment("tt1", "attempt_test_0003_r_000000_0 on tt1");
+    checkAssignment("tt2", "attempt_test_0001_m_000001_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0001_r_000001_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0004_m_000000_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0004_r_000000_0 on tt2");
+  }
+
+  /**
+   * This test uses the mapred.fairscheduler.pool property to assign jobs to pools.
+   */
+  public void testPoolAssignment() throws Exception {
+    // Set up pools file
+    PrintWriter out = new PrintWriter(new FileWriter(ALLOC_FILE));
+    out.println("<?xml version=\"1.0\"?>");
+    out.println("<allocations>");
+    out.println("<pool name=\"default\">");
+    out.println("<schedulingMode>fair</schedulingMode>");
+    out.println("</pool>");
+    out.println("<pool name=\"poolA\">");
+    out.println("<schedulingMode>fair</schedulingMode>");
+    out.println("</pool>");
+    out.println("</allocations>");
+    out.close();
+    scheduler.getPoolManager().reloadAllocs();
+    Pool defaultPool = scheduler.getPoolManager().getPool("default");
+    Pool poolA = scheduler.getPoolManager().getPool("poolA");
+ 
+    // Submit a job to the default pool.  All specifications take default values.
+    JobInProgress job1 = submitJob(JobStatus.RUNNING, 1, 3);
+
+    assertEquals(1,    defaultPool.getMapSchedulable().getDemand());
+    assertEquals(3,    defaultPool.getReduceSchedulable().getDemand());
+    assertEquals(0,    poolA.getMapSchedulable().getDemand());
+    assertEquals(0,    poolA.getReduceSchedulable().getDemand());
+
+    // Submit a job to the default pool and move it to poolA using setPool.
+    JobInProgress job2 = submitJob(JobStatus.RUNNING, 5, 7);
+
+    assertEquals(6,    defaultPool.getMapSchedulable().getDemand());
+    assertEquals(10,   defaultPool.getReduceSchedulable().getDemand());
+    assertEquals(0,    poolA.getMapSchedulable().getDemand());
+    assertEquals(0,    poolA.getReduceSchedulable().getDemand());
+
+    scheduler.getPoolManager().setPool(job2, "poolA");
+    assertEquals("poolA", scheduler.getPoolManager().getPoolName(job2));
+
+    defaultPool.getMapSchedulable().updateDemand();
+    defaultPool.getReduceSchedulable().updateDemand();
+    poolA.getMapSchedulable().updateDemand();
+    poolA.getReduceSchedulable().updateDemand();
+
+    assertEquals(1,    defaultPool.getMapSchedulable().getDemand());
+    assertEquals(3,    defaultPool.getReduceSchedulable().getDemand());
+    assertEquals(5,    poolA.getMapSchedulable().getDemand());
+    assertEquals(7,    poolA.getReduceSchedulable().getDemand());
+
+    // Submit a job to poolA by specifying mapred.fairscheduler.pool
+    JobConf jobConf = new JobConf(conf);
+    jobConf.setNumMapTasks(11);
+    jobConf.setNumReduceTasks(13);
+    jobConf.set(POOL_PROPERTY, "nonsense"); // test that this is overridden
+    jobConf.set(EXPLICIT_POOL_PROPERTY, "poolA");
+    JobInProgress job3 = new FakeJobInProgress(jobConf, taskTrackerManager,
+        null, jobTracker);
+    job3.initTasks();
+    job3.getStatus().setRunState(JobStatus.RUNNING);
+    taskTrackerManager.submitJob(job3);
+
+    assertEquals(1,    defaultPool.getMapSchedulable().getDemand());
+    assertEquals(3,    defaultPool.getReduceSchedulable().getDemand());
+    assertEquals(16,   poolA.getMapSchedulable().getDemand());
+    assertEquals(20,   poolA.getReduceSchedulable().getDemand());
+
+    // Submit a job to poolA by specifying pool and not mapred.fairscheduler.pool
+    JobConf jobConf2 = new JobConf(conf);
+    jobConf2.setNumMapTasks(17);
+    jobConf2.setNumReduceTasks(19);
+    jobConf2.set(POOL_PROPERTY, "poolA");
+    JobInProgress job4 = new FakeJobInProgress(jobConf2, taskTrackerManager,
+        null, jobTracker);
+    job4.initTasks();
+    job4.getStatus().setRunState(JobStatus.RUNNING);
+    taskTrackerManager.submitJob(job4);
+
+    assertEquals(1,    defaultPool.getMapSchedulable().getDemand());
+    assertEquals(3,    defaultPool.getReduceSchedulable().getDemand());
+    assertEquals(33,   poolA.getMapSchedulable().getDemand());
+    assertEquals(39,   poolA.getReduceSchedulable().getDemand());
+  }
+
+
+  /**
+   * Test switching a job from one pool to another, then back to the original
+   * one. This is a regression test for a bug seen during development of
+   * MAPREDUCE-2323 (fair scheduler metrics).
+   */
+  public void testSetPoolTwice() throws Exception {
+    // Set up pools file
+    PrintWriter out = new PrintWriter(new FileWriter(ALLOC_FILE));
+    out.println("<?xml version=\"1.0\"?>");
+    out.println("<allocations>");
+    out.println("<pool name=\"default\">");
+    out.println("<schedulingMode>fair</schedulingMode>");
+    out.println("</pool>");
+    out.println("<pool name=\"poolA\">");
+    out.println("<schedulingMode>fair</schedulingMode>");
+    out.println("</pool>");
+    out.println("</allocations>");
+    out.close();
+    scheduler.getPoolManager().reloadAllocs();
+    Pool defaultPool = scheduler.getPoolManager().getPool("default");
+    Pool poolA = scheduler.getPoolManager().getPool("poolA");
+
+    // Submit a job to the default pool.  All specifications take default values.
+    JobInProgress job1 = submitJob(JobStatus.RUNNING, 1, 3);
+    assertEquals(1,    defaultPool.getMapSchedulable().getDemand());
+    assertEquals(3,    defaultPool.getReduceSchedulable().getDemand());
+    assertEquals(0,    poolA.getMapSchedulable().getDemand());
+    assertEquals(0,    poolA.getReduceSchedulable().getDemand());
+
+    // Move job to poolA and make sure demand moves with it
+    scheduler.getPoolManager().setPool(job1, "poolA");
+    assertEquals("poolA", scheduler.getPoolManager().getPoolName(job1));
+
+    defaultPool.getMapSchedulable().updateDemand();
+    defaultPool.getReduceSchedulable().updateDemand();
+    poolA.getMapSchedulable().updateDemand();
+    poolA.getReduceSchedulable().updateDemand();
+
+    assertEquals(0,    defaultPool.getMapSchedulable().getDemand());
+    assertEquals(0,    defaultPool.getReduceSchedulable().getDemand());
+    assertEquals(1,    poolA.getMapSchedulable().getDemand());
+    assertEquals(3,    poolA.getReduceSchedulable().getDemand());
+
+    // Move back to default pool and make sure demand goes back
+    scheduler.getPoolManager().setPool(job1, "default");
+    assertEquals("default", scheduler.getPoolManager().getPoolName(job1));
+
+    defaultPool.getMapSchedulable().updateDemand();
+    defaultPool.getReduceSchedulable().updateDemand();
+    poolA.getMapSchedulable().updateDemand();
+    poolA.getReduceSchedulable().updateDemand();
+
+    assertEquals(1,    defaultPool.getMapSchedulable().getDemand());
+    assertEquals(3,    defaultPool.getReduceSchedulable().getDemand());
+    assertEquals(0,    poolA.getMapSchedulable().getDemand());
+    assertEquals(0,    poolA.getReduceSchedulable().getDemand());
+  }
+  
+  private void advanceTime(long time) {
+    clock.advance(time);
+    scheduler.update();
+  }
+
+  protected TaskTracker tracker(String taskTrackerName) {
+    return taskTrackerManager.getTaskTracker(taskTrackerName);
+  }
+  
+  protected void checkAssignment(String taskTrackerName,
+      String... expectedTasks) throws IOException {
+    List<Task> tasks = scheduler.assignTasks(tracker(taskTrackerName));
+    for (Task t : tasks) {
+      taskTrackerManager.reportTaskOnTracker(taskTrackerName, t);
+    }
+    assertNotNull(tasks);
+    System.out.println("Assigned tasks:");
+    for (int i = 0; i < tasks.size(); i++)
+      System.out.println("- " + tasks.get(i));
+    assertEquals(expectedTasks.length, tasks.size());
+    for (int i = 0; i < tasks.size(); i++)
+      assertEquals("assignment " + i, expectedTasks[i], tasks.get(i).toString());
+  }
+
+  /**
+   * This test submits a job that takes all 2 slots in a pool has both a min
+   * share of 2 slots with minshare timeout of 5s, and then a second job in
+   * default pool with a fair share timeout of 5s. After 60 seconds, this pool
+   * will be starved of fair share (2 slots of each type), and we test that it
+   * does not kill more than 2 tasks of each type.
+   */
+  public void testFairSharePreemptionWithShortTimeout() throws Exception {
+    // Enable preemption in scheduler
+    scheduler.preemptionEnabled = true;
+    // Set up pools file
+    PrintWriter out = new PrintWriter(new FileWriter(ALLOC_FILE));
+    out.println("<?xml version=\"1.0\"?>");
+    out.println("<allocations>");
+    out.println("<fairSharePreemptionTimeout>5</fairSharePreemptionTimeout>");
+    out.println("<pool name=\"pool1\">");
+    out.println("<minMaps>2</minMaps>");
+    out.println("<minReduces>2</minReduces>");
+    out.println("<minSharePreemptionTimeout>5</minSharePreemptionTimeout>");
+    out.println("</pool>");
+    out.println("</allocations>");
+    out.close();
+    scheduler.getPoolManager().reloadAllocs();
+    Pool pool1 = scheduler.getPoolManager().getPool("pool1");
+    Pool defaultPool = scheduler.getPoolManager().getPool("default");
+
+    // Submit job 1 and assign all slots to it. Sleep a bit before assigning
+    // tasks on tt1 and tt2 to ensure that the ones on tt2 get preempted first.
+    JobInProgress job1 = submitJob(JobStatus.RUNNING, 10, 10, "pool1");
+    JobInfo info1 = scheduler.infos.get(job1);
+    checkAssignment("tt1", "attempt_test_0001_m_000000_0 on tt1");
+    checkAssignment("tt1", "attempt_test_0001_r_000000_0 on tt1");
+    checkAssignment("tt1", "attempt_test_0001_m_000001_0 on tt1");
+    checkAssignment("tt1", "attempt_test_0001_r_000001_0 on tt1");
+    advanceTime(100);
+    checkAssignment("tt2", "attempt_test_0001_m_000002_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0001_r_000002_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0001_m_000003_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0001_r_000003_0 on tt2");
+
+    advanceTime(10000);
+    assertEquals(4,    info1.mapSchedulable.getRunningTasks());
+    assertEquals(4,    info1.reduceSchedulable.getRunningTasks());
+    assertEquals(4.0,  info1.mapSchedulable.getFairShare());
+    assertEquals(4.0,  info1.reduceSchedulable.getFairShare());
+    // Ten seconds later, submit job 2.
+    JobInProgress job2 = submitJob(JobStatus.RUNNING, 10, 10, "default");
+
+    // Advance time by 6 seconds without update the scheduler.
+    // This simulates the time gap between update and task preemption.
+    clock.advance(6000);
+    assertEquals(4,    info1.mapSchedulable.getRunningTasks());
+    assertEquals(4,    info1.reduceSchedulable.getRunningTasks());
+    assertEquals(2.0,  info1.mapSchedulable.getFairShare());
+    assertEquals(2.0,  info1.reduceSchedulable.getFairShare());
+    assertEquals(0, scheduler.tasksToPreempt(pool1.getMapSchedulable(),
+        clock.getTime()));
+    assertEquals(0, scheduler.tasksToPreempt(pool1.getReduceSchedulable(),
+        clock.getTime()));
+    assertEquals(2, scheduler.tasksToPreempt(defaultPool.getMapSchedulable(),
+        clock.getTime()));
+    assertEquals(2, scheduler.tasksToPreempt(defaultPool.getReduceSchedulable(),
+        clock.getTime()));
+
+    // Test that the tasks actually get preempted and we can assign new ones
+    scheduler.preemptTasksIfNecessary();
+    scheduler.update();
+    assertEquals(2, job1.runningMaps());
+    assertEquals(2, job1.runningReduces());
+    checkAssignment("tt2", "attempt_test_0002_m_000000_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0002_r_000000_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0002_m_000001_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0002_r_000001_0 on tt2");
+    assertNull(scheduler.assignTasks(tracker("tt1")));
+    assertNull(scheduler.assignTasks(tracker("tt2")));
+  }
+  
+  
+  /**
+   * Ask scheduler to update metrics and then verify that they're all
+   * correctly published to the metrics context
+   */
+  private void verifyMetrics() {
+    scheduler.updateMetrics();
+    verifyPoolMetrics();
+    verifyJobMetrics();
+  }
+  
+  /**
+   * Verify that pool-level metrics match internal data
+   */
+  private void verifyPoolMetrics() {
+    MetricsContext ctx = MetricsUtil.getContext("fairscheduler");
+    Collection<OutputRecord> records = ctx.getAllRecords().get("pools");
+
+    try {
+      assertEquals(scheduler.getPoolSchedulables(TaskType.MAP).size() * 2,
+          records.size());
+    } catch (Error e) {
+      for (OutputRecord rec : records) {
+        System.err.println("record:");
+        System.err.println(" name: " + rec.getTag("name"));
+        System.err.println(" type: " + rec.getTag("type"));
+      }
+
+      throw e;
+    }
+    
+    Map<String, OutputRecord> byPoolAndType =
+      new HashMap<String, OutputRecord>();
+    for (OutputRecord rec : records) {
+      String pool = (String)rec.getTag("name");
+      String type = (String)rec.getTag("taskType");
+      assertNotNull(pool);
+      assertNotNull(type);
+      byPoolAndType.put(pool + "_" + type, rec);
+    }
+    
+    List<PoolSchedulable> poolScheds = new ArrayList<PoolSchedulable>();
+    poolScheds.addAll(scheduler.getPoolSchedulables(TaskType.MAP));
+    poolScheds.addAll(scheduler.getPoolSchedulables(TaskType.REDUCE));
+    
+    for (PoolSchedulable pool : poolScheds) {
+      String poolName = pool.getName();
+      OutputRecord metrics = byPoolAndType.get(
+          poolName + "_" + pool.getTaskType().toString());
+      assertNotNull("Need metrics for " + pool, metrics);
+      
+      verifySchedulableMetrics(pool, metrics);
+    }
+    
+  }
+  
+  /**
+   * Verify that the job-level metrics match internal data
+   */
+  private void verifyJobMetrics() {
+    MetricsContext ctx = MetricsUtil.getContext("fairscheduler");
+    Collection<OutputRecord> records = ctx.getAllRecords().get("jobs");
+    
+    System.out.println("Checking job metrics...");
+    Map<String, OutputRecord> byJobIdAndType =
+      new HashMap<String, OutputRecord>();
+    for (OutputRecord rec : records) {
+      String jobId = (String)rec.getTag("name");
+      String type = (String)rec.getTag("taskType");
+      assertNotNull(jobId);
+      assertNotNull(type);
+      byJobIdAndType.put(jobId + "_" + type, rec);
+      System.out.println("Got " + type + " metrics for job: " + jobId);
+    }
+    assertEquals(scheduler.infos.size() * 2, byJobIdAndType.size());
+    
+    for (Map.Entry<JobInProgress, JobInfo> entry :
+            scheduler.infos.entrySet()) {
+      JobInfo info = entry.getValue();
+      String jobId = entry.getKey().getJobID().toString();
+      
+      OutputRecord mapMetrics = byJobIdAndType.get(jobId + "_MAP");
+      assertNotNull("Job " + jobId + " should have map metrics", mapMetrics);
+      verifySchedulableMetrics(info.mapSchedulable, mapMetrics);
+      
+      OutputRecord reduceMetrics = byJobIdAndType.get(jobId + "_REDUCE");
+      assertNotNull("Job " + jobId + " should have reduce metrics", reduceMetrics);
+      verifySchedulableMetrics(info.reduceSchedulable, reduceMetrics);
+    }
+  }
+
+  /**
+   * Verify that the metrics for a given Schedulable are correct
+   */
+  private void verifySchedulableMetrics(
+      Schedulable sched, OutputRecord metrics) {
+    assertEquals(sched.getRunningTasks(), metrics.getMetric("runningTasks"));
+    assertEquals(sched.getDemand(), metrics.getMetric("demand"));
+    assertEquals(sched.getFairShare(),
+        metrics.getMetric("fairShare").doubleValue(), .001);
+    assertEquals(sched.getWeight(),
+        metrics.getMetric("weight").doubleValue(), .001);
+  }
+}
Index: src/contrib/approximationscheduler/src/test/org/apache/hadoop/mapred/TestFairSchedulerPoolNames.java
===================================================================
--- src/contrib/approximationscheduler/src/test/org/apache/hadoop/mapred/TestFairSchedulerPoolNames.java	(revision 0)
+++ src/contrib/approximationscheduler/src/test/org/apache/hadoop/mapred/TestFairSchedulerPoolNames.java	(working copy)
@@ -0,0 +1,193 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.mapred;
+
+import java.io.DataOutputStream;
+import java.io.File;
+import java.io.FileWriter;
+import java.io.IOException;
+import java.io.PrintWriter;
+import java.net.URI;
+
+import static org.junit.Assert.*;
+
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.hdfs.MiniDFSCluster;
+import org.apache.hadoop.io.IntWritable;
+import org.apache.hadoop.io.Text;
+import org.apache.hadoop.mapred.Pool;
+import org.junit.After;
+import org.junit.Before;
+import org.junit.Test;
+
+public class TestFairSchedulerPoolNames {
+
+  final static String TEST_DIR = new File(System.getProperty("test.build.data",
+      "build/contrib/streaming/test/data")).getAbsolutePath();
+  final static String ALLOC_FILE = new File(TEST_DIR, "test-pools")
+      .getAbsolutePath();
+
+  private static final String POOL_PROPERTY = "pool";
+  private String namenode;
+  private MiniDFSCluster miniDFSCluster = null;
+  private MiniMRCluster miniMRCluster = null;
+
+  /**
+   * Note that The PoolManager.ALLOW_UNDECLARED_POOLS_KEY property is set to
+   * false. So, the default pool is not added, and only pool names in the
+   * scheduler allocation file are considered valid.
+   */
+  @Before
+  public void setUp() throws Exception {
+    new File(TEST_DIR).mkdirs(); // Make sure data directory exists
+    // Create an allocation file with only one pool defined.
+    PrintWriter out = new PrintWriter(new FileWriter(ALLOC_FILE));
+    out.println("<?xml version=\"1.0\"?>");
+    out.println("<allocations>");
+    out.println("<pool name=\"poolA\">");
+    out.println("<minMaps>1</minMaps>");
+    out.println("<minReduces>1</minReduces>");
+    out.println("</pool>");
+    out.println("</allocations>");
+    out.close();
+
+    namenode = "file:///";
+
+    JobConf clusterConf = new JobConf();
+    clusterConf.set("mapred.jobtracker.taskScheduler", FairScheduler.class
+        .getName());
+    clusterConf.set("mapred.fairscheduler.allocation.file", ALLOC_FILE);
+    clusterConf.set("mapred.fairscheduler.poolnameproperty", POOL_PROPERTY);
+    clusterConf.setBoolean(FairScheduler.ALLOW_UNDECLARED_POOLS_KEY, false);
+    miniMRCluster = new MiniMRCluster(1, namenode, 1, null, null, clusterConf);
+  }
+
+  @After
+  public void tearDown() throws Exception {
+    if (miniDFSCluster != null) {
+      miniDFSCluster.shutdown();
+    }
+    if (miniMRCluster != null) {
+      miniMRCluster.shutdown();
+    }
+  }
+
+  private void submitJob(String pool) throws IOException {
+    JobConf conf = new JobConf();
+    final Path inDir = new Path("/tmp/testing/wc/input");
+    final Path outDir = new Path("/tmp/testing/wc/output");
+    FileSystem fs = FileSystem.get(URI.create(namenode), conf);
+    fs.delete(outDir, true);
+    if (!fs.mkdirs(inDir)) {
+      throw new IOException("Mkdirs failed to create " + inDir.toString());
+    }
+    DataOutputStream file = fs.create(new Path(inDir, "part-00000"));
+    file.writeBytes("Sample text");
+    file.close();
+
+    FileSystem.setDefaultUri(conf, namenode);
+    conf.set("mapred.job.tracker", "localhost:"
+        + miniMRCluster.getJobTrackerPort());
+    conf.setJobName("wordcount");
+    conf.setInputFormat(TextInputFormat.class);
+
+    // the keys are words (strings)
+    conf.setOutputKeyClass(Text.class);
+    // the values are counts (ints)
+    conf.setOutputValueClass(IntWritable.class);
+
+    conf.setMapperClass(WordCount.MapClass.class);
+    conf.setCombinerClass(WordCount.Reduce.class);
+    conf.setReducerClass(WordCount.Reduce.class);
+
+    FileInputFormat.setInputPaths(conf, inDir);
+    FileOutputFormat.setOutputPath(conf, outDir);
+    conf.setNumMapTasks(1);
+    conf.setNumReduceTasks(0);
+
+    if (pool != null) {
+      conf.set(POOL_PROPERTY, pool);
+    }
+    JobClient.runJob(conf);
+  }
+
+  /**
+   * Tests job submission using the default pool name.
+   */
+  @Test
+  public void testDefaultPoolName() {
+    Throwable t = null;
+    try {
+      submitJob(null);
+    } catch (Exception e) {
+      t = e;
+    }
+    assertNotNull("No exception during submission", t);
+    assertTrue("Incorrect exception message", t.getMessage().contains(
+        "Add pool name to the fair scheduler allocation file"));
+  }
+
+  /**
+   * Tests job submission using a valid pool name (i.e., name exists in the fair
+   * scheduler allocation file).
+   */
+  @Test
+  public void testValidPoolName() {
+    Throwable t = null;
+    try {
+      submitJob("poolA");
+    } catch (Exception e) {
+      t = e;
+    }
+    assertNull("Exception during submission", t);
+  }
+
+  /**
+   * Tests job submission using an invalid pool name (i.e., name doesn't exist
+   * in the fair scheduler allocation file).
+   */
+  @Test
+  public void testInvalidPoolName() {
+    Throwable t = null;
+    try {
+      submitJob("poolB");
+    } catch (Exception e) {
+      t = e;
+    }
+    assertNotNull("No exception during submission", t);
+    assertTrue("Incorrect exception message", t.getMessage().contains(
+        "Add pool name to the fair scheduler allocation file"));
+  }
+
+  /**
+   * Tests that no Pool object can be created with a null string.
+   */
+  @Test
+  public void testPoolNameNotNull() {
+    try {
+      Pool pool = new Pool(null, null);
+      fail("Pool object got created with a null name somehow.");
+    } catch (IllegalArgumentException e) {
+      // Pass
+    } catch (Exception e) {
+      fail("Pool object got created with a null name and failed only later.");
+    }
+  }
+}
Index: src/contrib/approximationscheduler/src/test/org/apache/hadoop/mapred/TestFairSchedulerSystem.java
===================================================================
--- src/contrib/approximationscheduler/src/test/org/apache/hadoop/mapred/TestFairSchedulerSystem.java	(revision 0)
+++ src/contrib/approximationscheduler/src/test/org/apache/hadoop/mapred/TestFairSchedulerSystem.java	(working copy)
@@ -0,0 +1,199 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.mapred;
+
+import org.apache.hadoop.security.UserGroupInformation;
+import org.apache.hadoop.mapreduce.TestSleepJob;
+import org.apache.hadoop.util.ToolRunner;
+import org.apache.hadoop.conf.Configuration;
+import java.io.BufferedReader;
+import java.io.InputStream;
+import java.io.InputStreamReader;
+import java.net.URL;
+import java.net.HttpURLConnection;
+import java.util.concurrent.Callable;
+import java.util.concurrent.Executors;
+import java.util.concurrent.ExecutorService;
+import java.util.concurrent.Future;
+import java.util.concurrent.TimeoutException;
+import java.util.concurrent.TimeUnit;
+import java.util.ArrayList;
+import java.util.List;
+
+import org.junit.Test;
+import org.junit.BeforeClass;
+import org.junit.AfterClass;
+import static org.junit.Assert.*;
+
+/**
+ * System tests for the fair scheduler. These run slower than the
+ * mock-based tests in TestFairScheduler but have a better chance
+ * of catching synchronization bugs with the real JT.
+ *
+ * This test suite will often be run inside JCarder in order to catch
+ * deadlock bugs which have plagued the scheduler in the past - hence
+ * it is a bit of a "grab-bag" of system tests, since it's important
+ * that they all run as part of the same JVM instantiation.
+ */
+public class TestFairSchedulerSystem {
+  static final int NUM_THREADS=2;
+
+  static MiniMRCluster mr;
+  static JobConf conf;
+
+  @BeforeClass
+  public static void setUp() throws Exception {
+    conf = new JobConf();
+    final int taskTrackers = 1;
+
+    // Bump up the frequency of preemption updates to test against
+    // deadlocks, etc.
+    conf.set("mapred.jobtracker.taskScheduler", FairScheduler.class.getCanonicalName());
+    conf.set("mapred.fairscheduler.update.interval", "1");
+    conf.set("mapred.fairscheduler.preemption.interval", "1");
+    conf.set("mapred.fairscheduler.preemption", "true");
+    conf.set("mapred.fairscheduler.eventlog.enabled", "true");
+    conf.set("mapred.fairscheduler.poolnameproperty", "group.name");
+    conf.set("mapred.job.tracker.persist.jobstatus.active", "false");
+    mr = new MiniMRCluster(taskTrackers, "file:///", 1, null, null, conf);
+  }
+
+  @AfterClass
+  public static void tearDown() throws Exception {
+    if (mr != null) {
+      mr.shutdown();
+    }
+  }
+
+  private void runSleepJob(JobConf conf) throws Exception {
+    String[] args = { "-m", "1", "-r", "1", "-mt", "1", "-rt", "1" };
+    ToolRunner.run(conf, new TestSleepJob(), args);
+  }
+
+  /**
+   * Submit some concurrent sleep jobs, and visit the scheduler servlet
+   * while they're running.
+   */
+  @Test
+  public void testFairSchedulerSystem() throws Exception {
+    ExecutorService exec = Executors.newFixedThreadPool(NUM_THREADS);
+    List<Future<Void>> futures = new ArrayList<Future<Void>>(NUM_THREADS);
+    for (int i = 0; i < NUM_THREADS; i++) {
+      futures.add(exec.submit(new Callable<Void>() {
+            public Void call() throws Exception {
+              JobConf jobConf = mr.createJobConf();
+              runSleepJob(jobConf);
+              return null;
+            }
+          }));
+    }
+
+    JobClient jc = new JobClient(mr.createJobConf(null));
+
+    // Wait for the tasks to finish, and visit the scheduler servlet
+    // every few seconds while waiting.
+    for (Future<Void> future : futures) {
+      while (true) {
+        try {
+          future.get(3, TimeUnit.SECONDS);
+          break;
+        } catch (TimeoutException te) {
+          // It's OK
+        }
+        checkServlet(true);
+        checkServlet(false);
+
+        JobStatus jobs[] = jc.getAllJobs();
+        if (jobs == null) {
+          System.err.println("No jobs running, not checking tasklog servlet");
+          continue;
+        }
+        for (JobStatus j : jobs) {
+          System.err.println("Checking task graph for " + j.getJobID());
+          try {
+            checkTaskGraphServlet(j.getJobID());
+          } catch (AssertionError err) {
+            // The task graph servlet will be empty if the job has retired.
+            // This is OK.
+            RunningJob rj = jc.getJob(j.getJobID());
+            if (!rj.isComplete()) {
+              throw err;
+            }
+          }
+        }
+      }
+    }
+  }
+
+  /**
+   * Check the fair scheduler servlet for good status code and smoke test
+   * for contents.
+   */
+  private void checkServlet(boolean advanced) throws Exception {
+    String jtURL = "http://localhost:" +
+      mr.getJobTrackerRunner().getJobTrackerInfoPort();
+    URL url = new URL(jtURL + "/scheduler" +
+                      (advanced ? "?advanced" : ""));
+    HttpURLConnection connection = (HttpURLConnection)url.openConnection();
+    connection.setRequestMethod("GET");
+    connection.connect();
+    assertEquals(200, connection.getResponseCode());
+
+    // Just to be sure, slurp the content and make sure it looks like the scheduler
+    BufferedReader reader = new BufferedReader(
+      new InputStreamReader(connection.getInputStream()));
+    StringBuilder sb = new StringBuilder();
+
+    String line = null;
+    while ((line = reader.readLine()) != null) {
+      sb.append(line).append('\n');
+    }
+
+    String contents = sb.toString();
+    assertTrue("Bad contents for fair scheduler servlet: " + contents,
+      contents.contains("Fair Scheduler Administration"));
+  }
+
+  private void checkTaskGraphServlet(JobID job) throws Exception {
+    String jtURL = "http://localhost:" +
+      mr.getJobTrackerRunner().getJobTrackerInfoPort();
+    URL url = new URL(jtURL + "/taskgraph?jobid=" + job.toString() + "&type=map");
+    HttpURLConnection connection = (HttpURLConnection)url.openConnection();
+    connection.setRequestMethod("GET");
+    connection.connect();
+    assertEquals(200, connection.getResponseCode());
+
+    // Just to be sure, slurp the content and make sure it looks like the scheduler
+    String contents = slurpContents(connection);
+    assertTrue("Bad contents for job " + job + ":\n" + contents,
+      contents.contains("</svg>"));
+  }
+
+  private String slurpContents(HttpURLConnection connection) throws Exception {
+    BufferedReader reader = new BufferedReader(
+      new InputStreamReader(connection.getInputStream()));
+    StringBuilder sb = new StringBuilder();
+
+    String line = null;
+    while ((line = reader.readLine()) != null) {
+      sb.append(line).append('\n');
+    }
+
+    return sb.toString();
+  }
+}
Index: src/contrib/build-contrib.xml
===================================================================
--- src/contrib/build-contrib.xml	(revision 1596572)
+++ src/contrib/build-contrib.xml	(working copy)
@@ -22,6 +22,9 @@
 <project name="hadoopbuildcontrib" xmlns:ivy="antlib:org.apache.ivy.ant">
 
   <property name="name" value="${ant.project.name}"/>
+  <property name="version" value="1.1.3"/>
+  <property name="target" value="1.6"/>
+  <property name="source" value="1.6"/>
   <property name="root" value="${basedir}"/>
   <property name="hadoop.root" location="${root}/../../../"/>
 
@@ -61,6 +64,7 @@
   <!-- all jars together -->
   <property name="javac.deprecation" value="off"/>
   <property name="javac.debug" value="on"/>
+  <property name="javac.version" value="1.6"/>
   <property name="build.ivy.lib.dir" value="${hadoop.root}/build/ivy/lib"/> 
 
   <property name="javadoc.link"
@@ -185,6 +189,7 @@
      includes="**/*.java"
      destdir="${build.classes}"
      debug="${javac.debug}"
+     compiler="javac1.6" target="1.6" source="1.6"
      deprecation="${javac.deprecation}">
      <classpath refid="contrib-classpath"/>
     </javac>
Index: src/contrib/build.xml
===================================================================
--- src/contrib/build.xml	(revision 1596572)
+++ src/contrib/build.xml	(working copy)
@@ -54,6 +54,7 @@
       <fileset dir="." includes="streaming/build.xml"/>
       <fileset dir="." includes="fairscheduler/build.xml"/>
       <fileset dir="." includes="capacity-scheduler/build.xml"/>
+      <fileset dir="." includes="partitionscheduler/build.xml"/>
       <fileset dir="." includes="gridmix/build.xml"/>
     </subant>
      <available file="${build.contrib.dir}/testsfailed" property="testsfailed"/>
@@ -78,6 +79,7 @@
        <fileset dir="." includes="streaming/build.xml"/>
        <fileset dir="." includes="fairscheduler/build.xml"/>
        <fileset dir="." includes="capacity-scheduler/build.xml"/>
+       <fileset dir="." includes="partitionscheduler/build.xml"/>
        <fileset dir="." includes="gridmix/build.xml"/>
     </subant>
     <available file="${build.contrib.dir}/testsfailed" property="testsfailed"/>
Index: src/contrib/fairscheduler/src/java/org/apache/hadoop/mapred/DisableNodesFairScheduler.java
===================================================================
--- src/contrib/fairscheduler/src/java/org/apache/hadoop/mapred/DisableNodesFairScheduler.java	(revision 0)
+++ src/contrib/fairscheduler/src/java/org/apache/hadoop/mapred/DisableNodesFairScheduler.java	(working copy)
@@ -0,0 +1,65 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.mapred;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.List;
+
+import org.apache.hadoop.mapreduce.server.jobtracker.TaskTracker;
+
+/**
+ * A modification of a job scheduler that doesn't send jobs to disable nodes.
+ */
+class DisableNodesFairScheduler extends FairScheduler {
+	public static final Log LOG = LogFactory.getLog(DisableNodesFairScheduler.class);
+
+	private DisableNodesPollingThread pollingThread;
+	private ArrayList<String> disabledNodesList;
+	
+	public DisableNodesFairScheduler() {
+		super();
+		this.disabledNodesList = new ArrayList<String>();
+	}
+	
+	@Override
+	public synchronized void start() {// throws IOException {
+		super.start();
+
+		pollingThread = new DisableNodesPollingThread("conf/disabledServers", disabledNodesList, LOG);
+		pollingThread.start();
+	}
+	
+	@Override
+	public synchronized void terminate() throws IOException {
+		pollingThread.terminate();
+		super.terminate();
+	}
+
+	@Override
+	public synchronized List<Task> assignTasks(TaskTracker taskTracker) throws IOException {
+		// If a node is on the disabled list, never give it any tasks.
+		if(disabledNodesList.contains(taskTracker.getStatus().getHost()))
+			return new ArrayList<Task>();
+
+		return super.assignTasks(taskTracker);
+	}
+}
Index: src/contrib/fairscheduler/src/java/org/apache/hadoop/mapred/DisableNodesPollingThread.java
===================================================================
--- src/contrib/fairscheduler/src/java/org/apache/hadoop/mapred/DisableNodesPollingThread.java	(revision 0)
+++ src/contrib/fairscheduler/src/java/org/apache/hadoop/mapred/DisableNodesPollingThread.java	(working copy)
@@ -0,0 +1,58 @@
+/*
+ * To change this template, choose Tools | Templates
+ * and open the template in the editor.
+ */
+
+package org.apache.hadoop.mapred;
+
+
+import java.io.*;
+import java.util.*;
+import java.util.logging.Level;
+import java.util.logging.Logger;
+import org.apache.commons.logging.Log;
+
+/**
+ * Reads a file containing the list of disabled nodes.
+ * @author William Katsak
+ */
+public class DisableNodesPollingThread extends Thread {
+	private File disabledNodesFile;
+	private ArrayList<String> disabledNodesList;
+	private boolean keepRunning;
+	private Log LOG;
+
+	public DisableNodesPollingThread(String disabledNodesFileName, ArrayList<String> disabledNodesList, Log log) {
+		this.disabledNodesFile = new File(disabledNodesFileName);
+		this.disabledNodesList = disabledNodesList;
+		this.keepRunning = true;
+		this.LOG = log;
+	}
+
+	public void run() {
+		try {
+			LOG.info("Opening disabled node file: " + disabledNodesFile.toString());
+			while (keepRunning) {
+				Scanner fileScanner = new Scanner(disabledNodesFile);
+				ArrayList<String> newList = new ArrayList<String>();
+				while(fileScanner.hasNext()) {
+					String next = fileScanner.next();
+					newList.add(next);
+				}
+				if(!disabledNodesList.equals(newList)) {
+					disabledNodesList.clear();
+					disabledNodesList.addAll(newList);
+
+					LOG.info("Disabled Nodes List Changed:" + disabledNodesList);
+				}
+				Thread.sleep(1000);
+			}
+		} catch (Exception ex) {
+			LOG.error(ex.getMessage());
+		}
+	}
+
+	public void terminate() {
+		this.keepRunning = false;
+	}
+}
Index: src/contrib/partitionscheduler/build.xml
===================================================================
--- src/contrib/partitionscheduler/build.xml	(revision 0)
+++ src/contrib/partitionscheduler/build.xml	(working copy)
@@ -0,0 +1,39 @@
+<?xml version="1.0"?>
+
+<!--
+   Licensed to the Apache Software Foundation (ASF) under one or more
+   contributor license agreements.  See the NOTICE file distributed with
+   this work for additional information regarding copyright ownership.
+   The ASF licenses this file to You under the Apache License, Version 2.0
+   (the "License"); you may not use this file except in compliance with
+   the License.  You may obtain a copy of the License at
+
+       http://www.apache.org/licenses/LICENSE-2.0
+
+   Unless required by applicable law or agreed to in writing, software
+   distributed under the License is distributed on an "AS IS" BASIS,
+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+   See the License for the specific language governing permissions and
+   limitations under the License.
+-->
+
+<!-- 
+Before you can run these subtargets directly, you need 
+to call at top-level: ant deploy-contrib compile-core-test
+-->
+<project name="partitionscheduler" default="jar">
+
+  <import file="../build-contrib.xml"/>
+
+  <!-- ====================================================== -->
+  <!-- Package a Hadoop contrib                               -->
+  <!-- ====================================================== -->
+  <target name="package" depends="jar, jar-examples" unless="skip.contrib">
+    <copy todir="${dist.dir}/lib" includeEmptyDirs="false" flatten="true">
+      <fileset dir="${build.dir}">
+        <include name="hadoop-${name}-${version}.jar" />
+      </fileset>
+    </copy>
+  </target>
+
+</project>
Index: src/contrib/partitionscheduler/ivy/libraries.properties
===================================================================
--- src/contrib/partitionscheduler/ivy/libraries.properties	(revision 0)
+++ src/contrib/partitionscheduler/ivy/libraries.properties	(working copy)
@@ -0,0 +1,5 @@
+#This properties file lists the versions of the various artifacts used by streaming.
+#It drives ivy and the generation of a maven POM
+
+#Please list the dependencies name with version if they are different from the ones 
+#listed in the global libraries.properties file (in alphabetical order)
Index: src/contrib/partitionscheduler/ivy.xml
===================================================================
--- src/contrib/partitionscheduler/ivy.xml	(revision 0)
+++ src/contrib/partitionscheduler/ivy.xml	(working copy)
@@ -0,0 +1,86 @@
+<?xml version="1.0" ?>
+<ivy-module version="1.0">
+  <info organisation="org.apache.hadoop" module="${ant.project.name}">
+    <license name="Apache 2.0"/>
+    <ivyauthor name="Apache Hadoop Team" url="http://hadoop.apache.org"/>
+    <description>
+        Apache Hadoop contrib
+    </description>
+  </info>
+  <configurations defaultconfmapping="default">
+    <!--these match the Maven configurations-->
+    <conf name="default" extends="master,runtime"/>
+    <conf name="master" description="contains the artifact but no dependencies"/>
+    <conf name="runtime" description="runtime but not the artifact" />
+
+    <conf name="common" visibility="private" 
+      description="artifacts needed to compile/test the application"/>
+  </configurations>
+
+  <publications>
+    <!--get the artifact from our module name-->
+    <artifact conf="master"/>
+  </publications>
+  <dependencies>
+    <dependency org="commons-logging"
+      name="commons-logging"
+      rev="${commons-logging.version}"
+      conf="common->default"/>
+    <dependency org="commons-collections"
+      name="commons-collections"
+      rev="${commons-collections.version}"
+      conf="common->default"/>
+    <dependency org="commons-cli"
+      name="commons-cli"
+      rev="${commons-cli.version}"
+      conf="common->default"/>
+    <dependency org="log4j"
+      name="log4j"
+      rev="${log4j.version}"
+      conf="common->master"/>
+   <dependency org="junit"
+      name="junit"
+      rev="${junit.version}"
+      conf="common->default"/>
+    <dependency org="org.mortbay.jetty"
+      name="jetty-util"
+      rev="${jetty-util.version}"
+      conf="common->master"/>
+    <dependency org="org.mortbay.jetty"
+      name="jetty"
+      rev="${jetty.version}"
+      conf="common->default"/>
+    <dependency org="org.mortbay.jetty"
+      name="jsp-api-2.1"
+      rev="${jsp-api-2.1.version}"
+      conf="common->master"/>
+    <dependency org="org.mortbay.jetty"
+      name="jetty"
+      rev="${jetty.version}"
+      conf="common->default"/>
+    <dependency org="org.codehaus.jackson"
+      name="jackson-core-asl"
+      rev="${jackson.version}"
+      conf="common->default"/>
+    <dependency org="org.codehaus.jackson"
+      name="jackson-mapper-asl"
+      rev="${jackson.version}"
+      conf="common->default"/>
+    <dependency org="commons-httpclient"
+      name="commons-httpclient"
+      rev="${commons-httpclient.version}"
+      conf="common->master"/> 
+    <dependency org="commons-configuration"
+      name="commons-configuration"
+      rev="${commons-configuration.version}"
+      conf="common->master"/>
+    <dependency org="org.apache.commons"
+      name="commons-math"
+      rev="${commons-math.version}"
+      conf="common->master"/>
+    <dependency org="commons-lang"
+      name="commons-lang"
+      rev="${commons-lang.version}"
+      conf="common->master"/>
+  </dependencies>
+</ivy-module>
Index: src/contrib/partitionscheduler/src/java/org/apache/hadoop/mapred/JobSizeStatistics.java
===================================================================
--- src/contrib/partitionscheduler/src/java/org/apache/hadoop/mapred/JobSizeStatistics.java	(revision 0)
+++ src/contrib/partitionscheduler/src/java/org/apache/hadoop/mapred/JobSizeStatistics.java	(working copy)
@@ -0,0 +1,43 @@
+package org.apache.hadoop.mapred;
+
+import java.util.LinkedList;
+
+/**
+ * Holds job size statistics.
+ */
+public class JobSizeStatistics {
+	public LinkedList<Integer> jobCount = new LinkedList<Integer>();
+	public int jobNum = 0;
+	public int groupSize = 5;
+	
+	public void add(int jobSize) {
+		// Calculate which is the job size group.
+		int jobSizeGroup = jobSize/this.groupSize;
+		// Add empty statistics
+		while (jobSizeGroup+1 > this.jobCount.size()) {
+			this.jobCount.add(0);
+		}
+		// Add another job to the group (we weight the group by the size of the job)
+		this.jobCount.set(jobSizeGroup, this.jobCount.get(jobSizeGroup) + jobSize);
+		// Add another job to the statistics
+		this.jobNum += jobSize;
+	}
+	
+	public int get(double percentile) {
+		int auxSum = 0;
+		for (int i=0; i<this.jobCount.size(); i++) {
+			auxSum += this.jobCount.get(i);
+			if (100.0*auxSum/this.jobNum >= percentile) {
+				return (i+1)*this.groupSize;
+			}
+		}
+		// Return the maximum value
+		return this.jobCount.size()*this.groupSize;
+	}
+
+	public void flush() {
+		this.jobCount = new LinkedList<Integer>();
+		this.jobNum = 0;
+	}
+}
+
Index: src/contrib/partitionscheduler/src/java/org/apache/hadoop/mapred/PartitionStatus.java
===================================================================
--- src/contrib/partitionscheduler/src/java/org/apache/hadoop/mapred/PartitionStatus.java	(revision 0)
+++ src/contrib/partitionscheduler/src/java/org/apache/hadoop/mapred/PartitionStatus.java	(working copy)
@@ -0,0 +1,35 @@
+package org.apache.hadoop.mapred;
+
+import java.util.List;
+import java.util.LinkedList;
+
+/**
+ * Represents the status of a partition.
+ */
+public class PartitionStatus {
+	public boolean available = true;
+
+	public String partitionId;
+	
+	public int numTrackers = 0;
+	
+	// Status of the trackers in the partition
+	public int runMap = 0;
+	public int queueMap = 0;
+	public int maxMap = 0;
+	
+	public int runRed = 0;
+	public int queueRed = 0;
+	public int maxRed = 0;
+
+	// Maximum job size in the partition
+	public int jobMaxSize = 0;
+	
+	// Queue
+	public int numJobs = 0;
+	public List<JobID> queue = new LinkedList<JobID>();
+	
+	public PartitionStatus(String partitionId) {
+		this.partitionId = partitionId;
+	}
+}
Index: src/contrib/partitionscheduler/src/java/org/apache/hadoop/mapred/PartitionTaskScheduler.java
===================================================================
--- src/contrib/partitionscheduler/src/java/org/apache/hadoop/mapred/PartitionTaskScheduler.java	(revision 0)
+++ src/contrib/partitionscheduler/src/java/org/apache/hadoop/mapred/PartitionTaskScheduler.java	(working copy)
@@ -0,0 +1,1100 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.mapred;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Collection;
+import java.util.Collections;
+import java.util.Set;
+import java.util.List;
+import java.util.ArrayList;
+import java.util.LinkedList;
+import java.util.Map;
+import java.util.HashMap;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.mapreduce.server.jobtracker.TaskTracker;
+
+
+import org.apache.hadoop.mapreduce.TaskType;
+
+
+/**
+ * A {@link TaskScheduler} that divides the cluster in partitions and schedules jobs on them.
+ * The main advantage is that is easier to decommission servers and partitions.
+ * This characteristics makes nodes go to sleep faster.
+ *
+ * Right now, every public method is synchronized and we do more synchronization than it might be actually needed.
+ * However, there are many chances for a deadlock to happen (including the browser querying a queue).
+ * As the scheduler performance is not a concern right now, it's better to be overconservative.
+ */
+class PartitionTaskScheduler extends TaskScheduler {
+	private static final int MIN_CLUSTER_SIZE_FOR_PADDING = 3;
+	public static final Log LOG = LogFactory.getLog(PartitionTaskScheduler.class);
+	
+	protected JobQueueJobInProgressListener jobQueueJobInProgressListener;
+	protected EagerTaskInitializationListener eagerTaskInitializationListener;
+	private float padFraction;
+
+	// Partition management
+	private PartitionTaskSchedulerReader  partitionReader;
+	private PartitionTaskSchedulerUpdater partitionUpdater;
+	
+	// Partition management
+	// Status of the partitions
+	public PartitionsStatus partitions;
+	// trackerHost -> partitionId
+	public Map<String,String> trackerPartition;
+	// [trackerHost...]
+	public List<String> disabledTrackers;
+	
+	// Job scheduling
+	// Internal queue to maintain the arrival order and reschedule (just in case)
+	private List<JobID> inJobQueue;
+	// Server -> [JobID,...]
+	private Map<String,List<JobID>> trackerJobs;
+	// Job -> Partition (+status)
+	private Map<JobID,JobSchedule> jobSchedule;
+	
+	// Job statistics
+	private JobSizeStatistics jobStats;
+	
+	// Synchronization management: this triggers actions
+	private boolean flushedPartitionData;
+	private boolean flushedSchedule;
+	
+	
+	/**
+	 * Represents the job schedule.
+	 */
+	class JobSchedule {
+		public String partitionId;
+		public boolean queue;
+		
+		public JobSchedule(String partitionId) {
+			this.partitionId = partitionId;
+			this.queue = true;
+		}
+		
+		public boolean isQueued() {
+			return this.queue;
+		}
+		
+		public void setRunning() {
+			this.queue = false;
+		}
+	}
+	
+	/**
+	 * Create a new partition scheduler.
+	 * Create all the internal structures to manage the partitions.
+	 */
+	public PartitionTaskScheduler() {
+		this.jobQueueJobInProgressListener = new JobQueueJobInProgressListener();
+		
+		// Partition management
+		this.partitions = new PartitionsStatus();
+		this.trackerPartition = new HashMap<String,String>();
+		this.disabledTrackers = new LinkedList<String>();
+		
+		// Job scheduling
+		this.inJobQueue = new LinkedList<JobID>();
+		this.trackerJobs = new HashMap<String,List<JobID>>();
+		this.jobSchedule = new HashMap<JobID,JobSchedule>();
+		
+		// Partition manager
+		this.partitionReader =  new PartitionTaskSchedulerReader(this);
+		this.partitionUpdater = new PartitionTaskSchedulerUpdater(this);
+		
+		// Job statistics
+		this.jobStats = new JobSizeStatistics();
+		
+		// At the begining everything is flushed
+		this.flushedPartitionData = true;
+		this.flushedPartitionData = true;
+		this.flushedSchedule = true;
+	}
+
+	@Override
+	public synchronized void start() throws IOException {
+		super.start();
+		taskTrackerManager.addJobInProgressListener(this.jobQueueJobInProgressListener);
+		this.eagerTaskInitializationListener.setTaskTrackerManager(taskTrackerManager);
+		this.eagerTaskInitializationListener.start();
+		taskTrackerManager.addJobInProgressListener(this.eagerTaskInitializationListener);
+		
+		// Partition status management
+		this.partitionReader.start();
+		this.partitionUpdater.start();
+	}
+  
+	@Override
+	public synchronized void terminate() throws IOException {
+		if (this.jobQueueJobInProgressListener != null) {
+			taskTrackerManager.removeJobInProgressListener(this.jobQueueJobInProgressListener);
+		}
+		if (this.eagerTaskInitializationListener != null) {
+			taskTrackerManager.removeJobInProgressListener(this.eagerTaskInitializationListener);
+			this.eagerTaskInitializationListener.terminate();
+		}
+		if (this.partitionReader != null) {
+			this.partitionReader.terminate();
+		}
+		if (this.partitionUpdater != null) {
+			this.partitionUpdater.terminate();
+		}
+		super.terminate();
+	}
+  
+	@Override
+	public synchronized void setConf(Configuration conf) {
+		super.setConf(conf);
+		padFraction = conf.getFloat("mapred.jobtracker.taskalloc.capacitypad", 0.01f);
+		this.eagerTaskInitializationListener = new EagerTaskInitializationListener(conf);
+	}
+
+	/**
+	 * This function assign tasks following FIFO order inside of a partition.
+	 * The scheduler has already assigned a job to a partition.
+	 * We assign tasks from jobs assigned to this partition.
+	 * @param taskTracker Task tracker to assign tasks to.
+	 * @return List of tasks assigned to this tracker.
+	 */
+	@Override
+	public synchronized List<Task> assignTasks(TaskTracker taskTracker) throws IOException {
+		// Check for JT safe-mode
+		if (taskTrackerManager.isInSafeMode()) {
+			LOG.info("JobTracker is in safe-mode, not scheduling any tasks.");
+			return null;
+		}
+		
+		TaskTrackerStatus taskTrackerStatus = taskTracker.getStatus(); 
+		ClusterStatus clusterStatus = taskTrackerManager.getClusterStatus();
+		final int numTaskTrackers = clusterStatus.getTaskTrackers();
+		final int clusterMapCapacity = clusterStatus.getMaxMapTasks();
+		final int clusterReduceCapacity = clusterStatus.getMaxReduceTasks();
+		
+		Collection<JobInProgress> jobQueue = this.getJobs();
+		
+		// Get map + reduce counts for the current tracker.
+		final int trackerMapCapacity = taskTrackerStatus.getMaxMapSlots();
+		final int trackerReduceCapacity = taskTrackerStatus.getMaxReduceSlots();
+		final int trackerRunningMaps = taskTrackerStatus.countMapTasks();
+		final int trackerRunningReduces = taskTrackerStatus.countReduceTasks();
+
+		// Assigned tasks
+		List<Task> assignedTasks = new ArrayList<Task>();
+
+		// Partition information
+		String trackerHost = taskTrackerStatus.getHost();
+		String trackerPartition = this.trackerPartition.get(trackerHost);
+		
+		// If we are disabled, we can only run jobs that were already in this node
+		Map<JobID,Boolean> filterJobs = null;
+		if (this.disabledTrackers.contains(trackerHost)) {
+			filterJobs = new HashMap<JobID,Boolean>();
+			// Get the jobs that have been running in this host
+			List<JobID> jobsInServer = this.trackerJobs.get(trackerHost);
+			if (jobsInServer != null) {
+				for (JobID auxJobID : jobsInServer) {
+					filterJobs.put(auxJobID, true);
+				}
+			}
+		}
+		
+		// Compute (running + pending) map and reduce task numbers across pool
+		int remainingReduceLoad = 0;
+		int remainingMapLoad = 0;
+		synchronized (jobQueue) {
+			for (JobInProgress job : jobQueue) {
+				if (job.getStatus().getRunState() == JobStatus.RUNNING) {
+					remainingMapLoad += (job.desiredMaps() - job.finishedMaps());
+					if (job.scheduleReduces()) {
+						remainingReduceLoad += (job.desiredReduces() - job.finishedReduces());
+					}
+				}
+			}
+		}
+
+		// Compute the 'load factor' for maps and reduces
+		double mapLoadFactor = 0.0;
+		if (clusterMapCapacity > 0) {
+			mapLoadFactor = (double)remainingMapLoad / clusterMapCapacity;
+		}
+		double reduceLoadFactor = 0.0;
+		if (clusterReduceCapacity > 0) {
+			reduceLoadFactor = (double)remainingReduceLoad / clusterReduceCapacity;
+		}
+		
+		// In the below steps, we allocate first map tasks (if appropriate),
+		// and then reduce tasks if appropriate.  We go through all jobs
+		// in order of job arrival; jobs only get serviced if their 
+		// predecessors are serviced, too.
+		
+		// We assign tasks to the current taskTracker if the given machine 
+		// has a workload that's less than the maximum load of that kind of
+		// task.
+		// However, if the cluster is close to getting loaded i.e. we don't
+		// have enough _padding_ for speculative executions etc., we only 
+		// schedule the "highest priority" task i.e. the task from the job 
+		// with the highest priority.
+		final int trackerCurrentMapCapacity = Math.min((int)Math.ceil(mapLoadFactor * trackerMapCapacity), trackerMapCapacity);
+		int availableMapSlots = trackerCurrentMapCapacity - trackerRunningMaps;
+		boolean exceededMapPadding = false;
+		if (availableMapSlots > 0) {
+			exceededMapPadding = exceededPadding(true, clusterStatus, trackerMapCapacity);
+		}
+		
+		int numLocalMaps = 0;
+		int numNonLocalMaps = 0;
+		scheduleMaps:
+		for (int i=0; i < availableMapSlots; ++i) {
+			synchronized (jobQueue) {
+				for (JobInProgress job : jobQueue) {
+					// Only schedule jobs that are ready to run
+					if (job.getStatus().getRunState() != JobStatus.RUNNING) {
+						continue;
+					}
+					// Only schedule jobs in our partition
+					if (!trackerPartition.equals(this.jobSchedule.get(job.getJobID()).partitionId)) {
+						continue;
+					}
+					// If we are disabled, we can only run jobs that were already in this node
+					if (filterJobs != null) {
+						if (!filterJobs.containsKey(job.getJobID())) {
+							continue;
+						}
+					}
+					
+					// Try to schedule a node-local or rack-local Map task
+					Task t = job.obtainNewNodeOrRackLocalMapTask(taskTrackerStatus, numTaskTrackers, taskTrackerManager.getNumberOfUniqueHosts());
+					if (t != null) {
+						assignedTasks.add(t);
+						++numLocalMaps;
+						
+						// Don't assign map tasks to the hilt!
+						// Leave some free slots in the cluster for future task-failures,
+						// speculative tasks etc. beyond the highest priority job
+						if (exceededMapPadding) {
+							break scheduleMaps;
+						}
+						
+						// Try all jobs again for the next Map task 
+						break;
+					}
+					
+					// Try to schedule a node-local or rack-local Map task
+					t = job.obtainNewNonLocalMapTask(taskTrackerStatus, numTaskTrackers, taskTrackerManager.getNumberOfUniqueHosts());
+					if (t != null) {
+						assignedTasks.add(t);
+						++numNonLocalMaps;
+						
+						// We assign at most 1 off-switch or speculative task
+						// This is to prevent TaskTrackers from stealing local-tasks
+						// from other TaskTrackers.
+						break scheduleMaps;
+					}
+				}
+			}
+		}
+		int assignedMaps = assignedTasks.size();
+
+		// Same thing, but for reduce tasks
+		// However we _never_ assign more than 1 reduce task per heartbeat
+		final int trackerCurrentReduceCapacity = Math.min((int)Math.ceil(reduceLoadFactor * trackerReduceCapacity), trackerReduceCapacity);
+		final int availableReduceSlots = Math.min((trackerCurrentReduceCapacity - trackerRunningReduces), 1);
+		boolean exceededReducePadding = false;
+		if (availableReduceSlots > 0) {
+			exceededReducePadding = exceededPadding(false, clusterStatus, trackerReduceCapacity);
+			synchronized (jobQueue) {
+				for (JobInProgress job : jobQueue) {
+					// Only schedule jobs that are ready to run
+					if (job.getStatus().getRunState() != JobStatus.RUNNING || job.numReduceTasks == 0) {
+						continue;
+					}
+					// Only schedule jobs in our partitions
+					if (!trackerPartition.equals(this.jobSchedule.get(job.getJobID()).partitionId)) {
+						continue;
+					}
+					// If we are disabled, we can only run jobs that were already in this node
+					if (filterJobs != null) {
+						if (!filterJobs.containsKey(job.getJobID())) {
+							continue;
+						}
+					}
+
+					// Try to schedule a Reduce task
+					Task t = job.obtainNewReduceTask(taskTrackerStatus, numTaskTrackers, taskTrackerManager.getNumberOfUniqueHosts());
+					if (t != null) {
+						assignedTasks.add(t);
+						break;
+					}
+					
+					// Don't assign reduce tasks to the hilt!
+					// Leave some free slots in the cluster for future task-failures,
+					// speculative tasks etc. beyond the highest priority job
+					if (exceededReducePadding) {
+						break;
+					}
+				}
+			}
+		}
+
+		if (LOG.isDebugEnabled()) {
+			LOG.debug("Task assignments for " + taskTrackerStatus.getTrackerName() + " --> " +
+					"[" + mapLoadFactor + ", " + trackerMapCapacity + ", " + 
+					trackerCurrentMapCapacity + ", " + trackerRunningMaps + "] -> [" + 
+					(trackerCurrentMapCapacity - trackerRunningMaps) + ", " +
+					assignedMaps + " (" + numLocalMaps + ", " + numNonLocalMaps + 
+					")] [" + reduceLoadFactor + ", " + trackerReduceCapacity + ", " + 
+					trackerCurrentReduceCapacity + "," + trackerRunningReduces + 
+					"] -> [" + (trackerCurrentReduceCapacity - trackerRunningReduces) + 
+					", " + (assignedTasks.size()-assignedMaps) + "]");
+		}
+		
+		// Log which tasks have been submitted to this tracker
+		if (assignedTasks.size()>0) {
+			for (Task t : assignedTasks) {
+				this.addJobToTracker(t.getTaskID().getJobID(), trackerHost);
+			}
+		}
+		
+		return assignedTasks;
+	}
+
+	/**
+	 * Check if there is enough padding to run extra maps.
+	 */
+	private boolean exceededPadding(boolean isMapTask, ClusterStatus clusterStatus, int maxTaskTrackerSlots) { 
+		int numTaskTrackers = clusterStatus.getTaskTrackers();
+		int totalTasks = (isMapTask) ? clusterStatus.getMapTasks() : clusterStatus.getReduceTasks();
+		int totalTaskCapacity = isMapTask ? clusterStatus.getMaxMapTasks() : clusterStatus.getMaxReduceTasks();
+
+		Collection<JobInProgress> jobQueue = this.getJobs();
+
+		boolean exceededPadding = false;
+		synchronized (jobQueue) {
+			int totalNeededTasks = 0;
+			for (JobInProgress job : jobQueue) {
+				if (job.getStatus().getRunState() != JobStatus.RUNNING || job.numReduceTasks == 0) {
+					continue;
+				}
+
+				// Beyond the highest-priority task, reserve a little 
+				// room for failures and speculative executions; don't 
+				// schedule tasks to the hilt.
+				totalNeededTasks += isMapTask ? job.desiredMaps() : job.desiredReduces();
+				int padding = 0;
+				if (numTaskTrackers > MIN_CLUSTER_SIZE_FOR_PADDING) {
+					padding = Math.min(maxTaskTrackerSlots, (int) (totalNeededTasks * padFraction));
+				}
+				if (totalTasks + padding >= totalTaskCapacity) {
+					exceededPadding = true;
+					break;
+				}
+			}
+		}
+		return exceededPadding;
+	}
+	
+	
+	/**
+	 * Performs the first scheduling for a job.
+	 * @param job The job that we try to schedule.
+	 * @throws IOException If there is an error with the submission.
+	 */
+	@Override
+	public synchronized void checkJobSubmission(JobInProgress job) throws IOException {
+		// Check if it has been checked before
+		if (this.jobSchedule.get(job.getJobID()) == null) {
+			// Add job statistics
+			this.jobStats.add(job.desiredMaps() + job.desiredReduces());
+			// Schedule job
+			String jobPartition = this.scheduleJob(job);
+			LOG.info("Job "+job.getJobID()+" initially scheduled to "+jobPartition);
+		}
+	}
+	
+	/**
+	 * Decide if a job should run in a tracker.
+	 * This function is used by the JobTracker to decide if a job should run in a tracker.
+	 * Right now, we only check the JOB_SETUP.
+	 * It indirectly performs the partition scheduler.
+	 */
+	public synchronized boolean runJobInTracker(JobInProgress job, String trackerHost) {
+		// Check if the job is actually available
+		if (job==null || !job.inited()) {
+			return false;
+		}
+			
+		// Local data for job and partition
+		JobID jobId = job.getJobID();
+		String trackerPartition = this.trackerPartition.get(trackerHost);
+			
+		// Check decommission: if the node or the partition are disabled, we do not accept any new jobs.
+		try {
+			if (this.disabledTrackers.contains(trackerHost) || !this.getPartitions().get(trackerPartition).available) {
+				return false;
+			}
+		} catch (Exception e) {
+			LOG.error("Checking if "+trackerHost+" from "+trackerPartition+" is disabled. Error: " + e.getMessage());
+		}
+			
+		// Check job scheduling
+		JobSchedule jobSchedule = this.jobSchedule.get(jobId);
+			
+		// If the job hasn't been scheduled
+		if (jobSchedule == null || jobSchedule.partitionId == null) {
+			this.scheduleJob(job);
+			jobSchedule = this.jobSchedule.get(jobId);
+		}
+			
+		// If the job is already ready to run
+		if (!jobSchedule.isQueued()) {
+			return true;
+		}
+			
+		// If we are in the right partition
+		if (trackerPartition.equals(jobSchedule.partitionId)) {
+			if (this.isReadyToSubmit(jobId, trackerPartition)) {
+				// We can submit the job into this partition, move it to the right queue
+				this.assignJob(jobId, trackerPartition);
+				return true;
+			}
+		}
+		
+		// By default, don't run the job yet
+		return false;
+	}
+	
+	/**
+	 * Check if a job is ready to be submitted to a partition.
+	 * A job is ready if it's the next in line and there is room in the partition (less than 200%).
+	 */
+	private boolean isReadyToSubmit(JobID jobId, String partitionId) {
+		// Get the first jobId in the queue
+		JobID firstJobId = null;
+		if (this.getPartitions().get(partitionId).queue.size() > 0) {
+			firstJobId = this.getPartitions().get(partitionId).queue.get(0);
+		}
+		
+		// If we are the first job in the partition queue
+		if (jobId.equals(firstJobId)) {
+			// Check if there is room in the tracker
+			PartitionStatus partition = this.getPartitions().get(partitionId);
+			double utilMap = 1.0*(partition.runMap) / partition.maxMap;
+			double utilRed = 1.0*(partition.runRed) / partition.maxRed;
+			// If we have room (<200%), schedule it 
+			if (utilMap<=2.0 && utilRed<=2.0) {
+				return true;
+			}
+		}
+		return false;
+	}
+	
+	/**
+	 * Remove all the scheduling we had for a partition.
+	 */
+	public synchronized void flushScheduling() {
+		this.flushedSchedule = true;
+		// Clear the temporary scheduling
+		for (JobSchedule jobSchedule : this.jobSchedule.values()) {
+			if (jobSchedule.isQueued()) {
+				jobSchedule.partitionId = null;
+			}
+		}
+		// Partitions
+		for (PartitionStatus partition : this.partitions.info.values()) {
+			partition.queue.clear();
+			partition.queueMap = 0;
+			partition.queueRed = 0;
+		}
+		// We also flush the data
+		this.flushPartitionData();
+	}
+	
+	/**
+	 * Check if the queues for the active partitions are balanced.
+	 */
+	public synchronized boolean isQueueBalanced() {
+		// Check if any partition is full or underutilized
+		boolean fullPartition = false;
+		boolean emptyPartition = false;
+		for (PartitionStatus partition : this.partitions.info.values()) {
+			if (partition.available) {
+				// Utilization
+				double utilMap = 1.0*(partition.runMap + partition.queueMap) / partition.maxMap;
+				double utilRed = 1.0*(partition.runRed + partition.queueRed) / partition.maxRed;
+				if (partition.queue.size() == 0) {
+					if (utilMap < 1.0 && utilRed < 1.0) {
+						emptyPartition = true;
+					}
+				} else if (partition.queue.size() > 1) {
+					if (utilMap > 1.0 || utilRed > 1.0) {
+						fullPartition = true;
+					}
+				}
+			}
+		}
+		
+		// If we have a partition that is empty and another one that is full, we are not balanced
+		if (fullPartition && emptyPartition) {
+			LOG.info("Partitions are not balanced.");
+			this.printLogPartitions();
+			return false;
+		}
+		
+		return true;
+	}
+	
+	/**
+	 * Schedule all jobs that need to be scheduled.
+	 * This usually happens when we flush the scheduling.
+	 */
+	public synchronized void scheduleAll() {
+		// Get the jobs that need to be rescheduled
+		LinkedList<JobInProgress> jobsToSchedule = new LinkedList<JobInProgress>();
+		Collection<JobInProgress> jobQueue = this.getJobs();
+		synchronized (jobQueue) {
+			for (JobID jobId : this.inJobQueue) {
+				// Search for the job in the queue
+				JobInProgress job = null;
+				for (JobInProgress auxJob : jobQueue) {
+					if (jobId.equals(auxJob.getJobID())) {
+						job = auxJob;
+						break;
+					}
+				}
+				// Added to the list of jobs that need to be scheduled
+				if (job != null) {
+					jobsToSchedule.add(job);
+				}
+			}
+		}
+		
+		if (LOG.isDebugEnabled()) {
+			String outQueue =  "";
+			for(JobInProgress job : jobsToSchedule) {
+				int jobShortId = Integer.parseInt(job.getJobID().toString().substring(job.getJobID().toString().lastIndexOf("_")+1));
+				outQueue += " "+jobShortId;
+			}
+			LOG.debug("We have to reschedule:"+outQueue);
+		}
+		
+		// Reschedule jobs in order
+		while (jobsToSchedule.size() > 0) {
+			JobInProgress job = jobsToSchedule.pollFirst();
+			LOG.debug("Reschedule: " + job.getJobID());
+			this.scheduleJob(job);
+		}
+		
+		if (LOG.isDebugEnabled()) {
+			this.printLogQueue();
+		}
+	}
+	
+	/**
+	 * Move a job from the temporary queue to the actual partition.
+	 * @param jobId Id of the job that we will start running.
+	 * @param partitionId Partition we are assigning the job to.
+	 */
+	private synchronized boolean assignJob(JobID jobId, String partitionId) {
+		// Check everything is right
+		if (!this.getPartitions().get(partitionId).queue.get(0).equals(jobId)) {
+			LOG.error("We tried to remove " + jobId + " from the queue of " + partitionId + " and it wasn't there.");
+			LOG.error("The queue is: " + this.getPartitions().get(partitionId).queue);
+			return false;
+		}
+		if (!partitionId.equals(this.jobSchedule.get(jobId).partitionId)) {
+			LOG.error("We tried to move " + jobId + " from "+partitionId+" and it was assigned to " + this.jobSchedule.get(jobId).partitionId);
+			return false;
+		}
+		
+		// Perform changes
+		// Remove from partition queue
+		JobInProgress job = this.getJob(jobId);
+		PartitionStatus partition = this.getPartitions().get(partitionId);
+		partition.queue.remove(0);
+		partition.queueMap -= job.desiredMaps();
+		partition.queueRed -= job.desiredReduces();
+		partition.runMap   += job.desiredMaps();
+		partition.runRed   += job.desiredReduces();
+						
+		// Set job as scheduled
+		this.jobSchedule.get(jobId).setRunning();
+		// We don't need to keep the order for this job anymore
+		this.inJobQueue.remove(jobId);
+		
+		return true;
+	}
+	
+	/**
+	 * Log that a job has started running in a host.
+	 */
+	private synchronized void addJobToTracker(JobID jobId, String trackerHost) {
+		List<JobID> jobsInServer = this.trackerJobs.get(trackerHost);
+		if (jobsInServer == null) {
+			jobsInServer = new ArrayList<JobID>();
+			this.trackerJobs.put(trackerHost, jobsInServer);
+		}
+		if (!jobsInServer.contains(jobId)) {
+			jobsInServer.add(jobId);
+		}
+	}
+	
+	/**
+	 * Select the best partition to run a job.
+	 * We use three criteria to select the partition:
+	 * 1) Try to keep partitions utilization balanced.
+	 * 3) Heavy jobs go to lower partitions.
+	 */
+	private String scheduleJob(JobInProgress job) {
+		// If the scheduling has been flushed, do it again
+		if (this.flushedSchedule) {
+			// We have to reconstruct the whole schedule
+			this.flushedSchedule = false;
+			// We reschedule everybody not just as
+			this.scheduleAll();
+		}
+		
+		// Return the partition if the job was already scheduled
+		JobSchedule jobSchedule = this.jobSchedule.get(job.getJobID());
+		if (jobSchedule != null && jobSchedule.partitionId != null) {
+			return jobSchedule.partitionId;
+		}
+		
+		// Get the best partition to run the job
+		String bestPartition = getBestPartition(job);
+		
+		// Maintain scheduling information
+		// Temporary assign the job to a partition
+		this.jobSchedule.put(job.getJobID(), new JobSchedule(bestPartition));
+		
+		// Assign to the partition queue
+		PartitionStatus partition = this.getPartitions().get(bestPartition);
+		partition.queue.add(job.getJobID());
+		partition.queueMap += job.desiredMaps();
+		partition.queueRed += job.desiredReduces();
+		
+		// Store the order of the job
+		if (!this.inJobQueue.contains(job.getJobID())) {
+			this.inJobQueue.add(job.getJobID());
+		}
+		
+		if (LOG.isDebugEnabled()) {
+			LOG.debug("==================================================================");
+			LOG.debug("Scheduling " + job.getJobID() + " M:"+job.desiredMaps()+" R:"+job.desiredReduces()+" on:");
+			this.printLogPartitions();
+			LOG.debug(job.getJobID() + " scheduled to run in " + bestPartition);
+			LOG.debug("==================================================================");
+		}
+		
+		return bestPartition;
+	}
+	
+	/**
+	 * Select the best partition to run a job.
+	 * We start from the bottom and get the partition that:
+	 * 1) Has room to run right now.
+	 * 2) Accomplishes the capacity maximum (percentile based).
+	 * 3) Balances the load.
+	 */
+	private String getBestPartition(JobInProgress job) {
+		String bestPartition = null;
+		double bestUtilization = 999999999; // Maximum utilization. I'm not religious, but if you have a higher utilization than this, god bless you
+		
+		List<PartitionStatus> partitions = this.getPartitions().getAvailables();
+		
+		// Check the percentiles for each partition
+		int totalNodes = 0;
+		for (PartitionStatus partition : partitions) {
+			totalNodes += partition.numTrackers;
+		}
+		int remainNodes = totalNodes;
+		
+		// Start assigning
+		for (PartitionStatus partition : partitions) {
+			// Utilization right now
+			double utilMap =     1.0*(partition.runMap + partition.queueMap) / partition.maxMap;
+			double utilRed =     1.0*(partition.runRed + partition.queueRed) / partition.maxRed;
+			// Utilization after adding this job
+			double utilMapPost = 1.0*(partition.runMap + partition.queueMap + job.desiredMaps())    / partition.maxMap;
+			double utilRedPost = 1.0*(partition.runRed + partition.queueRed + job.desiredReduces()) / partition.maxRed;
+			// Get percentile and max job size
+			double percentile = Math.ceil(100.0*remainNodes/totalNodes);
+			remainNodes -= partition.numTrackers;
+			partition.jobMaxSize = this.jobStats.get(percentile);
+			if (job.desiredMaps() + job.desiredReduces() <= partition.jobMaxSize) {
+				// Choose the partition that has room to run the job right now
+				if (utilMapPost <= 1.0 && utilRedPost <= 1.0) {
+					return partition.partitionId;
+				}
+				// If we have to wait, we try to balance the queues and use the less utilized
+				if (utilMap < bestUtilization && utilRed < bestUtilization) {
+					bestPartition = partition.partitionId;
+					bestUtilization = Math.max(utilMap, utilRed);
+				}
+			}
+		}
+		return bestPartition;
+	}
+	
+	/**
+	 * Get the list of jobs that are in the queue.
+	 */
+	public synchronized Collection<JobInProgress> getJobs() {
+		return this.getJobs(null);
+	}
+	
+	/**
+	 * Get the list of jobs that are in the queue.
+	 * @param queueName Name of the queue (we use it as partition id).
+	 */
+	@Override
+	public synchronized Collection<JobInProgress> getJobs(String queueName) {
+		if (queueName == null || queueName.equals("default")) {
+			return this.jobQueueJobInProgressListener.getJobQueue();
+		} else {
+			LinkedList<JobInProgress> ret = new LinkedList<JobInProgress>();
+			for (JobInProgress job : this.jobQueueJobInProgressListener.getJobQueue()) {
+				if (queueName.equals(this.jobSchedule.get(job.getJobID()).partitionId)) {
+					ret.add(job);
+				}
+			}
+			return ret;
+		}
+	}
+	
+	/**
+	 * Get a job from the queue.
+	 */
+	private synchronized JobInProgress getJob(JobID jobId) {
+		JobInProgress job = null;
+		Collection<JobInProgress> jobQueue = this.getJobs();
+		synchronized(jobQueue) {
+			for (JobInProgress auxJob : jobQueue) {
+				if (jobId.equals(auxJob.getJobID())) {
+					job = auxJob;
+					break;
+				}
+			}
+		}
+		return job;
+	}
+	
+	/**
+	 * Get the partitions on the system.
+	 * If the data is out of date (flushed), get it again.
+	 */
+	public synchronized PartitionsStatus getPartitions() {
+		if (this.flushedPartitionData) {
+			this.flushedPartitionData = false;
+			this.updatePartitionData();
+		}
+		return this.partitions;
+	}
+	
+	/**
+	 * Flush partitions data.
+	 * Our internal information can be out of date.
+	 * We have a thread that periodically flushes this information.
+	 * Wehn we try to access it, we will update it.
+	 */
+	public synchronized void flushPartitionData() {
+		this.flushedPartitionData = true;
+		for (PartitionStatus partition : this.partitions.info.values()) {
+			// Reset values
+			partition.numTrackers = 0;
+			
+			partition.numJobs = 0;
+			
+			partition.runMap = 0;
+			partition.queueMap = 0;
+			partition.maxMap = 0;
+			
+			partition.runRed = 0;
+			partition.queueRed = 0;
+			partition.maxRed = 0;
+		}
+
+		// We restart the stats of the job sizes
+		this.jobStats.flush();
+		
+		// Expose information about the partitions using queues
+		QueueManager queueManager = taskTrackerManager.getQueueManager();
+		for (String queueName : queueManager.getQueues()) {
+			// Get queue
+			Queue q = queueManager.getQueue(queueName);
+			// Set decommission partitions
+			PartitionStatus partition = this.partitions.get(queueName);
+			if (partition != null && !partition.available) {
+				q.setState(Queue.QueueState.STOPPED);
+			}
+		}
+	}
+	
+	/**
+	 * Update the current status of the partition.
+	 * We include the jobs that are running and the ones in the queue.
+	 */
+	private synchronized void updatePartitionData() {
+		// Update partitions
+		int numTrackers = 0;
+		for (PartitionStatus partition : this.getPartitions().getAll()) {
+			// Check each host
+			for (TaskTrackerStatus other : taskTrackerManager.taskTrackers()) {
+				if (!this.disabledTrackers.contains(other.getHost())) {
+					String otherPartitionId = this.trackerPartition.get(other.getHost());
+					if (partition.partitionId.equals(otherPartitionId)) {
+						numTrackers++;
+						partition.numTrackers++;
+						partition.maxMap += other.getMaxMapSlots();
+						partition.maxRed += other.getMaxReduceSlots();
+					}
+				}
+			}
+		}
+			
+		// Add the tasks for the jobs (both running and internally queued)
+		Collection<JobInProgress> jobQueue = this.getJobs();
+		synchronized (jobQueue) {
+			for (JobInProgress jobCheck : jobQueue) {
+				JobSchedule jobSchedule = this.jobSchedule.get(jobCheck.getJobID());
+				PartitionStatus partition = this.partitions.get(jobSchedule.partitionId);
+				if (partition != null) {
+					// Set scheduling info for the job
+					if (jobSchedule.isQueued()) {
+						jobCheck.setSchedulingInfo("Queued in partition "+partition.partitionId);
+					} else if (partition.available) {
+						jobCheck.setSchedulingInfo("Partition "+partition.partitionId);
+					} else {
+						if (partition.numTrackers == 0) {
+							jobCheck.setSchedulingInfo("Frozen in partition "+partition.partitionId);
+						} else {
+							jobCheck.setSchedulingInfo("Decommission in partition "+partition.partitionId);
+						}
+					}
+					// Account the number of jobs in the partition
+					partition.numJobs++;
+					// Account the tasks the job is still running
+					int maps = 0;
+					for (TaskInProgress taskCheck : jobCheck.getTasks(TaskType.MAP)) {
+						if (taskCheck.getTaskStatuses().length == 0) {
+							// It hasn't been initialized, so it will have just that map to run
+							maps++;
+						} else {
+							for (TaskStatus taskStatus : taskCheck.getTaskStatuses()) {
+								// Multiple attempts
+								if (taskStatus.getRunState() == TaskStatus.State.RUNNING) {
+									maps++;
+					}	}	}	}
+					int reds = 0;
+					for (TaskInProgress taskCheck : jobCheck.getTasks(TaskType.REDUCE)) {
+						if (taskCheck.getTaskStatuses().length == 0) {
+							// It hasn't been initialized, so it will have just that map to run
+							reds++;
+						} else {
+							for (TaskStatus taskStatus : taskCheck.getTaskStatuses()) {
+								// Multiple attempts
+								if (taskStatus.getRunState() == TaskStatus.State.RUNNING) {
+									reds++;
+					}	}	}	}
+					// Account for the tasks
+					if (jobSchedule.isQueued()) {
+						partition.queueMap += maps;
+						partition.queueRed += reds;
+					} else {
+						partition.runMap += maps;
+						partition.runRed += reds;
+					}
+				}
+
+				// Add job stats
+				this.jobStats.add(jobCheck.desiredMaps() + jobCheck.desiredReduces());
+			}
+			
+			// Expose information about the partitions using queues
+			QueueManager queueManager = taskTrackerManager.getQueueManager();
+			for (String queueName : queueManager.getQueues()) {
+				// Get queue
+				Queue q = queueManager.getQueue(queueName);
+				q.setState(Queue.QueueState.RUNNING);
+				// Get partition info and set it to the queue
+				PartitionStatus partition = this.partitions.get(queueName);
+				if (partition != null) {
+					if (partition.numJobs == 0 && partition.numTrackers == 0) {
+						if (!partition.available) {
+							q.setSchedulingInfo("0 jobs 0 nodes");
+						} else {
+							q.setSchedulingInfo("OFF");
+						}
+					} else {
+						q.setSchedulingInfo((partition.numJobs-partition.queue.size())+"/"+partition.numJobs+" jobs (<"+partition.jobMaxSize+") "+partition.numTrackers+" nodes");
+					}
+					if (!partition.available) {
+						q.setState(Queue.QueueState.STOPPED);
+					}
+				// Default queue that represents the whole system
+				} else if ("default".equals(queueName)){
+					q.setSchedulingInfo(jobQueue.size()+" jobs "+numTrackers+" nodes");
+				// Unkwnown partition
+				} else {
+					q.setState(Queue.QueueState.STOPPED);
+					q.setSchedulingInfo("Unknown");
+				}
+			}
+		}
+		
+		if (LOG.isDebugEnabled()) {
+			LOG.debug("Updated data:");
+			this.printLogPartitions();
+		}
+	}
+	
+	/**
+	 * Report the status of the partitions.
+	 */
+	private void printLogPartitions() {
+		List<PartitionStatus> partitions = this.partitions.getAll();
+		// Get number of available
+		int available = 0;
+		int totalNodes = 0;
+		for (PartitionStatus partition : partitions) {
+			if (partition.available) {
+				available++;
+				totalNodes += partition.numTrackers;
+			}
+		}
+		int remainNodes = totalNodes;
+		for (PartitionStatus partition : partitions) {
+			// Get percentile and max job size
+			double percentile = Math.ceil(100.0*remainNodes/totalNodes);
+			if (partition.available) {
+				remainNodes -= partition.numTrackers;
+			}
+			partition.jobMaxSize = this.jobStats.get(percentile);
+			
+			// Output
+			String out = " ";
+			if (partition.available) {
+				out += "  ";
+			} else {
+				out += "X ";
+			}
+			out += partition.partitionId+" ";
+			out += "N:"+partition.numTrackers+" ";
+			out += "<"+partition.jobMaxSize+" "+String.format("%5.1f%%", percentile)+"\t";
+			out += "M:"+partition.runMap+"+"+partition.queueMap+"/"+partition.maxMap+" ";
+			out += "R:"+partition.runRed+"+"+partition.queueRed+"/"+partition.maxRed+"\t";
+			out += "Q:";
+			for (JobID jobId : partition.queue) {
+				int jobShortId = Integer.parseInt(jobId.toString().substring(jobId.toString().lastIndexOf("_")+1));
+				out += jobShortId+" ";
+			}
+			LOG.info(out);
+		}
+	}
+	
+	/**
+	 * Report the status of the jobs in the system.
+	 */
+	private void printLogQueue() {
+		Collection<JobInProgress> jobQueue = this.getJobs();
+		synchronized (jobQueue) {
+			LOG.info("Queue: " + jobQueue.size() + " jobs");
+			for (JobInProgress jobCheck : jobQueue) {
+				JobID jobId = jobCheck.getJobID();
+				if (!this.jobSchedule.containsKey(jobId)) {
+					LOG.info("   " + jobId +
+						" M:" + jobCheck.finishedMaps() + "/" + jobCheck.desiredMaps() +
+						" R:" + jobCheck.finishedReduces() + "/" + jobCheck.desiredReduces());
+				// Already assigned to a partition
+				} else if (!this.jobSchedule.get(jobId).isQueued()) {
+					LOG.info(" R " + jobId +
+						" M:" + jobCheck.finishedMaps() + "/" + jobCheck.desiredMaps() +
+						" R:" + jobCheck.finishedReduces() + "/" + jobCheck.desiredReduces() +
+						" Part:" + this.jobSchedule.get(jobId).partitionId);
+				// Assigned temporally to a partition
+				} else {
+					LOG.info(" Q " + jobId +
+						" M:" + jobCheck.finishedMaps() + "/" + jobCheck.desiredMaps() +
+						" R:" + jobCheck.finishedReduces() + "/" + jobCheck.desiredReduces() +
+						" Part:" + this.jobSchedule.get(jobId).partitionId);
+				}
+			}
+		}
+		// Check the order of the jobs that are not submitted yet
+		String outQueue = "   Order:" ;
+		for(JobID jobId : this.inJobQueue) {
+			int jobShortId = Integer.parseInt(jobId.toString().substring(jobId.toString().lastIndexOf("_")+1));
+			outQueue += " "+jobShortId;
+		}
+		LOG.info(outQueue);
+	}
+	
+	/**
+	 * Cleans the internal structures.
+	 * Sometimes an internal data structure might get outdated.
+	 */
+	public synchronized void cleanOldData() {
+		// Get the list of jobs that are in the system
+		Map<JobID,Boolean> outdatedJobs = new HashMap<JobID,Boolean>();
+		for (JobID jobId : this.jobSchedule.keySet()) {
+			if (!outdatedJobs.containsKey(jobId)) {
+				outdatedJobs.put(jobId, true);
+			}
+		}
+		for (JobID jobId : this.inJobQueue) {
+			if (!outdatedJobs.containsKey(jobId)) {
+				outdatedJobs.put(jobId, true);
+			}
+		}
+		for (List<JobID> listJobs : this.trackerJobs.values()) {
+			for (JobID jobId : listJobs) {
+				if (!outdatedJobs.containsKey(jobId)) {
+					outdatedJobs.put(jobId, true);
+				}
+			}
+		}
+		
+		LOG.info("Jobs in the internal structures: " + outdatedJobs.keySet());
+		
+		// Remove the jobs are in the queue
+		Collection<JobInProgress> jobQueue = this.getJobs();
+		synchronized (jobQueue) {
+			for (JobInProgress auxJob : jobQueue) {
+				outdatedJobs.remove(auxJob.getJobID());
+			}
+		}
+		
+		LOG.info("Jobs not used: " + outdatedJobs.keySet());
+			
+		// Remove outdated jobs from everywhere
+		for (JobID jobId : outdatedJobs.keySet()) {
+			LOG.info("Job "+jobId+" is not in the system anymore. Remove from internal data structures.");
+			this.jobSchedule.remove(jobId);
+			this.inJobQueue.remove(jobId);
+			for (List<JobID> listJobs : this.trackerJobs.values()) {
+				listJobs.remove(jobId);
+			}
+		}
+	}
+}
Index: src/contrib/partitionscheduler/src/java/org/apache/hadoop/mapred/PartitionTaskScheduler.java.bak
===================================================================
--- src/contrib/partitionscheduler/src/java/org/apache/hadoop/mapred/PartitionTaskScheduler.java.bak	(revision 0)
+++ src/contrib/partitionscheduler/src/java/org/apache/hadoop/mapred/PartitionTaskScheduler.java.bak	(working copy)
@@ -0,0 +1,1010 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.mapred;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Collection;
+import java.util.Collections;
+import java.util.List;
+import java.util.ArrayList;
+import java.util.LinkedList;
+import java.util.Map;
+import java.util.HashMap;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.mapreduce.server.jobtracker.TaskTracker;
+
+
+import org.apache.hadoop.mapreduce.TaskType;
+
+
+/**
+ * A {@link TaskScheduler} that divides the cluster in partitions and schedules jobs on them.
+ * The main advantage is that is easier to decommission servers and partitions.
+ * This characteristics makes servers sleep faster.
+ */
+class PartitionTaskScheduler extends TaskScheduler {
+	private static final int MIN_CLUSTER_SIZE_FOR_PADDING = 3;
+	public static final Log LOG = LogFactory.getLog(PartitionTaskScheduler.class);
+	
+	// Boundaries for the maximum size of the jobs
+	private static final int MIN_JOB_SIZE_BOUND = 10;
+	private static final int MAX_JOB_SIZE_BOUND = 1000;
+	
+	protected JobQueueJobInProgressListener jobQueueJobInProgressListener;
+	protected EagerTaskInitializationListener eagerTaskInitializationListener;
+	private float padFraction;
+
+	// Partition management
+	private PartitionTaskSchedulerReader  partitionReader;
+	private PartitionTaskSchedulerCleaner partitionCleaner;
+	
+	// Lock for scheduling
+	private Boolean scheduling = true;
+	
+	// Partition management
+	// Status of the partitions
+	private PartitionsStatus partitions;
+	// trackerHost -> partitionId
+	private Map<String,String> trackerPartition;
+	// [trackerHost...]
+	private List<String> disabledTracker;
+	// [partitionId...]
+	private List<String> disabledPartitions;
+	
+	// Job scheduling
+	// Internal queue to maintain the arrival order
+	private List<JobID> inJobQueue;
+	// Server -> [JobID,...]
+	private Map<String,List<JobID>> trackerJobs;
+	// Job -> Partition (+status)
+	private Map<JobID,JobSchedule> jobToPartition;
+	
+	class JobSchedule {
+		public JobID jobId;
+		public String partitionId;
+		public boolean queue;
+		
+		public PartitionSchedule(JobID jobId, String partitionId) {
+			this.jobId = jobId;
+			this.partitionId = partitionId;
+			this.queue = true;
+		}
+		
+		public boolean isQueued() {
+			return this.queue;
+		}
+	}
+	
+	/**
+	 * Create a new partition scheduler.
+	 * Create all the internal structures to manage the partitions.
+	 */
+	public PartitionTaskScheduler() {
+		this.jobQueueJobInProgressListener = new JobQueueJobInProgressListener();
+		
+		// Partition management
+		this.partitions = new PartitionsStatus();
+		this.trackerPartition = new HashMap<>();
+		this.disabledTracker = new LinkedList<String>;
+		this.disabledPartitions= new LinkedList<String>;
+		
+		// Job scheduling
+		this.inJobQueue = new LinkedList<JobID>();
+		this.trackerJobs = new HashMap<String,List<JobID>>();
+		this.jobToPartition = new HashMap<JobID,JobSchedule>();
+		
+		// Partition manager
+		this.partitionReader =  new PartitionTaskSchedulerReader(this);
+		this.partitionCleaner = new PartitionTaskSchedulerCleaner(this);
+	}
+
+	@Override
+	public synchronized void start() throws IOException {
+		super.start();
+		taskTrackerManager.addJobInProgressListener(this.jobQueueJobInProgressListener);
+		this.eagerTaskInitializationListener.setTaskTrackerManager(taskTrackerManager);
+		this.eagerTaskInitializationListener.start();
+		taskTrackerManager.addJobInProgressListener(this.eagerTaskInitializationListener);
+		
+		// Partition status management
+		this.partitionReader.start();
+		this.partitionCleaner.start();
+	}
+  
+	@Override
+	public synchronized void terminate() throws IOException {
+		if (this.jobQueueJobInProgressListener != null) {
+			taskTrackerManager.removeJobInProgressListener(this.jobQueueJobInProgressListener);
+		}
+		if (this.eagerTaskInitializationListener != null) {
+			taskTrackerManager.removeJobInProgressListener(this.eagerTaskInitializationListener);
+			this.eagerTaskInitializationListener.terminate();
+		}
+		if (this.partitionReader != null) {
+			this.partitionReader.terminate();
+		}
+		if (this.partitionCleaner != null) {
+			this.partitionCleaner.terminate();
+		}
+		super.terminate();
+	}
+  
+	@Override
+	public synchronized void setConf(Configuration conf) {
+		super.setConf(conf);
+		padFraction = conf.getFloat("mapred.jobtracker.taskalloc.capacitypad", 0.01f);
+		this.eagerTaskInitializationListener = new EagerTaskInitializationListener(conf);
+	}
+
+	/**
+	 * This function assign tasks following FIFO order inside of a partition.
+	 */
+	@Override
+	public synchronized List<Task> assignTasks(TaskTracker taskTracker) throws IOException {
+		LOG.info("DEBUG assignTasks " + taskTracker.getStatus().getHost());
+		// Check for JT safe-mode
+		if (taskTrackerManager.isInSafeMode()) {
+			LOG.info("JobTracker is in safe-mode, not scheduling any tasks.");
+			return null;
+		}
+		
+		TaskTrackerStatus taskTrackerStatus = taskTracker.getStatus(); 
+		ClusterStatus clusterStatus = taskTrackerManager.getClusterStatus();
+		final int numTaskTrackers = clusterStatus.getTaskTrackers();
+		final int clusterMapCapacity = clusterStatus.getMaxMapTasks();
+		final int clusterReduceCapacity = clusterStatus.getMaxReduceTasks();
+		
+		Collection<JobInProgress> jobQueue = this.getJobs();
+		
+		// Get map + reduce counts for the current tracker.
+		final int trackerMapCapacity = taskTrackerStatus.getMaxMapSlots();
+		final int trackerReduceCapacity = taskTrackerStatus.getMaxReduceSlots();
+		final int trackerRunningMaps = taskTrackerStatus.countMapTasks();
+		final int trackerRunningReduces = taskTrackerStatus.countReduceTasks();
+
+		// Assigned tasks
+		List<Task> assignedTasks = new ArrayList<Task>();
+
+		// Partition information
+		String trackerHost = taskTrackerStatus.getHost();
+		String trackerPartition = this.trackerPartition.get(trackerHost);
+		
+		// If we are disabled, we can only run jobs that were already in this node
+		Map<JobID,Boolean> filterJobs = null;
+		if (this.disabledTracker.contains(trackerHost)) {
+			filterJobs = new HashMap<JobID,Boolean>();
+			// Get the jobs that have been running in this host
+			synchronized (this.trackerJobs) {
+				List<JobID> jobsInServer = this.trackerJobs.get(trackerHost);
+				if (jobsInServer != null) {
+					for (JobID auxJobID : jobsInServer) {
+						filterJobs.put(auxJobID, true);
+					}
+				}
+			}
+		}
+		
+		// Compute (running + pending) map and reduce task numbers across pool
+		int remainingReduceLoad = 0;
+		int remainingMapLoad = 0;
+		synchronized (jobQueue) {
+			for (JobInProgress job : jobQueue) {
+				if (job.getStatus().getRunState() == JobStatus.RUNNING) {
+					remainingMapLoad += (job.desiredMaps() - job.finishedMaps());
+					if (job.scheduleReduces()) {
+						remainingReduceLoad += (job.desiredReduces() - job.finishedReduces());
+					}
+				}
+			}
+		}
+
+		// Compute the 'load factor' for maps and reduces
+		double mapLoadFactor = 0.0;
+		if (clusterMapCapacity > 0) {
+			mapLoadFactor = (double)remainingMapLoad / clusterMapCapacity;
+		}
+		double reduceLoadFactor = 0.0;
+		if (clusterReduceCapacity > 0) {
+			reduceLoadFactor = (double)remainingReduceLoad / clusterReduceCapacity;
+		}
+		
+		// In the below steps, we allocate first map tasks (if appropriate),
+		// and then reduce tasks if appropriate.  We go through all jobs
+		// in order of job arrival; jobs only get serviced if their 
+		// predecessors are serviced, too.
+		
+		// We assign tasks to the current taskTracker if the given machine 
+		// has a workload that's less than the maximum load of that kind of
+		// task.
+		// However, if the cluster is close to getting loaded i.e. we don't
+		// have enough _padding_ for speculative executions etc., we only 
+		// schedule the "highest priority" task i.e. the task from the job 
+		// with the highest priority.
+		final int trackerCurrentMapCapacity = Math.min((int)Math.ceil(mapLoadFactor * trackerMapCapacity), trackerMapCapacity);
+		int availableMapSlots = trackerCurrentMapCapacity - trackerRunningMaps;
+		boolean exceededMapPadding = false;
+		if (availableMapSlots > 0) {
+			exceededMapPadding = exceededPadding(true, clusterStatus, trackerMapCapacity);
+		}
+		
+		int numLocalMaps = 0;
+		int numNonLocalMaps = 0;
+		scheduleMaps:
+		for (int i=0; i < availableMapSlots; ++i) {
+			synchronized (jobQueue) {
+				for (JobInProgress job : jobQueue) {
+					// Only schedule jobs that are ready to run
+					if (job.getStatus().getRunState() != JobStatus.RUNNING) {
+						continue;
+					}
+					// Only schedule jobs in our partition
+					String jobPartition = this.jobToPartition.get(job.getJobID());
+					if (!trackerPartition.equals(jobPartition)) {
+						continue;
+					}
+					// If we are disabled, we can only run jobs that were already in this node
+					if (filterJobs != null) {
+						if (!filterJobs.containsKey(job.getJobID())) {
+							continue;
+						}
+					}
+					
+					// Try to schedule a node-local or rack-local Map task
+					Task t = job.obtainNewNodeOrRackLocalMapTask(taskTrackerStatus, numTaskTrackers, taskTrackerManager.getNumberOfUniqueHosts());
+					if (t != null) {
+						assignedTasks.add(t);
+						++numLocalMaps;
+						
+						// Don't assign map tasks to the hilt!
+						// Leave some free slots in the cluster for future task-failures,
+						// speculative tasks etc. beyond the highest priority job
+						if (exceededMapPadding) {
+							break scheduleMaps;
+						}
+						
+						// Try all jobs again for the next Map task 
+						break;
+					}
+					
+					// Try to schedule a node-local or rack-local Map task
+					t = job.obtainNewNonLocalMapTask(taskTrackerStatus, numTaskTrackers, taskTrackerManager.getNumberOfUniqueHosts());
+					if (t != null) {
+						assignedTasks.add(t);
+						++numNonLocalMaps;
+						
+						// We assign at most 1 off-switch or speculative task
+						// This is to prevent TaskTrackers from stealing local-tasks
+						// from other TaskTrackers.
+						break scheduleMaps;
+					}
+				}
+			}
+		}
+		int assignedMaps = assignedTasks.size();
+
+		// Same thing, but for reduce tasks
+		// However we _never_ assign more than 1 reduce task per heartbeat
+		final int trackerCurrentReduceCapacity = Math.min((int)Math.ceil(reduceLoadFactor * trackerReduceCapacity), trackerReduceCapacity);
+		final int availableReduceSlots = Math.min((trackerCurrentReduceCapacity - trackerRunningReduces), 1);
+		boolean exceededReducePadding = false;
+		if (availableReduceSlots > 0) {
+			exceededReducePadding = exceededPadding(false, clusterStatus, trackerReduceCapacity);
+			synchronized (jobQueue) {
+				for (JobInProgress job : jobQueue) {
+					// Only schedule jobs that are ready to run
+					if (job.getStatus().getRunState() != JobStatus.RUNNING || job.numReduceTasks == 0) {
+						continue;
+					}
+					// Only schedule jobs in our partitions
+					String jobPartition = this.jobToPartition.get(job.getJobID());
+					if (!trackerPartition.equals(jobPartition)) {
+						continue;
+					}
+					// If we are disabled, we can only run jobs that were already in this node
+					if (filterJobs != null) {
+						if (!filterJobs.containsKey(job.getJobID())) {
+							continue;
+						}
+					}
+
+					// Try to schedule a Reduce task
+					Task t = job.obtainNewReduceTask(taskTrackerStatus, numTaskTrackers, taskTrackerManager.getNumberOfUniqueHosts());
+					if (t != null) {
+						assignedTasks.add(t);
+						break;
+					}
+					
+					// Don't assign reduce tasks to the hilt!
+					// Leave some free slots in the cluster for future task-failures,
+					// speculative tasks etc. beyond the highest priority job
+					if (exceededReducePadding) {
+						break;
+					}
+				}
+			}
+		}
+
+		if (LOG.isDebugEnabled()) {
+			LOG.debug("Task assignments for " + taskTrackerStatus.getTrackerName() + " --> " +
+					"[" + mapLoadFactor + ", " + trackerMapCapacity + ", " + 
+					trackerCurrentMapCapacity + ", " + trackerRunningMaps + "] -> [" + 
+					(trackerCurrentMapCapacity - trackerRunningMaps) + ", " +
+					assignedMaps + " (" + numLocalMaps + ", " + numNonLocalMaps + 
+					")] [" + reduceLoadFactor + ", " + trackerReduceCapacity + ", " + 
+					trackerCurrentReduceCapacity + "," + trackerRunningReduces + 
+					"] -> [" + (trackerCurrentReduceCapacity - trackerRunningReduces) + 
+					", " + (assignedTasks.size()-assignedMaps) + "]");
+		}
+		
+		
+		if (assignedTasks.size() > 0) {
+			LOG.info("Assigned "+assignedTasks+" to "+trackerHost+"@"+trackerPartition);
+			// Store which job has been running in this tracker
+			for (Task task : assignedTasks) {
+				JobID auxJobID = task.getTaskID().getJobID();
+				this.addJobToTracker(auxJobID, trackerHost);
+			}
+		}
+		
+		return assignedTasks;
+	}
+	
+	/**
+	 * Logs that a job has started running in a host.
+	 */
+	private synchronized void addJobToTracker(JobID jobId, String trackerHost) {
+		synchronized(this.trackerJobs) {
+			List<JobID> jobsInServer = this.trackerJobs.get(trackerHost);
+			if (jobsInServer == null) {
+				jobsInServer = new ArrayList<JobID>();
+				this.trackerJobs.put(trackerHost, jobsInServer);
+			}
+			if (!jobsInServer.contains(jobId)) {
+				jobsInServer.add(jobId);
+			}
+		}
+	}
+
+	/**
+	 * Check if there is enough padding to run extra maps.
+	 */
+	private boolean exceededPadding(boolean isMapTask, ClusterStatus clusterStatus, int maxTaskTrackerSlots) { 
+		int numTaskTrackers = clusterStatus.getTaskTrackers();
+		int totalTasks = (isMapTask) ? clusterStatus.getMapTasks() : clusterStatus.getReduceTasks();
+		int totalTaskCapacity = isMapTask ? clusterStatus.getMaxMapTasks() : clusterStatus.getMaxReduceTasks();
+
+		Collection<JobInProgress> jobQueue = this.getJobs();
+
+		boolean exceededPadding = false;
+		synchronized (jobQueue) {
+			int totalNeededTasks = 0;
+			for (JobInProgress job : jobQueue) {
+				if (job.getStatus().getRunState() != JobStatus.RUNNING || job.numReduceTasks == 0) {
+					continue;
+				}
+
+				// Beyond the highest-priority task, reserve a little 
+				// room for failures and speculative executions; don't 
+				// schedule tasks to the hilt.
+				totalNeededTasks += isMapTask ? job.desiredMaps() : job.desiredReduces();
+				int padding = 0;
+				if (numTaskTrackers > MIN_CLUSTER_SIZE_FOR_PADDING) {
+					padding = Math.min(maxTaskTrackerSlots, (int) (totalNeededTasks * padFraction));
+				}
+				if (totalTasks + padding >= totalTaskCapacity) {
+					exceededPadding = true;
+					break;
+				}
+			}
+		}
+		return exceededPadding;
+	}
+	
+	
+	/**
+	 * Performs the first scheduling for a job.
+	 * @param job
+	 * @throws IOException If there is an error with the submission.
+	 */
+	public void checkJobSubmission(JobInProgress job) throws IOException {
+		LOG.info("DEBUG checkJobSubmission " + job.getJobID());
+		synchronized (this.scheduling) {
+			// Store the order of the job
+			synchronized (this.inJobQueue) {
+				if (!this.inJobQueue.contains(job.getJobID())) {
+					this.inJobQueue.add(job.getJobID());
+				}
+			}
+			LOG.info("************************* Calling scheduleJob from checkJobSubmission");
+			String jobPartition = this.scheduleJob(job);
+			LOG.info("Job "+job.getJobID()+" initially scheduled to "+jobPartition);
+		}
+	}
+	
+	/**
+	 * Decide if a job should run in a tracker.
+	 * This function is used by the JobTracker to decide if a job should run in a tracker.
+	 * Right now, we only check the JOB_SETUP.
+	 * It indirectly performs the partition scheduler.
+	 */
+	public synchronized boolean runJobInTracker(JobInProgress job, String trackerHost) {
+		synchronized (this.scheduling) {
+			LOG.info("DEBUG runJobInTracker 1 " + job.getJobID() + " " + trackerHost);
+			// Check if the job is actually available
+			if (job==null || !job.inited()) {
+				return false;
+			}
+			
+			// Local data for job and partition
+			JobID jobId = job.getJobID();
+			String trackerPartition = this.getTrackerPartition(trackerHost);
+			
+			// Check decommission: if the node or the partition are disabled, we do not accept any new jobs.
+			if (this.disabledTracker.contains(trackerHost) || this.disabledPartitions.contains(trackerPartition)) {
+				return false;
+			}
+			
+			// Check job scheduling
+			LOG.info("DEBUG runJobInTracker 2 " + job.getJobID() + " " + trackerHost);
+			// If the job is already scheduled and we are in the right partition, we can go ahead
+			if (trackerPartition.equals(this.jobToPartition.get(jobId))) {
+				return true;
+			// If the job hasn't been actually submitted to a partition, check what's going on
+			} else if (this.jobToPartition.get(jobId) == null) {
+				// Check scheduled partition
+				String jobPartition = this.jobToPartitionQueue.get(jobId);
+				if (jobPartition == null) {
+					LOG.error("************************* Job "+job.getJobID()+" hasn't been scheduled before.");
+					jobPartition = this.scheduleJob(job);
+				}
+				// If we are in the right partition
+				if (trackerPartition.equals(jobPartition)) {
+					if (this.isReadyToSubmit(jobId, trackerPartition)) {
+						// We can submit the job into this partition, move it to the right queue
+						this.assignJob(jobId, trackerPartition);
+						return true;
+					}
+				}
+			}
+			
+			// Don't run the job yet
+			return false;
+		}
+	}
+	
+	/**
+	 * Check if a job is ready to be submitted to a partition.
+	 */
+	private boolean isReadyToSubmit(JobID jobId, String partitionId) {
+		// Get the first jobId in the queue
+		JobID firstJobId = null;
+		synchronized (this.partitionQueues) {
+			List<JobID> partitionQueue = this.partitionQueues.get(partitionId);
+			if (partitionQueue != null && partitionQueue.size()>0) {
+				firstJobId = partitionQueue.get(0);
+			}
+		}
+		// If we are the first job in the partition queue
+		if (jobId.equals(firstJobId)) {
+			// Check if there is room in the tracker
+			ClusterStatus partition = this.getPartitionStatus(partitionId);
+			double utilMap = 1.0*(partition.getMapTasks())    / partition.getMaxMapTasks();
+			double utilRed = 1.0*(partition.getReduceTasks()) / partition.getMaxReduceTasks();
+			// If we have room (<200%), schedule it 
+			if (utilMap<=2.0 && utilRed<=2.0) {
+				return true;
+			}
+		}
+		return false;
+	}
+
+	/**
+	 * Move the job from the temporary queue to the actual partition.
+	 */
+	private synchronized boolean assignJob(JobID jobId, String trackerPartition) {
+		// Check everything is right
+		if (!this.partitionQueues.get(trackerPartition).get(0).equals(jobId)) {
+			LOG.error("We tried to remove " + jobId + " from the queue of " + trackerPartition + " and it wasn't there.");
+			LOG.error("The queue is: " + this.partitionQueues.get(trackerPartition));
+			return false;
+		}
+		if (!trackerPartition.equals(this.jobToPartitionQueue.get(jobId))) {
+			LOG.error("We tried to move " + jobId + " from "+trackerPartition+" and it was assigned to " + this.jobToPartitionQueue.get(jobId));
+			return false;
+		}
+		// Perform changes
+		// Remove from partition queue
+		synchronized (this.partitionQueues) {
+			this.partitionQueues.get(trackerPartition).remove(0);
+		}
+		// jobToPartitionQueue -> jobToPartition
+		synchronized (this.jobToPartitionQueue) {
+			synchronized (this.jobToPartition) {
+				this.jobToPartitionQueue.remove(jobId);
+				this.jobToPartition.put(jobId, trackerPartition);
+			}
+		}
+		// We don't need to keep the order for this job anymore
+		synchronized (this.inJobQueue) {
+			this.inJobQueue.remove(jobId);
+		}
+		return true;
+	}
+	
+	/**
+	 * Remove all the scheduling we had for a partition.
+	 */
+	public void flushScheduling() {
+		LOG.info("Flushing the scheduling for everybody in the queue!");
+		// We block everybody else
+		synchronized (this.scheduling) {
+			// Clear the temporary scheduling
+			this.jobToPartitionQueue.clear();
+			this.partitionQueues.clear();
+			
+			// Get the jobs that need to be rescheduled
+			LinkedList<JobInProgress> jobsToSchedule = new LinkedList<JobInProgress>();
+			Collection<JobInProgress> jobQueue = this.getJobs();
+			synchronized (jobQueue) {
+				for (JobID jobId : this.inJobQueue) {
+					// Search for the job in the queue
+					JobInProgress job = null;
+					for (JobInProgress auxJob : jobQueue) {
+						if (jobId.equals(auxJob.getJobID())) {
+							job = auxJob;
+							break;
+						}
+					}
+					// Added to the list of jobs that need to be scheduled
+					if (job != null) {
+						jobsToSchedule.add(job);
+					}
+				}
+			}
+			
+			String outQueue =  "";
+			for(JobInProgress job : jobsToSchedule) {
+				outQueue += " "+job.getJobID().toString().substring(job.getJobID().toString().lastIndexOf("_")+1);
+			}
+			LOG.info("We have to reschedule:"+outQueue);
+			// Reschedule jobs in order
+			while (jobsToSchedule.size() > 0) {
+				JobInProgress job = jobsToSchedule.pollFirst();
+				LOG.info("*************************  Reschedule: " + job.getJobID());
+				this.scheduleJob(job);
+				LOG.info(" Rescheduled: " + job.getJobID());
+			}
+		}
+		this.printLogQueue();
+	}
+	
+	// Decide if a job should run in a tracker.
+	/*public synchronized boolean runJobInTrackerOld(JobID jobId, String trackerHost) {
+		// Get our partition
+		Map<String,String> serverPartitions = this.partitionReader.getServerPartition();
+		String trackerPartition = this.getTrackerPartition(trackerHost);
+		
+		// Check if the job was already assigned to the partition
+		String jobPartition = this.jobToPartition.get(jobId);
+		if (jobPartition != null) {
+			if (trackerPartition.equals(jobPartition)) {
+				return true;
+			} else {
+				return false;
+			}
+		} else {
+			// If the node is disabled, we do not accept any new jobs.
+			if (this.partitionReader.getDisabledServers().contains(trackerHost)) {
+				LOG.info(trackerHost+"@"+trackerPartition + ": The node is under decomision! No new jobs!");
+				return false;
+			}
+			// If the whole partition is disabled, we do not accept any new jobs.
+			if (this.partitionReader.getDisabledPartitions().contains(trackerPartition)) {
+				LOG.info(trackerHost+"@"+trackerPartition + ": The partition is under decomision! No new jobs!");
+				return false;
+			}
+			
+			// Get job information
+			Collection<JobInProgress> jobQueue = this.getJobs();
+			JobInProgress job = null;
+			for (JobInProgress jobCheck : jobQueue) {
+				if (jobCheck.getJobID().equals(jobId)) {
+					job = jobCheck;
+				}
+			}
+		
+			// Get partitions (without decommission)
+			List<String> partitions = new ArrayList<String>();
+			for (String partitionId : this.partitionReader.getPartitions()) {
+				if (!this.partitionReader.getDisabledPartitions().contains(partitionId)) {
+					partitions.add(partitionId);
+				}
+			}
+			Map<String, ClusterStatus> partitionsStats = getPartitionStats(partitions);
+			
+			// DEBUGGING
+			LOG.info("Partitions:");
+			for (String partitionId : partitions) {
+				ClusterStatus cluster = partitionsStats.get(partitionId);
+				double utilMap = 1.0*(cluster.getMapTasks())    / cluster.getMaxMapTasks();
+				double utilRed = 1.0*(cluster.getReduceTasks()) / cluster.getMaxReduceTasks();
+				double utilMapPost = 1.0*(cluster.getMapTasks()    + job.desiredMaps())    / cluster.getMaxMapTasks();
+				double utilRedPost = 1.0*(cluster.getReduceTasks() + job.desiredReduces()) / cluster.getMaxReduceTasks();
+				LOG.info("  " + partitionId + " N:" + cluster.getTaskTrackers() +
+					" M:" + String.format("%5.1f%%", utilMap*100.0) + " R:" + String.format("%5.1f%%", utilRed*100.0) +
+					" M:" + String.format("%5.1f%%", utilMapPost*100.0) + " R:" + String.format("%5.1f%%", utilRedPost*100.0));
+			}
+			
+			// Select the best partition to run this jobs
+			String bestPartition = getBestPartition(partitions, partitionsStats, job);
+			LOG.info(trackerHost+"@"+trackerPartition + ": The best partition is " + bestPartition);
+			
+			// Check if we are in that partition
+			if (trackerPartition.equals(bestPartition)) {
+				// Assign job to partition
+				LOG.info(trackerHost+"@"+trackerPartition + ": This is the best partition. Run "+jobId+" here!");
+				// Job -> Partition
+				synchronized (this.jobToPartition) {
+					this.jobToPartition.put(jobId, trackerPartition);
+				}
+				// Server -> [Jobs]
+				this.addJobToTracker(jobId, trackerHost);
+				return true;
+			} else {
+				return false;
+			}
+		}
+	}*/
+	
+	/**
+	 * Selects the best partition to run a job.
+	 * We use three criteria to select the partition:
+	 * 1) Try to keep partitions utilization balanced.
+	 * 3) Heavy jobs go to lower partitions.
+	 */
+	private synchronized String scheduleJob(JobInProgress job) {
+		// Collect the status of the partitions (including the queues)
+		List<String> partitions = new ArrayList<String>();
+		for (String partitionId : this.partitionReader.getPartitions()) {
+			if (!this.isPartitionDisabled(partitionId)) {
+				partitions.add(partitionId);
+			}
+		}
+		Map<String, ClusterStatus> partitionsStats = this.getPartitionsStatus(partitions, true);
+		
+		// Get the best partition to run the job
+		String bestPartition = getBestPartition(partitions, partitionsStats, job);
+		
+		// Maintain scheduling information
+		synchronized (this.jobToPartitionQueue) {
+			synchronized (this.partitionQueues) {
+				synchronized (this.inJobQueue) {
+		LOG.info("3......assigning");
+					// Temporary assign the job to a partition
+					this.jobToPartitionQueue.put(job.getJobID(), bestPartition);
+		
+					// Assign to the partition queue
+					List<JobID> partitionQueue = this.partitionQueues.get(bestPartition);
+					if (partitionQueue == null) {
+						partitionQueue = new LinkedList<JobID>();
+						this.partitionQueues.put(bestPartition, partitionQueue);
+					}
+					partitionQueue.add(job.getJobID());
+		
+		LOG.info("4......assigning");
+					// Store the order of the job
+					if (!this.inJobQueue.contains(job.getJobID())) {
+						this.inJobQueue.add(job.getJobID());
+					}
+		LOG.info("5......internal queue");
+				}
+			}
+		}
+		
+		// Debug
+		LOG.info("==================================================================");
+		LOG.info("Scheduling " + job.getJobID());
+		LOG.info("Partitions status:");
+		for (String partitionId : partitions) {
+			// Internal queue
+			String outQueue = "  Q:" ;
+			List<JobID> partitionQueue = this.partitionQueues.get(partitionId);
+			if (partitionQueue != null && partitionQueue.size()>0) {
+				for(JobID jobId : partitionQueue) {
+					outQueue += " "+jobId.toString().substring(jobId.toString().lastIndexOf("_")+1);
+				}
+			}
+			// Cluster status
+			ClusterStatus cluster = partitionsStats.get(partitionId);
+			double utilMap = 1.0*(cluster.getMapTasks())    / cluster.getMaxMapTasks();
+			double utilRed = 1.0*(cluster.getReduceTasks()) / cluster.getMaxReduceTasks();
+			double utilMapPost = 1.0*(cluster.getMapTasks()    + job.desiredMaps())    / cluster.getMaxMapTasks();
+			double utilRedPost = 1.0*(cluster.getReduceTasks() + job.desiredReduces()) / cluster.getMaxReduceTasks();
+			LOG.info("   " + partitionId + "\tN:" + cluster.getTaskTrackers() + " " + 
+				"M:" + cluster.getMapTasks() + "/" + cluster.getMaxMapTasks() + " R:" + cluster.getReduceTasks() + "/" + cluster.getMaxReduceTasks() +
+				"\t=> " +
+				"M:" + String.format("%5.1f%%", utilMap*100.0) +     " R:" + String.format("%5.1f%%", utilRed*100.0) + "   " +
+				"M:" + String.format("%5.1f%%", utilMapPost*100.0) + " R:" + String.format("%5.1f%%", utilRedPost*100.0) +
+				outQueue);
+		}
+		this.printLogQueue();
+		LOG.info(job.getJobID() + " assigned to " + bestPartition);
+		LOG.info("==================================================================");
+		// Debug
+		
+		return bestPartition;
+	}
+	
+	/**
+	 * Function to print the jobs in the system.
+	 */
+	public void printLogQueue() {
+		LOG.info("Queue:");
+		Collection<JobInProgress> jobQueue = this.getJobs();
+		synchronized (jobQueue) {
+			for (JobInProgress jobCheck : jobQueue) {
+				JobID jobId = jobCheck.getJobID();
+				// Assigned temporally to a partition
+				if (this.jobToPartitionQueue.containsKey(jobId)) {
+					LOG.info(" Q " + jobId +
+						" M:" + jobCheck.finishedMaps() + "/" + jobCheck.desiredMaps() +
+						" R:" + jobCheck.finishedReduces() + "/" + jobCheck.desiredReduces() +
+						" Part:" + this.getJobPartition(jobId));
+				// Not assigned to any partition
+				} else if (this.getJobPartition(jobId) == null) {
+					LOG.info("   " + jobId +
+						" M:" + jobCheck.finishedMaps() + "/" + jobCheck.desiredMaps() +
+						" R:" + jobCheck.finishedReduces() + "/" + jobCheck.desiredReduces());
+				// Already assigned to a partition
+				} else {
+					LOG.info(" R " + jobId +
+						" M:" + jobCheck.finishedMaps() + "/" + jobCheck.desiredMaps() +
+						" R:" + jobCheck.finishedReduces() + "/" + jobCheck.desiredReduces() +
+						" Part:" + this.getJobPartition(jobId));
+				}
+			}
+		}
+		// Check the order of the jobs that are not submitted yet
+		String outQueue = "   Order:" ;
+		for(JobID jobId : this.inJobQueue) {
+			outQueue += " "+jobId.toString().substring(jobId.toString().lastIndexOf("_")+1);
+		}
+		LOG.info(outQueue);
+	}
+	
+	/**
+	 * Select the best partition to run a job.
+	 */
+	private String getBestPartition(List<String> partitions, Map<String, ClusterStatus> partitionsStats, JobInProgress job) {
+		// Choose the partition that has room to run the job right now
+		for (String partitionId : partitions) {
+			ClusterStatus cluster = partitionsStats.get(partitionId);
+			double utilMapPost = 1.0*(cluster.getMapTasks()    + job.desiredMaps())    / cluster.getMaxMapTasks();
+			double utilRedPost = 1.0*(cluster.getReduceTasks() + job.desiredReduces()) / cluster.getMaxReduceTasks();
+			if (utilMapPost <= 1.0 && utilRedPost <= 1.0) {
+				return partitionId;
+			}
+		}
+		
+		// If we have to wait, we try to balance the queues
+		String bestPartition = null;
+		double bestUtilization = 999999999;
+		for (String partitionId : partitions) {
+			// We try to assign smaller jobs to the top (piramid style)
+			// Limit the size of the jobs for partitions
+			//   100% partition4:   0%   10
+			//    75% partition3:  33%  336
+			//    50% partition2:  66%  663
+			//    25% partition1: 100% 1000 (this is the maximum job size)
+			double partitionRanking = 1.0 - (1.0*partitions.indexOf(partitionId)/(partitions.size()-1));
+			if (partitions.size() <= 1) {
+				partitionRanking = 1.0;
+			}
+			double partitionMaxSize = MIN_JOB_SIZE_BOUND + partitionRanking*(MAX_JOB_SIZE_BOUND-MIN_JOB_SIZE_BOUND);
+			
+			LOG.info("-----> " + partitionId + "\t" + partitions.indexOf(partitionId) + "/" + (partitions.size()-1) + "=> Rank:" + partitionRanking + " MaxSize:"+partitionMaxSize);
+			
+			if (job.desiredMaps() <= partitionMaxSize && job.desiredReduces() <= partitionMaxSize) {
+				ClusterStatus cluster = partitionsStats.get(partitionId);
+				double utilMap = 1.0*(cluster.getMapTasks())    / cluster.getMaxMapTasks();
+				double utilRed = 1.0*(cluster.getReduceTasks()) / cluster.getMaxReduceTasks();
+				// Get the less utilized
+				if (utilMap < bestUtilization && utilRed < bestUtilization) {
+					bestPartition = partitionId;
+					bestUtilization = Math.max(utilMap, utilRed);
+				}
+			}
+		}
+		
+		return bestPartition;
+		
+		// We better have something in the queue
+		/*for (String partitionId : partitions) {
+			ClusterStatus cluster = partitionsStats.get(partitionId);
+			double utilMapPost = 1.0*(cluster.getMapTasks()    + job.desiredMaps())    / cluster.getMaxMapTasks();
+			double utilRedPost = 1.0*(cluster.getReduceTasks() + job.desiredReduces()) / cluster.getMaxReduceTasks();
+			if (utilMapPost <= 2.0 && utilRedPost <= 2.0) {
+				return partitionId;
+			}
+		}*/
+		
+		// Choose the partition that gets not too crowded
+		/*for (String partitionId : partitions) {
+			ClusterStatus cluster = partitionsStats.get(partitionId);
+			double utilMap = 1.0*(cluster.getMapTasks()    + job.desiredMaps())    / cluster.getMaxMapTasks();
+			double utilRed = 1.0*(cluster.getReduceTasks() + job.desiredReduces()) / cluster.getMaxReduceTasks();
+			if (utilMap <= 1.5 && utilRed <= 1.5) {
+				return partitionId;
+			}
+		}*/
+		
+		// Choose the partition that is not overassigned (<200%), hold the job otherwise
+		// Get the last available partition
+		/*String lastPartitionId = null;
+		if (partitions.size() > 1) {
+			lastPartitionId = partitions.get(partitions.size()-1);
+		}
+		// Choose the partition that is not overassigned (<200%), hold the job otherwise
+		double bestUtilization = 2.0;
+		for (String partitionId : partitions) {
+			// Don't oversubscribe the last partition
+			if (!partitionId.equals(lastPartitionId)) {
+				ClusterStatus cluster = partitionsStats.get(partitionId);
+				double utilMap = 1.0*(cluster.getMapTasks())    / cluster.getMaxMapTasks();
+				double utilRed = 1.0*(cluster.getReduceTasks()) / cluster.getMaxReduceTasks();
+				if (utilMap < bestUtilization && utilRed < bestUtilization) {
+					bestPartition = partitionId;
+					bestUtilization = Math.max(utilMap, utilRed);
+				}
+			}
+		}*/
+		
+		/*         10       2000
+		| pod100 | |
+		|  pod75 |    |
+		|  pod50 |       |
+		|  pod25 |          |
+		*/
+		
+// 		TODO
+		/* String bestPartition = null;
+		for (int i=0; i<partitions.size(); i++) {
+			String partitionId = partitions.get(i);
+			ClusterStatus cluster = partitionsStats.get(partitionId);
+			double utilMap = 1.0*(cluster.getMapTasks())    / cluster.getMaxMapTasks();
+			double utilRed = 1.0*(cluster.getReduceTasks()) / cluster.getMaxReduceTasks();
+			double utilMapPost = 1.0*(cluster.getMapTasks()    + job.desiredMaps())    / cluster.getMaxMapTasks();
+			double utilRedPost = 1.0*(cluster.getReduceTasks() + job.desiredReduces()) / cluster.getMaxReduceTasks();
+			if (utilMap <= 1.5 && utilRed <= 1.5) {
+				if () {
+				
+				}
+			}
+		}*/
+	}
+	
+	/**
+	 * Cleans the internal structures.
+	 * Sometimes an internal data structure might get outdated.
+	 */
+	public void cleanInternalStructures() {
+		synchronized(this.trackerJobs) {
+			synchronized(this.jobToPartitionQueue) {
+				synchronized(this.jobToPartition) {
+					synchronized(this.inJobQueue) {
+						// Get the list of jobs that are in the system
+						Map<JobID,Boolean> outdatedJobs = new HashMap<JobID,Boolean>();
+						for (JobID jobId : this.jobToPartition.keySet()) {
+							if (!outdatedJobs.containsKey(jobId)) {
+								outdatedJobs.put(jobId, true);
+							}
+						}
+						for (JobID jobId : this.jobToPartitionQueue.keySet()) {
+							if (!outdatedJobs.containsKey(jobId)) {
+								outdatedJobs.put(jobId, true);
+							}
+						}
+						for (JobID jobId : this.inJobQueue) {
+							if (!outdatedJobs.containsKey(jobId)) {
+								outdatedJobs.put(jobId, true);
+							}
+						}
+						for (List<JobID> listJobs : this.trackerJobs.values()) {
+							for (JobID jobId : listJobs) {
+								if (!outdatedJobs.containsKey(jobId)) {
+									outdatedJobs.put(jobId, true);
+								}
+							}
+						}
+						
+						
+						LOG.info("Jobs in the internal structures: " + outdatedJobs.keySet());
+						
+						// Remove the jobs are in the queue
+						Collection<JobInProgress> jobQueue = this.getJobs();
+						synchronized (jobQueue) {
+							for (JobInProgress auxJob : jobQueue) {
+								outdatedJobs.remove(auxJob.getJobID());
+							}
+						}
+						
+						
+						LOG.info("Jobs not used: " + outdatedJobs.keySet());
+						
+						// Remove outdated jobs from everywhere
+						for (JobID jobId : outdatedJobs.keySet()) {
+							LOG.info("Job "+jobId+" is not in the system anymore. Remove from internal data structures.");
+							//this.jobToPartition.remove(jobId);
+							this.jobToPartitionQueue.remove(jobId);
+							this.inJobQueue.remove(jobId);
+							for (List<JobID> listJobs : this.trackerJobs.values()) {
+								listJobs.remove(jobId);
+							}
+						}
+					}
+				}
+			}
+		}
+	}
+	
+	/**
+	 * Get the list of jobs that are in the queue.
+	 */
+	public synchronized Collection<JobInProgress> getJobs() {
+		return this.getJobs(null);
+	}
+	
+	/**
+	 * Get the list of jobs that are in the queue.
+	 */
+	@Override
+	public synchronized Collection<JobInProgress> getJobs(String queueName) {
+		if (queueName == null || queueName.equals("default")) {
+			return this.jobQueueJobInProgressListener.getJobQueue();
+		} else {
+			LinkedList<JobInProgress> ret = new LinkedList<JobInProgress>();
+			for (JobInProgress job : this.jobQueueJobInProgressListener.getJobQueue()) {
+				if (queueName.equals(this.getJobPartition(job.getJobID()))) {
+					ret.add(job);
+				}
+			}
+			return ret;
+		}
+	}
+	
+	/**
+	 * Get a job from the queue.
+	 */
+	private JobInProgress getJob(JobID jobId) {
+		JobInProgress job = null;
+		Collection<JobInProgress> jobQueue = this.getJobs();
+		synchronized(jobQueue) {
+			for (JobInProgress auxJob : jobQueue) {
+				if (jobId.equals(auxJob.getJobID())) {
+					job = auxJob;
+					break;
+				}
+			}
+		}
+		return job;
+	}
+}
Index: src/contrib/partitionscheduler/src/java/org/apache/hadoop/mapred/PartitionTaskSchedulerReader.java
===================================================================
--- src/contrib/partitionscheduler/src/java/org/apache/hadoop/mapred/PartitionTaskSchedulerReader.java	(revision 0)
+++ src/contrib/partitionscheduler/src/java/org/apache/hadoop/mapred/PartitionTaskSchedulerReader.java	(working copy)
@@ -0,0 +1,186 @@
+package org.apache.hadoop.mapred;
+
+import java.io.File;
+import java.io.IOException;
+
+import java.util.Scanner;
+
+import java.util.Map;
+import java.util.List;
+import java.util.HashMap;
+import java.util.ArrayList;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+
+/**
+ * Reads a file containing the list of disabled nodes.
+ */
+public class PartitionTaskSchedulerReader extends Thread {
+	public static final Log LOG = LogFactory.getLog(PartitionTaskSchedulerReader.class);
+	
+	private static final int SLEEP_PERIOD = 1*1000; // 1 second
+	
+	private static final String FILE_SERVERS_PARTITION =     "conf/serverPods";
+	private static final String FILE_PARTITIONS =            "conf/pods";
+	private static final String FILE_DECOMISSION_PARTITION = "conf/disabledPods";
+	private static final String FILE_DECOMISSION_SERVER =    "conf/disabledServers";
+	
+	private PartitionTaskScheduler scheduler;
+
+	private boolean keepRunning;
+
+	/**
+	 * Default constructor that initializes the data structures.
+	 */
+	public PartitionTaskSchedulerReader(PartitionTaskScheduler scheduler) {
+		super("partitionReader");
+		this.scheduler = scheduler;
+		
+		// Set it ready to run
+		this.keepRunning = true;
+	}
+
+	/**
+	 * Stop the polling of the files.
+	 */
+	public void terminate() {
+		this.keepRunning = false;
+	}
+
+	/**
+	 * Periodically checks if the file have changed and updates the data structure.
+	 */
+	public void run() {
+		while (keepRunning) {
+			try {
+				this.readFiles();
+				
+				// Wait to re-read the files
+				Thread.sleep(SLEEP_PERIOD);
+			} catch (Exception ex) {
+				LOG.error(ex.getMessage());
+			}
+		}
+	}
+	
+	/**
+	 * Read the list of files.
+	 * Notifies if there is any chance.
+	 */
+	public boolean readFiles() {
+		boolean change = false;
+	
+		// trackerHost -> partitionId
+		synchronized (this.scheduler.trackerPartition) {
+			Map<String,String> trackerPartition = convertListToMap(readHostFile(FILE_SERVERS_PARTITION));
+			if (!this.scheduler.trackerPartition.equals(trackerPartition)) {
+				this.scheduler.trackerPartition.clear();
+				this.scheduler.trackerPartition.putAll(trackerPartition);
+				LOG.info("Server partitions: " + this.scheduler.trackerPartition);
+				this.scheduler.flushScheduling();
+				change = true;
+			}
+		}
+		
+		// Disable trackers
+		synchronized (this.scheduler.disabledTrackers) {
+			List<String> disabledTrackers = readHostFile(FILE_DECOMISSION_SERVER);
+			if (!this.scheduler.disabledTrackers.equals(disabledTrackers)) {
+				this.scheduler.disabledTrackers.clear();
+				this.scheduler.disabledTrackers.addAll(disabledTrackers);
+				LOG.info("Decomission servers: " + this.scheduler.disabledTrackers);
+				this.scheduler.flushScheduling();
+				change = true;
+			}
+		}
+		
+		synchronized (this.scheduler.partitions) {
+			// Partitions
+			List<String> partitions = readHostFile(FILE_PARTITIONS);
+			if (!this.scheduler.partitions.order.equals(partitions)) {
+				// Update order
+				this.scheduler.partitions.order.clear();
+				this.scheduler.partitions.order.addAll(partitions);
+				
+				// Remove the partitons that are
+				for (String oldPartitionId : this.scheduler.partitions.order) {
+					if (!partitions.contains(oldPartitionId)) {
+						this.scheduler.partitions.info.remove(oldPartitionId);
+					}
+				}
+				
+				// Add the new partitions
+				for (String newPartitionId : partitions) {
+					if (!this.scheduler.partitions.info.containsKey(newPartitionId)) {
+						this.scheduler.partitions.info.put(newPartitionId, new PartitionStatus(newPartitionId));
+					}
+				}
+				
+				LOG.info("Partitions: " + this.scheduler.partitions.order);
+				this.scheduler.flushScheduling();
+				change = true;
+			}
+			
+			// Disable partitions
+			List<String> disablePartitions = readHostFile(FILE_DECOMISSION_PARTITION);
+			boolean partitionChange = false;
+			for (PartitionStatus partition : this.scheduler.partitions.getAll()) {
+				if (!partition.available && !disablePartitions.contains(partition.partitionId)) {
+					partition.available = true;
+					partitionChange = true;
+				} else if (partition.available && disablePartitions.contains(partition.partitionId)) {
+					partition.available = false;
+					partitionChange = true;
+				}
+			}
+			if (partitionChange) {
+				LOG.info("Decomission partitions: " + disablePartitions);
+				this.scheduler.flushScheduling();
+				change = true;
+			}
+		}
+		
+		return change;
+	}
+	
+	/**
+	 * Convert a list into a map.
+	 */
+	private static Map<String,String> convertListToMap(List<String> list) {
+		Map<String,String> ret = new HashMap<String,String>();
+		for (String s : list) {
+			s = s.replaceAll("\t", " ");
+			while (s.indexOf("  ") >= 0) {
+				s = s.replaceAll("  ", " ");
+			}
+			String[] ssplit = s.split(" ");
+			if (ssplit.length==2) {
+				ret.put(ssplit[0], ssplit[1]);
+			}
+		}
+		return ret;
+	}
+	
+	/**
+	 * Read a file containing a list of hosts.
+	 */
+	private static List<String> readHostFile(String filename) {
+		List<String> ret = new ArrayList<String>();
+		try {
+			File file = new File(filename);
+			Scanner fileScanner = new Scanner(file);
+			while(fileScanner.hasNext()) {
+				String next = fileScanner.nextLine();
+				next = next.trim();
+				if (!next.startsWith("#") && !next.equals("")) {
+					ret.add(next);
+				}
+			}
+		} catch (IOException ex) {
+			LOG.error("Error reading file " + filename + ": " + ex.getMessage());
+		}
+		return ret;
+	}
+}
+
Index: src/contrib/partitionscheduler/src/java/org/apache/hadoop/mapred/PartitionTaskSchedulerUpdater.java
===================================================================
--- src/contrib/partitionscheduler/src/java/org/apache/hadoop/mapred/PartitionTaskSchedulerUpdater.java	(revision 0)
+++ src/contrib/partitionscheduler/src/java/org/apache/hadoop/mapred/PartitionTaskSchedulerUpdater.java	(working copy)
@@ -0,0 +1,64 @@
+package org.apache.hadoop.mapred;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+
+/**
+ * Periodically updates the internal data of the scheduler.
+ */
+public class PartitionTaskSchedulerUpdater extends Thread {
+	public static final Log LOG = LogFactory.getLog(PartitionTaskSchedulerUpdater.class);
+	
+	private static final int SLEEP_PERIOD = 5*1000; // 5 seconds
+	
+	private PartitionTaskScheduler scheduler;
+
+	private boolean keepRunning;
+	
+	/**
+	 * Default constructor that initializes the data structures.
+	 */
+	public PartitionTaskSchedulerUpdater(PartitionTaskScheduler scheduler) {
+		super("partitionUpdater");
+		this.scheduler = scheduler;
+		
+		// Set it ready to run
+		this.keepRunning = true;
+	}
+
+	/**
+	 * Stop the polling of the files.
+	 */
+	public void terminate() {
+		this.keepRunning = false;
+	}
+
+	/**
+	 * Periodically checks if the file have changed and updates the data structure.
+	 */
+	public void run() {
+		for (int i=0; keepRunning; i++) {
+			try {
+				// Regenerate the scheduling every 10 minutes 
+				if (i%120 == 0) {
+					// This also flushes the partitions
+					this.scheduler.flushScheduling();
+				// Check if the queues are balanced, and triggets rescheduling otherwise
+				} else if (i%5 == 0 && !this.scheduler.isQueueBalanced()) {
+					// This also flushes the partitions
+					this.scheduler.flushScheduling();
+				} else {
+					// Update the data periodically
+					this.scheduler.flushPartitionData();
+				}
+				
+				// Wait to re-read the files
+				Thread.sleep(SLEEP_PERIOD);
+			} catch (Exception ex) {
+				LOG.error(ex.getMessage());
+			}
+		}
+	}
+}
+	
+	
Index: src/contrib/partitionscheduler/src/java/org/apache/hadoop/mapred/PartitionsStatus.java
===================================================================
--- src/contrib/partitionscheduler/src/java/org/apache/hadoop/mapred/PartitionsStatus.java	(revision 0)
+++ src/contrib/partitionscheduler/src/java/org/apache/hadoop/mapred/PartitionsStatus.java	(working copy)
@@ -0,0 +1,50 @@
+package org.apache.hadoop.mapred;
+
+import java.util.List;
+import java.util.LinkedList;
+import java.util.Map;
+import java.util.HashMap;
+
+/**
+ * Represents the status of all the partitions.
+ */
+public class PartitionsStatus {
+	public List<String> order = new LinkedList<String>();
+	public Map<String,PartitionStatus> info = new HashMap<String,PartitionStatus>();
+	
+	/**
+	 * Get the status of a partition.
+	 */
+	public PartitionStatus get(String partitionId) {
+		return this.info.get(partitionId);
+	}
+	
+	/**
+	 * Get the list of all partitions in order.
+	 */
+	public List<PartitionStatus> getAll() {
+		return this.getAll(false);
+	}
+	
+	/**
+	 * Get the list of available partitions in order.
+	 */
+	public List<PartitionStatus> getAvailables() {
+		return this.getAll(true);
+	}
+	
+	/**
+	 * Get the list of available partitions in order.
+	 * We can filter the partitions that are available.
+	 */
+	public List<PartitionStatus> getAll(boolean filterAvailable) {
+		List<PartitionStatus> ret = new LinkedList<PartitionStatus>();
+		for (String partitionId : this.order) {
+			PartitionStatus partition = this.info.get(partitionId);
+			if (!filterAvailable || partition.available) {
+				ret.add(partition);
+			}
+		}
+		return ret;
+	}
+}
Index: src/contrib/partitionscheduler/src/test/org/apache/hadoop/mapred/FakeSchedulable.java
===================================================================
--- src/contrib/partitionscheduler/src/test/org/apache/hadoop/mapred/FakeSchedulable.java	(revision 0)
+++ src/contrib/partitionscheduler/src/test/org/apache/hadoop/mapred/FakeSchedulable.java	(working copy)
@@ -0,0 +1,124 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.mapred;
+
+import java.io.IOException;
+import java.util.Collection;
+
+import org.apache.hadoop.mapreduce.TaskType;
+
+/**
+ * Dummy implementation of Schedulable for unit testing.
+ */
+public class FakeSchedulable extends Schedulable {
+  private int demand;
+  private int runningTasks;
+  private int minShare;
+  private double weight;
+  private JobPriority priority;
+  private long startTime;
+  
+  public FakeSchedulable() {
+    this(0, 0, 1, 0, 0, JobPriority.NORMAL, 0);
+  }
+  
+  public FakeSchedulable(int demand) {
+    this(demand, 0, 1, 0, 0, JobPriority.NORMAL, 0);
+  }
+  
+  public FakeSchedulable(int demand, int minShare) {
+    this(demand, minShare, 1, 0, 0, JobPriority.NORMAL, 0);
+  }
+  
+  public FakeSchedulable(int demand, int minShare, double weight) {
+    this(demand, minShare, weight, 0, 0, JobPriority.NORMAL, 0);
+  }
+  
+  public FakeSchedulable(int demand, int minShare, double weight, int fairShare,
+      int runningTasks, JobPriority priority, long startTime) {
+    this.demand = demand;
+    this.minShare = minShare;
+    this.weight = weight;
+    setFairShare(fairShare);
+    this.runningTasks = runningTasks;
+    this.priority = priority;
+    this.startTime = startTime;
+  }
+  
+  @Override
+  public Task assignTask(TaskTrackerStatus tts, long currentTime,
+      Collection<JobInProgress> visited) throws IOException {
+    return null;
+  }
+
+  @Override
+  public int getDemand() {
+    return demand;
+  }
+
+  @Override
+  public String getName() {
+    return "FakeSchedulable" + this.hashCode();
+  }
+
+  @Override
+  public JobPriority getPriority() {
+    return priority;
+  }
+
+  @Override
+  public int getRunningTasks() {
+    return runningTasks;
+  }
+
+  @Override
+  public long getStartTime() {
+    return startTime;
+  }
+  
+  @Override
+  public double getWeight() {
+    return weight;
+  }
+  
+  @Override
+  public int getMinShare() {
+    return minShare;
+  }
+
+  @Override
+  public void redistributeShare() {}
+
+  @Override
+  public void updateDemand() {}
+
+  @Override
+  public TaskType getTaskType() {
+    return TaskType.MAP;
+  }
+
+  @Override
+  protected String getMetricsContextName() {
+    return "fake";
+  }
+
+  @Override
+  void updateMetrics() {
+  }
+}
Index: src/contrib/partitionscheduler/src/test/org/apache/hadoop/mapred/TestCapBasedLoadManager.java
===================================================================
--- src/contrib/partitionscheduler/src/test/org/apache/hadoop/mapred/TestCapBasedLoadManager.java	(revision 0)
+++ src/contrib/partitionscheduler/src/test/org/apache/hadoop/mapred/TestCapBasedLoadManager.java	(working copy)
@@ -0,0 +1,160 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.mapred;
+
+import java.util.ArrayList;
+import java.util.List;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.mapred.TaskStatus.State;
+
+import junit.framework.TestCase;
+
+/**
+ * Exercise the canAssignMap and canAssignReduce methods in 
+ * CapBasedLoadManager.
+ */
+public class TestCapBasedLoadManager extends TestCase {
+  
+  /**
+   * Returns a running MapTaskStatus.
+   */
+  private TaskStatus getRunningMapTaskStatus() {
+    TaskStatus ts = new MapTaskStatus();
+    ts.setRunState(State.RUNNING);
+    return ts;
+  }
+
+  /**
+   * Returns a running ReduceTaskStatus.
+   */
+  private TaskStatus getRunningReduceTaskStatus() {
+    TaskStatus ts = new ReduceTaskStatus();
+    ts.setRunState(State.RUNNING);
+    return ts;
+  }
+  
+  /**
+   * Returns a TaskTrackerStatus with the specified statistics. 
+   * @param mapCap        The capacity of map tasks 
+   * @param reduceCap     The capacity of reduce tasks
+   * @param runningMap    The number of running map tasks
+   * @param runningReduce The number of running reduce tasks
+   */
+  private TaskTrackerStatus getTaskTrackerStatus(int mapCap, int reduceCap, 
+      int runningMap, int runningReduce) {
+    List<TaskStatus> ts = new ArrayList<TaskStatus>();
+    for (int i = 0; i < runningMap; i++) {
+      ts.add(getRunningMapTaskStatus());
+    }
+    for (int i = 0; i < runningReduce; i++) {
+      ts.add(getRunningReduceTaskStatus());
+    }
+    TaskTrackerStatus tracker = new TaskTrackerStatus("tracker", 
+        "tracker_host", 1234, ts, 0, 0, mapCap, reduceCap);
+    return tracker;
+  }
+
+  /**
+   * A single test of canAssignMap.
+   */
+  private void oneTestCanAssignMap(float maxDiff, int mapCap, int runningMap,
+      int totalMapSlots, int totalRunnableMap, int expectedAssigned) {
+    
+    CapBasedLoadManager manager = new CapBasedLoadManager();
+    Configuration conf = new Configuration();
+    conf.setFloat("mapred.fairscheduler.load.max.diff", maxDiff);
+    manager.setConf(conf);
+    
+    TaskTrackerStatus ts = getTaskTrackerStatus(mapCap, 1, runningMap, 1);
+    
+    int numAssigned = 0;
+    while (manager.canAssignMap(ts, totalRunnableMap, totalMapSlots, numAssigned)) {
+      numAssigned++;
+    }
+      
+    assertEquals( "When maxDiff=" + maxDiff + ", with totalRunnableMap=" 
+        + totalRunnableMap + " and totalMapSlots=" + totalMapSlots
+        + ", a tracker with runningMap=" + runningMap + " and mapCap="
+        + mapCap + " should be able to assign " + expectedAssigned + " maps",
+        expectedAssigned, numAssigned);
+  }
+  
+  
+  /** 
+   * Test canAssignMap method.
+   */
+  public void testCanAssignMap() {
+    oneTestCanAssignMap(0.0f, 5, 0, 50, 1, 1);
+    oneTestCanAssignMap(0.0f, 5, 1, 50, 10, 0);
+    // 20% load + 20% diff = 40% of available slots, but rounds
+    // up with floating point error: so we get 3/5 slots on TT.
+    // 1 already taken, so assigns 2 more
+    oneTestCanAssignMap(0.2f, 5, 1, 50, 10, 2);
+    oneTestCanAssignMap(0.0f, 5, 1, 50, 11, 1);
+    oneTestCanAssignMap(0.0f, 5, 2, 50, 11, 0);
+    oneTestCanAssignMap(0.3f, 5, 2, 50, 6, 1);
+    oneTestCanAssignMap(1.0f, 5, 5, 50, 50, 0);
+  }
+  
+  
+  /**
+   * A single test of canAssignReduce.
+   */
+  private void oneTestCanAssignReduce(float maxDiff, int reduceCap,
+      int runningReduce, int totalReduceSlots, int totalRunnableReduce,
+      int expectedAssigned) {
+    
+    CapBasedLoadManager manager = new CapBasedLoadManager();
+    Configuration conf = new Configuration();
+    conf.setFloat("mapred.fairscheduler.load.max.diff", maxDiff);
+    manager.setConf(conf);
+    
+    TaskTrackerStatus ts = getTaskTrackerStatus(1, reduceCap, 1,
+        runningReduce);
+    
+    int numAssigned = 0;
+    while (manager.canAssignReduce(ts, totalRunnableReduce, totalReduceSlots, numAssigned)) {
+      numAssigned++;
+    }
+      
+    assertEquals( "When maxDiff=" + maxDiff + ", with totalRunnableReduce=" 
+        + totalRunnableReduce + " and totalReduceSlots=" + totalReduceSlots
+        + ", a tracker with runningReduce=" + runningReduce + " and reduceCap="
+        + reduceCap + " should be able to assign " + expectedAssigned + " reduces",
+        expectedAssigned, numAssigned);
+  }
+    
+  /** 
+   * Test canAssignReduce method.
+   */
+  public void testCanAssignReduce() {
+    oneTestCanAssignReduce(0.0f, 5, 0, 50, 1, 1);
+    oneTestCanAssignReduce(0.0f, 5, 1, 50, 10, 0);
+    // 20% load + 20% diff = 40% of available slots, but rounds
+    // up with floating point error: so we get 3/5 slots on TT.
+    // 1 already taken, so assigns 2 more
+    oneTestCanAssignReduce(0.2f, 5, 1, 50, 10, 2);
+    oneTestCanAssignReduce(0.0f, 5, 1, 50, 11, 1);
+    oneTestCanAssignReduce(0.0f, 5, 2, 50, 11, 0);
+    oneTestCanAssignReduce(0.3f, 5, 2, 50, 6, 1);
+    oneTestCanAssignReduce(1.0f, 5, 5, 50, 50, 0);
+  }
+  
+}
Index: src/contrib/partitionscheduler/src/test/org/apache/hadoop/mapred/TestComputeFairShares.java
===================================================================
--- src/contrib/partitionscheduler/src/test/org/apache/hadoop/mapred/TestComputeFairShares.java	(revision 0)
+++ src/contrib/partitionscheduler/src/test/org/apache/hadoop/mapred/TestComputeFairShares.java	(working copy)
@@ -0,0 +1,184 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.mapred;
+
+import java.util.ArrayList;
+import java.util.List;
+
+import junit.framework.TestCase;
+
+/**
+ * Exercise the computeFairShares method in SchedulingAlgorithms.
+ */
+public class TestComputeFairShares extends TestCase {
+  private List<Schedulable> scheds;
+  
+  @Override
+  protected void setUp() throws Exception {
+    scheds = new ArrayList<Schedulable>();
+  }
+  
+  /** 
+   * Basic test - pools with different demands that are all higher than their
+   * fair share (of 10 slots) should each get their fair share.
+   */
+  public void testEqualSharing() {
+    scheds.add(new FakeSchedulable(100));
+    scheds.add(new FakeSchedulable(50));
+    scheds.add(new FakeSchedulable(30));
+    scheds.add(new FakeSchedulable(20));
+    SchedulingAlgorithms.computeFairShares(scheds, 40);
+    verifyShares(10, 10, 10, 10);
+  }
+  
+  /**
+   * In this test, pool 4 has a smaller demand than the 40 / 4 = 10 slots that
+   * it would be assigned with equal sharing. It should only get the 3 slots
+   * it demands. The other pools must then split the remaining 37 slots, but
+   * pool 3, with 11 slots demanded, is now below its share of 37/3 ~= 12.3,
+   * so it only gets 11 slots. Pools 1 and 2 split the rest and get 13 each. 
+   */
+  public void testLowDemands() {
+    scheds.add(new FakeSchedulable(100));
+    scheds.add(new FakeSchedulable(50));
+    scheds.add(new FakeSchedulable(11));
+    scheds.add(new FakeSchedulable(3));
+    SchedulingAlgorithms.computeFairShares(scheds, 40);
+    verifyShares(13, 13, 11, 3);
+  }
+  
+  /**
+   * In this test, some pools have minimum shares set. Pool 1 has a min share
+   * of 20 so it gets 20 slots. Pool 2 also has a min share of 20, but its
+   * demand is only 10 so it can only get 10 slots. The remaining pools have
+   * 10 slots to split between them. Pool 4 gets 3 slots because its demand is
+   * only 3, and pool 3 gets the remaining 7 slots. Pool 4 also had a min share
+   * of 2 slots but this should not affect the outcome.
+   */
+  public void testMinShares() {
+    scheds.add(new FakeSchedulable(100, 20));
+    scheds.add(new FakeSchedulable(10, 20));
+    scheds.add(new FakeSchedulable(10, 0));
+    scheds.add(new FakeSchedulable(3, 2));
+    SchedulingAlgorithms.computeFairShares(scheds, 40);
+    verifyShares(20, 10, 7, 3);
+  }
+  
+  /**
+   * Basic test for weighted shares with no minimum shares and no low demands.
+   * Each pool should get slots in proportion to its weight.
+   */
+  public void testWeightedSharing() {
+    scheds.add(new FakeSchedulable(100, 0, 2.0));
+    scheds.add(new FakeSchedulable(50,  0, 1.0));
+    scheds.add(new FakeSchedulable(30,  0, 1.0));
+    scheds.add(new FakeSchedulable(20,  0, 0.5));
+    SchedulingAlgorithms.computeFairShares(scheds, 45);
+    verifyShares(20, 10, 10, 5);
+  }
+
+  /**
+   * Weighted sharing test where pools 1 and 2 are now given lower demands than
+   * above. Pool 1 stops at 10 slots, leaving 35. If the remaining pools split
+   * this into a 1:1:0.5 ratio, they would get 14:14:7 slots respectively, but
+   * pool 2's demand is only 11, so it only gets 11. The remaining 2 pools split
+   * the 24 slots left into a 1:0.5 ratio, getting 16 and 8 slots respectively.
+   */
+  public void testWeightedSharingWithLowDemands() {
+    scheds.add(new FakeSchedulable(10, 0, 2.0));
+    scheds.add(new FakeSchedulable(11, 0, 1.0));
+    scheds.add(new FakeSchedulable(30, 0, 1.0));
+    scheds.add(new FakeSchedulable(20, 0, 0.5));
+    SchedulingAlgorithms.computeFairShares(scheds, 45);
+    verifyShares(10, 11, 16, 8);
+  }
+
+  /**
+   * Weighted fair sharing test with min shares. As in the min share test above,
+   * pool 1 has a min share greater than its demand so it only gets its demand.
+   * Pool 3 has a min share of 15 even though its weight is very small, so it
+   * gets 15 slots. The remaining pools share the remaining 20 slots equally,
+   * getting 10 each. Pool 3's min share of 5 slots doesn't affect this.
+   */
+  public void testWeightedSharingWithMinShares() {
+    scheds.add(new FakeSchedulable(10, 20, 2.0));
+    scheds.add(new FakeSchedulable(11, 0, 1.0));
+    scheds.add(new FakeSchedulable(30, 5, 1.0));
+    scheds.add(new FakeSchedulable(20, 15, 0.5));
+    SchedulingAlgorithms.computeFairShares(scheds, 45);
+    verifyShares(10, 10, 10, 15);
+  }
+
+  /**
+   * Test that shares are computed accurately even when there are many more
+   * frameworks than available slots.
+   */
+  public void testSmallShares() {
+    scheds.add(new FakeSchedulable(10));
+    scheds.add(new FakeSchedulable(5));
+    scheds.add(new FakeSchedulable(3));
+    scheds.add(new FakeSchedulable(2));
+    SchedulingAlgorithms.computeFairShares(scheds, 1);
+    verifyShares(0.25, 0.25, 0.25, 0.25);
+  }
+
+  /**
+   * Test that shares are computed accurately even when the number of slots is
+   * very large.
+   */  
+  public void testLargeShares() {
+    int million = 1000 * 1000;
+    scheds.add(new FakeSchedulable(100 * million));
+    scheds.add(new FakeSchedulable(50 * million));
+    scheds.add(new FakeSchedulable(30 * million));
+    scheds.add(new FakeSchedulable(20 * million));
+    SchedulingAlgorithms.computeFairShares(scheds, 40 * million);
+    verifyShares(10 * million, 10 * million, 10 * million, 10 * million);
+  }
+
+  /**
+   * Test that having a pool with 0 demand doesn't confuse the algorithm.
+   */
+  public void testZeroDemand() {
+    scheds.add(new FakeSchedulable(100));
+    scheds.add(new FakeSchedulable(50));
+    scheds.add(new FakeSchedulable(30));
+    scheds.add(new FakeSchedulable(0));
+    SchedulingAlgorithms.computeFairShares(scheds, 30);
+    verifyShares(10, 10, 10, 0);
+  }
+  
+  /**
+   * Test that being called on an empty list doesn't confuse the algorithm.
+   */
+  public void testEmptyList() {
+    SchedulingAlgorithms.computeFairShares(scheds, 40);
+    verifyShares();
+  }
+  
+  /**
+   * Check that a given list of shares have been assigned to this.scheds.
+   */
+  private void verifyShares(double... shares) {
+    assertEquals(scheds.size(), shares.length);
+    for (int i = 0; i < shares.length; i++) {
+      assertEquals(shares[i], scheds.get(i).getFairShare(), 0.01);
+    }
+  }
+}
Index: src/contrib/partitionscheduler/src/test/org/apache/hadoop/mapred/TestFairScheduler.java
===================================================================
--- src/contrib/partitionscheduler/src/test/org/apache/hadoop/mapred/TestFairScheduler.java	(revision 0)
+++ src/contrib/partitionscheduler/src/test/org/apache/hadoop/mapred/TestFairScheduler.java	(working copy)
@@ -0,0 +1,3062 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.mapred;
+
+import java.io.File;
+import java.io.FileWriter;
+import java.io.IOException;
+import java.io.PrintWriter;
+import java.util.ArrayList;
+import java.util.Collection;
+import java.util.HashMap;
+import java.util.IdentityHashMap;
+import java.util.LinkedHashSet;
+import java.util.LinkedList;
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
+import java.util.TreeMap;
+
+import junit.framework.TestCase;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.io.BytesWritable;
+import org.apache.hadoop.mapred.FairScheduler.JobInfo;
+import org.apache.hadoop.mapred.MRConstants;
+import org.apache.hadoop.mapreduce.Job;
+import org.apache.hadoop.mapred.JobInProgress.KillInterruptedException;
+import org.apache.hadoop.mapred.UtilsForTests.FakeClock;
+import org.apache.hadoop.mapreduce.TaskType;
+import org.apache.hadoop.mapreduce.server.jobtracker.TaskTracker;
+import org.apache.hadoop.mapreduce.split.JobSplit;
+import org.apache.hadoop.metrics.ContextFactory;
+import org.apache.hadoop.metrics.MetricsContext;
+import org.apache.hadoop.metrics.MetricsUtil;
+import org.apache.hadoop.metrics.spi.NoEmitMetricsContext;
+import org.apache.hadoop.metrics.spi.OutputRecord;
+import org.apache.hadoop.net.Node;
+import org.mortbay.log.Log;
+
+public class TestFairScheduler extends TestCase {
+  final static String TEST_DIR = new File(System.getProperty("test.build.data",
+      "build/contrib/streaming/test/data")).getAbsolutePath();
+  final static String ALLOC_FILE = new File(TEST_DIR, 
+      "test-pools").getAbsolutePath();
+  
+  private static final String POOL_PROPERTY = "pool";
+  private static final String EXPLICIT_POOL_PROPERTY = "mapred.fairscheduler.pool";
+  
+  private static int jobCounter;
+  
+  class FakeJobInProgress extends JobInProgress {
+    
+    private FakeTaskTrackerManager taskTrackerManager;
+    private int mapCounter = 0;
+    private int reduceCounter = 0;
+    private final String[][] mapInputLocations; // Array of hosts for each map
+    private boolean initialized;
+    
+    public FakeJobInProgress(JobConf jobConf,
+        FakeTaskTrackerManager taskTrackerManager, 
+        String[][] mapInputLocations, JobTracker jt) throws IOException {
+      super(new JobID("test", ++jobCounter), jobConf, jt);
+      this.taskTrackerManager = taskTrackerManager;
+      this.mapInputLocations = mapInputLocations;
+      this.startTime = System.currentTimeMillis();
+      this.status = new JobStatus();
+      this.status.setRunState(JobStatus.PREP);
+      this.nonLocalRunningMaps = new LinkedHashSet<TaskInProgress>();
+      this.runningMapCache = new IdentityHashMap<Node, Set<TaskInProgress>>();
+      this.nonRunningReduces = new LinkedHashSet<TaskInProgress>();   
+      this.runningReduces = new LinkedHashSet<TaskInProgress>();
+      this.initialized = false;
+    }
+    
+    @Override
+    public synchronized void initTasks() throws IOException {
+      // initTasks is needed to create non-empty cleanup and setup TIP
+      // arrays, otherwise calls such as job.getTaskInProgress will fail
+      JobID jobId = getJobID();
+      JobConf conf = getJobConf();
+      String jobFile = "";
+      // create two cleanup tips, one map and one reduce.
+      cleanup = new TaskInProgress[2];
+      // cleanup map tip.
+      cleanup[0] = new TaskInProgress(jobId, jobFile, null, 
+              jobtracker, conf, this, numMapTasks, 1);
+      cleanup[0].setJobCleanupTask();
+      // cleanup reduce tip.
+      cleanup[1] = new TaskInProgress(jobId, jobFile, numMapTasks,
+                         numReduceTasks, jobtracker, conf, this, 1);
+      cleanup[1].setJobCleanupTask();
+      // create two setup tips, one map and one reduce.
+      setup = new TaskInProgress[2];
+      // setup map tip.
+      setup[0] = new TaskInProgress(jobId, jobFile, null, 
+              jobtracker, conf, this, numMapTasks + 1, 1);
+      setup[0].setJobSetupTask();
+      // setup reduce tip.
+      setup[1] = new TaskInProgress(jobId, jobFile, numMapTasks,
+                         numReduceTasks + 1, jobtracker, conf, this, 1);
+      setup[1].setJobSetupTask();
+      // create maps
+      numMapTasks = conf.getNumMapTasks();
+      maps = new TaskInProgress[numMapTasks];
+      // empty format
+      JobSplit.TaskSplitMetaInfo split = JobSplit.EMPTY_TASK_SPLIT;
+      for (int i = 0; i < numMapTasks; i++) {
+        String[] inputLocations = null;
+        if (mapInputLocations != null)
+          inputLocations = mapInputLocations[i];
+        maps[i] = new FakeTaskInProgress(getJobID(), i,
+            getJobConf(), this, inputLocations, split, jobtracker);
+        if (mapInputLocations == null) // Job has no locality info
+          nonLocalMaps.add(maps[i]);
+      }
+      // create reduces
+      numReduceTasks = conf.getNumReduceTasks();
+      reduces = new TaskInProgress[numReduceTasks];
+      for (int i = 0; i < numReduceTasks; i++) {
+        reduces[i] = new FakeTaskInProgress(getJobID(), i,
+            getJobConf(), this, jobtracker);
+      }
+      
+      initialized = true;
+    }
+    
+    @Override
+    public boolean inited() {
+      return initialized;
+    }
+    
+    @Override
+    public Task obtainNewMapTask(final TaskTrackerStatus tts, int clusterSize,
+        int numUniqueHosts) throws IOException {
+      return obtainNewMapTask(tts, clusterSize, numUniqueHosts, Integer.MAX_VALUE);
+    }
+    
+    @Override
+    public Task obtainNewNodeLocalMapTask(final TaskTrackerStatus tts, int clusterSize,
+        int numUniqueHosts) throws IOException {
+      return obtainNewMapTask(tts, clusterSize, numUniqueHosts, 1);
+    }
+    
+    @Override
+    public Task obtainNewNodeOrRackLocalMapTask(final TaskTrackerStatus tts, int clusterSize,
+        int numUniqueHosts) throws IOException {
+      return obtainNewMapTask(tts, clusterSize, numUniqueHosts, 2);
+    }
+
+    public Task obtainNewMapTask(final TaskTrackerStatus tts, int clusterSize,
+        int numUniqueHosts, int localityLevel) throws IOException {
+      for (int map = 0; map < maps.length; map++) {
+        FakeTaskInProgress tip = (FakeTaskInProgress) maps[map];
+        if (!tip.isRunning() && !tip.isComplete() &&
+            getLocalityLevel(tip, tts) < localityLevel) {
+          TaskAttemptID attemptId = getTaskAttemptID(tip);
+          JobSplit.TaskSplitMetaInfo split = JobSplit.EMPTY_TASK_SPLIT;
+          Task task = new MapTask("", attemptId, 0, split.getSplitIndex(), 1) {
+            @Override
+            public String toString() {
+              return String.format("%s on %s", getTaskID(), tts.getTrackerName());
+            }
+          };
+          runningMapTasks++;
+          tip.createTaskAttempt(task, tts.getTrackerName());
+          nonLocalRunningMaps.add(tip);
+          taskTrackerManager.startTask(tts.getTrackerName(), task, tip);
+          return task;
+        }
+      }
+      return null;
+    }
+    
+    @Override
+    public Task obtainNewReduceTask(final TaskTrackerStatus tts,
+        int clusterSize, int ignored) throws IOException {
+      for (int reduce = 0; reduce < reduces.length; reduce++) {
+        FakeTaskInProgress tip = 
+          (FakeTaskInProgress) reduces[reduce];
+        if (!tip.isRunning() && !tip.isComplete()) {
+          TaskAttemptID attemptId = getTaskAttemptID(tip);
+          Task task = new ReduceTask("", attemptId, 0, maps.length, 1) {
+            @Override
+            public String toString() {
+              return String.format("%s on %s", getTaskID(), tts.getTrackerName());
+            }
+          };
+          runningReduceTasks++;
+          tip.createTaskAttempt(task, tts.getTrackerName());
+          runningReduces.add(tip);
+          taskTrackerManager.startTask(tts.getTrackerName(), task, tip);
+          return task;
+        }
+      }
+      return null;
+    }
+    
+    public void mapTaskFinished(TaskInProgress tip) {
+      runningMapTasks--;
+      finishedMapTasks++;
+      nonLocalRunningMaps.remove(tip);
+    }
+    
+    public void reduceTaskFinished(TaskInProgress tip) {
+      runningReduceTasks--;
+      finishedReduceTasks++;
+      runningReduces.remove(tip);
+    }
+    
+    private TaskAttemptID getTaskAttemptID(TaskInProgress tip) {
+      JobID jobId = getJobID();
+      return new TaskAttemptID(jobId.getJtIdentifier(),
+          jobId.getId(), tip.isMapTask(), tip.getIdWithinJob(), tip.nextTaskId++);
+    }
+    
+    @Override
+    int getLocalityLevel(TaskInProgress tip, TaskTrackerStatus tts) {
+      FakeTaskInProgress ftip = (FakeTaskInProgress) tip;
+      if (ftip.inputLocations != null) {
+        // Check whether we're on the same host as an input split
+        for (String location: ftip.inputLocations) {
+          if (location.equals(tts.host)) {
+            return 0;
+          }
+        }
+        // Check whether we're on the same rack as an input split
+        for (String location: ftip.inputLocations) {
+          if (getRack(location).equals(getRack(tts.host))) {
+            return 1;
+          }
+        }
+        // Not on same rack or host
+        return 2;
+      } else {
+        // Job has no locality info  
+        return -1;
+      }
+    }
+  }
+  
+  class FakeTaskInProgress extends TaskInProgress {
+    private boolean isMap;
+    private FakeJobInProgress fakeJob;
+    private TreeMap<TaskAttemptID, String> activeTasks;
+    private TaskStatus taskStatus;
+    private boolean isComplete = false;
+    private String[] inputLocations;
+    
+    // Constructor for map
+    FakeTaskInProgress(JobID jId, int id, JobConf jobConf,
+        FakeJobInProgress job, String[] inputLocations, 
+        JobSplit.TaskSplitMetaInfo split, JobTracker jt) {
+      super(jId, "", split, jt, jobConf, job, id, 1);
+      this.isMap = true;
+      this.fakeJob = job;
+      this.inputLocations = inputLocations;
+      activeTasks = new TreeMap<TaskAttemptID, String>();
+      taskStatus = TaskStatus.createTaskStatus(isMap);
+      taskStatus.setRunState(TaskStatus.State.UNASSIGNED);
+    }
+
+    // Constructor for reduce
+    FakeTaskInProgress(JobID jId, int id, JobConf jobConf,
+        FakeJobInProgress job, JobTracker jt) {
+      super(jId, "", jobConf.getNumMapTasks(), id, jt, jobConf, job, 1);
+      this.isMap = false;
+      this.fakeJob = job;
+      activeTasks = new TreeMap<TaskAttemptID, String>();
+      taskStatus = TaskStatus.createTaskStatus(isMap);
+      taskStatus.setRunState(TaskStatus.State.UNASSIGNED);
+    }
+    
+    private void createTaskAttempt(Task task, String taskTracker) {
+      activeTasks.put(task.getTaskID(), taskTracker);
+      taskStatus = TaskStatus.createTaskStatus(isMap, task.getTaskID(),
+          0.5f, 1, TaskStatus.State.RUNNING, "", "", "", 
+          TaskStatus.Phase.STARTING, new Counters());
+      taskStatus.setStartTime(clock.getTime());
+    }
+    
+    @Override
+    TreeMap<TaskAttemptID, String> getActiveTasks() {
+      return activeTasks;
+    }
+    
+    public synchronized boolean isComplete() {
+      return isComplete;
+    }
+    
+    public boolean isRunning() {
+      return activeTasks.size() > 0;
+    }
+    
+    @Override
+    public TaskStatus getTaskStatus(TaskAttemptID taskid) {
+      return taskStatus;
+    }
+    
+    void killAttempt() {
+      if (isMap) {
+        fakeJob.mapTaskFinished(this);
+      }
+      else {
+        fakeJob.reduceTaskFinished(this);
+      }
+      activeTasks.clear();
+      taskStatus.setRunState(TaskStatus.State.UNASSIGNED);
+    }
+    
+    void finishAttempt() {
+      isComplete = true;
+      if (isMap) {
+        fakeJob.mapTaskFinished(this);
+      }
+      else {
+        fakeJob.reduceTaskFinished(this);
+      }
+      activeTasks.clear();
+      taskStatus.setRunState(TaskStatus.State.UNASSIGNED);
+    }
+  }
+  
+  static class FakeQueueManager extends QueueManager {
+    private Set<String> queues = null;
+    FakeQueueManager() {
+      super(new Configuration());
+    }
+    void setQueues(Set<String> queues) {
+      this.queues = queues;
+    }
+    public synchronized Set<String> getLeafQueueNames() {
+      return queues;
+    }
+  }
+  
+  static class FakeTaskTrackerManager implements TaskTrackerManager {
+    int maps = 0;
+    int reduces = 0;
+    int maxMapTasksPerTracker = 2;
+    int maxReduceTasksPerTracker = 2;
+    long ttExpiryInterval = 10 * 60 * 1000L; // default interval
+    List<JobInProgressListener> listeners =
+      new ArrayList<JobInProgressListener>();
+    Map<JobID, JobInProgress> jobs = new HashMap<JobID, JobInProgress>();
+    
+    private Map<String, TaskTracker> trackers =
+      new HashMap<String, TaskTracker>();
+    private Map<String, TaskStatus> statuses = 
+      new HashMap<String, TaskStatus>();
+    private Map<String, FakeTaskInProgress> tips = 
+      new HashMap<String, FakeTaskInProgress>();
+    private Map<String, TaskTrackerStatus> trackerForTip =
+      new HashMap<String, TaskTrackerStatus>();
+    
+    public FakeTaskTrackerManager(int numRacks, int numTrackersPerRack) {
+      int nextTrackerId = 1;
+      for (int rack = 1; rack <= numRacks; rack++) {
+        for (int node = 1; node <= numTrackersPerRack; node++) {
+          int id = nextTrackerId++;
+          String host = "rack" + rack + ".node" + node;
+          System.out.println("Creating TaskTracker tt" + id + " on " + host);
+          TaskTracker tt = new TaskTracker("tt" + id);
+          tt.setStatus(new TaskTrackerStatus("tt" + id, host, 0,
+              new ArrayList<TaskStatus>(), 0, 0,
+              maxMapTasksPerTracker, maxReduceTasksPerTracker));
+          trackers.put("tt" + id, tt);
+        }
+      }
+    }
+    
+    @Override
+    public ClusterStatus getClusterStatus() {
+      int numTrackers = trackers.size();
+
+      return new ClusterStatus(numTrackers, 0, 0,
+          ttExpiryInterval, maps, reduces,
+          numTrackers * maxMapTasksPerTracker,
+          numTrackers * maxReduceTasksPerTracker,
+          JobTracker.State.RUNNING);
+    }
+
+    @Override
+    public QueueManager getQueueManager() {
+      return null;
+    }
+    
+    @Override
+    public int getNumberOfUniqueHosts() {
+      return trackers.size();
+    }
+
+    @Override
+    public Collection<TaskTrackerStatus> taskTrackers() {
+      List<TaskTrackerStatus> statuses = new ArrayList<TaskTrackerStatus>();
+      for (TaskTracker tt : trackers.values()) {
+        statuses.add(tt.getStatus());
+      }
+      return statuses;
+    }
+
+
+    @Override
+    public void addJobInProgressListener(JobInProgressListener listener) {
+      listeners.add(listener);
+    }
+
+    @Override
+    public void removeJobInProgressListener(JobInProgressListener listener) {
+      listeners.remove(listener);
+    }
+    
+    @Override
+    public int getNextHeartbeatInterval() {
+      return MRConstants.HEARTBEAT_INTERVAL_MIN;
+    }
+
+    @Override
+    public void killJob(JobID jobid) {
+      return;
+    }
+
+    @Override
+    public JobInProgress getJob(JobID jobid) {
+      return jobs.get(jobid);
+    }
+
+    public void initJob (JobInProgress job) {
+      try {
+        job.initTasks();
+      } catch (KillInterruptedException e) {
+      } catch (IOException e) {
+      }
+    }
+    
+    public void failJob (JobInProgress job) {
+      // do nothing
+    }
+    
+    // Test methods
+    
+    public void submitJob(JobInProgress job) throws IOException {
+      jobs.put(job.getJobID(), job);
+      for (JobInProgressListener listener : listeners) {
+        listener.jobAdded(job);
+      }
+    }
+    
+    public TaskTracker getTaskTracker(String trackerID) {
+      return trackers.get(trackerID);
+    }
+    
+    public void startTask(String trackerName, Task t, FakeTaskInProgress tip) {
+      final boolean isMap = t.isMapTask();
+      if (isMap) {
+        maps++;
+      } else {
+        reduces++;
+      }
+      String attemptId = t.getTaskID().toString();
+      TaskStatus status = tip.getTaskStatus(t.getTaskID());
+      TaskTrackerStatus trackerStatus = trackers.get(trackerName).getStatus();
+      tips.put(attemptId, tip);
+      statuses.put(attemptId, status);
+      trackerForTip.put(attemptId, trackerStatus);
+      status.setRunState(TaskStatus.State.RUNNING);
+    }
+    
+    public void reportTaskOnTracker(String trackerName, Task t) {
+      FakeTaskInProgress tip = tips.get(t.getTaskID().toString());
+      TaskTrackerStatus trackerStatus = trackers.get(trackerName).getStatus();
+      trackerStatus.getTaskReports().add(tip.getTaskStatus(t.getTaskID()));
+    }
+    
+    public void finishTask(String taskTrackerName, String attemptId) {
+      FakeTaskInProgress tip = tips.get(attemptId);
+      if (tip.isMapTask()) {
+        maps--;
+      } else {
+        reduces--;
+      }
+      tip.finishAttempt();
+      TaskStatus status = statuses.get(attemptId);
+      trackers.get(taskTrackerName).getStatus().getTaskReports().remove(status);
+    }
+
+    @Override
+    public boolean killTask(TaskAttemptID attemptId, boolean shouldFail) {
+      String attemptIdStr = attemptId.toString();
+      FakeTaskInProgress tip = tips.get(attemptIdStr);
+      if (tip.isMapTask()) {
+        maps--;
+      } else {
+        reduces--;
+      }
+      tip.killAttempt();
+      TaskStatus status = statuses.get(attemptIdStr);
+      trackerForTip.get(attemptIdStr).getTaskReports().remove(status);
+      return true;
+    }
+
+    @Override
+    public boolean isInSafeMode() {
+      // TODO Auto-generated method stub
+      return false;
+    }
+  }
+  
+  protected JobConf conf;
+  protected FairScheduler scheduler;
+  private FakeTaskTrackerManager taskTrackerManager;
+  private FakeClock clock;
+  private JobTracker jobTracker;
+
+  @Override
+  protected void setUp() throws Exception {
+    jobCounter = 0;
+    new File(TEST_DIR).mkdirs(); // Make sure data directory exists
+    // Create an empty pools file (so we can add/remove pools later)
+    FileWriter fileWriter = new FileWriter(ALLOC_FILE);
+    fileWriter.write("<?xml version=\"1.0\"?>\n");
+    fileWriter.write("<allocations />\n");
+    fileWriter.close();
+    setUpCluster(1, 2, false);
+  }
+
+  public String getRack(String hostname) {
+    // Host names are of the form rackN.nodeM, so split at the dot.
+    return hostname.split("\\.")[0];
+  }
+
+  private void setUpCluster(int numRacks, int numNodesPerRack,
+      boolean assignMultiple) throws IOException {
+    
+    resetMetrics();
+    
+    conf = new JobConf();
+    conf.set("mapred.fairscheduler.allocation.file", ALLOC_FILE);
+    conf.set("mapred.fairscheduler.poolnameproperty", POOL_PROPERTY);
+    conf.setBoolean("mapred.fairscheduler.assignmultiple", assignMultiple);
+    // Manually set locality delay because we aren't using a JobTracker so
+    // we can't auto-compute it from the heartbeat interval.
+    conf.setLong("mapred.fairscheduler.locality.delay.node", 5000);
+    conf.setLong("mapred.fairscheduler.locality.delay.rack", 10000);
+    conf.set("mapred.job.tracker", "localhost:0");
+    conf.set("mapred.job.tracker.http.address", "0.0.0.0:0");
+    taskTrackerManager = new FakeTaskTrackerManager(numRacks, numNodesPerRack);
+    clock = new FakeClock();
+    try {
+      jobTracker = new JobTracker(conf, clock);
+      jobTracker.setSafeModeInternal(JobTracker.SafeModeAction.SAFEMODE_ENTER);
+      jobTracker.initializeFilesystem();
+      jobTracker.setSafeModeInternal(JobTracker.SafeModeAction.SAFEMODE_LEAVE);
+      jobTracker.initialize();
+    } catch (Exception e) {
+      throw new RuntimeException("Could not start JT", e);
+    }
+    scheduler = new FairScheduler(clock, true);
+    scheduler.waitForMapsBeforeLaunchingReduces = false;
+    scheduler.setConf(conf);
+    scheduler.setTaskTrackerManager(taskTrackerManager);
+    scheduler.start();
+    // TaskStatus complains if a task's start time is 0, so advance it a bit
+    advanceTime(100);
+  }
+  
+  /**
+   * Set up a metrics context that doesn't emit anywhere but stores the data
+   * so we can verify it. Also clears it of any data so that different test
+   * cases don't pollute each other.
+   */
+  private void resetMetrics() throws IOException {
+    ContextFactory factory = ContextFactory.getFactory();
+    factory.setAttribute("fairscheduler.class",
+        NoEmitMetricsContext.class.getName());
+    
+    MetricsUtil.getContext("fairscheduler").createRecord("jobs").remove();
+    MetricsUtil.getContext("fairscheduler").createRecord("pools").remove();
+  }
+
+  @Override
+  protected void tearDown() throws Exception {
+    if (scheduler != null) {
+      scheduler.terminate();
+    }
+  }
+  
+  private JobInProgress submitJobNotInitialized(int state, int maps, int reduces)
+	    throws IOException {
+    return submitJob(state, maps, reduces, null, null, false);
+  }
+
+  private JobInProgress submitJob(int state, int maps, int reduces)
+      throws IOException {
+    return submitJob(state, maps, reduces, null, null, true);
+  }
+  
+  private JobInProgress submitJob(int state, int maps, int reduces, String pool)
+      throws IOException {
+    return submitJob(state, maps, reduces, pool, null, true);
+  }
+  
+  private JobInProgress submitJob(int state, int maps, int reduces, String pool,
+      String[][] mapInputLocations, boolean initializeJob) throws IOException {
+    JobConf jobConf = new JobConf(conf);
+    jobConf.setNumMapTasks(maps);
+    jobConf.setNumReduceTasks(reduces);
+    if (pool != null)
+      jobConf.set(POOL_PROPERTY, pool);
+    JobInProgress job = new FakeJobInProgress(jobConf, taskTrackerManager,
+        mapInputLocations, jobTracker);
+    if (initializeJob) {
+      taskTrackerManager.initJob(job);
+    }
+    job.getStatus().setRunState(state);
+    taskTrackerManager.submitJob(job);
+    job.startTime = clock.time;
+    return job;
+  }
+  
+  protected void submitJobs(int number, int state, int maps, int reduces)
+    throws IOException {
+    for (int i = 0; i < number; i++) {
+      submitJob(state, maps, reduces);
+    }
+  }
+
+  public void testAllocationFileParsing() throws Exception {
+    PrintWriter out = new PrintWriter(new FileWriter(ALLOC_FILE));
+    out.println("<?xml version=\"1.0\"?>");
+    out.println("<allocations>"); 
+    // Give pool A a minimum of 1 map, 2 reduces
+    out.println("<pool name=\"poolA\">");
+    out.println("<minMaps>1</minMaps>");
+    out.println("<minReduces>2</minReduces>");
+    out.println("</pool>");
+    // Give pool B a minimum of 2 maps, 1 reduce
+    out.println("<pool name=\"poolB\">");
+    out.println("<minMaps>2</minMaps>");
+    out.println("<minReduces>1</minReduces>");
+    out.println("</pool>");
+    // Give pool C min maps but no min reduces
+    out.println("<pool name=\"poolC\">");
+    out.println("<minMaps>2</minMaps>");
+    out.println("</pool>");
+    // Give pool D a limit of 3 running jobs
+    out.println("<pool name=\"poolD\">");
+    out.println("<maxRunningJobs>3</maxRunningJobs>");
+    out.println("</pool>");
+    // Give pool E a preemption timeout of one minute
+    out.println("<pool name=\"poolE\">");
+    out.println("<minSharePreemptionTimeout>60</minSharePreemptionTimeout>");
+    out.println("</pool>");
+    // Set default limit of jobs per pool to 15
+    out.println("<poolMaxJobsDefault>15</poolMaxJobsDefault>");
+    // Set default limit of jobs per user to 5
+    out.println("<userMaxJobsDefault>5</userMaxJobsDefault>");
+    // Give user1 a limit of 10 jobs
+    out.println("<user name=\"user1\">");
+    out.println("<maxRunningJobs>10</maxRunningJobs>");
+    out.println("</user>");
+    // Set default min share preemption timeout to 2 minutes
+    out.println("<defaultMinSharePreemptionTimeout>120" 
+        + "</defaultMinSharePreemptionTimeout>"); 
+    // Set fair share preemption timeout to 5 minutes
+    out.println("<fairSharePreemptionTimeout>300</fairSharePreemptionTimeout>"); 
+    out.println("</allocations>"); 
+    out.close();
+    
+    PoolManager poolManager = scheduler.getPoolManager();
+    poolManager.reloadAllocs();
+    
+    assertEquals(6, poolManager.getPools().size()); // 5 in file + default pool
+    assertEquals(0, poolManager.getAllocation(Pool.DEFAULT_POOL_NAME,
+        TaskType.MAP));
+    assertEquals(0, poolManager.getAllocation(Pool.DEFAULT_POOL_NAME,
+        TaskType.REDUCE));
+    assertEquals(1, poolManager.getAllocation("poolA", TaskType.MAP));
+    assertEquals(2, poolManager.getAllocation("poolA", TaskType.REDUCE));
+    assertEquals(2, poolManager.getAllocation("poolB", TaskType.MAP));
+    assertEquals(1, poolManager.getAllocation("poolB", TaskType.REDUCE));
+    assertEquals(2, poolManager.getAllocation("poolC", TaskType.MAP));
+    assertEquals(0, poolManager.getAllocation("poolC", TaskType.REDUCE));
+    assertEquals(0, poolManager.getAllocation("poolD", TaskType.MAP));
+    assertEquals(0, poolManager.getAllocation("poolD", TaskType.REDUCE));
+    assertEquals(0, poolManager.getAllocation("poolE", TaskType.MAP));
+    assertEquals(0, poolManager.getAllocation("poolE", TaskType.REDUCE));
+    assertEquals(15, poolManager.getPoolMaxJobs(Pool.DEFAULT_POOL_NAME));
+    assertEquals(15, poolManager.getPoolMaxJobs("poolA"));
+    assertEquals(15, poolManager.getPoolMaxJobs("poolB"));
+    assertEquals(15, poolManager.getPoolMaxJobs("poolC"));
+    assertEquals(3, poolManager.getPoolMaxJobs("poolD"));
+    assertEquals(15, poolManager.getPoolMaxJobs("poolE"));
+    assertEquals(10, poolManager.getUserMaxJobs("user1"));
+    assertEquals(5, poolManager.getUserMaxJobs("user2"));
+    assertEquals(120000, poolManager.getMinSharePreemptionTimeout(
+        Pool.DEFAULT_POOL_NAME));
+    assertEquals(120000, poolManager.getMinSharePreemptionTimeout("poolA"));
+    assertEquals(120000, poolManager.getMinSharePreemptionTimeout("poolB"));
+    assertEquals(120000, poolManager.getMinSharePreemptionTimeout("poolC"));
+    assertEquals(120000, poolManager.getMinSharePreemptionTimeout("poolD"));
+    assertEquals(120000, poolManager.getMinSharePreemptionTimeout("poolA"));
+    assertEquals(60000, poolManager.getMinSharePreemptionTimeout("poolE"));
+    assertEquals(300000, poolManager.getFairSharePreemptionTimeout());
+  }
+  
+  public void testTaskNotAssignedWhenNoJobsArePresent() throws IOException {
+    assertNull(scheduler.assignTasks(tracker("tt1")));
+  }
+
+  public void testNonRunningJobsAreIgnored() throws IOException {
+    submitJobs(1, JobStatus.SUCCEEDED, 10, 10);
+    submitJobs(1, JobStatus.FAILED, 10, 10);
+    submitJobs(1, JobStatus.KILLED, 10, 10);
+    assertNull(scheduler.assignTasks(tracker("tt1")));
+    advanceTime(100); // Check that we still don't assign jobs after an update
+    assertNull(scheduler.assignTasks(tracker("tt1")));
+  }
+
+  /**
+   * This test contains two jobs with fewer required tasks than there are slots.
+   * We check that all tasks are assigned, but job 1 gets them first because it
+   * was submitted earlier.
+   */
+  public void testSmallJobs() throws IOException {
+    JobInProgress job1 = submitJob(JobStatus.RUNNING, 2, 1);
+    JobInfo info1 = scheduler.infos.get(job1);
+    
+    // Check scheduler variables
+    assertEquals(0,    info1.mapSchedulable.getRunningTasks());
+    assertEquals(0,    info1.reduceSchedulable.getRunningTasks());
+    assertEquals(2,    info1.mapSchedulable.getDemand());
+    assertEquals(1,    info1.reduceSchedulable.getDemand());
+    assertEquals(2.0,  info1.mapSchedulable.getFairShare());
+    assertEquals(1.0,  info1.reduceSchedulable.getFairShare());
+    verifyMetrics();
+
+    // Advance time before submitting another job j2, to make j1 run before j2
+    // deterministically.
+    advanceTime(100);
+    JobInProgress job2 = submitJob(JobStatus.RUNNING, 1, 2);
+    JobInfo info2 = scheduler.infos.get(job2);
+    
+    // Check scheduler variables
+    assertEquals(0,    info1.mapSchedulable.getRunningTasks());
+    assertEquals(0,    info1.reduceSchedulable.getRunningTasks());
+    assertEquals(2,    info1.mapSchedulable.getDemand());
+    assertEquals(1,    info1.reduceSchedulable.getDemand());
+    assertEquals(2.0,  info1.mapSchedulable.getFairShare());
+    assertEquals(1.0,  info1.reduceSchedulable.getFairShare());
+    assertEquals(0,    info2.mapSchedulable.getRunningTasks());
+    assertEquals(0,    info2.reduceSchedulable.getRunningTasks());
+    assertEquals(1,    info2.mapSchedulable.getDemand());
+    assertEquals(2,    info2.reduceSchedulable.getDemand());
+    assertEquals(1.0,  info2.mapSchedulable.getFairShare());
+    assertEquals(2.0,  info2.reduceSchedulable.getFairShare());
+    verifyMetrics();
+    
+    // Assign tasks and check that jobs alternate in filling slots
+    checkAssignment("tt1", "attempt_test_0001_m_000000_0 on tt1");
+    checkAssignment("tt1", "attempt_test_0001_r_000000_0 on tt1");
+    checkAssignment("tt1", "attempt_test_0002_m_000000_0 on tt1");
+    checkAssignment("tt1", "attempt_test_0002_r_000000_0 on tt1");
+    checkAssignment("tt2", "attempt_test_0001_m_000001_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0002_r_000001_0 on tt2");
+    assertNull(scheduler.assignTasks(tracker("tt2")));
+    
+    // Check that the scheduler has started counting the tasks as running
+    // as soon as it launched them.
+    assertEquals(2,  info1.mapSchedulable.getRunningTasks());
+    assertEquals(1,  info1.reduceSchedulable.getRunningTasks());
+    assertEquals(2,  info1.mapSchedulable.getDemand());
+    assertEquals(1,  info1.reduceSchedulable.getDemand());
+    assertEquals(1,  info2.mapSchedulable.getRunningTasks());
+    assertEquals(2,  info2.reduceSchedulable.getRunningTasks());
+    assertEquals(1, info2.mapSchedulable.getDemand());
+    assertEquals(2, info2.reduceSchedulable.getDemand());
+    verifyMetrics();
+  }
+  /**
+   * This test is identical to testSmallJobs but sets assignMultiple to
+   * true so that multiple tasks can be assigned per heartbeat.
+   */
+  public void testSmallJobsWithAssignMultiple() throws IOException {
+    setUpCluster(1, 2, true);
+    
+    JobInProgress job1 = submitJob(JobStatus.RUNNING, 2, 1);
+    JobInfo info1 = scheduler.infos.get(job1);
+    
+    // Check scheduler variables
+    assertEquals(0,    info1.mapSchedulable.getRunningTasks());
+    assertEquals(0,    info1.reduceSchedulable.getRunningTasks());
+    assertEquals(2,    info1.mapSchedulable.getDemand());
+    assertEquals(1,    info1.reduceSchedulable.getDemand());
+    assertEquals(2.0,  info1.mapSchedulable.getFairShare());
+    assertEquals(1.0,  info1.reduceSchedulable.getFairShare());
+    verifyMetrics();
+    
+    // Advance time before submitting another job j2, to make j1 run before j2
+    // deterministically.
+    advanceTime(100);
+    JobInProgress job2 = submitJob(JobStatus.RUNNING, 1, 2);
+    JobInfo info2 = scheduler.infos.get(job2);
+    
+    // Check scheduler variables
+    assertEquals(0,    info1.mapSchedulable.getRunningTasks());
+    assertEquals(0,    info1.reduceSchedulable.getRunningTasks());
+    assertEquals(2,    info1.mapSchedulable.getDemand());
+    assertEquals(1,    info1.reduceSchedulable.getDemand());
+    assertEquals(2.0,  info1.mapSchedulable.getFairShare());
+    assertEquals(1.0,  info1.reduceSchedulable.getFairShare());
+    assertEquals(0,    info2.mapSchedulable.getRunningTasks());
+    assertEquals(0,    info2.reduceSchedulable.getRunningTasks());
+    assertEquals(1,    info2.mapSchedulable.getDemand());
+    assertEquals(2,    info2.reduceSchedulable.getDemand());
+    assertEquals(1.0,  info2.mapSchedulable.getFairShare());
+    assertEquals(2.0,  info2.reduceSchedulable.getFairShare());
+    verifyMetrics();
+    
+    // Assign tasks and check that jobs alternate in filling slots
+    checkAssignment("tt1", "attempt_test_0001_m_000000_0 on tt1",
+                           "attempt_test_0001_r_000000_0 on tt1",
+                           "attempt_test_0002_m_000000_0 on tt1",
+                           "attempt_test_0002_r_000000_0 on tt1");
+    checkAssignment("tt2", "attempt_test_0001_m_000001_0 on tt2",
+                           "attempt_test_0002_r_000001_0 on tt2");
+    assertNull(scheduler.assignTasks(tracker("tt2")));
+    
+    // Check that the scheduler has started counting the tasks as running
+    // as soon as it launched them.
+    assertEquals(2,  info1.mapSchedulable.getRunningTasks());
+    assertEquals(1,  info1.reduceSchedulable.getRunningTasks());
+    assertEquals(2,  info1.mapSchedulable.getDemand());
+    assertEquals(1,  info1.reduceSchedulable.getDemand());
+    assertEquals(1,  info2.mapSchedulable.getRunningTasks());
+    assertEquals(2,  info2.reduceSchedulable.getRunningTasks());
+    assertEquals(1, info2.mapSchedulable.getDemand());
+    assertEquals(2, info2.reduceSchedulable.getDemand());
+    verifyMetrics();
+  }
+  
+  /**
+   * This test begins by submitting two jobs with 10 maps and reduces each.
+   * The first job is submitted 100ms after the second, to make it get slots
+   * first deterministically. We then assign a wave of tasks and check that
+   * they are given alternately to job1, job2, job1, job2, etc. We finish
+   * these tasks and assign a second wave, which should continue to be
+   * allocated in this manner.
+   */
+  public void testLargeJobs() throws IOException {
+    JobInProgress job1 = submitJob(JobStatus.RUNNING, 10, 10);
+    JobInfo info1 = scheduler.infos.get(job1);
+    
+    // Check scheduler variables
+    assertEquals(0,    info1.mapSchedulable.getRunningTasks());
+    assertEquals(0,    info1.reduceSchedulable.getRunningTasks());
+    assertEquals(10,   info1.mapSchedulable.getDemand());
+    assertEquals(10,   info1.reduceSchedulable.getDemand());
+    assertEquals(4.0,  info1.mapSchedulable.getFairShare());
+    assertEquals(4.0,  info1.reduceSchedulable.getFairShare());
+    
+    // Advance time before submitting another job j2, to make j1 run before j2
+    // deterministically.
+    advanceTime(100);
+    JobInProgress job2 = submitJob(JobStatus.RUNNING, 10, 10);
+    JobInfo info2 = scheduler.infos.get(job2);
+    
+    // Check scheduler variables
+    assertEquals(0,    info1.mapSchedulable.getRunningTasks());
+    assertEquals(0,    info1.reduceSchedulable.getRunningTasks());
+    assertEquals(10,   info1.mapSchedulable.getDemand());
+    assertEquals(10,   info1.reduceSchedulable.getDemand());
+    assertEquals(2.0,  info1.mapSchedulable.getFairShare());
+    assertEquals(2.0,  info1.reduceSchedulable.getFairShare());
+    assertEquals(0,    info2.mapSchedulable.getRunningTasks());
+    assertEquals(0,    info2.reduceSchedulable.getRunningTasks());
+    assertEquals(10,   info2.mapSchedulable.getDemand());
+    assertEquals(10,   info2.reduceSchedulable.getDemand());
+    assertEquals(2.0,  info2.mapSchedulable.getFairShare());
+    assertEquals(2.0,  info2.reduceSchedulable.getFairShare());
+    
+    // Check that tasks are filled alternately by the jobs
+    checkAssignment("tt1", "attempt_test_0001_m_000000_0 on tt1");
+    checkAssignment("tt1", "attempt_test_0001_r_000000_0 on tt1");
+    checkAssignment("tt1", "attempt_test_0002_m_000000_0 on tt1");
+    checkAssignment("tt1", "attempt_test_0002_r_000000_0 on tt1");
+    checkAssignment("tt2", "attempt_test_0001_m_000001_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0001_r_000001_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0002_m_000001_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0002_r_000001_0 on tt2");
+    
+    // Check that no new tasks can be launched once the tasktrackers are full
+    assertNull(scheduler.assignTasks(tracker("tt1")));
+    assertNull(scheduler.assignTasks(tracker("tt2")));
+    
+    // Check that the scheduler has started counting the tasks as running
+    // as soon as it launched them.
+    assertEquals(2,  info1.mapSchedulable.getRunningTasks());
+    assertEquals(2,  info1.reduceSchedulable.getRunningTasks());
+    assertEquals(10,  info1.mapSchedulable.getDemand());
+    assertEquals(10,  info1.reduceSchedulable.getDemand());
+    assertEquals(2,  info2.mapSchedulable.getRunningTasks());
+    assertEquals(2,  info2.reduceSchedulable.getRunningTasks());
+    assertEquals(10, info2.mapSchedulable.getDemand());
+    assertEquals(10, info2.reduceSchedulable.getDemand());
+    
+    // Finish up the tasks and advance time again. Note that we must finish
+    // the task since FakeJobInProgress does not properly maintain running
+    // tasks, so the scheduler will always get an empty task list from
+    // the JobInProgress's getTasks(TaskType.MAP)/getTasks(TaskType.REDUCE) and 
+    // think they finished.
+    taskTrackerManager.finishTask("tt1", "attempt_test_0001_m_000000_0");
+    taskTrackerManager.finishTask("tt1", "attempt_test_0002_m_000000_0");
+    taskTrackerManager.finishTask("tt1", "attempt_test_0001_r_000000_0");
+    taskTrackerManager.finishTask("tt1", "attempt_test_0002_r_000000_0");
+    taskTrackerManager.finishTask("tt2", "attempt_test_0001_m_000001_0");
+    taskTrackerManager.finishTask("tt2", "attempt_test_0002_m_000001_0");
+    taskTrackerManager.finishTask("tt2", "attempt_test_0001_r_000001_0");
+    taskTrackerManager.finishTask("tt2", "attempt_test_0002_r_000001_0");
+    advanceTime(200);
+    assertEquals(0,   info1.mapSchedulable.getRunningTasks());
+    assertEquals(0,   info1.reduceSchedulable.getRunningTasks());
+    assertEquals(0,   info2.mapSchedulable.getRunningTasks());
+    assertEquals(0,   info2.reduceSchedulable.getRunningTasks());
+
+    // Check that tasks are filled alternately by the jobs
+    checkAssignment("tt1", "attempt_test_0001_m_000002_0 on tt1");
+    checkAssignment("tt1", "attempt_test_0001_r_000002_0 on tt1");
+    checkAssignment("tt1", "attempt_test_0002_m_000002_0 on tt1");
+    checkAssignment("tt1", "attempt_test_0002_r_000002_0 on tt1");
+    checkAssignment("tt2", "attempt_test_0001_m_000003_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0001_r_000003_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0002_m_000003_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0002_r_000003_0 on tt2");
+    
+    // Check scheduler variables; the demands should now be 8 because 2 tasks
+    // of each type have finished in each job
+    assertEquals(2,    info1.mapSchedulable.getRunningTasks());
+    assertEquals(2,    info1.reduceSchedulable.getRunningTasks());
+    assertEquals(8,   info1.mapSchedulable.getDemand());
+    assertEquals(8,   info1.reduceSchedulable.getDemand());
+    assertEquals(2.0,  info1.mapSchedulable.getFairShare());
+    assertEquals(2.0,  info1.reduceSchedulable.getFairShare());
+    assertEquals(2,    info2.mapSchedulable.getRunningTasks());
+    assertEquals(2,    info2.reduceSchedulable.getRunningTasks());
+    assertEquals(8,   info2.mapSchedulable.getDemand());
+    assertEquals(8,   info2.reduceSchedulable.getDemand());
+    assertEquals(2.0,  info2.mapSchedulable.getFairShare());
+    assertEquals(2.0,  info2.reduceSchedulable.getFairShare());
+  }
+  
+  /**
+   * A copy of testLargeJobs that enables the assignMultiple feature to launch
+   * multiple tasks per heartbeat. Results should be the same as testLargeJobs.
+   */
+  public void testLargeJobsWithAssignMultiple() throws IOException {
+    setUpCluster(1, 2, true);
+    
+    JobInProgress job1 = submitJob(JobStatus.RUNNING, 10, 10);
+    JobInfo info1 = scheduler.infos.get(job1);
+    
+    // Check scheduler variables
+    assertEquals(0,    info1.mapSchedulable.getRunningTasks());
+    assertEquals(0,    info1.reduceSchedulable.getRunningTasks());
+    assertEquals(10,   info1.mapSchedulable.getDemand());
+    assertEquals(10,   info1.reduceSchedulable.getDemand());
+    assertEquals(4.0,  info1.mapSchedulable.getFairShare());
+    assertEquals(4.0,  info1.reduceSchedulable.getFairShare());
+    
+    // Advance time before submitting another job j2, to make j1 run before j2
+    // deterministically.
+    advanceTime(100);
+    JobInProgress job2 = submitJob(JobStatus.RUNNING, 10, 10);
+    JobInfo info2 = scheduler.infos.get(job2);
+    
+    // Check scheduler variables; the fair shares should now have been allocated
+    // equally between j1 and j2, but j1 should have (4 slots)*(100 ms) deficit
+    assertEquals(0,    info1.mapSchedulable.getRunningTasks());
+    assertEquals(0,    info1.reduceSchedulable.getRunningTasks());
+    assertEquals(10,   info1.mapSchedulable.getDemand());
+    assertEquals(10,   info1.reduceSchedulable.getDemand());
+    assertEquals(2.0,  info1.mapSchedulable.getFairShare());
+    assertEquals(2.0,  info1.reduceSchedulable.getFairShare());
+    assertEquals(0,    info2.mapSchedulable.getRunningTasks());
+    assertEquals(0,    info2.reduceSchedulable.getRunningTasks());
+    assertEquals(10,   info2.mapSchedulable.getDemand());
+    assertEquals(10,   info2.reduceSchedulable.getDemand());
+    assertEquals(2.0,  info2.mapSchedulable.getFairShare());
+    assertEquals(2.0,  info2.reduceSchedulable.getFairShare());
+    
+    // Check that tasks are filled alternately by the jobs
+    checkAssignment("tt1", "attempt_test_0001_m_000000_0 on tt1",
+                           "attempt_test_0001_r_000000_0 on tt1",
+                           "attempt_test_0002_m_000000_0 on tt1",
+                           "attempt_test_0002_r_000000_0 on tt1");
+    checkAssignment("tt2", "attempt_test_0001_m_000001_0 on tt2",
+                           "attempt_test_0001_r_000001_0 on tt2",
+                           "attempt_test_0002_m_000001_0 on tt2",
+                           "attempt_test_0002_r_000001_0 on tt2");
+    
+    // Check that no new tasks can be launched once the tasktrackers are full
+    assertNull(scheduler.assignTasks(tracker("tt1")));
+    assertNull(scheduler.assignTasks(tracker("tt2")));
+    
+    // Check that the scheduler has started counting the tasks as running
+    // as soon as it launched them.
+    assertEquals(2,  info1.mapSchedulable.getRunningTasks());
+    assertEquals(2,  info1.reduceSchedulable.getRunningTasks());
+    assertEquals(10,  info1.mapSchedulable.getDemand());
+    assertEquals(10,  info1.reduceSchedulable.getDemand());
+    assertEquals(2,  info2.mapSchedulable.getRunningTasks());
+    assertEquals(2,  info2.reduceSchedulable.getRunningTasks());
+    assertEquals(10, info2.mapSchedulable.getDemand());
+    assertEquals(10, info2.reduceSchedulable.getDemand());
+    
+    // Finish up the tasks and advance time again. Note that we must finish
+    // the task since FakeJobInProgress does not properly maintain running
+    // tasks, so the scheduler will always get an empty task list from
+    // the JobInProgress's getTasks(TaskType.MAP)/getTasks(TaskType.REDUCE) and
+    // think they finished.
+    taskTrackerManager.finishTask("tt1", "attempt_test_0001_m_000000_0");
+    taskTrackerManager.finishTask("tt1", "attempt_test_0002_m_000000_0");
+    taskTrackerManager.finishTask("tt1", "attempt_test_0001_r_000000_0");
+    taskTrackerManager.finishTask("tt1", "attempt_test_0002_r_000000_0");
+    taskTrackerManager.finishTask("tt2", "attempt_test_0001_m_000001_0");
+    taskTrackerManager.finishTask("tt2", "attempt_test_0002_m_000001_0");
+    taskTrackerManager.finishTask("tt2", "attempt_test_0001_r_000001_0");
+    taskTrackerManager.finishTask("tt2", "attempt_test_0002_r_000001_0");
+    advanceTime(200);
+    assertEquals(0,   info1.mapSchedulable.getRunningTasks());
+    assertEquals(0,   info1.reduceSchedulable.getRunningTasks());
+    assertEquals(0,   info2.mapSchedulable.getRunningTasks());
+    assertEquals(0,   info2.reduceSchedulable.getRunningTasks());
+
+    // Check that tasks are filled alternately by the jobs
+    checkAssignment("tt1", "attempt_test_0001_m_000002_0 on tt1",
+                           "attempt_test_0001_r_000002_0 on tt1",
+                           "attempt_test_0002_m_000002_0 on tt1",
+                           "attempt_test_0002_r_000002_0 on tt1");
+    checkAssignment("tt2", "attempt_test_0001_m_000003_0 on tt2",
+                           "attempt_test_0001_r_000003_0 on tt2",
+                           "attempt_test_0002_m_000003_0 on tt2",
+                           "attempt_test_0002_r_000003_0 on tt2");
+    
+    // Check scheduler variables; the demands should now be 8 because 2 tasks
+    // of each type have finished in each job
+    assertEquals(2,    info1.mapSchedulable.getRunningTasks());
+    assertEquals(2,    info1.reduceSchedulable.getRunningTasks());
+    assertEquals(8,   info1.mapSchedulable.getDemand());
+    assertEquals(8,   info1.reduceSchedulable.getDemand());
+    assertEquals(2.0,  info1.mapSchedulable.getFairShare());
+    assertEquals(2.0,  info1.reduceSchedulable.getFairShare());
+    assertEquals(2,    info2.mapSchedulable.getRunningTasks());
+    assertEquals(2,    info2.reduceSchedulable.getRunningTasks());
+    assertEquals(8,   info2.mapSchedulable.getDemand());
+    assertEquals(8,   info2.reduceSchedulable.getDemand());
+    assertEquals(2.0,  info2.mapSchedulable.getFairShare());
+    assertEquals(2.0,  info2.reduceSchedulable.getFairShare());
+  }
+
+  /**
+   * We submit two jobs such that one has 2x the priority of the other to 
+   * a cluster of 3 nodes, wait for 100 ms, and check that the weights/shares 
+   * the high-priority job gets 4 tasks while the normal-priority job gets 2.
+   */
+  public void testJobsWithPriorities() throws IOException {
+    setUpCluster(1, 3, false);
+    
+    JobInProgress job1 = submitJob(JobStatus.RUNNING, 10, 10);
+    JobInfo info1 = scheduler.infos.get(job1);
+    JobInProgress job2 = submitJob(JobStatus.RUNNING, 10, 10);
+    JobInfo info2 = scheduler.infos.get(job2);
+    job2.setPriority(JobPriority.HIGH);
+    scheduler.update();
+    
+    // Check scheduler variables
+    assertEquals(0,   info1.mapSchedulable.getRunningTasks());
+    assertEquals(0,   info1.reduceSchedulable.getRunningTasks());
+    assertEquals(10,  info1.mapSchedulable.getDemand());
+    assertEquals(10,  info1.reduceSchedulable.getDemand());
+    assertEquals(2.0, info1.mapSchedulable.getFairShare(), 0.1);
+    assertEquals(2.0, info1.reduceSchedulable.getFairShare(), 0.1);
+    assertEquals(0,   info2.mapSchedulable.getRunningTasks());
+    assertEquals(0,   info2.reduceSchedulable.getRunningTasks());
+    assertEquals(10,  info2.mapSchedulable.getDemand());
+    assertEquals(10,  info2.reduceSchedulable.getDemand());
+    assertEquals(4.0, info2.mapSchedulable.getFairShare(), 0.1);
+    assertEquals(4.0, info2.reduceSchedulable.getFairShare(), 0.1);
+    
+    // Advance time
+    advanceTime(100);
+    
+    // Assign tasks and check that j2 gets 2x more tasks than j1. In addition,
+    // whenever the jobs' runningTasks/weight ratios are tied, j1 should get
+    // the new task first because it started first; thus the tasks of each
+    // type should be handed out alternately to 1, 2, 2, 1, 2, 2, etc.
+    System.out.println("HEREEEE");
+    checkAssignment("tt1", "attempt_test_0001_m_000000_0 on tt1");
+    checkAssignment("tt1", "attempt_test_0001_r_000000_0 on tt1");
+    checkAssignment("tt1", "attempt_test_0002_m_000000_0 on tt1");
+    checkAssignment("tt1", "attempt_test_0002_r_000000_0 on tt1");
+    checkAssignment("tt2", "attempt_test_0002_m_000001_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0002_r_000001_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0001_m_000001_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0001_r_000001_0 on tt2");
+    checkAssignment("tt3", "attempt_test_0002_m_000002_0 on tt3");
+    checkAssignment("tt3", "attempt_test_0002_r_000002_0 on tt3");
+    checkAssignment("tt3", "attempt_test_0002_m_000003_0 on tt3");
+    checkAssignment("tt3", "attempt_test_0002_r_000003_0 on tt3");
+  }
+  
+  /**
+   * This test starts by submitting three large jobs:
+   * - job1 in the default pool, at time 0
+   * - job2 in poolA, with an allocation of 1 map / 2 reduces, at time 200
+   * - job3 in poolB, with an allocation of 2 maps / 1 reduce, at time 300
+   * 
+   * We then assign tasks to all slots. The maps should be assigned in the
+   * order job2, job3, job 3, job1 because jobs 3 and 2 have guaranteed slots
+   * (1 and 2 respectively). Job2 comes before job3 when they are both at 0
+   * slots because it has an earlier start time. In a similar manner,
+   * reduces should be assigned as job2, job3, job2, job1.
+   */
+  public void testLargeJobsWithPools() throws Exception {
+    // Set up pools file
+    PrintWriter out = new PrintWriter(new FileWriter(ALLOC_FILE));
+    out.println("<?xml version=\"1.0\"?>");
+    out.println("<allocations>");
+    // Give pool A a minimum of 1 map, 2 reduces
+    out.println("<pool name=\"poolA\">");
+    out.println("<minMaps>1</minMaps>");
+    out.println("<minReduces>2</minReduces>");
+    out.println("</pool>");
+    // Give pool B a minimum of 2 maps, 1 reduce
+    out.println("<pool name=\"poolB\">");
+    out.println("<minMaps>2</minMaps>");
+    out.println("<minReduces>1</minReduces>");
+    out.println("</pool>");
+    out.println("</allocations>");
+    out.close();
+    scheduler.getPoolManager().reloadAllocs();
+    Pool defaultPool = scheduler.getPoolManager().getPool("default");
+    Pool poolA = scheduler.getPoolManager().getPool("poolA");
+    Pool poolB = scheduler.getPoolManager().getPool("poolB");
+    
+    JobInProgress job1 = submitJob(JobStatus.RUNNING, 10, 10);
+    JobInfo info1 = scheduler.infos.get(job1);
+    
+    // Check scheduler variables
+    assertEquals(0,    info1.mapSchedulable.getRunningTasks());
+    assertEquals(0,    info1.reduceSchedulable.getRunningTasks());
+    assertEquals(10,   info1.mapSchedulable.getDemand());
+    assertEquals(10,   info1.reduceSchedulable.getDemand());
+    assertEquals(4.0,  info1.mapSchedulable.getFairShare());
+    assertEquals(4.0,  info1.reduceSchedulable.getFairShare());
+    
+    // Advance time 200ms and submit jobs 2 and 3
+    advanceTime(200);
+    JobInProgress job2 = submitJob(JobStatus.RUNNING, 10, 10, "poolA");
+    JobInfo info2 = scheduler.infos.get(job2);
+    advanceTime(100);
+    JobInProgress job3 = submitJob(JobStatus.RUNNING, 10, 10, "poolB");
+    JobInfo info3 = scheduler.infos.get(job3);
+    
+    // Check that minimum and fair shares have been allocated
+    assertEquals(0,    defaultPool.getMapSchedulable().getMinShare());
+    assertEquals(0,    defaultPool.getReduceSchedulable().getMinShare());
+    assertEquals(1.0,  info1.mapSchedulable.getFairShare());
+    assertEquals(1.0,  info1.reduceSchedulable.getFairShare());
+    assertEquals(1,    poolA.getMapSchedulable().getMinShare());
+    assertEquals(2,    poolA.getReduceSchedulable().getMinShare());
+    assertEquals(1.0,  info2.mapSchedulable.getFairShare());
+    assertEquals(2.0,  info2.reduceSchedulable.getFairShare());
+    assertEquals(2,    poolB.getMapSchedulable().getMinShare());
+    assertEquals(1,    poolB.getReduceSchedulable().getMinShare());
+    assertEquals(2.0,  info3.mapSchedulable.getFairShare());
+    assertEquals(1.0,  info3.reduceSchedulable.getFairShare());
+    
+    // Advance time 100ms
+    advanceTime(100);
+    
+    // Assign tasks and check that slots are first given to needy jobs
+    checkAssignment("tt1", "attempt_test_0002_m_000000_0 on tt1");
+    checkAssignment("tt1", "attempt_test_0002_r_000000_0 on tt1");
+    checkAssignment("tt1", "attempt_test_0003_m_000000_0 on tt1");
+    checkAssignment("tt1", "attempt_test_0003_r_000000_0 on tt1");
+    checkAssignment("tt2", "attempt_test_0003_m_000001_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0002_r_000001_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0001_m_000000_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0001_r_000000_0 on tt2");
+  }
+
+  /**
+   * This test starts by submitting three large jobs:
+   * - job1 in the default pool, at time 0
+   * - job2 in poolA, with an allocation of 2 maps / 2 reduces, at time 200
+   * - job3 in poolA, with an allocation of 2 maps / 2 reduces, at time 300
+   * 
+   * After this, we start assigning tasks. The first two tasks of each type
+   * should be assigned to job2 and job3 since they are in a pool with an
+   * allocation guarantee, but the next two slots should be assigned to job 3
+   * because the pool will no longer be needy.
+   */
+  public void testLargeJobsWithExcessCapacity() throws Exception {
+    // Set up pools file
+    PrintWriter out = new PrintWriter(new FileWriter(ALLOC_FILE));
+    out.println("<?xml version=\"1.0\"?>");
+    out.println("<allocations>");
+    // Give pool A a minimum of 2 maps, 2 reduces
+    out.println("<pool name=\"poolA\">");
+    out.println("<minMaps>2</minMaps>");
+    out.println("<minReduces>2</minReduces>");
+    out.println("</pool>");
+    out.println("</allocations>");
+    out.close();
+    scheduler.getPoolManager().reloadAllocs();
+    Pool poolA = scheduler.getPoolManager().getPool("poolA");
+    
+    JobInProgress job1 = submitJob(JobStatus.RUNNING, 10, 10);
+    JobInfo info1 = scheduler.infos.get(job1);
+    
+    // Check scheduler variables
+    assertEquals(0,    info1.mapSchedulable.getRunningTasks());
+    assertEquals(0,    info1.reduceSchedulable.getRunningTasks());
+    assertEquals(10,   info1.mapSchedulable.getDemand());
+    assertEquals(10,   info1.reduceSchedulable.getDemand());
+    assertEquals(4.0,  info1.mapSchedulable.getFairShare());
+    assertEquals(4.0,  info1.reduceSchedulable.getFairShare());
+    
+    // Advance time 200ms and submit job 2
+    advanceTime(200);
+    JobInProgress job2 = submitJob(JobStatus.RUNNING, 10, 10, "poolA");
+    JobInfo info2 = scheduler.infos.get(job2);
+    
+    // Check that minimum and fair shares have been allocated
+    assertEquals(2,    poolA.getMapSchedulable().getMinShare());
+    assertEquals(2,    poolA.getReduceSchedulable().getMinShare());
+    assertEquals(2.0,  info1.mapSchedulable.getFairShare());
+    assertEquals(2.0,  info1.reduceSchedulable.getFairShare());
+    assertEquals(2.0,  info2.mapSchedulable.getFairShare());
+    assertEquals(2.0,  info2.reduceSchedulable.getFairShare());
+    
+    // Advance time 100ms and submit job 3
+    advanceTime(100);
+    JobInProgress job3 = submitJob(JobStatus.RUNNING, 10, 10, "poolA");
+    JobInfo info3 = scheduler.infos.get(job3);
+    
+    // Check that minimum and fair shares have been allocated
+    assertEquals(2,    poolA.getMapSchedulable().getMinShare());
+    assertEquals(2,    poolA.getReduceSchedulable().getMinShare());
+    assertEquals(2.0,  info1.mapSchedulable.getFairShare());
+    assertEquals(2.0,  info1.reduceSchedulable.getFairShare());
+    assertEquals(1.0,  info2.mapSchedulable.getFairShare());
+    assertEquals(1.0,  info2.reduceSchedulable.getFairShare());
+    assertEquals(1.0,  info3.mapSchedulable.getFairShare());
+    assertEquals(1.0,  info3.reduceSchedulable.getFairShare());
+    
+    // Advance time
+    advanceTime(100);
+    
+    // Assign tasks and check that slots are first given to needy jobs, but
+    // that job 1 gets two tasks after due to having a larger share.
+    checkAssignment("tt1", "attempt_test_0002_m_000000_0 on tt1");
+    checkAssignment("tt1", "attempt_test_0002_r_000000_0 on tt1");
+    checkAssignment("tt1", "attempt_test_0003_m_000000_0 on tt1");
+    checkAssignment("tt1", "attempt_test_0003_r_000000_0 on tt1");
+    checkAssignment("tt2", "attempt_test_0001_m_000000_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0001_r_000000_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0001_m_000001_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0001_r_000001_0 on tt2");
+  }
+  
+  /**
+   * A copy of testLargeJobsWithExcessCapacity that enables assigning multiple
+   * tasks per heartbeat. Results should match testLargeJobsWithExcessCapacity.
+   */
+  public void testLargeJobsWithExcessCapacityAndAssignMultiple() 
+      throws Exception {
+    setUpCluster(1, 2, true);
+    
+    // Set up pools file
+    PrintWriter out = new PrintWriter(new FileWriter(ALLOC_FILE));
+    out.println("<?xml version=\"1.0\"?>");
+    out.println("<allocations>");
+    // Give pool A a minimum of 2 maps, 2 reduces
+    out.println("<pool name=\"poolA\">");
+    out.println("<minMaps>2</minMaps>");
+    out.println("<minReduces>2</minReduces>");
+    out.println("</pool>");
+    out.println("</allocations>");
+    out.close();
+    scheduler.getPoolManager().reloadAllocs();
+    Pool poolA = scheduler.getPoolManager().getPool("poolA");
+    
+    JobInProgress job1 = submitJob(JobStatus.RUNNING, 10, 10);
+    JobInfo info1 = scheduler.infos.get(job1);
+    
+    // Check scheduler variables
+    assertEquals(0,    info1.mapSchedulable.getRunningTasks());
+    assertEquals(0,    info1.reduceSchedulable.getRunningTasks());
+    assertEquals(10,   info1.mapSchedulable.getDemand());
+    assertEquals(10,   info1.reduceSchedulable.getDemand());
+    assertEquals(4.0,  info1.mapSchedulable.getFairShare());
+    assertEquals(4.0,  info1.reduceSchedulable.getFairShare());
+    
+    // Advance time 200ms and submit job 2
+    advanceTime(200);
+    JobInProgress job2 = submitJob(JobStatus.RUNNING, 10, 10, "poolA");
+    JobInfo info2 = scheduler.infos.get(job2);
+    
+    // Check that minimum and fair shares have been allocated
+    assertEquals(2,    poolA.getMapSchedulable().getMinShare());
+    assertEquals(2,    poolA.getReduceSchedulable().getMinShare());
+    assertEquals(2.0,  info1.mapSchedulable.getFairShare());
+    assertEquals(2.0,  info1.reduceSchedulable.getFairShare());
+    assertEquals(2.0,  info2.mapSchedulable.getFairShare());
+    assertEquals(2.0,  info2.reduceSchedulable.getFairShare());
+    
+    // Advance time 100ms and submit job 3
+    advanceTime(100);
+    JobInProgress job3 = submitJob(JobStatus.RUNNING, 10, 10, "poolA");
+    JobInfo info3 = scheduler.infos.get(job3);
+    
+    // Check that minimum and fair shares have been allocated
+    assertEquals(2,    poolA.getMapSchedulable().getMinShare());
+    assertEquals(2,    poolA.getReduceSchedulable().getMinShare());
+    assertEquals(2.0,  info1.mapSchedulable.getFairShare());
+    assertEquals(2.0,  info1.reduceSchedulable.getFairShare());
+    assertEquals(1.0,  info2.mapSchedulable.getFairShare());
+    assertEquals(1.0,  info2.reduceSchedulable.getFairShare());
+    assertEquals(1.0,  info3.mapSchedulable.getFairShare());
+    assertEquals(1.0,  info3.reduceSchedulable.getFairShare());
+    
+    // Advance time
+    advanceTime(100);
+    
+    // Assign tasks and check that slots are first given to needy jobs, but
+    // that job 1 gets two tasks after due to having a larger share.
+    checkAssignment("tt1", "attempt_test_0002_m_000000_0 on tt1",
+                           "attempt_test_0002_r_000000_0 on tt1",
+                           "attempt_test_0003_m_000000_0 on tt1",
+                           "attempt_test_0003_r_000000_0 on tt1");
+    checkAssignment("tt2", "attempt_test_0001_m_000000_0 on tt2",
+                           "attempt_test_0001_r_000000_0 on tt2",
+                           "attempt_test_0001_m_000001_0 on tt2",
+                           "attempt_test_0001_r_000001_0 on tt2");
+  }
+  
+  /**
+   * This test starts by submitting two jobs at time 0:
+   * - job1 in the default pool
+   * - job2, with 1 map and 1 reduce, in poolA, which has an alloc of 4
+   *   maps and 4 reduces
+   * 
+   * When we assign the slots, job2 should only get 1 of each type of task.
+   */
+  public void testSmallJobInLargePool() throws Exception {
+    // Set up pools file
+    PrintWriter out = new PrintWriter(new FileWriter(ALLOC_FILE));
+    out.println("<?xml version=\"1.0\"?>");
+    out.println("<allocations>");
+    // Give pool A a minimum of 4 maps, 4 reduces
+    out.println("<pool name=\"poolA\">");
+    out.println("<minMaps>4</minMaps>");
+    out.println("<minReduces>4</minReduces>");
+    out.println("</pool>");
+    out.println("</allocations>");
+    out.close();
+    scheduler.getPoolManager().reloadAllocs();
+    
+    JobInProgress job1 = submitJob(JobStatus.RUNNING, 10, 10);
+    JobInfo info1 = scheduler.infos.get(job1);
+    JobInProgress job2 = submitJob(JobStatus.RUNNING, 1, 1, "poolA");
+    JobInfo info2 = scheduler.infos.get(job2);
+    
+    // Check scheduler variables
+    assertEquals(0,    info1.mapSchedulable.getRunningTasks());
+    assertEquals(0,    info1.reduceSchedulable.getRunningTasks());
+    assertEquals(10,   info1.mapSchedulable.getDemand());
+    assertEquals(10,   info1.reduceSchedulable.getDemand());
+    assertEquals(3.0,  info1.mapSchedulable.getFairShare());
+    assertEquals(3.0,  info1.reduceSchedulable.getFairShare());
+    assertEquals(0,    info2.mapSchedulable.getRunningTasks());
+    assertEquals(0,    info2.reduceSchedulable.getRunningTasks());
+    assertEquals(1,    info2.mapSchedulable.getDemand());
+    assertEquals(1,    info2.reduceSchedulable.getDemand());
+    assertEquals(1.0,  info2.mapSchedulable.getFairShare());
+    assertEquals(1.0,  info2.reduceSchedulable.getFairShare());
+    
+    // Assign tasks and check that slots are first given to needy jobs
+    checkAssignment("tt1", "attempt_test_0002_m_000000_0 on tt1");
+    checkAssignment("tt1", "attempt_test_0002_r_000000_0 on tt1");
+    checkAssignment("tt1", "attempt_test_0001_m_000000_0 on tt1");
+    checkAssignment("tt1", "attempt_test_0001_r_000000_0 on tt1");
+    checkAssignment("tt2", "attempt_test_0001_m_000001_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0001_r_000001_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0001_m_000002_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0001_r_000002_0 on tt2");
+  }
+  
+  /**
+   * This test starts by submitting four jobs in the default pool. However, the
+   * maxRunningJobs limit for this pool has been set to two. We should see only
+   * the first two jobs get scheduled, each with half the total slots.
+   */
+  public void testPoolMaxJobs() throws Exception {
+    // Set up pools file
+    PrintWriter out = new PrintWriter(new FileWriter(ALLOC_FILE));
+    out.println("<?xml version=\"1.0\"?>");
+    out.println("<allocations>");
+    out.println("<pool name=\"default\">");
+    out.println("<maxRunningJobs>2</maxRunningJobs>");
+    out.println("</pool>");
+    out.println("</allocations>");
+    out.close();
+    scheduler.getPoolManager().reloadAllocs();
+    
+    // Submit jobs, advancing time in-between to make sure that they are
+    // all submitted at distinct times.
+    JobInProgress job1 = submitJobNotInitialized(JobStatus.PREP, 10, 10);
+    assertTrue(((FakeJobInProgress)job1).inited());
+    job1.getStatus().setRunState(JobStatus.RUNNING);
+    JobInfo info1 = scheduler.infos.get(job1);
+    advanceTime(10);
+    JobInProgress job2 = submitJobNotInitialized(JobStatus.PREP, 10, 10);
+    assertTrue(((FakeJobInProgress)job2).inited());
+    job2.getStatus().setRunState(JobStatus.RUNNING);
+    JobInfo info2 = scheduler.infos.get(job2);
+    advanceTime(10);
+    JobInProgress job3 = submitJobNotInitialized(JobStatus.PREP, 10, 10);
+    JobInfo info3 = scheduler.infos.get(job3);
+    advanceTime(10);
+    JobInProgress job4 = submitJobNotInitialized(JobStatus.PREP, 10, 10);
+    JobInfo info4 = scheduler.infos.get(job4);
+    
+    // Only two of the jobs should be initialized.
+    assertTrue(((FakeJobInProgress)job1).inited());
+    assertTrue(((FakeJobInProgress)job2).inited());
+    assertFalse(((FakeJobInProgress)job3).inited());
+    assertFalse(((FakeJobInProgress)job4).inited());
+    
+    // Check scheduler variables
+    assertEquals(2.0,  info1.mapSchedulable.getFairShare());
+    assertEquals(2.0,  info1.reduceSchedulable.getFairShare());
+    assertEquals(2.0,  info2.mapSchedulable.getFairShare());
+    assertEquals(2.0,  info2.reduceSchedulable.getFairShare());
+    assertEquals(0.0,  info3.mapSchedulable.getFairShare());
+    assertEquals(0.0,  info3.reduceSchedulable.getFairShare());
+    assertEquals(0.0,  info4.mapSchedulable.getFairShare());
+    assertEquals(0.0,  info4.reduceSchedulable.getFairShare());
+    
+    // Assign tasks and check that only jobs 1 and 2 get them
+    checkAssignment("tt1", "attempt_test_0001_m_000000_0 on tt1");
+    checkAssignment("tt1", "attempt_test_0001_r_000000_0 on tt1");
+    checkAssignment("tt1", "attempt_test_0002_m_000000_0 on tt1");
+    checkAssignment("tt1", "attempt_test_0002_r_000000_0 on tt1");
+    advanceTime(100);
+    checkAssignment("tt2", "attempt_test_0001_m_000001_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0001_r_000001_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0002_m_000001_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0002_r_000001_0 on tt2");
+  }
+
+  /**
+   * This test starts by submitting two jobs by user "user1" to the default
+   * pool, and two jobs by "user2". We set user1's job limit to 1. We should
+   * see one job from user1 and two from user2. 
+   */
+  public void testUserMaxJobs() throws Exception {
+    // Set up pools file
+    PrintWriter out = new PrintWriter(new FileWriter(ALLOC_FILE));
+    out.println("<?xml version=\"1.0\"?>");
+    out.println("<allocations>");
+    out.println("<user name=\"user1\">");
+    out.println("<maxRunningJobs>1</maxRunningJobs>");
+    out.println("</user>");
+    out.println("</allocations>");
+    out.close();
+    scheduler.getPoolManager().reloadAllocs();
+    
+    // Submit jobs, advancing time in-between to make sure that they are
+    // all submitted at distinct times.
+    JobInProgress job1 = submitJob(JobStatus.RUNNING, 10, 10);
+    job1.getJobConf().set("user.name", "user1");
+    JobInfo info1 = scheduler.infos.get(job1);
+    advanceTime(10);
+    JobInProgress job2 = submitJob(JobStatus.RUNNING, 10, 10);
+    job2.getJobConf().set("user.name", "user1");
+    JobInfo info2 = scheduler.infos.get(job2);
+    advanceTime(10);
+    JobInProgress job3 = submitJob(JobStatus.RUNNING, 10, 10);
+    job3.getJobConf().set("user.name", "user2");
+    JobInfo info3 = scheduler.infos.get(job3);
+    advanceTime(10);
+    JobInProgress job4 = submitJob(JobStatus.RUNNING, 10, 10);
+    job4.getJobConf().set("user.name", "user2");
+    JobInfo info4 = scheduler.infos.get(job4);
+    
+    // Check scheduler variables
+    assertEquals(1.33,  info1.mapSchedulable.getFairShare(), 0.1);
+    assertEquals(1.33,  info1.reduceSchedulable.getFairShare(), 0.1);
+    assertEquals(0.0,   info2.mapSchedulable.getFairShare());
+    assertEquals(0.0,   info2.reduceSchedulable.getFairShare());
+    assertEquals(1.33,  info3.mapSchedulable.getFairShare(), 0.1);
+    assertEquals(1.33,  info3.reduceSchedulable.getFairShare(), 0.1);
+    assertEquals(1.33,  info4.mapSchedulable.getFairShare(), 0.1);
+    assertEquals(1.33,  info4.reduceSchedulable.getFairShare(), 0.1);
+    
+    // Assign tasks and check that slots are given only to jobs 1, 3 and 4
+    checkAssignment("tt1", "attempt_test_0001_m_000000_0 on tt1");
+    checkAssignment("tt1", "attempt_test_0001_r_000000_0 on tt1");
+    checkAssignment("tt1", "attempt_test_0003_m_000000_0 on tt1");
+    checkAssignment("tt1", "attempt_test_0003_r_000000_0 on tt1");
+    advanceTime(100);
+    checkAssignment("tt2", "attempt_test_0004_m_000000_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0004_r_000000_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0001_m_000001_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0001_r_000001_0 on tt2");
+  }
+  
+  /**
+   * Test a combination of pool job limits and user job limits, the latter
+   * specified through both the userMaxJobsDefaults (for some users) and
+   * user-specific &lt;user&gt; elements in the allocations file. 
+   */
+  public void testComplexJobLimits() throws Exception {
+    // Set up pools file
+    PrintWriter out = new PrintWriter(new FileWriter(ALLOC_FILE));
+    out.println("<?xml version=\"1.0\"?>");
+    out.println("<allocations>");
+    out.println("<pool name=\"poolA\">");
+    out.println("<maxRunningJobs>1</maxRunningJobs>");
+    out.println("</pool>");
+    out.println("<user name=\"user1\">");
+    out.println("<maxRunningJobs>1</maxRunningJobs>");
+    out.println("</user>");
+    out.println("<user name=\"user2\">");
+    out.println("<maxRunningJobs>10</maxRunningJobs>");
+    out.println("</user>");
+    out.println("<userMaxJobsDefault>2</userMaxJobsDefault>");
+    out.println("</allocations>");
+    out.close();
+    scheduler.getPoolManager().reloadAllocs();
+    
+    // Submit jobs, advancing time in-between to make sure that they are
+    // all submitted at distinct times.
+    
+    // Two jobs for user1; only one should get to run
+    JobInProgress job1 = submitJob(JobStatus.RUNNING, 10, 10);
+    job1.getJobConf().set("user.name", "user1");
+    JobInfo info1 = scheduler.infos.get(job1);
+    advanceTime(10);
+    JobInProgress job2 = submitJob(JobStatus.RUNNING, 10, 10);
+    job2.getJobConf().set("user.name", "user1");
+    JobInfo info2 = scheduler.infos.get(job2);
+    advanceTime(10);
+    
+    // Three jobs for user2; all should get to run
+    JobInProgress job3 = submitJob(JobStatus.RUNNING, 10, 10);
+    job3.getJobConf().set("user.name", "user2");
+    JobInfo info3 = scheduler.infos.get(job3);
+    advanceTime(10);
+    JobInProgress job4 = submitJob(JobStatus.RUNNING, 10, 10);
+    job4.getJobConf().set("user.name", "user2");
+    JobInfo info4 = scheduler.infos.get(job4);
+    advanceTime(10);
+    JobInProgress job5 = submitJob(JobStatus.RUNNING, 10, 10);
+    job5.getJobConf().set("user.name", "user2");
+    JobInfo info5 = scheduler.infos.get(job5);
+    advanceTime(10);
+    
+    // Three jobs for user3; only two should get to run
+    JobInProgress job6 = submitJob(JobStatus.RUNNING, 10, 10);
+    job6.getJobConf().set("user.name", "user3");
+    JobInfo info6 = scheduler.infos.get(job6);
+    advanceTime(10);
+    JobInProgress job7 = submitJob(JobStatus.RUNNING, 10, 10);
+    job7.getJobConf().set("user.name", "user3");
+    JobInfo info7 = scheduler.infos.get(job7);
+    advanceTime(10);
+    JobInProgress job8 = submitJob(JobStatus.RUNNING, 10, 10);
+    job8.getJobConf().set("user.name", "user3");
+    JobInfo info8 = scheduler.infos.get(job8);
+    advanceTime(10);
+    
+    // Two jobs for user4, in poolA; only one should get to run
+    JobInProgress job9 = submitJob(JobStatus.RUNNING, 10, 10, "poolA");
+    job9.getJobConf().set("user.name", "user4");
+    JobInfo info9 = scheduler.infos.get(job9);
+    advanceTime(10);
+    JobInProgress job10 = submitJob(JobStatus.RUNNING, 10, 10, "poolA");
+    job10.getJobConf().set("user.name", "user4");
+    JobInfo info10 = scheduler.infos.get(job10);
+    advanceTime(10);
+    
+    // Check scheduler variables. The jobs in poolA should get half
+    // the total share, while those in the default pool should get
+    // the other half. This works out to 2 slots each for the jobs
+    // in poolA and 1/3 each for the jobs in the default pool because
+    // there are 2 runnable jobs in poolA and 6 jobs in the default pool.
+    assertEquals(0.33,   info1.mapSchedulable.getFairShare(), 0.1);
+    assertEquals(0.33,   info1.reduceSchedulable.getFairShare(), 0.1);
+    assertEquals(0.0,    info2.mapSchedulable.getFairShare());
+    assertEquals(0.0,    info2.reduceSchedulable.getFairShare());
+    assertEquals(0.33,   info3.mapSchedulable.getFairShare(), 0.1);
+    assertEquals(0.33,   info3.reduceSchedulable.getFairShare(), 0.1);
+    assertEquals(0.33,   info4.mapSchedulable.getFairShare(), 0.1);
+    assertEquals(0.33,   info4.reduceSchedulable.getFairShare(), 0.1);
+    assertEquals(0.33,   info5.mapSchedulable.getFairShare(), 0.1);
+    assertEquals(0.33,   info5.reduceSchedulable.getFairShare(), 0.1);
+    assertEquals(0.33,   info6.mapSchedulable.getFairShare(), 0.1);
+    assertEquals(0.33,   info6.reduceSchedulable.getFairShare(), 0.1);
+    assertEquals(0.33,   info7.mapSchedulable.getFairShare(), 0.1);
+    assertEquals(0.33,   info7.reduceSchedulable.getFairShare(), 0.1);
+    assertEquals(0.0,    info8.mapSchedulable.getFairShare());
+    assertEquals(0.0,    info8.reduceSchedulable.getFairShare());
+    assertEquals(2.0,    info9.mapSchedulable.getFairShare(), 0.1);
+    assertEquals(2.0,    info9.reduceSchedulable.getFairShare(), 0.1);
+    assertEquals(0.0,    info10.mapSchedulable.getFairShare());
+    assertEquals(0.0,    info10.reduceSchedulable.getFairShare());
+  }
+  
+  public void testSizeBasedWeight() throws Exception {
+    scheduler.sizeBasedWeight = true;
+    JobInProgress job1 = submitJob(JobStatus.RUNNING, 2, 10);
+    JobInProgress job2 = submitJob(JobStatus.RUNNING, 20, 1);
+    assertTrue(scheduler.infos.get(job2).mapSchedulable.getFairShare() >
+               scheduler.infos.get(job1).mapSchedulable.getFairShare());
+    assertTrue(scheduler.infos.get(job1).reduceSchedulable.getFairShare() >
+               scheduler.infos.get(job2).reduceSchedulable.getFairShare());
+  }
+
+  /**
+   * This test submits jobs in three pools: poolA, which has a weight
+   * of 2.0; poolB, which has a weight of 0.5; and the default pool, which
+   * should have a weight of 1.0. It then checks that the map and reduce
+   * fair shares are given out accordingly. We then submit a second job to
+   * pool B and check that each gets half of the pool (weight of 0.25).
+   */
+  public void testPoolWeights() throws Exception {
+    // Set up pools file
+    PrintWriter out = new PrintWriter(new FileWriter(ALLOC_FILE));
+    out.println("<?xml version=\"1.0\"?>");
+    out.println("<allocations>");
+    out.println("<pool name=\"poolA\">");
+    out.println("<weight>2.0</weight>");
+    out.println("</pool>");
+    out.println("<pool name=\"poolB\">");
+    out.println("<weight>0.5</weight>");
+    out.println("</pool>");
+    out.println("</allocations>");
+    out.close();
+    scheduler.getPoolManager().reloadAllocs();
+    
+    // Submit jobs, advancing time in-between to make sure that they are
+    // all submitted at distinct times.
+    JobInProgress job1 = submitJob(JobStatus.RUNNING, 10, 10);
+    JobInfo info1 = scheduler.infos.get(job1);
+    JobInProgress job2 = submitJob(JobStatus.RUNNING, 10, 10, "poolA");
+    JobInfo info2 = scheduler.infos.get(job2);
+    JobInProgress job3 = submitJob(JobStatus.RUNNING, 10, 10, "poolB");
+    JobInfo info3 = scheduler.infos.get(job3);
+    advanceTime(10);
+    
+    assertEquals(1.14,  info1.mapSchedulable.getFairShare(), 0.01);
+    assertEquals(1.14,  info1.reduceSchedulable.getFairShare(), 0.01);
+    assertEquals(2.28,  info2.mapSchedulable.getFairShare(), 0.01);
+    assertEquals(2.28,  info2.reduceSchedulable.getFairShare(), 0.01);
+    assertEquals(0.57,  info3.mapSchedulable.getFairShare(), 0.01);
+    assertEquals(0.57,  info3.reduceSchedulable.getFairShare(), 0.01);
+    
+    JobInProgress job4 = submitJob(JobStatus.RUNNING, 10, 10, "poolB");
+    JobInfo info4 = scheduler.infos.get(job4);
+    advanceTime(10);
+    
+    assertEquals(1.14,  info1.mapSchedulable.getFairShare(), 0.01);
+    assertEquals(1.14,  info1.reduceSchedulable.getFairShare(), 0.01);
+    assertEquals(2.28,  info2.mapSchedulable.getFairShare(), 0.01);
+    assertEquals(2.28,  info2.reduceSchedulable.getFairShare(), 0.01);
+    assertEquals(0.28,  info3.mapSchedulable.getFairShare(), 0.01);
+    assertEquals(0.28,  info3.reduceSchedulable.getFairShare(), 0.01);
+    assertEquals(0.28,  info4.mapSchedulable.getFairShare(), 0.01);
+    assertEquals(0.28,  info4.reduceSchedulable.getFairShare(), 0.01);
+    verifyMetrics();    
+  }
+
+  /**
+   * This test submits jobs in two pools, poolA and poolB. None of the
+   * jobs in poolA have maps, but this should not affect their reduce
+   * share.
+   */
+  public void testPoolWeightsWhenNoMaps() throws Exception {
+    // Set up pools file
+    PrintWriter out = new PrintWriter(new FileWriter(ALLOC_FILE));
+    out.println("<?xml version=\"1.0\"?>");
+    out.println("<allocations>");
+    out.println("<pool name=\"poolA\">");
+    out.println("<weight>2.0</weight>");
+    out.println("</pool>");
+    out.println("<pool name=\"poolB\">");
+    out.println("<weight>1.0</weight>");
+    out.println("</pool>");
+    out.println("</allocations>");
+    out.close();
+    scheduler.getPoolManager().reloadAllocs();
+    
+    // Submit jobs, advancing time in-between to make sure that they are
+    // all submitted at distinct times.
+    JobInProgress job1 = submitJob(JobStatus.RUNNING, 0, 10, "poolA");
+    JobInfo info1 = scheduler.infos.get(job1);
+    JobInProgress job2 = submitJob(JobStatus.RUNNING, 0, 10, "poolA");
+    JobInfo info2 = scheduler.infos.get(job2);
+    JobInProgress job3 = submitJob(JobStatus.RUNNING, 10, 10, "poolB");
+    JobInfo info3 = scheduler.infos.get(job3);
+    advanceTime(10);
+    
+    /*
+    assertEquals(0,     info1.mapWeight, 0.01);
+    assertEquals(1.0,   info1.reduceWeight, 0.01);
+    assertEquals(0,     info2.mapWeight, 0.01);
+    assertEquals(1.0,   info2.reduceWeight, 0.01);
+    assertEquals(1.0,   info3.mapWeight, 0.01);
+    assertEquals(1.0,   info3.reduceWeight, 0.01);
+    */
+    
+    assertEquals(0,     info1.mapSchedulable.getFairShare(), 0.01);
+    assertEquals(1.33,  info1.reduceSchedulable.getFairShare(), 0.01);
+    assertEquals(0,     info2.mapSchedulable.getFairShare(), 0.01);
+    assertEquals(1.33,  info2.reduceSchedulable.getFairShare(), 0.01);
+    assertEquals(4,     info3.mapSchedulable.getFairShare(), 0.01);
+    assertEquals(1.33,  info3.reduceSchedulable.getFairShare(), 0.01);
+  }
+
+  public void testPoolMaxMapsReduces() throws Exception {
+    PrintWriter out = new PrintWriter(new FileWriter(ALLOC_FILE));
+    out.println("<?xml version=\"1.0\"?>");
+    out.println("<allocations>");
+    // Pool with upper bound
+    out.println("<pool name=\"poolLimited\">");
+    out.println("<weight>1.0</weight>");
+    out.println("<maxMaps>2</maxMaps>");
+    out.println("<maxReduces>1</maxReduces>");
+    out.println("</pool>");
+    out.println("</allocations>");
+    out.close();
+    scheduler.getPoolManager().reloadAllocs();
+    // Create two jobs with ten maps
+    JobInProgress job1 = submitJob(JobStatus.RUNNING, 10, 5, "poolLimited");
+    advanceTime(10);
+    JobInProgress job2 = submitJob(JobStatus.RUNNING, 10, 5);
+    checkAssignment("tt1", "attempt_test_0002_m_000000_0 on tt1");
+    checkAssignment("tt1", "attempt_test_0002_r_000000_0 on tt1");
+    checkAssignment("tt1", "attempt_test_0001_m_000000_0 on tt1");
+    checkAssignment("tt1", "attempt_test_0001_r_000000_0 on tt1");
+    checkAssignment("tt2", "attempt_test_0002_m_000001_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0002_r_000001_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0001_m_000001_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0002_r_000002_0 on tt2");
+
+    Pool limited = scheduler.getPoolManager().getPool("poolLimited");
+    assertEquals(2, limited.getSchedulable(TaskType.MAP).getRunningTasks());
+    assertEquals(1, limited.getSchedulable(TaskType.REDUCE).getRunningTasks());
+    Pool defaultPool = scheduler.getPoolManager().getPool("default");
+    assertEquals(2, defaultPool.getSchedulable(TaskType.MAP).getRunningTasks());
+    assertEquals(3, defaultPool.getSchedulable(TaskType.REDUCE)
+        .getRunningTasks());
+    assertEquals(2, job1.runningMapTasks);
+    assertEquals(1, job1.runningReduceTasks);
+    assertEquals(2, job2.runningMapTasks);
+    assertEquals(3, job2.runningReduceTasks);
+  }
+
+  /**
+   * Tests that max-running-tasks per node are set by assigning load
+   * equally accross the cluster in CapBasedLoadManager.
+   */
+  public void testCapBasedLoadManager() {
+    CapBasedLoadManager loadMgr = new CapBasedLoadManager();
+    // Arguments to getCap: totalRunnableTasks, nodeCap, totalSlots
+    // Desired behavior: return ceil(nodeCap * min(1, runnableTasks/totalSlots))
+    assertEquals(1, loadMgr.getCap(1, 1, 100));
+    assertEquals(1, loadMgr.getCap(1, 2, 100));
+    assertEquals(1, loadMgr.getCap(1, 10, 100));
+    assertEquals(1, loadMgr.getCap(200, 1, 100));
+    assertEquals(1, loadMgr.getCap(1, 5, 100));
+    assertEquals(3, loadMgr.getCap(50, 5, 100));
+    assertEquals(5, loadMgr.getCap(100, 5, 100));
+    assertEquals(5, loadMgr.getCap(200, 5, 100));
+  }
+
+  /**
+   * This test starts by launching a job in the default pool that takes
+   * all the slots in the cluster. We then submit a job in a pool with
+   * min share of 2 maps and 1 reduce task. After the min share preemption
+   * timeout, this pool should be allowed to preempt tasks. 
+   */
+  public void testMinSharePreemption() throws Exception {
+    // Enable preemption in scheduler
+    scheduler.preemptionEnabled = true;
+    // Set up pools file
+    PrintWriter out = new PrintWriter(new FileWriter(ALLOC_FILE));
+    out.println("<?xml version=\"1.0\"?>");
+    out.println("<allocations>");
+    // Give pool A a min share of 2 maps and 1 reduce, and a preemption
+    // timeout of 1 minute
+    out.println("<pool name=\"poolA\">");
+    out.println("<minMaps>2</minMaps>");
+    out.println("<minReduces>1</minReduces>");
+    out.println("<minSharePreemptionTimeout>60</minSharePreemptionTimeout>");
+    out.println("</pool>");
+    out.println("</allocations>");
+    out.close();
+    scheduler.getPoolManager().reloadAllocs();
+    Pool poolA = scheduler.getPoolManager().getPool("poolA");
+
+    // Submit job 1 and assign all slots to it. Sleep a bit before assigning
+    // tasks on tt1 and tt2 to ensure that the ones on tt2 get preempted first.
+    JobInProgress job1 = submitJob(JobStatus.RUNNING, 10, 10);
+    checkAssignment("tt1", "attempt_test_0001_m_000000_0 on tt1");
+    checkAssignment("tt1", "attempt_test_0001_r_000000_0 on tt1");
+    checkAssignment("tt1", "attempt_test_0001_m_000001_0 on tt1");
+    checkAssignment("tt1", "attempt_test_0001_r_000001_0 on tt1");
+    advanceTime(100);
+    checkAssignment("tt2", "attempt_test_0001_m_000002_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0001_r_000002_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0001_m_000003_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0001_r_000003_0 on tt2");
+    
+    // Ten seconds later, submit job 2.
+    advanceTime(10000);
+    JobInProgress job2 = submitJob(JobStatus.RUNNING, 10, 10, "poolA");
+    
+    // Ten seconds later, check that job 2 is not able to preempt tasks.
+    advanceTime(10000);
+    assertEquals(0, scheduler.tasksToPreempt(poolA.getMapSchedulable(),
+        clock.getTime()));
+    assertEquals(0, scheduler.tasksToPreempt(poolA.getReduceSchedulable(),
+        clock.getTime()));
+    
+    // Advance time by 49 more seconds, putting us at 59s after the
+    // submission of job 2. It should still not be able to preempt.
+    advanceTime(49000);
+    assertEquals(0, scheduler.tasksToPreempt(poolA.getMapSchedulable(),
+        clock.getTime()));
+    assertEquals(0, scheduler.tasksToPreempt(poolA.getReduceSchedulable(),
+        clock.getTime()));
+    
+    // Advance time by 2 seconds, putting us at 61s after the submission
+    // of job 2. It should now be able to preempt 2 maps and 1 reduce.
+    advanceTime(2000);
+    assertEquals(2, scheduler.tasksToPreempt(poolA.getMapSchedulable(),
+        clock.getTime()));
+    assertEquals(1, scheduler.tasksToPreempt(poolA.getReduceSchedulable(),
+        clock.getTime()));
+    
+    // Test that the tasks actually get preempted and we can assign new ones
+    scheduler.preemptTasksIfNecessary();
+    scheduler.update();
+    assertEquals(2, job1.runningMaps());
+    assertEquals(3, job1.runningReduces());
+    checkAssignment("tt2", "attempt_test_0002_m_000000_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0002_m_000001_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0002_r_000000_0 on tt2");
+    assertNull(scheduler.assignTasks(tracker("tt1")));
+    assertNull(scheduler.assignTasks(tracker("tt2")));
+  }
+
+  /**
+   * This test starts by launching a job in the default pool that takes
+   * all the slots in the cluster. We then submit a job in a pool with
+   * min share of 3 maps and 3 reduce tasks, but which only actually
+   * needs 1 map and 2 reduces. We check that this pool does not prempt
+   * more than this many tasks despite its min share being higher. 
+   */
+  public void testMinSharePreemptionWithSmallJob() throws Exception {
+    // Enable preemption in scheduler
+    scheduler.preemptionEnabled = true;
+    // Set up pools file
+    PrintWriter out = new PrintWriter(new FileWriter(ALLOC_FILE));
+    out.println("<?xml version=\"1.0\"?>");
+    out.println("<allocations>");
+    // Give pool A a min share of 2 maps and 1 reduce, and a preemption
+    // timeout of 1 minute
+    out.println("<pool name=\"poolA\">");
+    out.println("<minMaps>3</minMaps>");
+    out.println("<minReduces>3</minReduces>");
+    out.println("<minSharePreemptionTimeout>60</minSharePreemptionTimeout>");
+    out.println("</pool>");
+    out.println("</allocations>");
+    out.close();
+    scheduler.getPoolManager().reloadAllocs();
+    Pool poolA = scheduler.getPoolManager().getPool("poolA");
+
+    // Submit job 1 and assign all slots to it. Sleep a bit before assigning
+    // tasks on tt1 and tt2 to ensure that the ones on tt2 get preempted first.
+    JobInProgress job1 = submitJob(JobStatus.RUNNING, 10, 10);
+    checkAssignment("tt1", "attempt_test_0001_m_000000_0 on tt1");
+    checkAssignment("tt1", "attempt_test_0001_r_000000_0 on tt1");
+    checkAssignment("tt1", "attempt_test_0001_m_000001_0 on tt1");
+    checkAssignment("tt1", "attempt_test_0001_r_000001_0 on tt1");
+    advanceTime(100);
+    checkAssignment("tt2", "attempt_test_0001_m_000002_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0001_r_000002_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0001_m_000003_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0001_r_000003_0 on tt2");
+    
+    // Ten seconds later, submit job 2.
+    advanceTime(10000);
+    JobInProgress job2 = submitJob(JobStatus.RUNNING, 1, 2, "poolA");
+    
+    // Advance time by 59 seconds and check that no preemption occurs.
+    advanceTime(59000);
+    assertEquals(0, scheduler.tasksToPreempt(poolA.getMapSchedulable(),
+        clock.getTime()));
+    assertEquals(0, scheduler.tasksToPreempt(poolA.getReduceSchedulable(),
+        clock.getTime()));
+    
+    // Advance time by 2 seconds, putting us at 61s after the submission
+    // of job 2. Job 2 should now preempt 1 map and 2 reduces.
+    advanceTime(2000);
+    assertEquals(1, scheduler.tasksToPreempt(poolA.getMapSchedulable(),
+        clock.getTime()));
+    assertEquals(2, scheduler.tasksToPreempt(poolA.getReduceSchedulable(),
+        clock.getTime()));
+
+    // Test that the tasks actually get preempted and we can assign new ones
+    scheduler.preemptTasksIfNecessary();
+    scheduler.update();
+    assertEquals(3, job1.runningMaps());
+    assertEquals(2, job1.runningReduces());
+    checkAssignment("tt2", "attempt_test_0002_r_000000_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0002_m_000000_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0002_r_000001_0 on tt2");
+    assertNull(scheduler.assignTasks(tracker("tt1")));
+    assertNull(scheduler.assignTasks(tracker("tt2")));
+  }
+
+  /**
+   * This test runs on a 4-node (8-slot) cluster to allow 3 pools with fair
+   * shares greater than 2 slots to coexist (which makes the half-fair-share 
+   * of each pool more than 1 so that fair share preemption can kick in). 
+   * 
+   * The test first starts job 1, which takes 6 map slots and 6 reduce slots,
+   * in pool 1.  We then submit job 2 in pool 2, which takes 2 slots of each
+   * type. Finally, we submit a third job, job 3 in pool3, which gets no slots. 
+   * At this point the fair share of each pool will be 8/3 ~= 2.7 slots. 
+   * Pool 1 will be above its fair share, pool 2 will be below it but at half
+   * fair share, and pool 3 will be below half fair share. Therefore pool 3 
+   * should preempt a task (after a timeout) but pools 1 and 2 shouldn't. 
+   */
+  public void testFairSharePreemption() throws Exception {
+    // Create a bigger cluster than normal (4 tasktrackers instead of 2)
+    setUpCluster(1, 4, false);
+    // Enable preemption in scheduler
+    scheduler.preemptionEnabled = true;
+    // Set up pools file with a fair share preemtion timeout of 1 minute
+    PrintWriter out = new PrintWriter(new FileWriter(ALLOC_FILE));
+    out.println("<?xml version=\"1.0\"?>");
+    out.println("<allocations>");
+    out.println("<fairSharePreemptionTimeout>60</fairSharePreemptionTimeout>");
+    out.println("</allocations>");
+    out.close();
+    scheduler.getPoolManager().reloadAllocs();
+    
+    // Grab pools (they'll be created even though they're not in the alloc file)
+    Pool pool1 = scheduler.getPoolManager().getPool("pool1");
+    Pool pool2 = scheduler.getPoolManager().getPool("pool2");
+    Pool pool3 = scheduler.getPoolManager().getPool("pool3");
+
+    // Submit job 1. We advance time by 100 between each task tracker
+    // assignment stage to ensure that the tasks from job1 on tt3 are the ones
+    // that are deterministically preempted first (being the latest launched
+    // tasks in an over-allocated job).
+    JobInProgress job1 = submitJob(JobStatus.RUNNING, 6, 6, "pool1");
+    advanceTime(100);
+    checkAssignment("tt1", "attempt_test_0001_m_000000_0 on tt1");
+    checkAssignment("tt1", "attempt_test_0001_r_000000_0 on tt1");
+    checkAssignment("tt1", "attempt_test_0001_m_000001_0 on tt1");
+    checkAssignment("tt1", "attempt_test_0001_r_000001_0 on tt1");
+    advanceTime(100);
+    checkAssignment("tt2", "attempt_test_0001_m_000002_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0001_r_000002_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0001_m_000003_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0001_r_000003_0 on tt2");
+    advanceTime(100);
+    checkAssignment("tt3", "attempt_test_0001_m_000004_0 on tt3");
+    checkAssignment("tt3", "attempt_test_0001_r_000004_0 on tt3");
+    checkAssignment("tt3", "attempt_test_0001_m_000005_0 on tt3");
+    checkAssignment("tt3", "attempt_test_0001_r_000005_0 on tt3");
+    advanceTime(100);
+    
+    // Submit job 2. It should get the last 2 slots.
+    JobInProgress job2 = submitJob(JobStatus.RUNNING, 10, 10, "pool2");
+    advanceTime(100);
+    checkAssignment("tt4", "attempt_test_0002_m_000000_0 on tt4");
+    checkAssignment("tt4", "attempt_test_0002_r_000000_0 on tt4");
+    checkAssignment("tt4", "attempt_test_0002_m_000001_0 on tt4");
+    checkAssignment("tt4", "attempt_test_0002_r_000001_0 on tt4");
+    
+    // Submit job 3.
+    JobInProgress job3 = submitJob(JobStatus.RUNNING, 10, 10, "pool3");
+    
+    // Check that after 59 seconds, neither pool can preempt
+    advanceTime(59000);
+    assertEquals(0, scheduler.tasksToPreempt(pool2.getMapSchedulable(),
+        clock.getTime()));
+    assertEquals(0, scheduler.tasksToPreempt(pool2.getReduceSchedulable(),
+        clock.getTime()));
+    assertEquals(0, scheduler.tasksToPreempt(pool3.getMapSchedulable(),
+        clock.getTime()));
+    assertEquals(0, scheduler.tasksToPreempt(pool3.getReduceSchedulable(),
+        clock.getTime()));
+    
+    // Wait 2 more seconds, so that job 3 has now been in the system for 61s.
+    // Now pool 3 should be able to preempt 2 tasks (its share of 2.7 rounded
+    // down to its floor), but pool 2 shouldn't.
+    advanceTime(2000);
+    assertEquals(0, scheduler.tasksToPreempt(pool2.getMapSchedulable(),
+        clock.getTime()));
+    assertEquals(0, scheduler.tasksToPreempt(pool2.getReduceSchedulable(),
+        clock.getTime()));
+    assertEquals(2, scheduler.tasksToPreempt(pool3.getMapSchedulable(),
+        clock.getTime()));
+    assertEquals(2, scheduler.tasksToPreempt(pool3.getReduceSchedulable(),
+        clock.getTime()));
+    
+    // Test that the tasks actually get preempted and we can assign new ones
+    scheduler.preemptTasksIfNecessary();
+    scheduler.update();
+    assertEquals(4, job1.runningMaps());
+    assertEquals(4, job1.runningReduces());
+    checkAssignment("tt3", "attempt_test_0003_m_000000_0 on tt3");
+    checkAssignment("tt3", "attempt_test_0003_r_000000_0 on tt3");
+    checkAssignment("tt3", "attempt_test_0003_m_000001_0 on tt3");
+    checkAssignment("tt3", "attempt_test_0003_r_000001_0 on tt3");
+    assertNull(scheduler.assignTasks(tracker("tt1")));
+    assertNull(scheduler.assignTasks(tracker("tt2")));
+    assertNull(scheduler.assignTasks(tracker("tt3")));
+    assertNull(scheduler.assignTasks(tracker("tt4")));
+  }
+  
+  /**
+   * This test runs on a 3-node (6-slot) cluster to allow 3 pools with fair
+   * shares equal 2 slots to coexist (which makes the half-fair-share 
+   * of each pool equal to 1 so that fair share preemption can kick in). 
+   * 
+   * The test first starts job 1, which takes 3 map slots and 0 reduce slots,
+   * in pool 1.  We then submit job 2 in pool 2, which takes 3 map slots and zero
+   * reduce slots. Finally, we submit a third job, job 3 in pool3, which gets no slots. 
+   * At this point the fair share of each pool will be 6/3 = 2 slots. 
+   * Pool 1 and 2 will be above their fair share and pool 3 will be below half fair share. 
+   * Therefore pool 3 should preempt tasks from both pool 1 & 2 (after a timeout) but 
+   * pools 1 and 2 shouldn't. 
+   */
+  public void testFairSharePreemptionFromMultiplePools() throws Exception {
+	// Create a bigger cluster than normal (3 tasktrackers instead of 2)
+	setUpCluster(1, 3, false);
+	// Enable preemption in scheduler
+	scheduler.preemptionEnabled = true;
+	// Set up pools file with a fair share preemtion timeout of 1 minute
+	PrintWriter out = new PrintWriter(new FileWriter(ALLOC_FILE));
+	out.println("<?xml version=\"1.0\"?>");
+	out.println("<allocations>");
+	out.println("<fairSharePreemptionTimeout>60</fairSharePreemptionTimeout>");
+	out.println("</allocations>");
+	out.close();
+	scheduler.getPoolManager().reloadAllocs();
+	 
+	// Grab pools (they'll be created even though they're not in the alloc file)
+	Pool pool1 = scheduler.getPoolManager().getPool("pool1");
+	Pool pool2 = scheduler.getPoolManager().getPool("pool2");
+	Pool pool3 = scheduler.getPoolManager().getPool("pool3");
+
+	// Submit job 1. We advance time by 100 between each task tracker
+	// assignment stage to ensure that the tasks from job1 on tt3 are the ones
+	// that are deterministically preempted first (being the latest launched
+	// tasks in an over-allocated job).
+	JobInProgress job1 = submitJob(JobStatus.RUNNING, 12, 0, "pool1");
+	advanceTime(100);
+	checkAssignment("tt1", "attempt_test_0001_m_000000_0 on tt1");
+	checkAssignment("tt1", "attempt_test_0001_m_000001_0 on tt1");
+	advanceTime(100);
+	checkAssignment("tt2", "attempt_test_0001_m_000002_0 on tt2");
+	advanceTime(100);
+	    
+	// Submit job 2. It should get the last 3 slots.
+	JobInProgress job2 = submitJob(JobStatus.RUNNING, 10, 0, "pool2");
+	advanceTime(100);
+	checkAssignment("tt2", "attempt_test_0002_m_000000_0 on tt2");
+	checkAssignment("tt3", "attempt_test_0002_m_000001_0 on tt3");
+	advanceTime(100);
+	checkAssignment("tt3", "attempt_test_0002_m_000002_0 on tt3");
+
+	advanceTime(100);
+	    
+	// Submit job 3.
+	JobInProgress job3 = submitJob(JobStatus.RUNNING, 10, 0, "pool3");
+	    
+	// Check that after 59 seconds, neither pool can preempt
+	advanceTime(59000);
+	assertEquals(0, scheduler.tasksToPreempt(pool2.getMapSchedulable(),
+			clock.getTime()));
+	assertEquals(0, scheduler.tasksToPreempt(pool2.getReduceSchedulable(),
+	        clock.getTime()));
+	assertEquals(0, scheduler.tasksToPreempt(pool3.getMapSchedulable(),
+	        clock.getTime()));
+	assertEquals(0, scheduler.tasksToPreempt(pool3.getReduceSchedulable(),
+	        clock.getTime()));
+	    
+	// Wait 2 more seconds, so that job 3 has now been in the system for 61s.
+	// Now pool 3 should be able to preempt 2 tasks (its share of 2 rounded
+	// down to its floor), but pool 1 & 2 shouldn't.
+	advanceTime(2000);
+	assertEquals(0, scheduler.tasksToPreempt(pool2.getMapSchedulable(),
+	        clock.getTime()));
+	assertEquals(0, scheduler.tasksToPreempt(pool2.getReduceSchedulable(),
+	        clock.getTime()));
+	assertEquals(2, scheduler.tasksToPreempt(pool3.getMapSchedulable(),
+	        clock.getTime()));
+	assertEquals(0, scheduler.tasksToPreempt(pool3.getReduceSchedulable(),
+	        clock.getTime()));
+	    
+	// Test that the tasks actually get preempted and we can assign new ones.
+	// This should preempt one task each from pool1 and pool2
+	scheduler.preemptTasksIfNecessary();
+	scheduler.update();
+	assertEquals(2, job2.runningMaps());  
+	assertEquals(2, job1.runningMaps());  
+	checkAssignment("tt2", "attempt_test_0003_m_000000_0 on tt2");
+	checkAssignment("tt3", "attempt_test_0003_m_000001_0 on tt3");
+	assertNull(scheduler.assignTasks(tracker("tt1")));
+	assertNull(scheduler.assignTasks(tracker("tt2")));
+	assertNull(scheduler.assignTasks(tracker("tt3")));
+  }
+  
+  
+  /**
+   * This test submits a job that takes all 4 slots, and then a second job in
+   * a pool that has both a min share of 2 slots with a 60s timeout and a
+   * fair share timeout of 60s. After 60 seconds, this pool will be starved
+   * of both min share (2 slots of each type) and fair share (2 slots of each
+   * type), and we test that it does not kill more than 2 tasks of each type
+   * in total.
+   */
+  public void testMinAndFairSharePreemption() throws Exception {
+    // Enable preemption in scheduler
+    scheduler.preemptionEnabled = true;
+    // Set up pools file
+    PrintWriter out = new PrintWriter(new FileWriter(ALLOC_FILE));
+    out.println("<?xml version=\"1.0\"?>");
+    out.println("<allocations>");
+    // Give pool A a min share of 2 maps and 1 reduce, and a preemption
+    // timeout of 1 minute
+    out.println("<pool name=\"poolA\">");
+    out.println("<minMaps>2</minMaps>");
+    out.println("<minReduces>2</minReduces>");
+    out.println("<minSharePreemptionTimeout>60</minSharePreemptionTimeout>");
+    out.println("</pool>");
+    out.println("<fairSharePreemptionTimeout>60</fairSharePreemptionTimeout>");
+    out.println("</allocations>");
+    out.close();
+    scheduler.getPoolManager().reloadAllocs();
+    Pool poolA = scheduler.getPoolManager().getPool("poolA");
+
+    // Submit job 1 and assign all slots to it. Sleep a bit before assigning
+    // tasks on tt1 and tt2 to ensure that the ones on tt2 get preempted first.
+    JobInProgress job1 = submitJob(JobStatus.RUNNING, 10, 10);
+    checkAssignment("tt1", "attempt_test_0001_m_000000_0 on tt1");
+    checkAssignment("tt1", "attempt_test_0001_r_000000_0 on tt1");
+    checkAssignment("tt1", "attempt_test_0001_m_000001_0 on tt1");
+    checkAssignment("tt1", "attempt_test_0001_r_000001_0 on tt1");
+    advanceTime(100);
+    checkAssignment("tt2", "attempt_test_0001_m_000002_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0001_r_000002_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0001_m_000003_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0001_r_000003_0 on tt2");
+    
+    // Ten seconds later, submit job 2.
+    advanceTime(10000);
+    JobInProgress job2 = submitJob(JobStatus.RUNNING, 10, 10, "poolA");
+    
+    // Ten seconds later, check that job 2 is not able to preempt tasks.
+    advanceTime(10000);
+    assertEquals(0, scheduler.tasksToPreempt(poolA.getMapSchedulable(),
+        clock.getTime()));
+    assertEquals(0, scheduler.tasksToPreempt(poolA.getReduceSchedulable(),
+        clock.getTime()));
+    
+    // Advance time by 49 more seconds, putting us at 59s after the
+    // submission of job 2. It should still not be able to preempt.
+    advanceTime(49000);
+    assertEquals(0, scheduler.tasksToPreempt(poolA.getMapSchedulable(),
+        clock.getTime()));
+    assertEquals(0, scheduler.tasksToPreempt(poolA.getReduceSchedulable(),
+        clock.getTime()));
+    
+    // Advance time by 2 seconds, putting us at 61s after the submission
+    // of job 2. It should now be able to preempt 2 maps and 1 reduce.
+    advanceTime(2000);
+    assertEquals(2, scheduler.tasksToPreempt(poolA.getMapSchedulable(),
+        clock.getTime()));
+    assertEquals(2, scheduler.tasksToPreempt(poolA.getReduceSchedulable(),
+        clock.getTime()));
+
+    // Test that the tasks actually get preempted and we can assign new ones
+    scheduler.preemptTasksIfNecessary();
+    scheduler.update();
+    assertEquals(2, job1.runningMaps());
+    assertEquals(2, job1.runningReduces());
+    checkAssignment("tt2", "attempt_test_0002_m_000000_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0002_r_000000_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0002_m_000001_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0002_r_000001_0 on tt2");
+    assertNull(scheduler.assignTasks(tracker("tt1")));
+    assertNull(scheduler.assignTasks(tracker("tt2")));
+  }
+  
+  /**
+   * This is a copy of testMinAndFairSharePreemption that turns preemption
+   * off and verifies that no tasks get killed.
+   */
+  public void testNoPreemptionIfDisabled() throws Exception {
+    // Set up pools file
+    PrintWriter out = new PrintWriter(new FileWriter(ALLOC_FILE));
+    out.println("<?xml version=\"1.0\"?>");
+    out.println("<allocations>");
+    // Give pool A a min share of 2 maps and 1 reduce, and a preemption
+    // timeout of 1 minute
+    out.println("<pool name=\"poolA\">");
+    out.println("<minMaps>2</minMaps>");
+    out.println("<minReduces>2</minReduces>");
+    out.println("<minSharePreemptionTimeout>60</minSharePreemptionTimeout>");
+    out.println("</pool>");
+    out.println("<fairSharePreemptionTimeout>60</fairSharePreemptionTimeout>");
+    out.println("</allocations>");
+    out.close();
+    scheduler.getPoolManager().reloadAllocs();
+
+    // Submit job 1 and assign all slots to it. Sleep a bit before assigning
+    // tasks on tt1 and tt2 to ensure that the ones on tt2 get preempted first.
+    JobInProgress job1 = submitJob(JobStatus.RUNNING, 10, 10);
+    checkAssignment("tt1", "attempt_test_0001_m_000000_0 on tt1");
+    checkAssignment("tt1", "attempt_test_0001_r_000000_0 on tt1");
+    checkAssignment("tt1", "attempt_test_0001_m_000001_0 on tt1");
+    checkAssignment("tt1", "attempt_test_0001_r_000001_0 on tt1");
+    advanceTime(100);
+    checkAssignment("tt2", "attempt_test_0001_m_000002_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0001_r_000002_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0001_m_000003_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0001_r_000003_0 on tt2");
+    
+    // Ten seconds later, submit job 2.
+    advanceTime(10000);
+    JobInProgress job2 = submitJob(JobStatus.RUNNING, 10, 10, "poolA");
+    
+    // Advance time by 61s, putting us past the preemption timeout,
+    // and check that no tasks get preempted.
+    advanceTime(61000);
+    scheduler.preemptTasksIfNecessary();
+    scheduler.update();
+    assertEquals(4, job1.runningMaps());
+    assertEquals(4, job1.runningReduces());
+    assertNull(scheduler.assignTasks(tracker("tt1")));
+    assertNull(scheduler.assignTasks(tracker("tt2")));
+  }
+
+  /**
+   * This is a copy of testMinAndFairSharePreemption that turns preemption
+   * on but also turns on mapred.fairscheduler.preemption.only.log (the
+   * "dry run" parameter for testing out preemption) and verifies that no
+   * tasks get killed.
+   */
+  public void testNoPreemptionIfOnlyLogging() throws Exception {
+    // Turn on preemption, but for logging only
+    scheduler.preemptionEnabled = true;
+    scheduler.onlyLogPreemption = true;
+    // Set up pools file
+    PrintWriter out = new PrintWriter(new FileWriter(ALLOC_FILE));
+    out.println("<?xml version=\"1.0\"?>");
+    out.println("<allocations>");
+    // Give pool A a min share of 2 maps and 1 reduce, and a preemption
+    // timeout of 1 minute
+    out.println("<pool name=\"poolA\">");
+    out.println("<minMaps>2</minMaps>");
+    out.println("<minReduces>2</minReduces>");
+    out.println("<minSharePreemptionTimeout>60</minSharePreemptionTimeout>");
+    out.println("</pool>");
+    out.println("<fairSharePreemptionTimeout>60</fairSharePreemptionTimeout>");
+    out.println("</allocations>");
+    out.close();
+    scheduler.getPoolManager().reloadAllocs();
+
+    // Submit job 1 and assign all slots to it. Sleep a bit before assigning
+    // tasks on tt1 and tt2 to ensure that the ones on tt2 get preempted first.
+    JobInProgress job1 = submitJob(JobStatus.RUNNING, 10, 10);
+    checkAssignment("tt1", "attempt_test_0001_m_000000_0 on tt1");
+    checkAssignment("tt1", "attempt_test_0001_r_000000_0 on tt1");
+    checkAssignment("tt1", "attempt_test_0001_m_000001_0 on tt1");
+    checkAssignment("tt1", "attempt_test_0001_r_000001_0 on tt1");
+    advanceTime(100);
+    checkAssignment("tt2", "attempt_test_0001_m_000002_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0001_r_000002_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0001_m_000003_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0001_r_000003_0 on tt2");
+    
+    // Ten seconds later, submit job 2.
+    advanceTime(10000);
+    JobInProgress job2 = submitJob(JobStatus.RUNNING, 10, 10, "poolA");
+    
+    // Advance time by 61s, putting us past the preemption timeout,
+    // and check that no tasks get preempted.
+    advanceTime(61000);
+    scheduler.preemptTasksIfNecessary();
+    scheduler.update();
+    assertEquals(4, job1.runningMaps());
+    assertEquals(4, job1.runningReduces());
+    assertNull(scheduler.assignTasks(tracker("tt1")));
+    assertNull(scheduler.assignTasks(tracker("tt2")));
+  }
+
+  /**
+   * This test exercises delay scheduling at the node level. We submit a job
+   * with data on rack1.node2 and check that it doesn't get assigned on earlier
+   * nodes. A second job with no locality info should get assigned instead.
+   * 
+   * TaskTracker names in this test map to nodes as follows:
+   * - tt1 = rack1.node1
+   * - tt2 = rack1.node2
+   * - tt3 = rack2.node1
+   * - tt4 = rack2.node2
+   */
+  public void testDelaySchedulingAtNodeLevel() throws IOException {
+    setUpCluster(2, 2, true);
+    scheduler.assignMultiple = true;
+    
+    JobInProgress job1 = submitJob(JobStatus.RUNNING, 1, 0, "pool1",
+        new String[][] {
+          {"rack2.node2"}
+        }, true);
+    JobInfo info1 = scheduler.infos.get(job1);
+    
+    // Advance time before submitting another job j2, to make j1 be ahead
+    // of j2 in the queue deterministically.
+    advanceTime(100);
+    JobInProgress job2 = submitJob(JobStatus.RUNNING, 10, 0);
+    
+    // Assign tasks on nodes 1-3 and check that j2 gets them
+    checkAssignment("tt1", "attempt_test_0002_m_000000_0 on tt1", 
+                           "attempt_test_0002_m_000001_0 on tt1");
+    checkAssignment("tt2", "attempt_test_0002_m_000002_0 on tt2",
+                           "attempt_test_0002_m_000003_0 on tt2");
+    checkAssignment("tt3", "attempt_test_0002_m_000004_0 on tt3",
+                           "attempt_test_0002_m_000005_0 on tt3");
+    
+    // Assign a task on node 4 now and check that j1 gets it. The other slot
+    // on the node should be given to j2 because j1 will be out of tasks.
+    checkAssignment("tt4", "attempt_test_0001_m_000000_0 on tt4",
+                           "attempt_test_0002_m_000006_0 on tt4");
+    
+    // Check that delay scheduling info is properly set
+    assertEquals(info1.lastMapLocalityLevel, LocalityLevel.NODE);
+    assertEquals(info1.timeWaitedForLocalMap, 0);
+    assertEquals(info1.skippedAtLastHeartbeat, false);
+  }
+  
+  /**
+   * This test submits a job and causes it to exceed its node-level delay,
+   * and thus to go on to launch a rack-local task. We submit one job with data
+   * on rack2.node4 and check that it does not get assigned on any of the other
+   * nodes until 10 seconds (the delay configured in setUpCluster) pass.
+   * Finally, after some delay, we let the job assign local tasks and check
+   * that it has returned to waiting for node locality.
+   * 
+   * TaskTracker names in this test map to nodes as follows:
+   * - tt1 = rack1.node1
+   * - tt2 = rack1.node2
+   * - tt3 = rack2.node1
+   * - tt4 = rack2.node2
+   */
+  public void testDelaySchedulingAtRackLevel() throws IOException {
+    setUpCluster(2, 2, true);
+    scheduler.assignMultiple = true;
+    
+    JobInProgress job1 = submitJob(JobStatus.RUNNING, 4, 0, "pool1",
+        new String[][] {
+          {"rack2.node2"}, {"rack2.node2"}, {"rack2.node2"}, {"rack2.node2"}
+        }, true);
+    JobInfo info1 = scheduler.infos.get(job1);
+    
+    // Advance time before submitting another job j2, to make j1 be ahead
+    // of j2 in the queue deterministically.
+    advanceTime(100);
+    JobInProgress job2 = submitJob(JobStatus.RUNNING, 20, 0);
+    
+    // Assign tasks on nodes 1-3 and check that j2 gets them
+    checkAssignment("tt1", "attempt_test_0002_m_000000_0 on tt1", 
+                           "attempt_test_0002_m_000001_0 on tt1");
+    checkAssignment("tt2", "attempt_test_0002_m_000002_0 on tt2",
+                           "attempt_test_0002_m_000003_0 on tt2");
+    checkAssignment("tt3", "attempt_test_0002_m_000004_0 on tt3",
+                           "attempt_test_0002_m_000005_0 on tt3");
+    
+    // Advance time by 6 seconds to put us past the 5-second node locality delay
+    advanceTime(6000);
+    
+    // Finish some tasks on each node
+    taskTrackerManager.finishTask("tt1", "attempt_test_0002_m_000000_0");
+    taskTrackerManager.finishTask("tt2", "attempt_test_0002_m_000002_0");
+    taskTrackerManager.finishTask("tt3", "attempt_test_0002_m_000004_0");
+    advanceTime(100);
+    
+    // Check that job 1 is only assigned on node 3 (which is rack-local)
+    checkAssignment("tt1", "attempt_test_0002_m_000006_0 on tt1");
+    checkAssignment("tt2", "attempt_test_0002_m_000007_0 on tt2");
+    checkAssignment("tt3", "attempt_test_0001_m_000000_0 on tt3");
+    
+    // Check that delay scheduling info is properly set
+    assertEquals(info1.lastMapLocalityLevel, LocalityLevel.RACK);
+    assertEquals(info1.timeWaitedForLocalMap, 0);
+    assertEquals(info1.skippedAtLastHeartbeat, false);
+    
+    // Also give job 1 some tasks on node 4. Its lastMapLocalityLevel
+    // should go back to 0 after it gets assigned these.
+    checkAssignment("tt4", "attempt_test_0001_m_000001_0 on tt4",
+                           "attempt_test_0001_m_000002_0 on tt4");
+    
+    // Check that delay scheduling info is properly set
+    assertEquals(info1.lastMapLocalityLevel, LocalityLevel.NODE);
+    assertEquals(info1.timeWaitedForLocalMap, 0);
+    assertEquals(info1.skippedAtLastHeartbeat, false);
+    
+    // Check that job 1 no longer assigns tasks in the same rack now
+    // that it has obtained a node-local task
+    taskTrackerManager.finishTask("tt1", "attempt_test_0002_m_000001_0");
+    taskTrackerManager.finishTask("tt2", "attempt_test_0002_m_000003_0");
+    taskTrackerManager.finishTask("tt3", "attempt_test_0002_m_000005_0");
+    advanceTime(100);
+    checkAssignment("tt1", "attempt_test_0002_m_000008_0 on tt1");
+    checkAssignment("tt2", "attempt_test_0002_m_000009_0 on tt2");
+    checkAssignment("tt3", "attempt_test_0002_m_000010_0 on tt3");
+    advanceTime(100);
+    
+    // However, job 1 should still be able to launch tasks on node 4
+    taskTrackerManager.finishTask("tt4", "attempt_test_0001_m_000001_0");
+    advanceTime(100);
+    checkAssignment("tt4", "attempt_test_0001_m_000003_0 on tt4");
+  }
+  
+  /**
+   * This test submits a job and causes it to exceed its node-level delay,
+   * then its rack-level delay. It should then launch tasks off-rack.
+   * However, once the job gets a rack-local slot it should stay in-rack,
+   * and once it gets a node-local slot it should stay in-node.
+   * For simplicity, we don't submit a second job in this test.
+   * 
+   * TaskTracker names in this test map to nodes as follows:
+   * - tt1 = rack1.node1
+   * - tt2 = rack1.node2
+   * - tt3 = rack2.node1
+   * - tt4 = rack2.node2
+   */
+  public void testDelaySchedulingOffRack() throws IOException {
+    setUpCluster(2, 2, true);
+    scheduler.assignMultiple = true;
+    
+    JobInProgress job1 = submitJob(JobStatus.RUNNING, 8, 0, "pool1",
+        new String[][] {
+          {"rack2.node2"}, {"rack2.node2"}, {"rack2.node2"}, {"rack2.node2"},
+          {"rack2.node2"}, {"rack2.node2"}, {"rack2.node2"}, {"rack2.node2"},
+        }, true);
+    JobInfo info1 = scheduler.infos.get(job1);
+    advanceTime(100);
+    
+    // Check that nothing is assigned on trackers 1-3
+    assertNull(scheduler.assignTasks(tracker("tt1")));
+    assertNull(scheduler.assignTasks(tracker("tt2")));
+    assertNull(scheduler.assignTasks(tracker("tt3")));
+    
+    // Advance time by 6 seconds to put us past the 5-sec node locality delay
+    advanceTime(6000);
+
+    // Check that nothing is assigned on trackers 1-2; the job would assign
+    // a task on tracker 3 (rack1.node2) so we skip that one 
+    assertNull(scheduler.assignTasks(tracker("tt1")));
+    assertNull(scheduler.assignTasks(tracker("tt2")));
+    
+    // Repeat to see that receiving multiple heartbeats works
+    advanceTime(100);
+    assertNull(scheduler.assignTasks(tracker("tt1")));
+    assertNull(scheduler.assignTasks(tracker("tt2")));
+    advanceTime(100);
+    assertNull(scheduler.assignTasks(tracker("tt1")));
+    assertNull(scheduler.assignTasks(tracker("tt2")));
+
+    // Check that delay scheduling info is properly set
+    assertEquals(info1.lastMapLocalityLevel, LocalityLevel.NODE);
+    assertEquals(info1.timeWaitedForLocalMap, 6200);
+    assertEquals(info1.skippedAtLastHeartbeat, true);
+    
+    // Advance time by 11 seconds to put us past the 10-sec rack locality delay
+    advanceTime(11000);
+    
+    // Now the job should be able to assign tasks on tt1 and tt2
+    checkAssignment("tt1", "attempt_test_0001_m_000000_0 on tt1",
+                           "attempt_test_0001_m_000001_0 on tt1");
+    checkAssignment("tt2", "attempt_test_0001_m_000002_0 on tt2",
+                           "attempt_test_0001_m_000003_0 on tt2");
+
+    // Check that delay scheduling info is properly set
+    assertEquals(info1.lastMapLocalityLevel, LocalityLevel.ANY);
+    assertEquals(info1.timeWaitedForLocalMap, 0);
+    assertEquals(info1.skippedAtLastHeartbeat, false);
+    
+    // Now assign a task on tt3. This should make the job stop assigning
+    // on tt1 and tt2 (checked after we finish some tasks there)
+    checkAssignment("tt3", "attempt_test_0001_m_000004_0 on tt3",
+                           "attempt_test_0001_m_000005_0 on tt3");
+
+    // Check that delay scheduling info is properly set
+    assertEquals(info1.lastMapLocalityLevel, LocalityLevel.RACK);
+    assertEquals(info1.timeWaitedForLocalMap, 0);
+    assertEquals(info1.skippedAtLastHeartbeat, false);
+    
+    // Check that j1 no longer assigns tasks on rack 1 now
+    taskTrackerManager.finishTask("tt1", "attempt_test_0001_m_000001_0");
+    taskTrackerManager.finishTask("tt2", "attempt_test_0001_m_000003_0");
+    advanceTime(100);
+    assertNull(scheduler.assignTasks(tracker("tt1")));
+    assertNull(scheduler.assignTasks(tracker("tt2")));
+    
+    // However, tasks on rack 2 should still be assigned
+    taskTrackerManager.finishTask("tt3", "attempt_test_0001_m_000004_0");
+    advanceTime(100);
+    checkAssignment("tt3", "attempt_test_0001_m_000006_0 on tt3");
+    
+    // Now assign a task on node 4
+    checkAssignment("tt4", "attempt_test_0001_m_000007_0 on tt4");
+
+    // Check that delay scheduling info is set so we are looking for node-local
+    // tasks at this point
+    assertEquals(info1.lastMapLocalityLevel, LocalityLevel.NODE);
+    assertEquals(info1.timeWaitedForLocalMap, 0);
+    assertEquals(info1.skippedAtLastHeartbeat, false);
+  }
+  
+  /**
+   * This test submits two jobs with 4 maps and 3 reduces in total to a
+   * 4-node cluster. Although the cluster has 2 map slots and 2 reduce
+   * slots per node, it should only launch one map and one reduce on each
+   * node to balance the load. We check that this happens even if
+   * assignMultiple is set to true so the scheduler has the opportunity
+   * to launch multiple tasks per heartbeat.
+   */
+  public void testAssignMultipleWithUnderloadedCluster() throws IOException {
+    setUpCluster(1, 4, true);
+    JobInProgress job1 = submitJob(JobStatus.RUNNING, 2, 2);
+    
+    // Advance to make j1 be scheduled before j2 deterministically.
+    advanceTime(100);
+    JobInProgress job2 = submitJob(JobStatus.RUNNING, 2, 1);
+    
+    // Assign tasks and check that at most one map and one reduce slot is used
+    // on each node, and that no tasks are assigned on subsequent heartbeats
+    checkAssignment("tt1", "attempt_test_0001_m_000000_0 on tt1",
+                           "attempt_test_0001_r_000000_0 on tt1");
+    assertNull(scheduler.assignTasks(tracker("tt1")));
+    checkAssignment("tt2", "attempt_test_0002_m_000000_0 on tt2",
+                           "attempt_test_0002_r_000000_0 on tt2");
+    assertNull(scheduler.assignTasks(tracker("tt2")));
+    checkAssignment("tt3", "attempt_test_0001_m_000001_0 on tt3",
+                           "attempt_test_0001_r_000001_0 on tt3");
+    assertNull(scheduler.assignTasks(tracker("tt3")));
+    checkAssignment("tt4", "attempt_test_0002_m_000001_0 on tt4");
+    assertNull(scheduler.assignTasks(tracker("tt4")));
+  }
+  
+  /**
+   * This test submits four jobs in the default pool, which is set to FIFO mode:
+   * - job1, with 1 map and 1 reduce
+   * - job2, with 3 maps and 3 reduces
+   * - job3, with 1 map, 1 reduce, and priority set to HIGH
+   * - job4, with 3 maps and 3 reduces
+   * 
+   * We check that the scheduler assigns tasks first to job3 (because it is
+   * high priority), then to job1, then to job2.
+   */
+  public void testFifoPool() throws Exception {
+    // Set up pools file
+    PrintWriter out = new PrintWriter(new FileWriter(ALLOC_FILE));
+    out.println("<?xml version=\"1.0\"?>");
+    out.println("<allocations>");
+    out.println("<pool name=\"default\">");
+    out.println("<schedulingMode>fifo</schedulingMode>");
+    out.println("</pool>");
+    out.println("</allocations>");
+    out.close();
+    scheduler.getPoolManager().reloadAllocs();
+    
+    // Submit jobs, advancing time in-between to make sure that they are
+    // all submitted at distinct times.
+    JobInProgress job1 = submitJob(JobStatus.RUNNING, 1, 1);
+    advanceTime(10);
+    JobInProgress job2 = submitJob(JobStatus.RUNNING, 3, 3);
+    advanceTime(10);
+    JobInProgress job3 = submitJob(JobStatus.RUNNING, 1, 1);
+    job3.setPriority(JobPriority.HIGH);
+    advanceTime(10);
+    JobInProgress job4 = submitJob(JobStatus.RUNNING, 3, 3);
+    
+    // Assign tasks and check that they're given first to job3 (because it is
+    // high priority), then to job1, then to job2.
+    checkAssignment("tt1", "attempt_test_0003_m_000000_0 on tt1");
+    checkAssignment("tt1", "attempt_test_0003_r_000000_0 on tt1");
+    checkAssignment("tt1", "attempt_test_0001_m_000000_0 on tt1");
+    checkAssignment("tt1", "attempt_test_0001_r_000000_0 on tt1");
+    checkAssignment("tt2", "attempt_test_0002_m_000000_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0002_r_000000_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0002_m_000001_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0002_r_000001_0 on tt2");
+  }
+  
+  /**
+   * This test submits 2 large jobs each to 2 pools, which are both set to FIFO
+   * mode through the global defaultPoolSchedulingMode setting. We check that
+   * the scheduler assigns tasks only to the first job within each pool, but
+   * alternates between the pools to give each pool a fair share.
+   */
+  public void testMultipleFifoPools() throws Exception {
+    // Set up pools file
+    PrintWriter out = new PrintWriter(new FileWriter(ALLOC_FILE));
+    out.println("<?xml version=\"1.0\"?>");
+    out.println("<allocations>");
+    out.println("<defaultPoolSchedulingMode>fifo</defaultPoolSchedulingMode>");
+    out.println("</allocations>");
+    out.close();
+    scheduler.getPoolManager().reloadAllocs();
+    
+    // Submit jobs, advancing time in-between to make sure that they are
+    // all submitted at distinct times.
+    JobInProgress job1 = submitJob(JobStatus.RUNNING, 10, 10, "poolA");
+    advanceTime(10);
+    JobInProgress job2 = submitJob(JobStatus.RUNNING, 10, 10, "poolA");
+    advanceTime(10);
+    JobInProgress job3 = submitJob(JobStatus.RUNNING, 10, 10, "poolB");
+    advanceTime(10);
+    JobInProgress job4 = submitJob(JobStatus.RUNNING, 10, 10, "poolB");
+    
+    // Assign tasks and check that they alternate between jobs 1 and 3, the
+    // head-of-line jobs in their respective pools.
+    checkAssignment("tt1", "attempt_test_0001_m_000000_0 on tt1");
+    checkAssignment("tt1", "attempt_test_0001_r_000000_0 on tt1");
+    checkAssignment("tt1", "attempt_test_0003_m_000000_0 on tt1");
+    checkAssignment("tt1", "attempt_test_0003_r_000000_0 on tt1");
+    checkAssignment("tt2", "attempt_test_0001_m_000001_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0001_r_000001_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0003_m_000001_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0003_r_000001_0 on tt2");
+  }
+  
+  /**
+   * This test submits 2 large jobs each to 2 pools, one of which is set to FIFO
+   * mode through the global defaultPoolSchedulingMode setting, and one of which
+   * is set to fair mode. We check that the scheduler assigns tasks only to the
+   * first job in the FIFO pool but to both jobs in the fair sharing pool.
+   */
+  public void testFifoAndFairPools() throws Exception {
+    // Set up pools file
+    PrintWriter out = new PrintWriter(new FileWriter(ALLOC_FILE));
+    out.println("<?xml version=\"1.0\"?>");
+    out.println("<allocations>");
+    out.println("<defaultPoolSchedulingMode>fifo</defaultPoolSchedulingMode>");
+    out.println("<pool name=\"poolB\">");
+    out.println("<schedulingMode>fair</schedulingMode>");
+    out.println("</pool>");
+    out.println("</allocations>");
+    out.close();
+    scheduler.getPoolManager().reloadAllocs();
+    
+    // Submit jobs, advancing time in-between to make sure that they are
+    // all submitted at distinct times.
+    JobInProgress job1 = submitJob(JobStatus.RUNNING, 10, 10, "poolA");
+    advanceTime(10);
+    JobInProgress job2 = submitJob(JobStatus.RUNNING, 10, 10, "poolA");
+    advanceTime(10);
+    JobInProgress job3 = submitJob(JobStatus.RUNNING, 10, 10, "poolB");
+    advanceTime(10);
+    JobInProgress job4 = submitJob(JobStatus.RUNNING, 10, 10, "poolB");
+    
+    // Assign tasks and check that only job 1 gets tasks in pool A, but
+    // jobs 3 and 4 both get tasks in pool B.
+    checkAssignment("tt1", "attempt_test_0001_m_000000_0 on tt1");
+    checkAssignment("tt1", "attempt_test_0001_r_000000_0 on tt1");
+    checkAssignment("tt1", "attempt_test_0003_m_000000_0 on tt1");
+    checkAssignment("tt1", "attempt_test_0003_r_000000_0 on tt1");
+    checkAssignment("tt2", "attempt_test_0001_m_000001_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0001_r_000001_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0004_m_000000_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0004_r_000000_0 on tt2");
+  }
+
+  /**
+   * This test uses the mapred.fairscheduler.pool property to assign jobs to pools.
+   */
+  public void testPoolAssignment() throws Exception {
+    // Set up pools file
+    PrintWriter out = new PrintWriter(new FileWriter(ALLOC_FILE));
+    out.println("<?xml version=\"1.0\"?>");
+    out.println("<allocations>");
+    out.println("<pool name=\"default\">");
+    out.println("<schedulingMode>fair</schedulingMode>");
+    out.println("</pool>");
+    out.println("<pool name=\"poolA\">");
+    out.println("<schedulingMode>fair</schedulingMode>");
+    out.println("</pool>");
+    out.println("</allocations>");
+    out.close();
+    scheduler.getPoolManager().reloadAllocs();
+    Pool defaultPool = scheduler.getPoolManager().getPool("default");
+    Pool poolA = scheduler.getPoolManager().getPool("poolA");
+ 
+    // Submit a job to the default pool.  All specifications take default values.
+    JobInProgress job1 = submitJob(JobStatus.RUNNING, 1, 3);
+
+    assertEquals(1,    defaultPool.getMapSchedulable().getDemand());
+    assertEquals(3,    defaultPool.getReduceSchedulable().getDemand());
+    assertEquals(0,    poolA.getMapSchedulable().getDemand());
+    assertEquals(0,    poolA.getReduceSchedulable().getDemand());
+
+    // Submit a job to the default pool and move it to poolA using setPool.
+    JobInProgress job2 = submitJob(JobStatus.RUNNING, 5, 7);
+
+    assertEquals(6,    defaultPool.getMapSchedulable().getDemand());
+    assertEquals(10,   defaultPool.getReduceSchedulable().getDemand());
+    assertEquals(0,    poolA.getMapSchedulable().getDemand());
+    assertEquals(0,    poolA.getReduceSchedulable().getDemand());
+
+    scheduler.getPoolManager().setPool(job2, "poolA");
+    assertEquals("poolA", scheduler.getPoolManager().getPoolName(job2));
+
+    defaultPool.getMapSchedulable().updateDemand();
+    defaultPool.getReduceSchedulable().updateDemand();
+    poolA.getMapSchedulable().updateDemand();
+    poolA.getReduceSchedulable().updateDemand();
+
+    assertEquals(1,    defaultPool.getMapSchedulable().getDemand());
+    assertEquals(3,    defaultPool.getReduceSchedulable().getDemand());
+    assertEquals(5,    poolA.getMapSchedulable().getDemand());
+    assertEquals(7,    poolA.getReduceSchedulable().getDemand());
+
+    // Submit a job to poolA by specifying mapred.fairscheduler.pool
+    JobConf jobConf = new JobConf(conf);
+    jobConf.setNumMapTasks(11);
+    jobConf.setNumReduceTasks(13);
+    jobConf.set(POOL_PROPERTY, "nonsense"); // test that this is overridden
+    jobConf.set(EXPLICIT_POOL_PROPERTY, "poolA");
+    JobInProgress job3 = new FakeJobInProgress(jobConf, taskTrackerManager,
+        null, jobTracker);
+    job3.initTasks();
+    job3.getStatus().setRunState(JobStatus.RUNNING);
+    taskTrackerManager.submitJob(job3);
+
+    assertEquals(1,    defaultPool.getMapSchedulable().getDemand());
+    assertEquals(3,    defaultPool.getReduceSchedulable().getDemand());
+    assertEquals(16,   poolA.getMapSchedulable().getDemand());
+    assertEquals(20,   poolA.getReduceSchedulable().getDemand());
+
+    // Submit a job to poolA by specifying pool and not mapred.fairscheduler.pool
+    JobConf jobConf2 = new JobConf(conf);
+    jobConf2.setNumMapTasks(17);
+    jobConf2.setNumReduceTasks(19);
+    jobConf2.set(POOL_PROPERTY, "poolA");
+    JobInProgress job4 = new FakeJobInProgress(jobConf2, taskTrackerManager,
+        null, jobTracker);
+    job4.initTasks();
+    job4.getStatus().setRunState(JobStatus.RUNNING);
+    taskTrackerManager.submitJob(job4);
+
+    assertEquals(1,    defaultPool.getMapSchedulable().getDemand());
+    assertEquals(3,    defaultPool.getReduceSchedulable().getDemand());
+    assertEquals(33,   poolA.getMapSchedulable().getDemand());
+    assertEquals(39,   poolA.getReduceSchedulable().getDemand());
+  }
+
+
+  /**
+   * Test switching a job from one pool to another, then back to the original
+   * one. This is a regression test for a bug seen during development of
+   * MAPREDUCE-2323 (fair scheduler metrics).
+   */
+  public void testSetPoolTwice() throws Exception {
+    // Set up pools file
+    PrintWriter out = new PrintWriter(new FileWriter(ALLOC_FILE));
+    out.println("<?xml version=\"1.0\"?>");
+    out.println("<allocations>");
+    out.println("<pool name=\"default\">");
+    out.println("<schedulingMode>fair</schedulingMode>");
+    out.println("</pool>");
+    out.println("<pool name=\"poolA\">");
+    out.println("<schedulingMode>fair</schedulingMode>");
+    out.println("</pool>");
+    out.println("</allocations>");
+    out.close();
+    scheduler.getPoolManager().reloadAllocs();
+    Pool defaultPool = scheduler.getPoolManager().getPool("default");
+    Pool poolA = scheduler.getPoolManager().getPool("poolA");
+
+    // Submit a job to the default pool.  All specifications take default values.
+    JobInProgress job1 = submitJob(JobStatus.RUNNING, 1, 3);
+    assertEquals(1,    defaultPool.getMapSchedulable().getDemand());
+    assertEquals(3,    defaultPool.getReduceSchedulable().getDemand());
+    assertEquals(0,    poolA.getMapSchedulable().getDemand());
+    assertEquals(0,    poolA.getReduceSchedulable().getDemand());
+
+    // Move job to poolA and make sure demand moves with it
+    scheduler.getPoolManager().setPool(job1, "poolA");
+    assertEquals("poolA", scheduler.getPoolManager().getPoolName(job1));
+
+    defaultPool.getMapSchedulable().updateDemand();
+    defaultPool.getReduceSchedulable().updateDemand();
+    poolA.getMapSchedulable().updateDemand();
+    poolA.getReduceSchedulable().updateDemand();
+
+    assertEquals(0,    defaultPool.getMapSchedulable().getDemand());
+    assertEquals(0,    defaultPool.getReduceSchedulable().getDemand());
+    assertEquals(1,    poolA.getMapSchedulable().getDemand());
+    assertEquals(3,    poolA.getReduceSchedulable().getDemand());
+
+    // Move back to default pool and make sure demand goes back
+    scheduler.getPoolManager().setPool(job1, "default");
+    assertEquals("default", scheduler.getPoolManager().getPoolName(job1));
+
+    defaultPool.getMapSchedulable().updateDemand();
+    defaultPool.getReduceSchedulable().updateDemand();
+    poolA.getMapSchedulable().updateDemand();
+    poolA.getReduceSchedulable().updateDemand();
+
+    assertEquals(1,    defaultPool.getMapSchedulable().getDemand());
+    assertEquals(3,    defaultPool.getReduceSchedulable().getDemand());
+    assertEquals(0,    poolA.getMapSchedulable().getDemand());
+    assertEquals(0,    poolA.getReduceSchedulable().getDemand());
+  }
+  
+  private void advanceTime(long time) {
+    clock.advance(time);
+    scheduler.update();
+  }
+
+  protected TaskTracker tracker(String taskTrackerName) {
+    return taskTrackerManager.getTaskTracker(taskTrackerName);
+  }
+  
+  protected void checkAssignment(String taskTrackerName,
+      String... expectedTasks) throws IOException {
+    List<Task> tasks = scheduler.assignTasks(tracker(taskTrackerName));
+    for (Task t : tasks) {
+      taskTrackerManager.reportTaskOnTracker(taskTrackerName, t);
+    }
+    assertNotNull(tasks);
+    System.out.println("Assigned tasks:");
+    for (int i = 0; i < tasks.size(); i++)
+      System.out.println("- " + tasks.get(i));
+    assertEquals(expectedTasks.length, tasks.size());
+    for (int i = 0; i < tasks.size(); i++)
+      assertEquals("assignment " + i, expectedTasks[i], tasks.get(i).toString());
+  }
+
+  /**
+   * This test submits a job that takes all 2 slots in a pool has both a min
+   * share of 2 slots with minshare timeout of 5s, and then a second job in
+   * default pool with a fair share timeout of 5s. After 60 seconds, this pool
+   * will be starved of fair share (2 slots of each type), and we test that it
+   * does not kill more than 2 tasks of each type.
+   */
+  public void testFairSharePreemptionWithShortTimeout() throws Exception {
+    // Enable preemption in scheduler
+    scheduler.preemptionEnabled = true;
+    // Set up pools file
+    PrintWriter out = new PrintWriter(new FileWriter(ALLOC_FILE));
+    out.println("<?xml version=\"1.0\"?>");
+    out.println("<allocations>");
+    out.println("<fairSharePreemptionTimeout>5</fairSharePreemptionTimeout>");
+    out.println("<pool name=\"pool1\">");
+    out.println("<minMaps>2</minMaps>");
+    out.println("<minReduces>2</minReduces>");
+    out.println("<minSharePreemptionTimeout>5</minSharePreemptionTimeout>");
+    out.println("</pool>");
+    out.println("</allocations>");
+    out.close();
+    scheduler.getPoolManager().reloadAllocs();
+    Pool pool1 = scheduler.getPoolManager().getPool("pool1");
+    Pool defaultPool = scheduler.getPoolManager().getPool("default");
+
+    // Submit job 1 and assign all slots to it. Sleep a bit before assigning
+    // tasks on tt1 and tt2 to ensure that the ones on tt2 get preempted first.
+    JobInProgress job1 = submitJob(JobStatus.RUNNING, 10, 10, "pool1");
+    JobInfo info1 = scheduler.infos.get(job1);
+    checkAssignment("tt1", "attempt_test_0001_m_000000_0 on tt1");
+    checkAssignment("tt1", "attempt_test_0001_r_000000_0 on tt1");
+    checkAssignment("tt1", "attempt_test_0001_m_000001_0 on tt1");
+    checkAssignment("tt1", "attempt_test_0001_r_000001_0 on tt1");
+    advanceTime(100);
+    checkAssignment("tt2", "attempt_test_0001_m_000002_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0001_r_000002_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0001_m_000003_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0001_r_000003_0 on tt2");
+
+    advanceTime(10000);
+    assertEquals(4,    info1.mapSchedulable.getRunningTasks());
+    assertEquals(4,    info1.reduceSchedulable.getRunningTasks());
+    assertEquals(4.0,  info1.mapSchedulable.getFairShare());
+    assertEquals(4.0,  info1.reduceSchedulable.getFairShare());
+    // Ten seconds later, submit job 2.
+    JobInProgress job2 = submitJob(JobStatus.RUNNING, 10, 10, "default");
+
+    // Advance time by 6 seconds without update the scheduler.
+    // This simulates the time gap between update and task preemption.
+    clock.advance(6000);
+    assertEquals(4,    info1.mapSchedulable.getRunningTasks());
+    assertEquals(4,    info1.reduceSchedulable.getRunningTasks());
+    assertEquals(2.0,  info1.mapSchedulable.getFairShare());
+    assertEquals(2.0,  info1.reduceSchedulable.getFairShare());
+    assertEquals(0, scheduler.tasksToPreempt(pool1.getMapSchedulable(),
+        clock.getTime()));
+    assertEquals(0, scheduler.tasksToPreempt(pool1.getReduceSchedulable(),
+        clock.getTime()));
+    assertEquals(2, scheduler.tasksToPreempt(defaultPool.getMapSchedulable(),
+        clock.getTime()));
+    assertEquals(2, scheduler.tasksToPreempt(defaultPool.getReduceSchedulable(),
+        clock.getTime()));
+
+    // Test that the tasks actually get preempted and we can assign new ones
+    scheduler.preemptTasksIfNecessary();
+    scheduler.update();
+    assertEquals(2, job1.runningMaps());
+    assertEquals(2, job1.runningReduces());
+    checkAssignment("tt2", "attempt_test_0002_m_000000_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0002_r_000000_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0002_m_000001_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0002_r_000001_0 on tt2");
+    assertNull(scheduler.assignTasks(tracker("tt1")));
+    assertNull(scheduler.assignTasks(tracker("tt2")));
+  }
+  
+  
+  /**
+   * Ask scheduler to update metrics and then verify that they're all
+   * correctly published to the metrics context
+   */
+  private void verifyMetrics() {
+    scheduler.updateMetrics();
+    verifyPoolMetrics();
+    verifyJobMetrics();
+  }
+  
+  /**
+   * Verify that pool-level metrics match internal data
+   */
+  private void verifyPoolMetrics() {
+    MetricsContext ctx = MetricsUtil.getContext("fairscheduler");
+    Collection<OutputRecord> records = ctx.getAllRecords().get("pools");
+
+    try {
+      assertEquals(scheduler.getPoolSchedulables(TaskType.MAP).size() * 2,
+          records.size());
+    } catch (Error e) {
+      for (OutputRecord rec : records) {
+        System.err.println("record:");
+        System.err.println(" name: " + rec.getTag("name"));
+        System.err.println(" type: " + rec.getTag("type"));
+      }
+
+      throw e;
+    }
+    
+    Map<String, OutputRecord> byPoolAndType =
+      new HashMap<String, OutputRecord>();
+    for (OutputRecord rec : records) {
+      String pool = (String)rec.getTag("name");
+      String type = (String)rec.getTag("taskType");
+      assertNotNull(pool);
+      assertNotNull(type);
+      byPoolAndType.put(pool + "_" + type, rec);
+    }
+    
+    List<PoolSchedulable> poolScheds = new ArrayList<PoolSchedulable>();
+    poolScheds.addAll(scheduler.getPoolSchedulables(TaskType.MAP));
+    poolScheds.addAll(scheduler.getPoolSchedulables(TaskType.REDUCE));
+    
+    for (PoolSchedulable pool : poolScheds) {
+      String poolName = pool.getName();
+      OutputRecord metrics = byPoolAndType.get(
+          poolName + "_" + pool.getTaskType().toString());
+      assertNotNull("Need metrics for " + pool, metrics);
+      
+      verifySchedulableMetrics(pool, metrics);
+    }
+    
+  }
+  
+  /**
+   * Verify that the job-level metrics match internal data
+   */
+  private void verifyJobMetrics() {
+    MetricsContext ctx = MetricsUtil.getContext("fairscheduler");
+    Collection<OutputRecord> records = ctx.getAllRecords().get("jobs");
+    
+    System.out.println("Checking job metrics...");
+    Map<String, OutputRecord> byJobIdAndType =
+      new HashMap<String, OutputRecord>();
+    for (OutputRecord rec : records) {
+      String jobId = (String)rec.getTag("name");
+      String type = (String)rec.getTag("taskType");
+      assertNotNull(jobId);
+      assertNotNull(type);
+      byJobIdAndType.put(jobId + "_" + type, rec);
+      System.out.println("Got " + type + " metrics for job: " + jobId);
+    }
+    assertEquals(scheduler.infos.size() * 2, byJobIdAndType.size());
+    
+    for (Map.Entry<JobInProgress, JobInfo> entry :
+            scheduler.infos.entrySet()) {
+      JobInfo info = entry.getValue();
+      String jobId = entry.getKey().getJobID().toString();
+      
+      OutputRecord mapMetrics = byJobIdAndType.get(jobId + "_MAP");
+      assertNotNull("Job " + jobId + " should have map metrics", mapMetrics);
+      verifySchedulableMetrics(info.mapSchedulable, mapMetrics);
+      
+      OutputRecord reduceMetrics = byJobIdAndType.get(jobId + "_REDUCE");
+      assertNotNull("Job " + jobId + " should have reduce metrics", reduceMetrics);
+      verifySchedulableMetrics(info.reduceSchedulable, reduceMetrics);
+    }
+  }
+
+  /**
+   * Verify that the metrics for a given Schedulable are correct
+   */
+  private void verifySchedulableMetrics(
+      Schedulable sched, OutputRecord metrics) {
+    assertEquals(sched.getRunningTasks(), metrics.getMetric("runningTasks"));
+    assertEquals(sched.getDemand(), metrics.getMetric("demand"));
+    assertEquals(sched.getFairShare(),
+        metrics.getMetric("fairShare").doubleValue(), .001);
+    assertEquals(sched.getWeight(),
+        metrics.getMetric("weight").doubleValue(), .001);
+  }
+}
Index: src/contrib/partitionscheduler/src/test/org/apache/hadoop/mapred/TestFairSchedulerPoolNames.java
===================================================================
--- src/contrib/partitionscheduler/src/test/org/apache/hadoop/mapred/TestFairSchedulerPoolNames.java	(revision 0)
+++ src/contrib/partitionscheduler/src/test/org/apache/hadoop/mapred/TestFairSchedulerPoolNames.java	(working copy)
@@ -0,0 +1,193 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.mapred;
+
+import java.io.DataOutputStream;
+import java.io.File;
+import java.io.FileWriter;
+import java.io.IOException;
+import java.io.PrintWriter;
+import java.net.URI;
+
+import static org.junit.Assert.*;
+
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.hdfs.MiniDFSCluster;
+import org.apache.hadoop.io.IntWritable;
+import org.apache.hadoop.io.Text;
+import org.apache.hadoop.mapred.Pool;
+import org.junit.After;
+import org.junit.Before;
+import org.junit.Test;
+
+public class TestFairSchedulerPoolNames {
+
+  final static String TEST_DIR = new File(System.getProperty("test.build.data",
+      "build/contrib/streaming/test/data")).getAbsolutePath();
+  final static String ALLOC_FILE = new File(TEST_DIR, "test-pools")
+      .getAbsolutePath();
+
+  private static final String POOL_PROPERTY = "pool";
+  private String namenode;
+  private MiniDFSCluster miniDFSCluster = null;
+  private MiniMRCluster miniMRCluster = null;
+
+  /**
+   * Note that The PoolManager.ALLOW_UNDECLARED_POOLS_KEY property is set to
+   * false. So, the default pool is not added, and only pool names in the
+   * scheduler allocation file are considered valid.
+   */
+  @Before
+  public void setUp() throws Exception {
+    new File(TEST_DIR).mkdirs(); // Make sure data directory exists
+    // Create an allocation file with only one pool defined.
+    PrintWriter out = new PrintWriter(new FileWriter(ALLOC_FILE));
+    out.println("<?xml version=\"1.0\"?>");
+    out.println("<allocations>");
+    out.println("<pool name=\"poolA\">");
+    out.println("<minMaps>1</minMaps>");
+    out.println("<minReduces>1</minReduces>");
+    out.println("</pool>");
+    out.println("</allocations>");
+    out.close();
+
+    namenode = "file:///";
+
+    JobConf clusterConf = new JobConf();
+    clusterConf.set("mapred.jobtracker.taskScheduler", FairScheduler.class
+        .getName());
+    clusterConf.set("mapred.fairscheduler.allocation.file", ALLOC_FILE);
+    clusterConf.set("mapred.fairscheduler.poolnameproperty", POOL_PROPERTY);
+    clusterConf.setBoolean(FairScheduler.ALLOW_UNDECLARED_POOLS_KEY, false);
+    miniMRCluster = new MiniMRCluster(1, namenode, 1, null, null, clusterConf);
+  }
+
+  @After
+  public void tearDown() throws Exception {
+    if (miniDFSCluster != null) {
+      miniDFSCluster.shutdown();
+    }
+    if (miniMRCluster != null) {
+      miniMRCluster.shutdown();
+    }
+  }
+
+  private void submitJob(String pool) throws IOException {
+    JobConf conf = new JobConf();
+    final Path inDir = new Path("/tmp/testing/wc/input");
+    final Path outDir = new Path("/tmp/testing/wc/output");
+    FileSystem fs = FileSystem.get(URI.create(namenode), conf);
+    fs.delete(outDir, true);
+    if (!fs.mkdirs(inDir)) {
+      throw new IOException("Mkdirs failed to create " + inDir.toString());
+    }
+    DataOutputStream file = fs.create(new Path(inDir, "part-00000"));
+    file.writeBytes("Sample text");
+    file.close();
+
+    FileSystem.setDefaultUri(conf, namenode);
+    conf.set("mapred.job.tracker", "localhost:"
+        + miniMRCluster.getJobTrackerPort());
+    conf.setJobName("wordcount");
+    conf.setInputFormat(TextInputFormat.class);
+
+    // the keys are words (strings)
+    conf.setOutputKeyClass(Text.class);
+    // the values are counts (ints)
+    conf.setOutputValueClass(IntWritable.class);
+
+    conf.setMapperClass(WordCount.MapClass.class);
+    conf.setCombinerClass(WordCount.Reduce.class);
+    conf.setReducerClass(WordCount.Reduce.class);
+
+    FileInputFormat.setInputPaths(conf, inDir);
+    FileOutputFormat.setOutputPath(conf, outDir);
+    conf.setNumMapTasks(1);
+    conf.setNumReduceTasks(0);
+
+    if (pool != null) {
+      conf.set(POOL_PROPERTY, pool);
+    }
+    JobClient.runJob(conf);
+  }
+
+  /**
+   * Tests job submission using the default pool name.
+   */
+  @Test
+  public void testDefaultPoolName() {
+    Throwable t = null;
+    try {
+      submitJob(null);
+    } catch (Exception e) {
+      t = e;
+    }
+    assertNotNull("No exception during submission", t);
+    assertTrue("Incorrect exception message", t.getMessage().contains(
+        "Add pool name to the fair scheduler allocation file"));
+  }
+
+  /**
+   * Tests job submission using a valid pool name (i.e., name exists in the fair
+   * scheduler allocation file).
+   */
+  @Test
+  public void testValidPoolName() {
+    Throwable t = null;
+    try {
+      submitJob("poolA");
+    } catch (Exception e) {
+      t = e;
+    }
+    assertNull("Exception during submission", t);
+  }
+
+  /**
+   * Tests job submission using an invalid pool name (i.e., name doesn't exist
+   * in the fair scheduler allocation file).
+   */
+  @Test
+  public void testInvalidPoolName() {
+    Throwable t = null;
+    try {
+      submitJob("poolB");
+    } catch (Exception e) {
+      t = e;
+    }
+    assertNotNull("No exception during submission", t);
+    assertTrue("Incorrect exception message", t.getMessage().contains(
+        "Add pool name to the fair scheduler allocation file"));
+  }
+
+  /**
+   * Tests that no Pool object can be created with a null string.
+   */
+  @Test
+  public void testPoolNameNotNull() {
+    try {
+      Pool pool = new Pool(null, null);
+      fail("Pool object got created with a null name somehow.");
+    } catch (IllegalArgumentException e) {
+      // Pass
+    } catch (Exception e) {
+      fail("Pool object got created with a null name and failed only later.");
+    }
+  }
+}
Index: src/contrib/partitionscheduler/src/test/org/apache/hadoop/mapred/TestFairSchedulerSystem.java
===================================================================
--- src/contrib/partitionscheduler/src/test/org/apache/hadoop/mapred/TestFairSchedulerSystem.java	(revision 0)
+++ src/contrib/partitionscheduler/src/test/org/apache/hadoop/mapred/TestFairSchedulerSystem.java	(working copy)
@@ -0,0 +1,199 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.mapred;
+
+import org.apache.hadoop.security.UserGroupInformation;
+import org.apache.hadoop.mapreduce.TestSleepJob;
+import org.apache.hadoop.util.ToolRunner;
+import org.apache.hadoop.conf.Configuration;
+import java.io.BufferedReader;
+import java.io.InputStream;
+import java.io.InputStreamReader;
+import java.net.URL;
+import java.net.HttpURLConnection;
+import java.util.concurrent.Callable;
+import java.util.concurrent.Executors;
+import java.util.concurrent.ExecutorService;
+import java.util.concurrent.Future;
+import java.util.concurrent.TimeoutException;
+import java.util.concurrent.TimeUnit;
+import java.util.ArrayList;
+import java.util.List;
+
+import org.junit.Test;
+import org.junit.BeforeClass;
+import org.junit.AfterClass;
+import static org.junit.Assert.*;
+
+/**
+ * System tests for the fair scheduler. These run slower than the
+ * mock-based tests in TestFairScheduler but have a better chance
+ * of catching synchronization bugs with the real JT.
+ *
+ * This test suite will often be run inside JCarder in order to catch
+ * deadlock bugs which have plagued the scheduler in the past - hence
+ * it is a bit of a "grab-bag" of system tests, since it's important
+ * that they all run as part of the same JVM instantiation.
+ */
+public class TestFairSchedulerSystem {
+  static final int NUM_THREADS=2;
+
+  static MiniMRCluster mr;
+  static JobConf conf;
+
+  @BeforeClass
+  public static void setUp() throws Exception {
+    conf = new JobConf();
+    final int taskTrackers = 1;
+
+    // Bump up the frequency of preemption updates to test against
+    // deadlocks, etc.
+    conf.set("mapred.jobtracker.taskScheduler", FairScheduler.class.getCanonicalName());
+    conf.set("mapred.fairscheduler.update.interval", "1");
+    conf.set("mapred.fairscheduler.preemption.interval", "1");
+    conf.set("mapred.fairscheduler.preemption", "true");
+    conf.set("mapred.fairscheduler.eventlog.enabled", "true");
+    conf.set("mapred.fairscheduler.poolnameproperty", "group.name");
+    conf.set("mapred.job.tracker.persist.jobstatus.active", "false");
+    mr = new MiniMRCluster(taskTrackers, "file:///", 1, null, null, conf);
+  }
+
+  @AfterClass
+  public static void tearDown() throws Exception {
+    if (mr != null) {
+      mr.shutdown();
+    }
+  }
+
+  private void runSleepJob(JobConf conf) throws Exception {
+    String[] args = { "-m", "1", "-r", "1", "-mt", "1", "-rt", "1" };
+    ToolRunner.run(conf, new TestSleepJob(), args);
+  }
+
+  /**
+   * Submit some concurrent sleep jobs, and visit the scheduler servlet
+   * while they're running.
+   */
+  @Test
+  public void testFairSchedulerSystem() throws Exception {
+    ExecutorService exec = Executors.newFixedThreadPool(NUM_THREADS);
+    List<Future<Void>> futures = new ArrayList<Future<Void>>(NUM_THREADS);
+    for (int i = 0; i < NUM_THREADS; i++) {
+      futures.add(exec.submit(new Callable<Void>() {
+            public Void call() throws Exception {
+              JobConf jobConf = mr.createJobConf();
+              runSleepJob(jobConf);
+              return null;
+            }
+          }));
+    }
+
+    JobClient jc = new JobClient(mr.createJobConf(null));
+
+    // Wait for the tasks to finish, and visit the scheduler servlet
+    // every few seconds while waiting.
+    for (Future<Void> future : futures) {
+      while (true) {
+        try {
+          future.get(3, TimeUnit.SECONDS);
+          break;
+        } catch (TimeoutException te) {
+          // It's OK
+        }
+        checkServlet(true);
+        checkServlet(false);
+
+        JobStatus jobs[] = jc.getAllJobs();
+        if (jobs == null) {
+          System.err.println("No jobs running, not checking tasklog servlet");
+          continue;
+        }
+        for (JobStatus j : jobs) {
+          System.err.println("Checking task graph for " + j.getJobID());
+          try {
+            checkTaskGraphServlet(j.getJobID());
+          } catch (AssertionError err) {
+            // The task graph servlet will be empty if the job has retired.
+            // This is OK.
+            RunningJob rj = jc.getJob(j.getJobID());
+            if (!rj.isComplete()) {
+              throw err;
+            }
+          }
+        }
+      }
+    }
+  }
+
+  /**
+   * Check the fair scheduler servlet for good status code and smoke test
+   * for contents.
+   */
+  private void checkServlet(boolean advanced) throws Exception {
+    String jtURL = "http://localhost:" +
+      mr.getJobTrackerRunner().getJobTrackerInfoPort();
+    URL url = new URL(jtURL + "/scheduler" +
+                      (advanced ? "?advanced" : ""));
+    HttpURLConnection connection = (HttpURLConnection)url.openConnection();
+    connection.setRequestMethod("GET");
+    connection.connect();
+    assertEquals(200, connection.getResponseCode());
+
+    // Just to be sure, slurp the content and make sure it looks like the scheduler
+    BufferedReader reader = new BufferedReader(
+      new InputStreamReader(connection.getInputStream()));
+    StringBuilder sb = new StringBuilder();
+
+    String line = null;
+    while ((line = reader.readLine()) != null) {
+      sb.append(line).append('\n');
+    }
+
+    String contents = sb.toString();
+    assertTrue("Bad contents for fair scheduler servlet: " + contents,
+      contents.contains("Fair Scheduler Administration"));
+  }
+
+  private void checkTaskGraphServlet(JobID job) throws Exception {
+    String jtURL = "http://localhost:" +
+      mr.getJobTrackerRunner().getJobTrackerInfoPort();
+    URL url = new URL(jtURL + "/taskgraph?jobid=" + job.toString() + "&type=map");
+    HttpURLConnection connection = (HttpURLConnection)url.openConnection();
+    connection.setRequestMethod("GET");
+    connection.connect();
+    assertEquals(200, connection.getResponseCode());
+
+    // Just to be sure, slurp the content and make sure it looks like the scheduler
+    String contents = slurpContents(connection);
+    assertTrue("Bad contents for job " + job + ":\n" + contents,
+      contents.contains("</svg>"));
+  }
+
+  private String slurpContents(HttpURLConnection connection) throws Exception {
+    BufferedReader reader = new BufferedReader(
+      new InputStreamReader(connection.getInputStream()));
+    StringBuilder sb = new StringBuilder();
+
+    String line = null;
+    while ((line = reader.readLine()) != null) {
+      sb.append(line).append('\n');
+    }
+
+    return sb.toString();
+  }
+}
Index: src/contrib/streaming/src/java/org/apache/hadoop/streaming/PipeMapper.java
===================================================================
--- src/contrib/streaming/src/java/org/apache/hadoop/streaming/PipeMapper.java	(revision 1596572)
+++ src/contrib/streaming/src/java/org/apache/hadoop/streaming/PipeMapper.java	(working copy)
@@ -23,6 +23,7 @@
 
 import org.apache.hadoop.mapred.JobConf;
 import org.apache.hadoop.mapred.Mapper;
+import org.apache.hadoop.mapred.ApproximateMapper;
 import org.apache.hadoop.mapred.Reporter;
 import org.apache.hadoop.mapred.OutputCollector;
 import org.apache.hadoop.mapred.SkipBadRecords;
@@ -35,7 +36,7 @@
 /** A generic Mapper bridge.
  *  It delegates operations to an external program via stdin and stdout.
  */
-public class PipeMapper extends PipeMapRed implements Mapper {
+public class PipeMapper extends PipeMapRed implements ApproximateMapper {
 
   private boolean ignoreKey = false;
   private boolean skipping = false;
@@ -46,6 +47,10 @@
   
   String getPipeCommand(JobConf job) {
     String str = job.get("stream.map.streamprocessor");
+    // Approximate Hadoop
+    if (job.getBoolean("map.approximate", false)) {
+      str = job.get("stream.mapapproximate.streamprocessor");
+    }
     if (str == null) {
       return str;
     }
@@ -129,6 +134,14 @@
       }
     }
   }
+  
+  /**
+   * Approximate Hadoop, we run the approximate command if available.
+   */
+  public void mapApproximate(Object key, Object value, OutputCollector output, Reporter reporter) throws IOException {
+    // We have already configure the task to be approximated at configuration time, so we are good.
+    map(key, value, output, reporter);
+  }
 
   public void close() {
     appendLogToJobLog("success");
Index: src/contrib/streaming/src/java/org/apache/hadoop/streaming/StreamJob.java
===================================================================
--- src/contrib/streaming/src/java/org/apache/hadoop/streaming/StreamJob.java	(revision 1596572)
+++ src/contrib/streaming/src/java/org/apache/hadoop/streaming/StreamJob.java	(working copy)
@@ -199,6 +199,10 @@
     mapCmd_ = unqualifyIfLocalPath(mapCmd_);
     comCmd_ = unqualifyIfLocalPath(comCmd_);
     redCmd_ = unqualifyIfLocalPath(redCmd_);
+    
+    // Approximate Hadoop
+    mapApproximateCmd_ = unqualifyIfLocalPath(mapApproximateCmd_);
+    redApproximateCmd_ = unqualifyIfLocalPath(redApproximateCmd_);
   }
 
   String unqualifyIfLocalPath(String cmd) throws IOException {
@@ -262,8 +266,12 @@
       
       mapCmd_ = (String)cmdLine.getOptionValue("mapper"); 
       comCmd_ = (String)cmdLine.getOptionValue("combiner"); 
-      redCmd_ = (String)cmdLine.getOptionValue("reducer"); 
+      redCmd_ = (String)cmdLine.getOptionValue("reducer");
       
+      // Approximate Hadoop
+      mapApproximateCmd_ = (String)cmdLine.getOptionValue("mapperApproximate"); 
+      redApproximateCmd_ = (String)cmdLine.getOptionValue("reducerApproximate"); 
+      
       values = cmdLine.getOptionValues("file");
       if (values != null && values.length > 0) {
         for (String file : values) {
@@ -377,6 +385,10 @@
     // reducer could be NONE 
     Option reducer = createOption("reducer", 
                                   "The streaming command to run", "cmd", 1, false); 
+    // Approximate Hadoop
+    Option mapperApproximate  = createOption("mapperApproximate",  "The streaming command to run", "cmd", 1, false);
+    Option reducerApproximate = createOption("reducerApproximate", "The streaming command to run", "cmd", 1, false); 
+    
     Option file = createOption("file", 
                                "File to be shipped in the Job jar file", 
                                "file", Integer.MAX_VALUE, false); 
@@ -428,6 +440,8 @@
       addOption(mapper).
       addOption(combiner).
       addOption(reducer).
+      addOption(mapperApproximate).
+      addOption(reducerApproximate).
       addOption(file).
       addOption(dfs).
       addOption(jt).
@@ -742,6 +756,18 @@
                      URLEncoder.encode(mapCmd_, "UTF-8"));
       }
     }
+    
+    // Approximate Hadoop
+    if (mapApproximateCmd_ != null) {
+      c = StreamUtil.goodClassOrNull(jobConf_, mapApproximateCmd_, defaultPackage);
+      if (c != null) {
+        jobConf_.setApproximateMapperClass(c);
+      } else {
+        jobConf_.setApproximateMapperClass(PipeMapper.class);
+        jobConf_.setApproximateMapRunnerClass(PipeMapRunner.class);
+        jobConf_.set("stream.mapapproximate.streamprocessor", URLEncoder.encode(mapApproximateCmd_, "UTF-8"));
+      }
+    }
 
     if (comCmd_ != null) {
       c = StreamUtil.goodClassOrNull(jobConf_, comCmd_, defaultPackage);
@@ -1000,6 +1026,10 @@
   protected String mapDebugSpec_;
   protected String reduceDebugSpec_;
   protected String ioSpec_;
+  
+  // Approximate Hadoop
+  protected String mapApproximateCmd_;
+  protected String redApproximateCmd_;
 
   // Use to communicate config to the external processes (ex env.var.HADOOP_USER)
   // encoding "a=b c=d"
Index: src/contrib/wikipedia/build.xml
===================================================================
--- src/contrib/wikipedia/build.xml	(revision 0)
+++ src/contrib/wikipedia/build.xml	(working copy)
@@ -0,0 +1,39 @@
+<?xml version="1.0"?>
+
+<!--
+   Licensed to the Apache Software Foundation (ASF) under one or more
+   contributor license agreements.  See the NOTICE file distributed with
+   this work for additional information regarding copyright ownership.
+   The ASF licenses this file to You under the Apache License, Version 2.0
+   (the "License"); you may not use this file except in compliance with
+   the License.  You may obtain a copy of the License at
+
+       http://www.apache.org/licenses/LICENSE-2.0
+
+   Unless required by applicable law or agreed to in writing, software
+   distributed under the License is distributed on an "AS IS" BASIS,
+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+   See the License for the specific language governing permissions and
+   limitations under the License.
+-->
+
+<!-- 
+Before you can run these subtargets directly, you need 
+to call at top-level: ant deploy-contrib compile-core-test
+-->
+<project name="wikipedia" default="jar">
+
+  <import file="../build-contrib.xml"/>
+
+  <!-- ====================================================== -->
+  <!-- Package a Hadoop contrib                               -->
+  <!-- ====================================================== -->
+  <target name="package" depends="jar, jar-examples" unless="skip.contrib">
+    <copy todir="${dist.dir}/lib" includeEmptyDirs="false" flatten="true">
+      <fileset dir="${build.dir}">
+        <include name="hadoop-${name}-${version}.jar" />
+      </fileset>
+    </copy>
+  </target>
+
+</project>
Index: src/contrib/wikipedia/ivy/libraries.properties
===================================================================
--- src/contrib/wikipedia/ivy/libraries.properties	(revision 0)
+++ src/contrib/wikipedia/ivy/libraries.properties	(working copy)
@@ -0,0 +1,5 @@
+#This properties file lists the versions of the various artifacts used by streaming.
+#It drives ivy and the generation of a maven POM
+
+#Please list the dependencies name with version if they are different from the ones 
+#listed in the global libraries.properties file (in alphabetical order)
Index: src/contrib/wikipedia/ivy.xml
===================================================================
--- src/contrib/wikipedia/ivy.xml	(revision 0)
+++ src/contrib/wikipedia/ivy.xml	(working copy)
@@ -0,0 +1,84 @@
+<?xml version="1.0" ?>
+<ivy-module version="1.0">
+  <info organisation="org.apache.hadoop" module="${ant.project.name}">
+    <license name="Apache 2.0"/>
+    <ivyauthor name="Apache Hadoop Team" url="http://hadoop.apache.org"/>
+    <description>
+        Apache Hadoop contrib
+    </description>
+  </info>
+  <configurations defaultconfmapping="default">
+    <!--these match the Maven configurations-->
+    <conf name="default" extends="master,runtime"/>
+    <conf name="master" description="contains the artifact but no dependencies"/>
+    <conf name="runtime" description="runtime but not the artifact" />
+
+    <conf name="common" visibility="private" 
+      description="artifacts needed to compile/test the application"/>
+  </configurations>
+
+  <publications>
+    <!--get the artifact from our module name-->
+    <artifact conf="master"/>
+  </publications>
+  <dependencies>
+    <dependency org="info.bliki.wiki" name="bliki-core" rev="3.0.16" conf="*->*,!sources,!javadoc">
+      <exclude org="commons-httpclient" module="commons-httpclient"/>
+    </dependency>
+    <dependency org="com.google.guava" name="guava" rev="13.0.1" conf="*->*,!sources,!javadoc"/>
+    <dependency org="com.google.code.gson" name="gson" rev="2.2.2" conf="*->*,!sources,!javadoc"/>
+
+    <dependency org="commons-math" name="commons-math" rev="1.1" conf="common->default"/>
+
+    <dependency org="org.mapdb" name="mapdb" rev="1.0.1" conf="common->default"/>
+
+    <dependency org="commons-cli"
+      name="commons-cli"
+      rev="${commons-cli.version}"
+      conf="common->default"/>
+    <dependency org="commons-logging"
+      name="commons-logging"
+      rev="${commons-logging.version}"
+      conf="common->default"/>
+   <dependency org="commons-collections"
+      name="commons-collections"
+      rev="${commons-collections.version}"
+      conf="common->default"/>
+    <dependency org="log4j"
+      name="log4j"
+      rev="${log4j.version}"
+      conf="common->master"/>
+   <dependency org="junit"
+      name="junit"
+      rev="${junit.version}"
+      conf="common->default"/>
+    <dependency org="org.mortbay.jetty"
+      name="jetty-util"
+      rev="${jetty-util.version}"
+      conf="common->master"/>
+    <dependency org="org.mortbay.jetty"
+      name="jetty"
+      rev="${jetty.version}"
+      conf="common->default"/>
+    <dependency org="org.mortbay.jetty"
+      name="jsp-api-2.1"
+      rev="${jsp-api-2.1.version}"
+      conf="common->master"/>
+    <dependency org="commons-httpclient"
+      name="commons-httpclient"
+      rev="${commons-httpclient.version}"
+      conf="common->master"/> 
+    <dependency org="commons-configuration"
+      name="commons-configuration"
+      rev="${commons-configuration.version}"
+      conf="common->master"/>
+    <dependency org="org.apache.commons"
+      name="commons-math"
+      rev="${commons-math.version}"
+      conf="common->master"/>
+    <dependency org="commons-lang"
+      name="commons-lang"
+      rev="${commons-lang.version}"
+      conf="common->master"/>
+  </dependencies>
+</ivy-module>
Index: src/contrib/wikipedia/runtest.sh
===================================================================
--- src/contrib/wikipedia/runtest.sh	(revision 0)
+++ src/contrib/wikipedia/runtest.sh	(working copy)
@@ -0,0 +1,119 @@
+#!/bin/bash
+
+export HADOOP_HOME="/home/goiri/hadoop-1.1.2"
+export HADOOP_WIKIPEDIA="/home/goiri/hadoop-1.1.2-src/build/contrib/wikipedia/hadoop-wikipedia-1.1.3.jar org.apache.hadoop.mapreduce.wikipedia.WikiPopularity"
+export HADOOP_APACHE="/home/goiri/hadoop-1.1.2-src/build/contrib/wikipedia/hadoop-wikipedia-1.1.3.jar org.apache.hadoop.mapreduce.apache.ApacheLogAnalysis"
+export HADOOP_WIKIPAGERANK="/home/goiri/hadoop-1.1.2-src/build/contrib/wikipedia/hadoop-wikipedia-1.1.3.jar org.apache.hadoop.mapreduce.wikipedia.WikiPageRank"
+export HADOOP_WIKILENGTHS="/home/goiri/hadoop-1.1.2-src/build/contrib/wikipedia/hadoop-wikipedia-1.1.3.jar org.apache.hadoop.mapreduce.wikipedia.WikiLengths"
+
+export INPUT_APACHE_LOG="/user/goiri/apachelog/"
+export INPUT_WIKIPEDIA_LOG="/user/goiri/wikipediaaccess/"
+export INPUT_WIKIPEDIA="/user/goiri/wikipedia-dump.xml.bz2"
+
+RUNS=10
+
+# PageRank
+if false; then
+	for RUN in `seq 1 $RUNS`; do
+		# Pagerank
+		t0=`date +%s`
+		$HADOOP_HOME/bin/hadoop jar $HADOOP_WIKIPAGERANK -input $INPUT_WIKIPEDIA -output /user/goiri/wiki-pagerank-precise-$RUN -r 10 -p
+		t1=`date +%s`
+		let t=$t1-$t0
+		echo "wiki-pagerank $RUN precise 100 $t" >> /tmp/results
+		# Approximations
+		for SAMPLING in `echo 1 2 5 10 100 1000 10000 100000`; do
+			for DROP in `echo 100 75 50 25`; do
+				# Pagerank
+				t0=`date +%s`
+				$HADOOP_HOME/bin/hadoop jar $HADOOP_WIKIPAGERANK -input $INPUT_WIKIPEDIA -output /user/goiri/wiki-pagerank-$SAMPLING-$DROP-$RUN -r 10 -s $SAMPLING -n $DROP
+				t1=`date +%s`
+				let t=$t1-$t0
+				echo "wiki-pagerank $RUN $SAMPLING $DROP $t" >> /tmp/results
+			done
+		done
+	done
+fi
+
+# Wikipedia length
+if false; then
+	for RUN in `seq 1 $RUNS`; do
+		# Lengths
+		t0=`date +%s`
+		$HADOOP_HOME/bin/hadoop jar $HADOOP_WIKILENGTHS  -input $INPUT_WIKIPEDIA -output /user/goiri/wiki-length-precise-$RUN -r 10 -p
+		t1=`date +%s`
+		let t=$t1-$t0
+		echo "wiki-length $RUN precise 100 $t" >> /tmp/results
+		# Approximations
+		for SAMPLING in `echo 1 2 5 10 100 1000 10000 100000`; do
+			for DROP in `echo 100 75 50 25`; do
+				# Lengths
+				echo "Run: wikilenghts drop=$DROP sampling=$SAMPLING run=$RUN"
+				t0=`date +%s`
+				$HADOOP_HOME/bin/hadoop jar $HADOOP_WIKILENGTHS  -input $INPUT_WIKIPEDIA -output /user/goiri/wiki-length-$SAMPLING-$DROP-$RUN -r 10 -s $SAMPLING -n $DROP
+				t1=`date +%s`
+				let t=$t1-$t0
+				echo "wiki-length $RUN $SAMPLING $DROP $t" >> /tmp/results
+			done
+		done
+	done
+fi
+
+# Apache
+if false; then
+	for RUN in `seq 1 10`; do
+		for TASK in `echo dateweek size page browser hack host totalsize pagesize`; do
+			# Apache logs
+			t0=`date +%s`
+			$HADOOP_HOME/bin/hadoop jar $HADOOP_APACHE -input $INPUT_APACHE_LOG -output /user/goiri/apache-$TASK-precise-$RUN -r 10 -t $TASK -p
+			t1=`date +%s`
+			let t=$t1-$t0
+			echo "This run took $t seconds"
+			echo "apache-$TASK $RUN precise 100 $t" >> /tmp/results
+			sleep 1
+			# Approximations
+			for SAMPLING in `echo 1 2 5 10 100 1000 10000 100000`; do
+				for DROP in `echo 100 75 50 25`; do
+					# Apache logs
+					t0=`date +%s`
+					$HADOOP_HOME/bin/hadoop jar $HADOOP_APACHE -input $INPUT_APACHE_LOG -output /user/goiri/apache-$TASK-$SAMPLING-$DROP-$RUN -r 10 -t $TASK -s $SAMPLING -n $DROP
+					t1=`date +%s`
+					let t=$t1-$t0
+					echo "This run took $t seconds"
+					echo "apache-$TASK $RUN $SAMPLING $DROP $t" >> /tmp/results
+					sleep 1
+				done
+			done
+		done
+	done
+fi
+
+
+
+# TODO generating the log...
+# Wikipedia access log
+if false; then
+	for TASK in `echo project page`; do
+		# Precise
+		t0=`date +%s`
+		$HADOOP_HOME/bin/hadoop jar $HADOOP_WIKIPEDIA -input $INPUT_WIKIPEDIA_LOG -output /user/goiri/wiki-$TASK-precise -r 10 -p -t $TASK
+		t1=`date +%s`
+		let t=$t1-$t0
+		echo "wiki-$TASK precise 100 $t" >> /tmp/results
+
+		# Approximations
+		SAMPLINGS=`echo 1 2 5 10 100 1000 10000`
+		if [[ "$TASK" == "page" ]]; then
+			SAMPLINGS=`echo 100 1000 10000`
+		fi
+		for SAMPLING in $SAMPLINGS; do
+			for DROP in `echo 100 75 50 25`; do
+				t0=`date +%s`
+				$HADOOP_HOME/bin/hadoop jar $HADOOP_WIKIPEDIA -input $INPUT_WIKIPEDIA_LOG -output /user/goiri/wiki-$TASK-$SAMPLING-$DROP -r 10 -t $TASK -s $SAMPLING -d $DROP
+				t1=`date +%s`
+				let t=$t1-$t0
+				echo "wiki-$TASK $SAMPLING $DROP $t" >> /tmp/results
+			done
+		done
+	done
+fi

Property changes on: src/contrib/wikipedia/runtest.sh
___________________________________________________________________
Added: svn:executable
## -0,0 +1 ##
+*
\ No newline at end of property
Index: src/contrib/wikipedia/src/java/org/apache/hadoop/mapreduce/ApproximateIntWritable.java
===================================================================
--- src/contrib/wikipedia/src/java/org/apache/hadoop/mapreduce/ApproximateIntWritable.java	(revision 0)
+++ src/contrib/wikipedia/src/java/org/apache/hadoop/mapreduce/ApproximateIntWritable.java	(working copy)
@@ -0,0 +1,99 @@
+package org.apache.hadoop.mapreduce;
+
+import org.apache.hadoop.io.IntWritable;
+
+
+/**
+ * This is just for outputting. The ideal would be to have value as protected in IntWritable but...
+ */
+public class ApproximateIntWritable extends IntWritable {
+	private int value;
+	private int range;
+
+	public ApproximateIntWritable(int value, double range) {
+		this.value = value;
+		this.range = (int) Math.ceil(range);
+	}
+	
+	public String toString() {
+		return Integer.toString(value)+"+/-"+range;
+	}
+	
+	/**
+	 * Normalized error.
+	 */
+	public double getError() {
+		if (this.value == 0) {
+			return 0.0;
+		}
+		return this.range/this.value;
+	}
+	
+	/*public ApproximateIntWritable() {}
+
+	public ApproximateIntWritable(int value) {
+		set(value);
+	}
+	
+	public ApproximateIntWritable(int value, double range) {
+		set(value);
+		this.range = (int) Math.ceil(range);
+	}
+
+	public void set(int value) {
+		this.value = value;
+	}
+
+	public int get() { return value; }
+
+	public void readFields(DataInput in) throws IOException {
+		value = in.readInt();
+		in.readChar();
+		in.readChar();
+		in.readChar();
+		range = in.readInt();
+	}
+
+	public void write(DataOutput out) throws IOException {
+		out.writeInt(value);
+		out.writeChars("+/-");
+		out.writeInt(range);
+	}
+
+	public boolean equals(Object o) {
+		if (!(o instanceof IntWritable))
+			return false;
+		ApproximateIntWritable other = (ApproximateIntWritable)o;
+		return this.value == other.value;
+	}
+
+	public int hashCode() {
+		return value;
+	}
+
+	public int compareTo(Object o) {
+		int thisValue = this.value;
+		int thatValue = ((ApproximateIntWritable)o).value;
+		return (thisValue<thatValue ? -1 : (thisValue==thatValue ? 0 : 1));
+	}
+
+	public String toString() {
+		return Integer.toString(value)+"+/-"+range;
+	}
+
+	public static class Comparator extends WritableComparator {
+		public Comparator() {
+			super(IntWritable.class);
+		}
+
+		public int compare(byte[] b1, int s1, int l1, byte[] b2, int s2, int l2) {
+			int thisValue = readInt(b1, s1);
+			int thatValue = readInt(b2, s2);
+			return (thisValue<thatValue ? -1 : (thisValue==thatValue ? 0 : 1));
+		}
+	}
+	
+	static {                                        // register this comparator
+		WritableComparator.define(IntWritable.class, new Comparator());
+	}*/
+}
\ No newline at end of file
Index: src/contrib/wikipedia/src/java/org/apache/hadoop/mapreduce/ApproximateLongWritable.java
===================================================================
--- src/contrib/wikipedia/src/java/org/apache/hadoop/mapreduce/ApproximateLongWritable.java	(revision 0)
+++ src/contrib/wikipedia/src/java/org/apache/hadoop/mapreduce/ApproximateLongWritable.java	(working copy)
@@ -0,0 +1,38 @@
+package org.apache.hadoop.mapreduce;
+
+import org.apache.hadoop.io.LongWritable;
+
+/**
+ * This is just for outputting. The ideal would be to have value as protected in IntWritable but...
+ */
+public class ApproximateLongWritable extends LongWritable {
+	private long value;
+	private long range;
+
+	public ApproximateLongWritable(long value, double range) {
+		this.value = value;
+		this.range = (long) Math.ceil(range);
+	}
+	
+	public String toString() {
+		return Long.toString(value)+"+/-"+range;
+	}
+
+	private long getValue() {
+		return this.value;
+	}
+	
+	private long getRange() {
+		return this.range;
+	}
+	
+	/**
+	 * Normalized error.
+	 */
+	public double getError() {
+		if (this.value == 0) {
+			return 0.0;
+		}
+		return this.range/this.value;
+	}
+}
\ No newline at end of file
Index: src/contrib/wikipedia/src/java/org/apache/hadoop/mapreduce/MultistageSamplingHelper.java
===================================================================
--- src/contrib/wikipedia/src/java/org/apache/hadoop/mapreduce/MultistageSamplingHelper.java	(revision 0)
+++ src/contrib/wikipedia/src/java/org/apache/hadoop/mapreduce/MultistageSamplingHelper.java	(working copy)
@@ -0,0 +1,102 @@
+package org.apache.hadoop.mapreduce;
+
+import org.apache.commons.math.distribution.TDistribution;
+import org.apache.commons.math.distribution.TDistributionImpl;
+
+
+/**
+ * Set of useful functions for multistage sampling
+ */
+public class MultistageSamplingHelper {
+	/**
+	 * Get the t-score based on the t-student distribution.
+	 */
+	public static double getTScore(int degrees, double confidence) {
+		double tscore = 1.96; // By default we use the normal distribution
+		try {
+			TDistribution tdist = new TDistributionImpl(degrees);
+			//double confidence = 0.95; // 95% confidence => 0.975
+			tscore = tdist.inverseCumulativeProbability(1.0-((1.0-confidence)/2.0)); // 95% confidence 1-alpha
+		} catch (Exception e) { }
+		return tscore;
+	}
+	
+	/**
+	 * Calculate summation of a list of numbers.
+	 */
+	protected static double sum(int[] arr) {
+		double sum = 0.0;
+		for (int a : arr) {
+			sum += a;
+		}
+		return sum;
+	}
+	protected static double sum(long[] arr) {
+		double sum = 0.0;
+		for (long a : arr) {
+			sum += a;
+		}
+		return sum;
+	}
+	
+	protected static double sum(double[] arr) {
+		double sum = 0.0;
+		for (double a : arr) {
+			sum += a;
+		}
+		return sum;
+	}
+	
+	/**
+	* Calculate variance of a list of numbers. Note that this is the estimated variance (n-1).
+	*/
+	protected static double var(double[] arr) {
+		// Infinite variance for a single element
+		if (arr.length <= 1) {
+			return Double.MAX_VALUE; // x/n-1 = x/0 = infinity
+		}
+		
+		double avg = 0.0;
+		for (double a : arr) {
+			avg += a;
+			//avg += a / arr.length; // It looks like we don't overflow, so let's do it outside
+		}
+		avg = avg/arr.length;
+		
+		// Calculate the variance
+		double variance = 0.0;
+		for (double a : arr) {
+			double aux = a-avg;
+			variance += (aux*aux);
+			//variance += (aux*aux) / (arr.length-1.0);  // It looks like we don't overflow, so let's do it outside
+		}
+		variance = variance/(arr.length-1);
+		
+		return variance;
+	}
+	
+	/**
+	 * Convert a format string into a match string.
+	 */
+	public static String formatToMatch(String format) {
+		String ret = new String(format);
+		ret = ret.replaceAll("%d", "(\\\\d)+");
+		ret = ret.replaceAll("\\$", "\\\\\\$");
+		return ret;
+	}
+	
+	/**
+	 * Creates a fancy string for a number
+	 */
+	public static String toNumberString(double number) {
+		String suffix = "";
+		if (number > 1000000) {
+			number = number/1000000.0;
+			suffix = "M";
+		} else if (number > 1000) {
+			number = number/1000.0;
+			suffix = "K";
+		}
+		return String.format("%.1f%s", number, suffix);
+	}
+}
\ No newline at end of file
Index: src/contrib/wikipedia/src/java/org/apache/hadoop/mapreduce/MultistageSamplingMapper.java
===================================================================
--- src/contrib/wikipedia/src/java/org/apache/hadoop/mapreduce/MultistageSamplingMapper.java	(revision 0)
+++ src/contrib/wikipedia/src/java/org/apache/hadoop/mapreduce/MultistageSamplingMapper.java	(working copy)
@@ -0,0 +1,204 @@
+package org.apache.hadoop.mapreduce;
+
+import java.io.IOException;
+
+import org.apache.hadoop.mapreduce.Mapper;
+
+import org.apache.hadoop.io.IntWritable;
+import org.apache.hadoop.io.LongWritable;
+import org.apache.hadoop.io.Text;
+import org.apache.hadoop.io.WritableComparable;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.io.RawComparator;
+import org.apache.hadoop.mapred.RawKeyValueIterator;
+import org.apache.hadoop.mapreduce.Counter;
+import org.apache.hadoop.mapreduce.StatusReporter;
+
+import org.apache.hadoop.mapred.JobClient;
+import org.apache.hadoop.mapreduce.lib.input.FileSplit;
+
+import org.apache.hadoop.mapreduce.lib.input.ApproximateLineRecordReader;
+import org.apache.hadoop.mapreduce.SamplingRecordReader;
+
+/**
+ * Mapper that allows sending parameters to all the reducers for multistage sampling.
+ */
+public abstract class MultistageSamplingMapper<KEYIN,VALUEIN,KEYOUT,VALUEOUT extends WritableComparable> extends Mapper<KEYIN,VALUEIN,KEYOUT,VALUEOUT> {
+	// We keep track of this for multistage sampling
+	protected long numM =  0;
+	protected long numT = -1; // We don't use this by default
+	protected long numS = -1; // We don't use this by default
+	
+	/**
+	 * This is a wrapper for Context that gets keys and adds an ID at the end.
+	 */
+	public class ClusteringContext extends Context {
+		Context context;
+		int sendTaskId = 0;
+		boolean precise = false;
+		
+		public ClusteringContext(Context context) throws IOException, InterruptedException {
+			// This is just a wrapper, so we don't create anything
+			super(context.getConfiguration(), context.getTaskAttemptID(), null, null, context.getOutputCommitter(), null, null);
+
+			// Save the context
+			this.context = context;
+			this.sendTaskId = context.getTaskAttemptID().getTaskID().getId();
+			this.precise = context.getConfiguration().getBoolean("mapred.job.precise", false);
+		}
+		
+		/**
+		 * Overwrite of regular write() to capture values and do clustering if needed. If we run precise, pass it to the actual context.
+		 */
+		@Override
+		public void write(KEYOUT key, VALUEOUT value) throws IOException,InterruptedException {
+			if (!this.precise && key instanceof Text) {
+				// Sort method with just one more character at the end
+				byte[] byteId = new byte[] {(byte) (sendTaskId/128), (byte) (sendTaskId%128)};
+				context.write((KEYOUT) new Text(key.toString()+new String(byteId)), value);
+				// Long method that is human readable
+				//context.write((KEYOUT) new Text(key.toString()+String.format("-%05d", sendTaskId)), value);
+			} else {
+				context.write(key, value);
+			}
+		}
+
+		// We overwrite the following methods to avoid problems, ideally we would forward everything
+		@Override
+		public float getProgress() {
+			return context.getProgress();
+		}
+		
+		@Override
+		public void progress() {
+			context.progress();
+		}
+		
+		@Override
+		public void setStatus(String status) {
+			context.setStatus(status);
+		}
+		
+		@Override
+		public Counter getCounter(Enum<?> counterName) {
+			return context.getCounter(counterName);
+		}
+		
+		@Override
+		public Counter getCounter(String groupName, String counterName) {
+			return context.getCounter(groupName, counterName);
+		}
+	}
+	
+	/**
+	 * We use this to keep track of the fields and send it to the reducers, the user can decide to use something else or add others.
+	 */
+	@Override
+	public void run(Context context) throws IOException, InterruptedException {
+		setup(context);
+		
+		// Create the context that adds an id for clustering
+		Context newcontext = context;
+		Configuration conf = context.getConfiguration();
+		// If we don't do incremental, we have to IDs to the keys
+		if (!conf.getBoolean("mapred.job.precise", false) && !conf.getBoolean("mapred.tasks.incremental.reduction", false)) {
+			newcontext = new ClusteringContext(context);
+		}
+		
+		while (context.nextKeyValue()) {
+			map(context.getCurrentKey(), context.getCurrentValue(), newcontext);
+			numM++;
+		}
+		cleanup(context);
+	}
+	
+	/**
+	 * Cleanup function that reports how many fields have been processed.
+	 * This is the default case where each process item is an element m
+	 */
+	@Override
+	public void cleanup(Context context) throws IOException, InterruptedException {
+		// Get the the sampling ratio from the reader
+		int samplingRatio = 1;
+		RecordReader<KEYIN,VALUEIN> reader = context.getReader();
+		if (reader instanceof SamplingRecordReader) {
+			samplingRatio = ((SamplingRecordReader)reader).getSamplingRatio();
+			System.out.println("The actual sampling ratio at the end was: " + samplingRatio);
+		}
+		
+		// We send the statistically relevant information to everybody if we are sampling
+		Configuration conf = context.getConfiguration();
+		if (!conf.getBoolean("mapred.job.precise", false)) {
+			// Integer format
+			if (IntWritable.class.equals(context.getMapOutputValueClass())) {
+				if (numM >= 0) {
+					sendValue(context, MultistageSamplingReducer.M_SAMPLED, (int) numM);
+				}
+				if (numT >= 0) {
+					sendValue(context, MultistageSamplingReducer.T_SAMPLED, (int) numT);
+				}
+				if (numS >= 0 || numS == -3) {
+					sendValue(context, MultistageSamplingReducer.S_SAMPLED, (int) numS);
+				}
+				sendValue(context, MultistageSamplingReducer.Mm_SAMPLED, (int) samplingRatio);
+				// Send the cluster marks
+				sendValue(context, MultistageSamplingReducer.CLUSTERINI, 0);
+				sendValue(context, MultistageSamplingReducer.CLUSTERFIN, 0);
+			// Long format
+			} else {
+				if (numM >= 0) {
+					sendValue(context, MultistageSamplingReducer.M_SAMPLED, numM);
+				}
+				if (numT >= 0) {
+					sendValue(context, MultistageSamplingReducer.T_SAMPLED, numT);
+				}
+				if (numS >= 0 || numS == -3) {
+					sendValue(context, MultistageSamplingReducer.S_SAMPLED, numS);
+				}
+				sendValue(context, MultistageSamplingReducer.Mm_SAMPLED, samplingRatio);
+				// Send the cluster marks
+				sendValue(context, MultistageSamplingReducer.CLUSTERINI, 0L);
+				sendValue(context, MultistageSamplingReducer.CLUSTERFIN, 0L);
+			}
+		}
+	}
+	
+	/**
+	 * Set the secondary cluster size.
+	 */
+	protected void setM(long numM) {
+		this.numM = numM;
+	}
+	
+	/**
+	 * Set the terciary cluster size.
+	 */
+	protected void setT(long numT) {
+		this.numT = numT;
+	}
+	
+	/**
+	 * Set the standard deviaiton for the cluster size.
+	 */
+	protected void setS(long numS) {
+		this.numS = numS;
+	}
+	
+	/**
+	 * Send a parameter (inside of the data) to all reducers.
+	 */
+	protected void sendValue(Context context, String param, Long value) throws IOException, InterruptedException {
+		int numReducers = context.getConfiguration().getInt("mapred.reduce.tasks", 1);
+		for (int r=0; r<numReducers; r++) {
+			context.write((KEYOUT) new Text(String.format(param, context.getTaskAttemptID().getTaskID().getId(), r)), (VALUEOUT) new LongWritable(value));
+		}
+	}
+	
+	protected void sendValue(Context context, String param, Integer value) throws IOException, InterruptedException {
+		int numReducers = context.getConfiguration().getInt("mapred.reduce.tasks", 1);
+		for (int r=0; r<numReducers; r++) {
+			context.write((KEYOUT) new Text(String.format(param, context.getTaskAttemptID().getTaskID().getId(), r)), (VALUEOUT) new IntWritable(value));
+		}
+	}
+}
Index: src/contrib/wikipedia/src/java/org/apache/hadoop/mapreduce/MultistageSamplingPartitioner.java
===================================================================
--- src/contrib/wikipedia/src/java/org/apache/hadoop/mapreduce/MultistageSamplingPartitioner.java	(revision 0)
+++ src/contrib/wikipedia/src/java/org/apache/hadoop/mapreduce/MultistageSamplingPartitioner.java	(working copy)
@@ -0,0 +1,36 @@
+package org.apache.hadoop.mapreduce;
+
+import org.apache.hadoop.io.Text;
+import org.apache.hadoop.io.IntWritable;
+
+import org.apache.hadoop.mapreduce.lib.partition.HashPartitioner;
+
+/**
+ * A partitioner that checks if we have a parameter and send it to the specified one.
+ * \0PARAMETER-1-1 -> 1
+ * origkey from map 10 -> origkey10
+ */
+public class MultistageSamplingPartitioner<K,V> extends HashPartitioner<K,V> {
+	/**
+	 * Overwrite the partitioner to send to everybody.
+	 */
+	@Override
+	public int getPartition(K key, V value, int numReduceTasks) {
+		if (key instanceof Text) {
+			String aux = ((Text)key).toString();
+			
+			// Check if it's a parameter, it has the \0 first to guarantee is the first when sorting
+			int lastIndex = aux.lastIndexOf('-');
+			if (aux.charAt(0) == '\0' && lastIndex>0) {
+				return Integer.parseInt(aux.substring(lastIndex+1, aux.length())) % numReduceTasks;
+			}
+			
+			// The MultistageClusteringMapper adds an ID to the cluster, we have to remove it. For example, origkey-0003 => origkey
+			byte[] bytes = aux.getBytes();
+			String originalKey = new String(bytes, 0, bytes.length-2);
+			return super.getPartition((K) new Text(originalKey), value, numReduceTasks);
+		}
+		// The default case shouldn't happen
+		return super.getPartition(key, value, numReduceTasks);
+	}
+}
Index: src/contrib/wikipedia/src/java/org/apache/hadoop/mapreduce/MultistageSamplingReducer.java
===================================================================
--- src/contrib/wikipedia/src/java/org/apache/hadoop/mapreduce/MultistageSamplingReducer.java	(revision 0)
+++ src/contrib/wikipedia/src/java/org/apache/hadoop/mapreduce/MultistageSamplingReducer.java	(working copy)
@@ -0,0 +1,393 @@
+package org.apache.hadoop.mapreduce;
+
+import java.io.IOException;
+
+import java.util.Iterator;
+import java.util.Date;
+import java.util.LinkedList;
+import java.util.Map;
+import java.util.HashMap;
+
+import org.apache.hadoop.mapreduce.Reducer;
+
+import org.apache.hadoop.io.WritableComparable;
+import org.apache.hadoop.io.Text;
+import org.apache.hadoop.io.LongWritable;
+import org.apache.hadoop.io.IntWritable;
+import org.apache.hadoop.io.FloatWritable;
+import org.apache.hadoop.io.DoubleWritable;
+
+import org.apache.commons.math.distribution.TDistribution;
+import org.apache.commons.math.distribution.TDistributionImpl;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.io.RawComparator;
+import org.apache.hadoop.mapred.RawKeyValueIterator;
+import org.apache.hadoop.mapreduce.Counter;
+import org.apache.hadoop.mapreduce.StatusReporter;
+
+import org.apache.hadoop.mapreduce.TaskAttemptID;
+import org.apache.hadoop.mapreduce.RecordWriter;
+import org.apache.hadoop.mapreduce.OutputCommitter;
+import org.apache.hadoop.util.Progressable;
+
+/**
+ * Perform multistage sampling.
+ * It gets the data from the reducers and performs sampling on the fly.
+ * The data needs to come properly sorted.
+ */
+public abstract class MultistageSamplingReducer<KEYIN extends Text,VALUEIN,KEYOUT,VALUEOUT extends WritableComparable> extends Reducer<KEYIN,VALUEIN,KEYOUT,VALUEOUT> {
+	public static final char MARK_PARAM = '\0';
+	// Formats that the multistage clustering reducer recognizes. It is [NAME][mapId]-[reduceId]
+	public static final String M_SAMPLED  = MARK_PARAM + "m%d-%d"; // number of samples in cluster
+	public static final String Mm_SAMPLED = MARK_PARAM + "M%d-%d"; // sampling ratio (M/m)
+	public static final String T_SAMPLED  = MARK_PARAM + "T%d-%d"; // yt_i
+	public static final String S_SAMPLED  = MARK_PARAM + "S%d-%d"; // Standard deviation
+	// We need two additional marks to guarrantee we identify clusters properly, the name doesn't really matter (neither the order)
+	public static final String CLUSTERINI = MARK_PARAM + "I%d-%d";
+	public static final String CLUSTERFIN = MARK_PARAM + "F%d-%d";
+	
+	public static final String NULLKEY = "NULLKEY";
+	
+	// If we are doing the precise or execution
+	protected boolean precise = false;
+	
+	// Keep track of what was the last key
+	private String prevKey = null;
+	
+	// Multistage sampling
+	protected int N = 0;
+	protected int n = 0;
+	
+	// Current values
+	protected double[] y;
+	// Value for multistage sampling in-place
+	protected long[]  m; // Number of samples in cluster i
+	protected long[] Mm; // Sampling ratio (Mi/mi)
+	// Value for multistage sampling in-place
+	protected double[] yt;
+	protected double[] s2;
+	
+	// Precomputed values
+	protected double Nn = 0.0;
+	protected double NNnn = 0.0;
+	protected double[] auxm1;
+	protected double[] auxm2;
+	protected double[] auxm3;
+	
+	// Score based on T-student distribution for estimating the range
+	protected double tscore = 1.96; // Default is 95% for high n
+	
+	/**
+	 * Initialize all the variables needed for multistage sampling.
+	 */
+	@Override
+	public void setup(Context context) {
+		// Check if we are precise or not
+		Configuration conf = context.getConfiguration();
+		precise = conf.getBoolean("mapred.job.precise", false);
+		
+		// If we are approximating, we use the rest
+		if (!precise) {
+			// The total number of clusters
+			N  = conf.getInt("mapred.map.tasks", -1);
+			
+			// Default value for the sampling ratio
+			int skipSamples = conf.getInt("mapred.input.approximate.skip", 1);
+			
+			// Initialize values we will use frequently
+			y  =  new double[N];
+			m  =  new long[N];
+			Mm =  new long[N];
+			yt =  new double[N];
+			s2  = new double[N];
+			for (int i=0; i<N; i++) {
+				// Default sampling ratio
+				Mm[i] = skipSamples;
+				// -1 means we have not received anything for this cluster 
+				m[i]  = -1;
+				yt[i] = -1;
+				s2[i] = -1;
+			}
+			
+			// Structure for precomputed values
+			auxm1 = new double[N]; // skipSamples = 1.0 * Mi / mi;
+			auxm2 = new double[N]; // (Mi*(Mi-mi)) / (mi-1.0);
+			auxm3 = new double[N]; // (Mi*(Mi-mi)) / mi;
+		}
+	}
+	
+	/**
+	 * This is a wrapper for Context that gets values and clusters them if required.
+	 */
+	public class ClusteringContext extends Context {
+		protected Context context;
+		
+		/**
+		 * We need to store the wrapped context to forward everything.
+		 */
+		public ClusteringContext(Context context) throws IOException, InterruptedException {
+			// This is just a wrapper, so we don't create anything
+			super(context.getConfiguration(), context.getTaskAttemptID(), null, null, null, null, context.getOutputCommitter(), null, (RawComparator<KEYIN>) context.getSortComparator(), (Class<KEYIN>) context.getMapOutputKeyClass(), (Class<VALUEIN>) context.getMapOutputValueClass());
+
+			// Save the wrapped context
+			this.context = context;
+		}
+		
+		/**
+		 * Update "n" and "N" values
+		 */
+		protected void updateNValues() {
+			// Update "n" and "N" values
+			n++;
+			Nn = 1.0*N/n;
+			NNnn = 1.0*N*(N-n)/n;
+			
+			// Update t-score
+			tscore = MultistageSamplingHelper.getTScore(n-1, 0.95);
+		}
+		
+		/**
+		 * Update the m/M related values
+		 */
+		protected void updateMValues(int clusterId) {
+			long Mi = m[clusterId]*Mm[clusterId];
+			auxm1[clusterId] = 1.0 * Mm[clusterId]; // 1.0 * Mi / mi;
+			auxm2[clusterId] = 1.0 * (Mi*(Mi-m[clusterId])) / (m[clusterId]-1.0); // Mi(Mi-mi)/(mi-1)
+			auxm3[clusterId] = 1.0 * (Mi*(Mi-m[clusterId])) /  m[clusterId];      // Mi(Mi-mi)/mi
+		}
+		
+		/**
+		 * Overwrite of regular write() to capture values and do clustering if needed. If we run precise, pass it to the actual context.
+		 */
+		@Override
+		public void write(KEYOUT key, VALUEOUT value) throws IOException,InterruptedException {
+			// For precise, we just forward it to the regular context
+			// In this implementation, the marks are always Strings
+			if (isPrecise() || !(key instanceof Text)) {
+				context.write(key, value);
+			// If we don't want precise, we save for multistage sampling
+			} else {
+				// We have to convert the writtable methods into numbers
+				Double res = 0.0;
+				if (value instanceof LongWritable) {
+					res = new Double(((LongWritable) value).get());
+				} else if (value instanceof IntWritable) {
+					res = new Double(((IntWritable) value).get());
+				} else if (value instanceof DoubleWritable) {
+					res = new Double(((DoubleWritable) value).get());
+				} else if (value instanceof FloatWritable) {
+					res = new Double(((FloatWritable) value).get());
+				}
+				
+				// Extract the actual key
+				String keyStr = ((Text) key).toString();
+				byte[] aux = keyStr.getBytes();
+				String origKey = new String(aux, 0, aux.length-2);
+				
+				// We changed the key and the previous one wasn't a parameter, write it!
+				if (!origKey.equals(prevKey) && prevKey != null && prevKey.charAt(0) != MARK_PARAM) {
+					double[] result = estimateCurrentResult();
+					double tauhat   = result[0];
+					double interval = result[1];
+					// Let's output the value
+					context.write((KEYOUT) new Text(prevKey), (VALUEOUT) new ApproximateLongWritable((long) tauhat, interval));
+				}
+				
+				// Parameters start by '\0' to be the first when sorting
+				if (keyStr.charAt(0) == MARK_PARAM) {
+					// The parameters always have format: [NAME][MAPID/CLUSTERID]-[REDUCEID]
+					int clusterId = Integer.parseInt(keyStr.substring(2, keyStr.lastIndexOf("-"))); // \0M[mapId]-[reduceId]
+					// m_i
+					if (keyStr.matches(MultistageSamplingHelper.formatToMatch(M_SAMPLED))) {
+						// Save the value
+						m[clusterId] = res.longValue();
+						// Update values depending on m
+						updateNValues();
+						updateMValues(clusterId);
+					// M_i/m_i
+					} else if (keyStr.matches(MultistageSamplingHelper.formatToMatch(Mm_SAMPLED))) {
+						// Save the value
+						Mm[clusterId] = res.longValue();
+						// Update values depending on m
+						updateMValues(clusterId);
+					// yt_i
+					} else if (keyStr.matches(MultistageSamplingHelper.formatToMatch(T_SAMPLED))) {
+						yt[clusterId] = res.longValue();
+					// standard deviation
+					} else if (keyStr.matches(MultistageSamplingHelper.formatToMatch(S_SAMPLED))) {
+						s2[clusterId] = res.longValue();
+					// Other parameters (marks mainly)
+					} else {
+						// We don't do anything with the marks, they are just... marks
+					}
+				// Regular values
+				} else {
+					// Calculate the map this key comes from, it is the last two bytes
+					int clusterId = 128*aux[aux.length-2]+aux[aux.length-1]; // Last two characters
+					// Save the value
+					y[clusterId] = res;
+				}
+				prevKey = origKey;
+			}
+		}
+
+		// We overwrite these method to avoid problems, ideally we would forward everything
+		public void getfirst() throws IOException { }
+		
+		public float getProgress() {
+			return context.getProgress();
+		}
+		
+		public void progress() {
+			context.progress();
+		}
+		
+		public void setStatus(String status) {
+			context.setStatus(status);
+		}
+		
+		public Counter getCounter(Enum<?> counterName) {
+			return context.getCounter(counterName);
+		}
+		
+		public Counter getCounter(String groupName, String counterName) {
+			return context.getCounter(groupName, counterName);
+		}
+	}
+	
+	/**
+	 * Reduce runner that uses a thread to check the results.
+	 */
+	@Override
+	public void run(Context context) throws IOException, InterruptedException {
+		setup(context);
+		
+		// Precise
+		if (isPrecise()) {
+			while (context.nextKey()) {
+				KEYIN key = context.getCurrentKey();
+				reduce(key, context.getValues(), context);
+			}
+		// Sampling
+		} else {
+			// We wrap the context to capture the writing of the keys
+			Context clusteringcontext = new ClusteringContext(context);
+			while (context.nextKey()) {
+				KEYIN key = context.getCurrentKey();
+				reduce(key, context.getValues(), clusteringcontext);
+			}
+			
+			// We need to treat the last key (not parameters)
+			if (prevKey != null && prevKey.charAt(0) != MARK_PARAM) {
+				double[] result = estimateCurrentResult();
+				double tauhat   = result[0];
+				double interval = result[1];
+				// Let's output the value
+				context.write((KEYOUT) new Text(prevKey), (VALUEOUT) new ApproximateLongWritable((long) tauhat, interval));
+			}
+		}
+		// Finish checker thread
+		cleanup(context);
+	}
+	
+	/**
+	 * To finish we estimate the error of keys that didn't show up because of sampling.
+	 */
+	@Override
+	public void cleanup(Context context) throws IOException, InterruptedException {
+		if (!isPrecise()) {
+			// We add a NULLKEY to estimate the range of everything we didn't see because of sampling
+			prevKey = NULLKEY;
+			
+			// Find the worst cluster
+			int worstCluster = 0;
+			for (int i=0; i<N; i++) {
+				// This is the cluster that brings the highest variance
+				if (auxm3[i] > auxm3[worstCluster]) {
+					worstCluster = i;
+				}
+			}
+			// We simulate we have one here
+			y[worstCluster] = 1.0;
+			
+			// Calculate the error for the null key
+			double[] result = estimateCurrentResult();
+			double tauhat   = result[0];
+			double interval = result[1];
+			// Let's output the value
+			context.write((KEYOUT) new Text(prevKey), (VALUEOUT) new ApproximateLongWritable((long) tauhat, interval));
+		}
+	}
+	
+	/**
+	 * Check if we run the job precisely.
+	 */
+	public void setPrecise() {
+		this.precise = true;
+	}
+	
+	public boolean isPrecise() {
+		return this.precise;
+	}
+	
+	/**
+	 * Get the estimated result for the current key.
+	 * @return [tauhat, tscore*setauhat]
+	 */
+	private double[] estimateCurrentResult() throws IOException, InterruptedException {
+		// Estimate the result
+		double[] yhat = new double[N];
+		double var2 = 0.0;
+		
+		// Process every cluster
+		for (int i=0; i<N; i++) {
+			// We can skip a few because of dropping and the result for yi==0 is 0
+			if (m[i] >= 0 && y[i] > 0) {
+				// Estimate the results in each cluster
+				yhat[i] = auxm1[i] * y[i];
+				// Calculate variance
+				if (s2[i] >= 0) {
+					// Based on standard deviation from the map
+					var2 += auxm3[i] * s2[i];
+				} else if (s2[i] == -3) {
+					// Worst case is a variance of all the values: si2 = 10% of yi
+					var2 += auxm3[i] * (0.1*y[i]);
+				} else {
+					// Estimate proportions
+					double pi = 1.0*y[i]/m[i];
+					// Variance to the secondary
+					var2 += auxm2[i] * pi * (1.0-pi);
+				}
+				// Reset yi=0 for the next key
+				y[i] = 0.0;
+			}
+		}
+		var2 = Nn * var2; // N/n
+		
+		// Estimate the total number
+		double tauhat = Nn * MultistageSamplingHelper.sum(yhat); // N/n
+		
+		// Calculate first variance variance
+		double var1 = 0.0;
+		if (n < N) {
+			var1 = NNnn * MultistageSamplingHelper.var(yhat);
+		}
+		
+		// Calculate standard error
+		double setauhat = 0.0;
+		if (var1 < 0 || var2 < 0) {
+			setauhat = Double.MAX_VALUE;
+		} else {
+			double vartauhat = var1 + var2;
+			setauhat = Math.sqrt(vartauhat);
+		}
+		
+		// The null key is 0 as we actually don't see it
+		if (prevKey.equals("NULLKEY")) {
+			tauhat = 0.0;
+		}
+		
+		return new double[] {tauhat, tscore*setauhat};
+	}
+}
Index: src/contrib/wikipedia/src/java/org/apache/hadoop/mapreduce/MultistageSamplingReducer.java.bak
===================================================================
--- src/contrib/wikipedia/src/java/org/apache/hadoop/mapreduce/MultistageSamplingReducer.java.bak	(revision 0)
+++ src/contrib/wikipedia/src/java/org/apache/hadoop/mapreduce/MultistageSamplingReducer.java.bak	(working copy)
@@ -0,0 +1,316 @@
+package org.apache.hadoop.mapred;
+
+import java.io.IOException;
+
+import java.util.Iterator;
+import java.util.Date;
+import java.util.LinkedList;
+import java.util.Map;
+import java.util.HashMap;
+
+import org.apache.hadoop.mapreduce.Reducer;
+
+import org.apache.hadoop.io.WritableComparable;
+import org.apache.hadoop.io.Text;
+import org.apache.hadoop.io.LongWritable;
+import org.apache.hadoop.io.IntWritable;
+import org.apache.hadoop.io.FloatWritable;
+import org.apache.hadoop.io.DoubleWritable;
+
+import org.apache.commons.math.distribution.TDistribution;
+import org.apache.commons.math.distribution.TDistributionImpl;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.io.RawComparator;
+import org.apache.hadoop.mapred.RawKeyValueIterator;
+import org.apache.hadoop.mapreduce.Counter;
+import org.apache.hadoop.mapreduce.StatusReporter;
+
+import org.apache.hadoop.mapreduce.TaskAttemptID;
+import org.apache.hadoop.mapreduce.RecordWriter;
+import org.apache.hadoop.mapreduce.OutputCommitter;
+import org.apache.hadoop.util.Progressable;
+
+/**
+ * Perform multistage sampling.
+ */
+public abstract class MultistageSamplingReducer<KEYIN extends WritableComparable,VALUEIN,KEYOUT,VALUEOUT extends WritableComparable> extends Reducer<KEYIN,VALUEIN,KEYOUT,VALUEOUT> {
+	// Parameters to estimate results based on sampling
+	private int skipSamples = 0; // Mi/mi
+	
+	private String prevKey = null;
+	
+	protected boolean precise = false;
+
+	// Formats that the multistage clustering reducer recognizes
+	public static final String M_SAMPLED = "\0M%d-%d"; // mapId, reduceId
+	public static final String T_SAMPLED = "\0T%d-%d"; // mapId, reduceId
+	public static final String S_SAMPLED = "\0S%d-%d"; // Standard deviation: mapId, reduceId
+	// We need two additional marks to guarrantee we identify clusters properly, the name doesn't really matter (neither the order)
+	public static final String CLUSTERINI = "\0INI%d-%d";
+	public static final String CLUSTERFIN = "\0FIN%d-%d";
+	
+	private int numCluster = 0;
+	
+	private double tscore = 1.96;
+	
+	// Multistage sampling
+	private double[] yi;
+	private long[]   mi;
+	private double[] yti;
+	private double[] si2;
+	private int N = 0;
+	private int n = 0;
+	private double Nn = 0.0;
+	private double NNnn = 0.0;
+	private double[] auxmi1;
+	private double[] auxmi2;
+	private double[] auxmi3;
+	
+	/**
+	 * Check if we are precise.
+	 */
+	public void setup(Context context) {
+		precise = context.getConfiguration().getBoolean("mapred.job.precise", false);
+		
+		if (!precise) {
+			numCluster  = context.getConfiguration().getInt("mapred.map.tasks", -1);
+			skipSamples = context.getConfiguration().getInt("mapred.input.approximate.skip", 1);
+			
+			yi  = new double[numCluster];
+			mi  = new long[numCluster];
+			yti = new double[numCluster];
+			si2  = new double[numCluster];
+			for (int i=0; i<numCluster; i++) {
+				// -1 means we have not received anything for this cluster 
+				mi[i]  = -1;
+				yti[i] = -1;
+				si2[i] = -1;
+			}
+			N = numCluster;
+			
+			auxmi1 = new double[N]; // skipSamples = 1.0 * Mi / mi;
+			auxmi2 = new double[N]; // (Mi*(Mi-mi)) / (mi-1.0);
+			auxmi3 = new double[N]; // (Mi*(Mi-mi)) / mi;
+		}
+	}
+	
+	/**
+	 * This is a wrapper for Context that gets values and clusters them if required.
+	 */
+	public class ClusteringContext extends Context {
+		Context context;
+		
+		public ClusteringContext(Context context) throws IOException, InterruptedException {
+			// This is just a wrapper, so we don't create anything
+			super(context.getConfiguration(), context.getTaskAttemptID(), null, null, null, null, context.getOutputCommitter(), null, (RawComparator<KEYIN>) context.getSortComparator(), (Class<KEYIN>) context.getMapOutputKeyClass(), (Class<VALUEIN>) context.getMapOutputValueClass());
+
+			// Save the context
+			this.context = context;
+		}
+		
+		/**
+		 * Overwrite of regular write() to capture values and do clustering if needed. If we run precise, pass it to the actual context.
+		 */
+		public void write(KEYOUT key, VALUEOUT value) throws IOException,InterruptedException {
+			// For precise, we just forward it to the regular context
+			if (isPrecise()) {
+				context.write(key, value);
+			// If we don't want precise, we save for multistage sampling
+			} else {
+				// We have to convert the writtable methods into numbers
+				Double res = 0.0;
+				if (value instanceof LongWritable) {
+					res = new Double(((LongWritable) value).get());
+				} else if (value instanceof IntWritable) {
+					res = new Double(((IntWritable) value).get());
+				} else if (value instanceof DoubleWritable) {
+					res = new Double(((DoubleWritable) value).get());
+				} else if (value instanceof FloatWritable) {
+					res = new Double(((FloatWritable) value).get());
+				}
+				
+				// If this is Text we do multistage sampling
+				if (key instanceof Text) {
+					// Get the actual key and the cluster Id
+					String keyStr = ((Text) key).toString();
+					
+					// Extract actual key and clusterId
+					byte[] aux = keyStr.getBytes();
+					String origKey = new String(aux, 0, aux.length-2);
+					
+					// We changed the key and the previous one wasn't a parameter
+					if (!origKey.equals(prevKey) && prevKey != null && prevKey.charAt(0) != '\0') {
+						saveCurrentResult(context);
+					}
+					
+					// Parameters start by '\0' to be the first when sorting
+					if (keyStr.charAt(0) == '\0') {
+						// Check the parameter
+						if (keyStr.matches(MultistageSamplingHelper.formatToMatch(M_SAMPLED))) {
+							int clusterId = Integer.parseInt(keyStr.substring(2, keyStr.lastIndexOf("-"))); // \0M[mapId]-[reduceId]
+							
+							mi[clusterId] = res.longValue();
+							
+							// Update "n" and "N" values
+							n++;
+							Nn = 1.0*N/n;
+							NNnn = 1.0*N*(N-n)/n;
+							
+							long Mi = mi[clusterId]*skipSamples;
+							auxmi1[clusterId] = 1.0 * skipSamples; // 1.0 * Mi / mi;
+							auxmi2[clusterId] = 1.0 * (Mi*(Mi-mi[clusterId])) / (mi[clusterId]-1.0); // Mi(Mi-mi)/(mi-1)
+							auxmi3[clusterId] = 1.0 * (Mi*(Mi-mi[clusterId])) /  mi[clusterId];      // Mi(Mi-mi)/mi
+							
+							// Update t-score
+							tscore = MultistageSamplingHelper.getTScore(n-1, 0.95);
+						// yt
+						} else if (keyStr.matches(MultistageSamplingHelper.formatToMatch(T_SAMPLED))) {
+							int clusterId = Integer.parseInt(keyStr.substring(2, keyStr.lastIndexOf("-"))); // \0M[mapId]-[reduceId]
+						
+							yti[clusterId] = res.longValue();
+						// standard deviation
+						} else if (keyStr.matches(MultistageSamplingHelper.formatToMatch(S_SAMPLED))) {
+							int clusterId = Integer.parseInt(keyStr.substring(2, keyStr.lastIndexOf("-"))); // \0M[mapId]-[reduceId]
+						
+							si2[clusterId] = res.longValue();
+						// Other parameters
+						} else {
+							// We should implement the other parameters but there's no point now
+						}
+					// Regular values
+					} else {
+						// Calculate the map this key comes from
+						//int clusterId  = Integer.parseInt(keyStr.substring(lastIndex+1));
+						int clusterId = 128*aux[aux.length-2]+aux[aux.length-1]; // Last two characters
+						// Save the value
+						yi[clusterId] = res;
+					
+					}
+					prevKey = origKey;
+				// We just write by default
+				} else {
+					context.write(key, value);
+				}
+			}
+		}
+
+		// We overwrite these method to avoid problems, ideally we would forward everything
+		public void getfirst() throws IOException { }
+		
+		public float getProgress() {
+			return context.getProgress();
+		}
+		
+		public void progress() {
+			context.progress();
+		}
+		
+		public void setStatus(String status) {
+			context.setStatus(status);
+		}
+		
+		public Counter getCounter(Enum<?> counterName) {
+			return context.getCounter(counterName);
+		}
+		
+		public Counter getCounter(String groupName, String counterName) {
+			return context.getCounter(groupName, counterName);
+		}
+	}
+	
+	
+	/**
+	 * Reduce runner that uses a thread to check the results.
+	 */
+	public void run(Context context) throws IOException, InterruptedException {
+		setup(context);
+		
+		Context clusteringcontext = new ClusteringContext(context);
+		
+		// Precise
+		if (isPrecise()) {
+			while (context.nextKey()) {
+				KEYIN key = context.getCurrentKey();
+				reduce(key, context.getValues(), context);
+			}
+		// Sampling
+		} else {
+			while (context.nextKey()) {
+				KEYIN key = context.getCurrentKey();
+				reduce(key, context.getValues(), clusteringcontext);
+			}
+			
+			// Don't forget to treat the last key
+			if (prevKey != null && prevKey.charAt(0) != '\0') {
+				saveCurrentResult(context);
+			}
+		}
+		// Finish checker thread
+		cleanup(context);
+	}
+	
+	
+	/**
+	 * Check if we run the job precisely.
+	 */
+	public void setPrecise() {
+		this.precise = true;
+	}
+	
+	public boolean isPrecise() {
+		return this.precise;
+	}
+	
+	/**
+	 * Save the result for the last key.
+	 */
+	public void saveCurrentResult(Context context) throws IOException, InterruptedException {
+		// Estimate the result
+		double[] yhati = new double[N];
+		double var2 = 0.0;
+		for (int i=0; i<N; i++) {
+			// We can skip a few because of dropping and the result for yi==0 is 0
+			if (mi[i] >= 0 && yi[i] > 0) {
+				// Estimate the results in each cluster
+				yhati[i] = auxmi1[i] * yi[i];
+				// Calculate variance
+				if (si2[i] >= 0) {
+					// Based on standard deviation from the map
+					var2 += auxmi3[i] * si2[i];
+				} else if (si2[i] == -3) {
+					// Worst case is a variance of all the values: si2 = 10% of yi
+					var2 += auxmi3[i] * (0.1*yi[i]);
+				} else {
+					// Estimate proportions
+					double pi = 1.0*yi[i]/mi[i];
+					// Variance to the secondary
+					var2 += auxmi2[i] * pi * (1.0-pi);
+				}
+				// Reset yi=0 for the next key
+				yi[i] = 0.0;
+			}
+		}
+		var2 = Nn * var2; // N/n
+		
+		// Estimate the total number
+		double tauhat = Nn * MultistageSamplingHelper.sum(yhati); // N/n
+		
+		// Calculate total variance and standard error
+		// The regular way to do this is:
+		double var1 = 0.0;
+		if (n < N) {
+			var1 = MultistageSamplingHelper.var(yhati)*NNnn;
+		}
+		double vartauhat = var1 + var2;
+		Double setauhat = Math.sqrt(vartauhat);
+		
+		// Check if we are not in a weird point
+		if (setauhat.isNaN()) {
+			setauhat = Double.MAX_VALUE;
+		}
+		
+		// Let's output the value
+		context.write((KEYOUT) new Text(prevKey), (VALUEOUT) new ApproximateLongWritable((long) tauhat, tscore*setauhat));
+	}
+}
Index: src/contrib/wikipedia/src/java/org/apache/hadoop/mapreduce/MultistageSamplingReducerIncr.java
===================================================================
--- src/contrib/wikipedia/src/java/org/apache/hadoop/mapreduce/MultistageSamplingReducerIncr.java	(revision 0)
+++ src/contrib/wikipedia/src/java/org/apache/hadoop/mapreduce/MultistageSamplingReducerIncr.java	(working copy)
@@ -0,0 +1,787 @@
+package org.apache.hadoop.mapreduce;
+
+import java.io.IOException;
+
+import java.util.Iterator;
+import java.util.Date;
+import java.util.LinkedList;
+import java.util.Map;
+import java.util.HashMap;
+import java.util.Collections;
+import java.util.concurrent.ConcurrentHashMap;
+
+import org.apache.hadoop.mapreduce.Reducer;
+
+import org.apache.hadoop.io.WritableComparable;
+import org.apache.hadoop.io.Text;
+import org.apache.hadoop.io.LongWritable;
+import org.apache.hadoop.io.IntWritable;
+import org.apache.hadoop.io.FloatWritable;
+import org.apache.hadoop.io.DoubleWritable;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.io.RawComparator;
+import org.apache.hadoop.mapred.RawKeyValueIterator;
+import org.apache.hadoop.mapreduce.Counter;
+import org.apache.hadoop.mapreduce.StatusReporter;
+
+import org.apache.hadoop.mapreduce.lib.input.ApproximateLineRecordReader;
+
+import org.apache.hadoop.mapreduce.TaskAttemptID;
+import org.apache.hadoop.mapreduce.RecordWriter;
+import org.apache.hadoop.mapreduce.OutputCommitter;
+import org.apache.hadoop.util.Progressable;
+
+/**
+ * Perform multistage sampling.
+ * This reducer stores all the memory and periodically checks the quality of the results.
+ * It can decide to drop.
+ */
+public abstract class MultistageSamplingReducerIncr<KEYIN extends Text,VALUEIN,KEYOUT,VALUEOUT extends WritableComparable> extends MultistageSamplingReducer<KEYIN,VALUEIN,KEYOUT,VALUEOUT> {
+	// Detect if we get a new cluster (it would break sorting)
+	private Text prevKey;
+	
+	// Current cluster receiving data for
+	private int curCluster = 0;
+	
+	// Clusters update counters: when were the values updated.
+	private int lastCurClusterCheck = 0;
+	private long lastUpdate = 0;
+	
+	// Actual clusters (we do everything in doubles to not lose any precission)
+	private Map<Object,SparseArray<Double>> clusters; // y_i for every single key
+	
+	// Adaptive mechanisms for sampling and dropping
+	private boolean adaptiveSampling = false;
+	private boolean adaptiveDropping = false;
+	private float targetError = 1/100f;
+	
+	/**
+	 * Initialize the date structure to store all the data required for multistage sampling.
+	 */
+	@Override
+	public void setup(Context context) {
+		super.setup(context);
+		
+		if (!precise) {
+			clusters = new ConcurrentHashMap<Object,SparseArray<Double>>();
+			
+			// Adaptive mechanisms
+			Configuration conf = context.getConfiguration();
+			adaptiveDropping = conf.getBoolean("mapred.map.approximate.drop.adaptive",   false);
+			adaptiveSampling = conf.getBoolean("mapred.input.approximate.skip.adaptive", false);
+			targetError      = conf.getFloat("mapred.approximate.error.target", 1/100f); // 1%
+		}
+	}
+	
+	/**
+	 * A thread that checks periodically if there are new clusters.
+	 */
+	class CheckerThread extends Thread {
+		private Context context;
+		private boolean done = false;
+		
+		// How long without receiving keys to process
+		private static final int MIN_IDLE_SECONDS = 1; 
+		
+		public CheckerThread(Context context) {
+			this.context = context;
+		}
+		
+		/**
+		 * Periodically check if there is an update and check results.
+		 */
+		public void run() {
+			while (!done) {
+				try {
+					// Check the quality if we are 2 seconds without new data
+					boolean enoughTime = System.currentTimeMillis()-lastUpdate > MIN_IDLE_SECONDS*1000 && lastCurClusterCheck < curCluster;
+					boolean enoughDistance = curCluster > lastCurClusterCheck + 10;
+					if (lastUpdate != 0 && (enoughTime || enoughDistance) && !done) {
+						// Go ahead and check the current quality of the results
+						lastCurClusterCheck = curCluster;
+						lastUpdate = 0;
+						checkQuality(context, false);
+						lastUpdate = 0;
+					}
+					Thread.sleep(1*1000);
+				} catch(Exception e) {
+					System.err.println("Error checking results: " + e);
+					e.printStackTrace();
+				}
+			}
+		}
+		
+		/**
+		 * We are done.
+		 */
+		public void setDone() {
+			done = true;
+		}
+	}
+	
+	/**
+	 * This is a wrapper for Context that gets values and clusters them if required.
+	 */
+	public class ClusteringContextIncr extends ClusteringContext {
+		public ClusteringContextIncr(Context context) throws IOException, InterruptedException {
+			super(context);
+		}
+	
+		/**
+		 * Overwrite of regular write() to capture values and do clustering if needed. If we run precise, pass it to the actual context.
+		 */
+		@Override
+		public void write(KEYOUT key, VALUEOUT value) throws IOException,InterruptedException {
+			// For precise, we just forward it to the regular context
+			// In this implementation, the marks are always Strings
+			if (isPrecise() || !(key instanceof Text)) {
+				context.write(key, value);
+			// If we don't want precise, we save for multistage sampling
+			} else {
+				// We have to convert the writtable methods into numbers
+				Double res = 0.0;
+				if (value instanceof LongWritable) {
+					res = new Double(((LongWritable) value).get());
+				} else if (value instanceof IntWritable) {
+					res = new Double(((IntWritable) value).get());
+				} else if (value instanceof DoubleWritable) {
+					res = new Double(((DoubleWritable) value).get());
+				} else if (value instanceof FloatWritable) {
+					res = new Double(((FloatWritable) value).get());
+				}
+				
+				// Extract the actual key
+				String keyStr = ((Text) key).toString();
+				
+				// Parameters start by '\0' to be the first when sorting
+				if (keyStr.charAt(0) == MARK_PARAM) {
+					// m_i
+					if (keyStr.matches(MultistageSamplingHelper.formatToMatch(M_SAMPLED))) {
+						// Save the value
+						m[curCluster] = res.longValue();
+						// Update values depending on m
+						updateNValues();
+						updateMValues(curCluster);
+					// M_i/m_i
+					} else if (keyStr.matches(MultistageSamplingHelper.formatToMatch(Mm_SAMPLED))) {
+						// Save the value
+						Mm[curCluster] = res.longValue();
+						// Update values depending on m
+						updateMValues(curCluster);
+System.out.println("M/m[" + curCluster + "]=" + Mm[curCluster]);
+					// yt_i
+					} else if (keyStr.matches(MultistageSamplingHelper.formatToMatch(T_SAMPLED))) {
+						yt[curCluster] = res.longValue();
+					// standard deviation
+					} else if (keyStr.matches(MultistageSamplingHelper.formatToMatch(S_SAMPLED))) {
+						s2[curCluster] = res.longValue();
+					// Other parameters (marks mainly)
+					} else {
+						// We don't do anything with the marks, they are just... marks
+					}
+				// Add to the proper cluster to approximate the results
+				} else {
+					// Add the value for that cluster
+					SparseArray<Double> cluster = clusters.get(keyStr);
+					if (cluster == null) {
+						// We have to create the cluster structure for a non-existing key
+						cluster = new SparseArray<Double>(N);
+						clusters.put(keyStr, cluster);
+					}
+					cluster.set(curCluster, res);
+				}
+			}
+		}
+	}
+	
+	/**
+	 * Reduce runner that uses a thread to check the results.
+	 * We overwrite the super-class method so we don't have to worry about saving results
+	 */
+	@Override
+	public void run(Context context) throws IOException, InterruptedException {
+		setup(context);
+		
+		// Precise
+		if (isPrecise()) {
+			while (context.nextKey()) {
+				KEYIN key = context.getCurrentKey();
+				reduce(key, context.getValues(), context);
+			}
+		// Sampling
+		} else {
+			// Start the checker thread
+			CheckerThread checker = new CheckerThread(context);
+			checker.start();
+			
+			// Wrap the Context for clustering
+			Context clusteringcontext = new ClusteringContextIncr(context);
+			
+			// Reduce using approximation
+			while (context.nextKey()) {
+				// Check if we have a new map output to create a new cluster
+				KEYIN key = context.getCurrentKey();
+				// The keys within a cluster are sorted, when we break the sorting, we have a new cluster
+				if (prevKey != null && prevKey.compareTo(key) > 0) {
+					curCluster++;
+				}
+				// Set the value of the previous key
+				if (prevKey == null) {
+					prevKey = new Text();
+				}
+				((Text) prevKey).set((Text)key);
+				
+				// Calling the actual reduce
+				reduce(key, context.getValues(), clusteringcontext);
+				
+				// Keep track on when we get new keys
+				lastUpdate = System.currentTimeMillis();
+			}
+			
+			// We can stop the checker thread
+			checker.setDone();
+		}
+		// Finish checker thread
+		cleanup(context);
+	}
+	
+	/**
+	 * We perform the final sampling theory here.
+	 */
+	@Override
+	public void cleanup(Context context) throws IOException, InterruptedException {
+		// Perform the final estimation and output
+		if (!isPrecise()) {
+			// We add the NULLKEY, which would be the worst case
+			SparseArray cluster = new SparseArray<Double>(N);
+			int worstCluster = 0;
+			for (int i=0; i<N; i++) {
+				// This is the cluster that brings the highest variance
+				if (auxm3[i] > auxm3[worstCluster]) {
+					worstCluster = i;
+				}
+			}
+			cluster.set(worstCluster, 1.0);
+			clusters.put(NULLKEY, cluster);
+			
+			// Estimate and output the final results
+			checkQuality(context, true);
+		}
+	}
+	
+	/**
+	 * Check the error in the results based on the current sampling.
+	 */
+	protected void checkQuality(Context context, boolean output) throws IOException, InterruptedException {
+		long t0 = System.currentTimeMillis();
+		
+		// Current error
+		double maxAbs = 0.0;
+		double maxError = 0.0;
+		double maxErrorRel = 0.0;
+		double maxAbsVal = 0.0;
+		Object maxAbsKey = null;
+		
+		// Go over all the keys
+		Iterator it = clusters.entrySet().iterator();
+		while (it.hasNext()) {
+			// Collect the results for each cluster
+			Map.Entry<Object,SparseArray<Double>> pairs = (Map.Entry<Object,SparseArray<Double>>) it.next();
+			SparseArray<Double> cluster = pairs.getValue();
+			
+			// Estimate the result for this key
+			double[] result = estimateResult(cluster);
+			double tauhat   = result[0];
+			double interval = result[1];
+			
+			// TODO check the error according to the user requirements
+			if (tauhat > maxAbsVal) {
+			//if (interval > maxError) {
+				// Calculate the relative error for the maximum absolute one
+				maxAbs = tauhat;
+				maxError = interval;
+				maxErrorRel = interval/tauhat;
+				maxAbsVal = tauhat;
+				maxAbsKey = pairs.getKey();
+			}
+			
+			// Output the estimated result
+			if (context != null && output) {
+				Object key = pairs.getKey();
+				// Get key and transform it into writable
+				/*KEYOUT outkey;
+				if (key instanceof Long) {
+					outkey = (KEYOUT) new LongWritable((Long)key);
+				} else if (key instanceof Integer) {
+					outkey = (KEYOUT) new IntWritable((Integer)key);
+				} else if (key instanceof Double) {
+					outkey = (KEYOUT) new DoubleWritable((Double)key);
+				} else if (key instanceof Float) {
+					outkey = (KEYOUT) new FloatWritable((Float)key);
+				} else {
+					outkey = (KEYOUT) new Text((String)key);
+				}*/
+				// Right now, we onyl support marks as strings, so...
+				KEYOUT outkey = (KEYOUT) new Text((String)key);
+				
+				// NULLKEY has actually 0 estimation, the range is the important
+				if (((String)key).equals("NULLKEY")) {
+					tauhat = 0;
+				}
+				
+				// Get value and transform it into approximate writable
+				VALUEOUT outvalue;
+				if (IntWritable.class.equals(context.getOutputValueClass())) {
+					outvalue = (VALUEOUT) new ApproximateIntWritable((int) tauhat, interval);
+				} else if (LongWritable.class.equals(context.getOutputValueClass())) {
+					outvalue = (VALUEOUT) new ApproximateLongWritable((long) tauhat, interval);
+				} else {
+					outvalue = (VALUEOUT) new ApproximateLongWritable((long) tauhat, interval);
+				}
+				
+				// Actual and final output
+				context.write(outkey, outvalue);
+			}
+		}
+		
+		// We can change the sampling ratio dynamically
+		if (adaptiveSampling) {
+			System.out.println("");
+			System.out.println("TESTING ADAPTIVE SAMPLING");
+			System.out.println("=========================");
+			
+			int maxSamplingRatio = getMaximumSamplingRatio(clusters.get(maxAbsKey), targetError);
+			
+			System.out.println("Set ratio M/m=" + maxSamplingRatio);
+			System.out.println("=========================");
+			
+			// Set the new parameter for the new maps
+			//Counter counter = context.getCounter(ApproximateLineRecordReader.Sampling.RATIO);
+			Counter counter = context.getCounter("SamplingRatio", Integer.toString(context.getTaskAttemptID().getTaskID().getId()));
+			if (counter != null) {
+				counter.setValue(maxSamplingRatio);
+			}
+		}
+		
+		// We can start dropping dynamically
+		if (adaptiveDropping) {
+			if (maxErrorRel < targetError) {
+				context.setStatus("dropping");
+			} else {
+				context.setStatus("reduce");
+			}
+		}
+		
+		// Info message for debugging
+		System.out.format("%s: %.1fs %d/%d max error for \"%s\" is %s+/-%s (+/-%.2f%%) Keys=%d\n",
+			new Date(),
+			(System.currentTimeMillis()-t0)/1000.0,
+			n, N,
+			maxAbsKey,
+			MultistageSamplingHelper.toNumberString(maxAbs),
+			MultistageSamplingHelper.toNumberString(maxError),
+			maxErrorRel*100,
+			clusters.size());
+	}
+	
+	/**
+	 * Get the maximum sampling ratio to get an error.
+	 */
+	private int getMaximumSamplingRatio(SparseArray<Double> cluster, float targetError) {
+		// Calculate the average M and s^2
+		double  Mavg = 0.0;
+		int    nMavg = 0;
+		double s2avg = 0.0;
+		int   ns2avg = 0;
+		for (int i=0; i<N; i++) {
+			// Number of samples
+			if (m[i] >= 0) {
+				Mavg += m[i]*Mm[i];
+				nMavg++;
+			}
+		}
+		
+		// Get the variation parameters for the maximum value
+		double[] yhat = new double[n];
+		double var2 = 0.0;
+		
+		// This thing about splitting in two ways is very ugly but... performance my friend.
+		if (cluster.isSparse()) {
+			// Sparse matrix, so we can skip a lot of computation
+			synchronized (cluster.sparse) {
+				// Go over the positions different than 0
+				for (SparseArray<Double>.SparseNode<Double> aux : (LinkedList<SparseArray<Double>.SparseNode<Double>>) cluster.sparse.clone()) {
+					int i = aux.p;
+					if (i < n) { // We could be checking newer values, stick to what we knew at the beginning
+						double yi = aux.v;
+						// Estimate the results in each cluster
+						yhat[i] = auxm1[i] * yi;
+						// Estimate variation
+						if (s2[i] >= 0) {
+							var2 += auxm3[i] * s2[i];
+							// Calculate average value
+							s2avg += s2[i];
+							ns2avg++;
+						} else if (s2[i] == -3) {
+							var2 += auxm3[i] * (0.1*yi);
+							// Calculate average value
+							s2avg += 0.1*yi;
+							ns2avg++;
+						} else {
+							// For boolean variables
+							double pi = 1.0*yi/m[i];
+							// Variance to the secondary
+							var2 += auxm2[i] * pi * (1.0-pi);
+							// Calculate average value
+							s2avg += (m[i]/(m[i]-1.0)) * pi * (1.0-pi);
+							ns2avg++;
+						}
+					}
+				}
+			}
+		// Now we go over everybody
+		} else {
+			synchronized (cluster.array) {
+				// Go over all the positions
+				int i = 0;
+				for (Double auxyi : cluster.array) {
+					// Check if there is a value, otherwise, we can leave the default yhat[i]=0.0
+					if (auxyi != null) {
+						double yi = auxyi;
+						// Estimate the results in each cluster
+						yhat[i] = auxm1[i] * yi;
+						// Estimate variation
+						if (s2[i] >= 0) {
+							var2 += auxm3[i] * s2[i];
+							// Calculate average value
+							s2avg += s2[i];
+							ns2avg++;
+						} else if (s2[i] == -3) {
+							var2 += auxm3[i] * (0.1*yi);
+							// Calculate average value
+							s2avg += 0.1*yi;
+							ns2avg++;
+						} else {
+							// For boolean variables
+							double pi = 1.0*yi/m[i];
+							// Variance to the secondary
+							var2 += auxm2[i] * pi * (1.0-pi);
+							// Calculate average value
+							s2avg += (m[i]/(m[i]-1.0)) * pi * (1.0-pi);
+							ns2avg++;
+// System.out.println(i+" => m="+m[i]+" M="+(m[i]*Mm[i])+" s2="+((m[i]/(m[i]-1.0)) * pi * (1.0-pi)));
+						}
+					}
+					i++;
+					// If we pass this point we would checking newer values
+					if (i >= n) {
+						break;
+					}
+				}
+			}
+		}
+		// We calculate what would happen if we had everybody
+		//var2 = Nn * var2; // N/n
+		
+		// Fianlize average values calculation
+		Mavg  = Mavg/nMavg;
+		s2avg = s2avg/ns2avg;
+		
+		// Estimate the total number
+		double tauhat = Nn * MultistageSamplingHelper.sum(yhat); // N/n
+		
+		// Solve the ratio to get a maximum relative error
+		/*
+		tscore*sqrt(var2)
+		----------------- < 10%
+		      tauhat
+		
+		 M    (target*tauhat/t)^2 - var2
+		--- < -------------------------- + 1
+		 m        (1-N/n) (N-n) M s^2
+		*/
+// System.out.println("n     = "+n);
+// System.out.println("N     = "+N);
+// System.out.println("var2  = "+var2);
+// System.out.println("Mavg  = "+Mavg);
+// System.out.println("s2avg = "+s2avg);
+		
+		double auxtscore = MultistageSamplingHelper.getTScore(N-1, 0.95);
+		double maxSamplingRatio = (Math.pow(targetError*tauhat/auxtscore, 2) - var2) / ((N-n) * Mavg * s2avg) + 1;
+		
+// Follow the progress of the approximation
+double interval = auxtscore*Math.sqrt(var2/(Nn));
+System.out.format("If we continue like this, the result would be: %.2f +/- %.2f (+/- %.2f%%) < target=%.2f%%\n", tauhat, interval, 100.0*interval/tauhat, 100*targetError);
+		
+		if (maxSamplingRatio < 1) {
+System.out.println("PROBLEM!! To achieve our goal, we need to sample more than what is possible!");
+			maxSamplingRatio = 1.0;
+		}
+		
+		return (int) Math.floor(maxSamplingRatio);
+	}
+	
+	/**
+	 * Get the estimated result for the current key.
+	 * @return [tauhat, tscore*setauhat]
+	 */
+	private double[] estimateResult(SparseArray<Double> cluster) throws IOException, InterruptedException {
+		// Start estimating
+		double[] yhat = new double[n];
+		double var2 = 0.0;
+		
+		// This thing about splitting in two ways is very ugly but... performance my friend.
+		if (cluster.isSparse()) {
+			// Sparse matrix, so we can skip a lot of computation
+			synchronized (cluster.sparse) {
+				// Go over the positions different than 0
+				for (SparseArray<Double>.SparseNode<Double> aux : (LinkedList<SparseArray<Double>.SparseNode<Double>>) cluster.sparse.clone()) {
+					int i = aux.p;
+					if (i < yhat.length) { // We could be checking newer values, stick to what we knew at the beginning
+						double yi = aux.v;
+						// Estimate the results in each cluster
+						yhat[i] = auxm1[i] * yi;
+						// Estimate proportions
+						/*double yti = 0.0;
+						if (clusterT != null && clusterT[i] != null) {
+							yti = clusterT[i];
+						} else if (clusterM != null && clusterM[i] != null) {
+							yti = clusterM[i];
+						}*/
+						if (s2[i] >= 0) {
+							var2 += auxm3[i] * s2[i];
+						} else if (s2[i] == -3) {
+							var2 += auxm3[i] * (0.1*yi);
+						} else {
+							// For boolean variables
+							double pi = 1.0*yi/m[i];
+							// Variance to the secondary
+							var2 += auxm2[i] * pi * (1.0-pi);
+						}
+					}
+				}
+			}
+		// Now we go over everybody
+		} else {
+			synchronized (cluster.array) {
+				// Go over all the positions
+				int i = 0;
+				for (Double auxyi : cluster.array) {
+					// Check if there is a value, otherwise, we can leave the default yhat[i]=0.0
+					if (auxyi != null) {
+						double yi = auxyi;
+						// Estimate the results in each cluster
+						yhat[i] = auxm1[i] * yi;
+						// Estimate proportions
+						/*double yti = 0.0;
+						if (clusterT != null && clusterT[i] != null) {
+							yti = clusterT[i];
+						} else if (clusterM != null && clusterM[i] != null) {
+							yti = clusterM[i];
+						}
+						double pi = 1.0*yi/yti;
+						// Variance to the secondary
+						var2 += auxm2[i] * pi * (1.0-pi);*/
+						if (s2[i] >= 0) {
+							var2 += auxm3[i] * s2[i];
+						} else if (s2[i] == -3) {
+							var2 += auxm3[i] * (0.1*yi);
+						} else {
+							// For boolean variables
+							double pi = 1.0*yi/m[i];
+							// Variance to the secondary
+							var2 += auxm2[i] * pi * (1.0-pi);
+						
+						}
+					}
+					i++;
+					// If we pass this point we would be checking newer values
+					if (i >= yhat.length) {
+						break;
+					}
+				}
+			}
+		}
+		// This is the regular implementation but we unfolded it for performance
+		/*for (int i=0; i<n; i++) {
+			Double auxyi = cluster.get(i);
+			if (auxyi != null) {
+				// y_i
+				double yi = auxyi;
+				// yt_i
+				double yti = 0.0;
+				if (clusterT[i] != null) {
+					yti = clusterT[i];
+				} else if (clusterM[i] != null) {
+					yti = clusterM[i];
+				}
+				// Estimate the results in each cluster
+				yhat[i] = auxm1[i] * yi;
+				// Estimate proportions
+				double pi = 1.0*yi/yti;
+				// Variance to the secondary
+				var2 += auxm2[i] * pi * (1.0-pi);
+			} else {
+				// If yi==0 we can say that the rest is 0
+				yhat[i] = 0.0;
+			}
+		}*/
+		var2 = Nn * var2; // N/n
+		
+		// Estimate the total number
+		double tauhat = Nn * MultistageSamplingHelper.sum(yhat); // N/n
+		
+		// Calculate first variance variance
+		double var1 = 0.0;
+		if (n < N) {
+			var1 = NNnn * MultistageSamplingHelper.var(yhat);
+		}
+		
+		// Calculate standard error
+		double setauhat = 0.0;
+		if (var1 < 0 || var2 < 0) {
+			setauhat = Double.MAX_VALUE;
+		} else {
+			double vartauhat = var1 + var2;
+			setauhat = Math.sqrt(vartauhat);
+		}
+		
+		return new double[] {tauhat, tscore*setauhat};
+	}
+}
+
+	/**
+	 * Check the error in the results based on the current sampling.
+	 */
+	/*protected void checkQualityComplete(Context context, boolean output) throws IOException, InterruptedException {
+		long t0 = System.currentTimeMillis();
+	
+		double maxAbs = 0.0;
+		double maxError = 0.0;
+		double maxErrorRel = 0.0;
+		
+		// Multistage sampling
+		int N = numCluster;
+		int n = curCluster+1;
+		
+		// Calculate populations in each cluster
+		long[] mi = new long[n];
+		long[] Mi = new long[n];
+		for (int i=0; i<n; i++) {
+			if (clusterM[i] != null) {
+				mi[i] = clusterM[i].longValue();//clusterLines[i];
+			} else {
+				// If not defned, we add all the values
+				mi[i] = 0;
+				for (Double v : clusters.get(i).values()) {
+					mi[i] += v.longValue();
+				}
+			}
+			Mi[i] = mi[i]*skipSamples; // This is an approximation based on the sampling ratio
+		}
+		
+		// Get the t-score for the current distribution
+		double tscore = getTScore(n-1, 0.95);
+		
+		// Go over all the keys
+		for (Object key : clusters.keySet()) {
+			// Collect the results for each cluster
+			double[] yi  = new double[n];
+			double[] yti = new double[n];
+			for (int i=0; i<n; i++) {
+				yi[i] = 0;
+				try {
+					yi[i] = clusters.get(key).get(i);
+				} catch(Exception e) {}
+				// y_t
+				if (clusterT[i] != null) {
+					yti[i] = clusterT[i];
+				} else {
+					yti[i] = clusterM[i];
+				}
+			}
+
+			// Estimate the results in each cluster
+			double[] yhat = new double[n];
+			for (int i=0; i<n; i++) {
+				yhat[i] = (1.0*Mi[i]/mi[i])* yi[i];
+			}
+			
+			// Estimate the total number
+			double tauhat = (1.0*N/n)*sum(yhat);
+			
+			// Calculate the errors
+			// Estimate the global deviation
+			double su2 = var(yhat);
+		
+			// Calculate proportions
+			double[] pi = new double[n];
+			for (int i=0; i<n; i++) {
+				pi[i] = 1.0*yi[i]/yti[i];
+			}
+			
+			// Estimate variance in primary
+			double[] si2 = new double[n];
+			for (int i=0; i<n; i++) {
+				si2[i] = (1.0*mi[i]/(mi[i]-1.0)) * pi[i] * (1.0-pi[i]);
+			}
+			
+			// Calculate total variance
+			double var1 = 1.0*(N*(N-n)*su2)/n;
+			if (n == N) {
+				var1 = 0.0;
+			}
+			double var2 = 0.0;
+			for (int i=0; i<n; i++) {
+				var2 += 1.0*(Mi[i]*(Mi[i]-mi[i])*si2[i])/mi[i];
+			}
+			var2 = (1.0*N/n)*var2;
+			
+			double vartauhat = var1 + var2;
+			Double setauhat = Math.sqrt(vartauhat);
+			if (setauhat.isNaN()) { 
+				setauhat = Double.MAX_VALUE;
+			}
+			
+			// TODO select error according to the user requirements
+			// Calculate the maximum relative error
+			if (tscore*setauhat > maxError) {
+				maxAbs = tauhat;
+				maxError = tscore*setauhat;
+				maxErrorRel = 100.0*tscore*setauhat/tauhat;
+			}
+			
+			// Output the estimation
+			if (output) {
+				// Get key and transform it into writable
+				KEYOUT outkey;
+				if (key instanceof Long) {
+					outkey = (KEYOUT) new LongWritable((Long)key);
+				} else if (key instanceof Integer) {
+					outkey = (KEYOUT) new IntWritable((Integer)key);
+				} else if (key instanceof Double) {
+					outkey = (KEYOUT) new DoubleWritable((Double)key);
+				} else if (key instanceof Float) {
+					outkey = (KEYOUT) new FloatWritable((Float)key);
+				} else {
+					outkey = (KEYOUT) new Text((String)key);
+				}
+				
+				// Get value and transform it into approximate writable
+				VALUEOUT outvalue;
+				if (IntWritable.class.equals(context.getOutputValueClass())) {
+					outvalue = (VALUEOUT) new ApproximateIntWritable((int) tauhat, tscore*setauhat);
+				} else if (LongWritable.class.equals(context.getOutputValueClass())) {
+					outvalue = (VALUEOUT) new ApproximateLongWritable((long) tauhat, tscore*setauhat);
+				} else {
+					outvalue = (VALUEOUT) new ApproximateLongWritable((long) tauhat, tscore*setauhat);
+				}
+				
+				// Actual output
+				context.write(outkey, outvalue);
+			}
+		}
+		// Info message
+		System.out.format("%s: %.1fs %d/%d max error %s+/-%s (+/-%.2f%%) Keys=%d\n", new Date(), (System.currentTimeMillis()-t0)/1000.0, n, N, toNumberString(maxAbs), toNumberString(maxError), maxErrorRel, clusters.size());
+	}*/
Index: src/contrib/wikipedia/src/java/org/apache/hadoop/mapreduce/MultistageSamplingReducerIncr.java.bak
===================================================================
--- src/contrib/wikipedia/src/java/org/apache/hadoop/mapreduce/MultistageSamplingReducerIncr.java.bak	(revision 0)
+++ src/contrib/wikipedia/src/java/org/apache/hadoop/mapreduce/MultistageSamplingReducerIncr.java.bak	(working copy)
@@ -0,0 +1,684 @@
+package org.apache.hadoop.mapred;
+
+import java.io.IOException;
+
+import java.util.Iterator;
+import java.util.Date;
+import java.util.LinkedList;
+import java.util.Map;
+import java.util.HashMap;
+import java.util.Collections;
+import java.util.concurrent.ConcurrentHashMap;
+
+import org.apache.hadoop.mapreduce.Reducer;
+
+import org.apache.hadoop.io.WritableComparable;
+import org.apache.hadoop.io.Text;
+import org.apache.hadoop.io.LongWritable;
+import org.apache.hadoop.io.IntWritable;
+import org.apache.hadoop.io.FloatWritable;
+import org.apache.hadoop.io.DoubleWritable;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.io.RawComparator;
+import org.apache.hadoop.mapred.RawKeyValueIterator;
+import org.apache.hadoop.mapreduce.Counter;
+import org.apache.hadoop.mapreduce.StatusReporter;
+
+import org.apache.hadoop.mapreduce.TaskAttemptID;
+import org.apache.hadoop.mapreduce.RecordWriter;
+import org.apache.hadoop.mapreduce.OutputCommitter;
+import org.apache.hadoop.util.Progressable;
+
+/**
+ * Perform multistage sampling.
+ * It can decide to drop.
+ */
+public abstract class MultistageSamplingReducerIncr<KEYIN extends WritableComparable,VALUEIN,KEYOUT,VALUEOUT extends WritableComparable> extends MultistageSamplingReducer<KEYIN,VALUEIN,KEYOUT,VALUEOUT> {
+	// Detect if we get a new cluster (break sorting)
+	private WritableComparable prevKey;
+	
+	// Clusters
+	private int curCluster = 0;
+	private int numCluster = 0;
+	private int lastCurClusterCheck = 0;
+	
+	// Parameters to estimate results based on sampling
+	private int skipSamples = 0; // Mi/mi
+	
+	// Actual clusters (we do everything in doubles to not lose precission)
+	private Map<Object,SparseArray<Double>> clusters; // y_i
+	private Double[] clusterM; // m_i
+	private Double[] clusterT; // yt_i
+	private Double[] clusterS; // s_i^2
+	
+	// Checker variables
+	private long lastUpdate = 0;
+	private boolean precise = false;
+
+// 	// Formats that the multistage clustering reducer recognizes
+// 	public static final String M_SAMPLED = "\0M%d-%d"; // mapId, reduceId
+// 	public static final String T_SAMPLED = "\0T%d-%d"; // mapId, reduceId
+// 	public static final String S_SAMPLED = "\0S%d-%d"; // Standard deviation: mapId, reduceId
+// 	// We need two additional marks to guarrantee we identify clusters properly, the name doesn't really matter (neither the order)
+// 	public static final String CLUSTERINI = "\0INI%d-%d"; // mapId, reduceId
+// 	public static final String CLUSTERFIN = "\0FIN%d-%d"; // mapId, reduceId
+	
+	/**
+	 * A thread that checks regularly if there are new clusters.
+	 */
+	class CheckerThread extends Thread {
+		private Context context;
+		private boolean done = false;
+		
+		private static final int MIN_IDLE_SECONDS = 1; 
+		
+		public CheckerThread(Context context) {
+			this.context = context;
+		}
+		
+		/**
+		 * Periodically check if there is an update and check results.
+		 */
+		public void run() {
+			while (!done) {
+				try {
+					// Check the quality if we are 2 seconds without new data
+					boolean enoughTime = System.currentTimeMillis()-lastUpdate > MIN_IDLE_SECONDS*1000 && lastCurClusterCheck < curCluster;
+					boolean enoughDistance = curCluster > lastCurClusterCheck + 10;
+					if (lastUpdate != 0 && (enoughTime || enoughDistance) && !done) {
+						lastCurClusterCheck = curCluster;
+						lastUpdate = 0;
+						checkQuality();
+						lastUpdate = 0;
+					}
+					Thread.sleep(1*1000);
+				} catch(Exception e) {
+					System.err.println("Error checking results: " + e);
+					e.printStackTrace();
+				}
+			}
+		}
+		
+		/**
+		 * We are done.
+		 */
+		public void setDone() {
+			done = true;
+		}
+	}
+	
+	/**
+	 * This is a wrapper for Context that gets values and clusters them if required.
+	 */
+	public class ClusteringContext extends Context {
+		Context context;
+		
+		public ClusteringContext(Context context) throws IOException, InterruptedException {
+			// This is just a wrapper, so we don't create anything
+			super(context.getConfiguration(), context.getTaskAttemptID(), null, null, null, null, context.getOutputCommitter(), null, (RawComparator<KEYIN>) context.getSortComparator(), (Class<KEYIN>) context.getMapOutputKeyClass(), (Class<VALUEIN>) context.getMapOutputValueClass());
+
+			// Save the context
+			this.context = context;
+		}
+		
+		/**
+		 * Overwrite of regular write() to capture values and do clustering if needed. If we run precise, pass it to the actual context.
+		 */
+		public void write(KEYOUT key, VALUEOUT value) throws IOException,InterruptedException {
+			// For precise, we just forward it to the regular context
+			if (isPrecise()) {
+				context.write(key, value);
+			// If we don't want precise, we save for multistage sampling
+			} else {
+				// We have to convert the writtable methods into numbers
+				Double res = 0.0;
+				if (value instanceof LongWritable) {
+					res = new Double(((LongWritable) value).get());
+				} else if (value instanceof IntWritable) {
+					res = new Double(((IntWritable) value).get());
+				} else if (value instanceof DoubleWritable) {
+					res = new Double(((DoubleWritable) value).get());
+				} else if (value instanceof FloatWritable) {
+					res = new Double(((FloatWritable) value).get());
+				}
+				
+				// Check for parameters on multistage clustering
+				boolean parameter = false;
+				Object outkey = null;
+				if (key instanceof Text) {
+					String textkey = ((Text) key).toString();
+					// Parameters start by '\0' to be the first when sorting
+					if (textkey.charAt(0)=='\0') {
+						// m_i parameter
+						if (textkey.matches(MultistageSamplingHelper.formatToMatch(M_SAMPLED))) {
+							if (clusterM == null) {
+								clusterM = new Double[numCluster];
+							}
+							clusterM[curCluster] = res;
+							parameter = true;
+						// yt_i parameter
+						} else if (textkey.matches(MultistageSamplingHelper.formatToMatch(T_SAMPLED))) {
+							if (clusterT == null) {
+								clusterT = new Double[numCluster];
+							}
+							clusterT[curCluster] = res;
+							parameter = true;
+						// yt_i parameter
+						} else if (textkey.matches(MultistageSamplingHelper.formatToMatch(S_SAMPLED))) {
+							if (clusterS == null) {
+								clusterS = new Double[numCluster];
+							}
+							clusterS[curCluster] = res;
+							parameter = true;
+						// Any other parameter
+						} else if (textkey.matches(MultistageSamplingHelper.formatToMatch(CLUSTERINI)) || textkey.matches(MultistageSamplingHelper.formatToMatch(CLUSTERFIN))) {
+							parameter = true;
+						}
+					}
+					// As we've already checked this for the parameters, we don't need to do it again
+					outkey = textkey;
+				}
+				
+				// Add to the proper cluster to approximate the results
+				if (!parameter) {
+					// Save key for storage into the cluster format
+					if (outkey == null) {
+						if (key instanceof LongWritable) {
+							outkey = ((LongWritable) key).get();
+						} else if (key instanceof IntWritable) {
+							outkey = ((IntWritable) key).get();
+						} else if (key instanceof DoubleWritable) {
+							outkey = ((DoubleWritable) key).get();
+						} else if (key instanceof FloatWritable) {
+							outkey = ((FloatWritable) key).get();
+						}
+					}
+					
+					// Add the value for that cluster
+					SparseArray<Double> cluster = clusters.get(outkey);
+					if (cluster == null) {
+						// We have to create the cluster structure for a non-existing key
+						cluster = new SparseArray<Double>(numCluster);
+						clusters.put(outkey, cluster);
+					}
+					cluster.set(curCluster, res);
+				}
+			}
+		}
+
+		// We overwrite these method to avoid problems, ideally we would forward everything
+		public void getfirst() throws IOException { }
+		
+		public float getProgress() {
+			return context.getProgress();
+		}
+		
+		public void progress() {
+			context.progress();
+		}
+		
+		public void setStatus(String status) {
+			context.setStatus(status);
+		}
+		
+		public Counter getCounter(Enum<?> counterName) {
+			return context.getCounter(counterName);
+		}
+		
+		public Counter getCounter(String groupName, String counterName) {
+			return context.getCounter(groupName, counterName);
+		}
+	}
+	
+	/**
+	 * Check if we are precise.
+	 */
+	public void setup(Context context) {
+		precise = context.getConfiguration().getBoolean("mapred.job.precise", false);
+		
+		if (!precise) {
+			numCluster  = context.getConfiguration().getInt("mapred.map.tasks", -1);
+			skipSamples = context.getConfiguration().getInt("mapred.input.approximate.skip", 1);
+			
+			clusters = new ConcurrentHashMap<Object,SparseArray<Double>>();
+		}
+	}
+	
+	/**
+	 * Reduce runner that uses a thread to check the results.
+	 */
+	public void run(Context context) throws IOException, InterruptedException {
+		setup(context);
+		
+		// Wrap the Context for clustering
+		Context clusteringcontext = new ClusteringContext(context);
+		
+		// Precise
+		if (isPrecise()) {
+			while (context.nextKey()) {
+				KEYIN key = context.getCurrentKey();
+				reduce(key, context.getValues(), context);
+			}
+		// Sampling
+		} else {
+			// Start the checker thread
+			CheckerThread checker = new CheckerThread(context);
+			checker.start();
+			
+			// Reduce using approximation
+			while (context.nextKey()) {
+				// Check if we have a new map output to create a new cluster
+				KEYIN key = context.getCurrentKey();
+				// The keys within a cluster are sorted, when we break the sorting, we have a new cluster
+				if (prevKey != null && prevKey.compareTo(key) > 0) {
+					curCluster++;
+				}
+				// Set the value of the previous key
+				if (key instanceof Text) {
+					if (prevKey == null) {
+						prevKey = new Text();
+					}
+					((Text) prevKey).set((Text)key);
+				} else if (key instanceof LongWritable) {
+					if (prevKey == null) {
+						prevKey = new LongWritable();
+					}
+					((LongWritable) prevKey).set(((LongWritable)key).get());
+				} else {
+					prevKey = key;
+				}
+				
+				// Calling the actual reduce
+				reduce(key, context.getValues(), clusteringcontext);
+				
+				// Keep track on when we get new keys
+				lastUpdate = System.currentTimeMillis();
+			}
+			
+			// We can stop the checker thread
+			checker.setDone();
+		}
+		// Finish checker thread
+		cleanup(context);
+	}
+	
+	/**
+	 * We perform the final sampling theory here.
+	 */
+	protected void cleanup(Context context) throws IOException, InterruptedException {
+		// Perform the final estimation and output
+		if (!isPrecise()) {
+			checkQuality(context);
+		}
+	}
+	
+	/**
+	 * This check doesn't write output.
+	 */
+	protected void checkQuality() throws IOException, InterruptedException {
+		checkQuality(null);
+	}
+	
+	/**
+	 * Check the error in the results based on the current sampling.
+	 */
+	protected void checkQuality(Context context) throws IOException, InterruptedException {
+		long t0 = System.currentTimeMillis();
+	
+		double maxAbs = 0.0;
+		double maxError = 0.0;
+		double maxErrorRel = 0.0;
+		Object maxAbsKey = null;
+		
+		// Multistage sampling
+		int N = numCluster;
+		int n = curCluster+1;
+		// Precomputed operations
+		double Nn  = 1.0*N/n;
+		double NNnn = 1.0*N*(N-n)/n;
+		
+		// Calculate populations in each cluster
+		//long[] mi = new long[n];
+		//long[] Mi = new long[n];
+		double[] auxmi1 = new double[n];
+		double[] auxmi2 = new double[n];
+		double[] auxmi3 = new double[n];
+		for (int i=0; i<n; i++) {
+			long mi = 0;
+			if (clusterM[i] != null) {
+				mi = clusterM[i].longValue();//clusterLines[i];
+			}
+			long Mi = mi*skipSamples; // This is an approximation based on the sampling ratio
+			
+			// Precomputed operations
+			auxmi1[i] = 1.0 * skipSamples; // 1.0 * Mi / mi;
+			auxmi2[i] = 1.0 * (Mi*(Mi-mi)) / (mi-1.0);
+			auxmi3[i] = 1.0 * (Mi*(Mi-mi)) / mi;
+		}
+		
+		// Get the t-score for the current distribution
+		double tscore = MultistageSamplingHelper.getTScore(n-1, 0.95);
+		
+		// Go over all the keys
+		Iterator it = clusters.entrySet().iterator();
+		while (it.hasNext()) {
+			// Collect the results for each cluster
+			Map.Entry<Object,SparseArray<Double>> pairs = (Map.Entry<Object,SparseArray<Double>>) it.next();
+			SparseArray<Double> cluster = pairs.getValue();
+			
+			// Start estimating
+			double[] yhati = new double[n];
+			double var2 = 0.0;
+			
+			// This thing about splitting in two ways is very ugly but... performance my friend.
+			if (cluster.isSparse()) {
+				// Sparse matrix, so we can skip a lot of computation
+				synchronized (cluster.sparse) {
+					// Go over the positions different than 0
+					for (SparseArray<Double>.SparseNode<Double> aux : (LinkedList<SparseArray<Double>.SparseNode<Double>>) cluster.sparse.clone()) {
+						int i = aux.p;
+						if (i < n) { // We could be checking newer values, stick to what we knew at the beginning
+							double yi = aux.v;
+							// Estimate the results in each cluster
+							yhati[i] = auxmi1[i] * yi;
+							// Estimate proportions
+							/*double yti = 0.0;
+							if (clusterT != null && clusterT[i] != null) {
+								yti = clusterT[i];
+							} else if (clusterM != null && clusterM[i] != null) {
+								yti = clusterM[i];
+							}*/
+							if (clusterS[i] >= 0) {
+								var2 += auxmi3[i] * clusterS[i];
+							} else {
+								// For boolean variables
+								double pi = 1.0*yi/clusterM[i];
+								// Variance to the secondary
+								var2 += auxmi2[i] * pi * (1.0-pi);
+							
+							}
+						}
+					}
+				}
+			// Now we go over everybody
+			} else {
+				synchronized (cluster.array) {
+					// Go over all the positions
+					int i = 0;
+					for (Double auxyi : cluster.array) {
+						// Check if there is a value, otherwise, we can leave the default yhati[i]=0.0
+						if (auxyi != null) {
+							double yi = auxyi;
+							// Estimate the results in each cluster
+							yhati[i] = auxmi1[i] * yi;
+							// Estimate proportions
+							/*double yti = 0.0;
+							if (clusterT != null && clusterT[i] != null) {
+								yti = clusterT[i];
+							} else if (clusterM != null && clusterM[i] != null) {
+								yti = clusterM[i];
+							}
+							double pi = 1.0*yi/yti;
+							// Variance to the secondary
+							var2 += auxmi2[i] * pi * (1.0-pi);*/
+							if (clusterS[i] >= 0) {
+								var2 += auxmi3[i] * clusterS[i];
+							} else {
+								// For boolean variables
+								double pi = 1.0*yi/clusterM[i];
+								// Variance to the secondary
+								var2 += auxmi2[i] * pi * (1.0-pi);
+							
+							}
+						}
+						i++;
+						// If we pass this point we would checking newer values
+						if (i >= n) {
+							break;
+						}
+					}
+				}
+			}
+			// This is the regular implementation but we unfolded for performance
+			/*for (int i=0; i<n; i++) {
+				Double auxyi = cluster.get(i);
+				if (auxyi != null) {
+					// y_i
+					double yi = auxyi;
+					// yt_i
+					double yti = 0.0;
+					if (clusterT[i] != null) {
+						yti = clusterT[i];
+					} else if (clusterM[i] != null) {
+						yti = clusterM[i];
+					}
+					// Estimate the results in each cluster
+					yhati[i] = auxmi1[i] * yi;
+					// Estimate proportions
+					double pi = 1.0*yi/yti;
+					// Variance to the secondary
+					var2 += auxmi2[i] * pi * (1.0-pi);
+				} else {
+					// If yi==0 we can say that the rest is 0
+					yhati[i] = 0.0;
+				}
+			}*/
+			var2 = Nn * var2; // N/n
+			
+			// Estimate the total number
+			double tauhat = Nn * MultistageSamplingHelper.sum(yhati); // N/n
+			
+			
+			// Calculate total variance and standard error
+			// The regular way to do this is:
+			double var1 = 0.0;
+			if (n < N) {
+				var1 = MultistageSamplingHelper.var(yhati)*NNnn;
+			}
+			double vartauhat = var1 + var2;
+			Double setauhat = Math.sqrt(vartauhat);
+			
+			// Check if we are not in a weird point
+			if (setauhat.isNaN()) { 
+				setauhat = Double.MAX_VALUE;
+			}
+			
+			// TODO select error according to the user requirements
+			// Calculate the maximum relative error
+			if (tscore*setauhat > maxError) {
+				maxAbs = tauhat;
+				maxError = tscore*setauhat;
+				maxErrorRel = 100.0*tscore*setauhat/tauhat;
+				maxAbsKey = pairs.getKey();
+			}
+			
+			// Output the estimated result
+			if (context != null) {
+				Object key = pairs.getKey();
+				// Get key and transform it into writable
+				KEYOUT outkey;
+				if (key instanceof Long) {
+					outkey = (KEYOUT) new LongWritable((Long)key);
+				} else if (key instanceof Integer) {
+					outkey = (KEYOUT) new IntWritable((Integer)key);
+				} else if (key instanceof Double) {
+					outkey = (KEYOUT) new DoubleWritable((Double)key);
+				} else if (key instanceof Float) {
+					outkey = (KEYOUT) new FloatWritable((Float)key);
+				} else {
+					outkey = (KEYOUT) new Text((String)key);
+				}
+				
+				// Get value and transform it into approximate writable
+				VALUEOUT outvalue;
+				if (IntWritable.class.equals(context.getOutputValueClass())) {
+					outvalue = (VALUEOUT) new ApproximateIntWritable((int) tauhat, tscore*setauhat);
+				} else if (LongWritable.class.equals(context.getOutputValueClass())) {
+					outvalue = (VALUEOUT) new ApproximateLongWritable((long) tauhat, tscore*setauhat);
+				} else {
+					outvalue = (VALUEOUT) new ApproximateLongWritable((long) tauhat, tscore*setauhat);
+				}
+				
+				// Actual and final output
+				context.write(outkey, outvalue);
+			}
+		}
+		
+		// Null key: keys that does not show up
+		// tauhat = 0
+		// var1 = 0
+		// var2 = 0
+		
+		
+		// Info message
+		System.out.format("%s: %.1fs %d/%d max error for \"%s\" is %s+/-%s (+/-%.2f%%) Keys=%d\n", new Date(), (System.currentTimeMillis()-t0)/1000.0, n, N, maxAbsKey, MultistageSamplingHelper.toNumberString(maxAbs), MultistageSamplingHelper.toNumberString(maxError), maxErrorRel, clusters.size());
+	}
+	
+	/**
+	 * Check the error in the results based on the current sampling.
+	 */
+	/*protected void checkQualityComplete(Context context, boolean output) throws IOException, InterruptedException {
+		long t0 = System.currentTimeMillis();
+	
+		double maxAbs = 0.0;
+		double maxError = 0.0;
+		double maxErrorRel = 0.0;
+		
+		// Multistage sampling
+		int N = numCluster;
+		int n = curCluster+1;
+		
+		// Calculate populations in each cluster
+		long[] mi = new long[n];
+		long[] Mi = new long[n];
+		for (int i=0; i<n; i++) {
+			if (clusterM[i] != null) {
+				mi[i] = clusterM[i].longValue();//clusterLines[i];
+			} else {
+				// If not defned, we add all the values
+				mi[i] = 0;
+				for (Double v : clusters.get(i).values()) {
+					mi[i] += v.longValue();
+				}
+			}
+			Mi[i] = mi[i]*skipSamples; // This is an approximation based on the sampling ratio
+		}
+		
+		// Get the t-score for the current distribution
+		double tscore = getTScore(n-1, 0.95);
+		
+		// Go over all the keys
+		for (Object key : clusters.keySet()) {
+			// Collect the results for each cluster
+			double[] yi  = new double[n];
+			double[] yti = new double[n];
+			for (int i=0; i<n; i++) {
+				yi[i] = 0;
+				try {
+					yi[i] = clusters.get(key).get(i);
+				} catch(Exception e) {}
+				// y_t
+				if (clusterT[i] != null) {
+					yti[i] = clusterT[i];
+				} else {
+					yti[i] = clusterM[i];
+				}
+			}
+
+			// Estimate the results in each cluster
+			double[] yhati = new double[n];
+			for (int i=0; i<n; i++) {
+				yhati[i] = (1.0*Mi[i]/mi[i])* yi[i];
+			}
+			
+			// Estimate the total number
+			double tauhat = (1.0*N/n)*sum(yhati);
+			
+			// Calculate the errors
+			// Estimate the global deviation
+			double su2 = var(yhati);
+		
+			// Calculate proportions
+			double[] pi = new double[n];
+			for (int i=0; i<n; i++) {
+				pi[i] = 1.0*yi[i]/yti[i];
+			}
+			
+			// Estimate variance in primary
+			double[] si2 = new double[n];
+			for (int i=0; i<n; i++) {
+				si2[i] = (1.0*mi[i]/(mi[i]-1.0)) * pi[i] * (1.0-pi[i]);
+			}
+			
+			// Calculate total variance
+			double var1 = 1.0*(N*(N-n)*su2)/n;
+			if (n == N) {
+				var1 = 0.0;
+			}
+			double var2 = 0.0;
+			for (int i=0; i<n; i++) {
+				var2 += 1.0*(Mi[i]*(Mi[i]-mi[i])*si2[i])/mi[i];
+			}
+			var2 = (1.0*N/n)*var2;
+			
+			double vartauhat = var1 + var2;
+			Double setauhat = Math.sqrt(vartauhat);
+			if (setauhat.isNaN()) { 
+				setauhat = Double.MAX_VALUE;
+			}
+			
+			// TODO select error according to the user requirements
+			// Calculate the maximum relative error
+			if (tscore*setauhat > maxError) {
+				maxAbs = tauhat;
+				maxError = tscore*setauhat;
+				maxErrorRel = 100.0*tscore*setauhat/tauhat;
+			}
+			
+			// Output the estimation
+			if (output) {
+				// Get key and transform it into writable
+				KEYOUT outkey;
+				if (key instanceof Long) {
+					outkey = (KEYOUT) new LongWritable((Long)key);
+				} else if (key instanceof Integer) {
+					outkey = (KEYOUT) new IntWritable((Integer)key);
+				} else if (key instanceof Double) {
+					outkey = (KEYOUT) new DoubleWritable((Double)key);
+				} else if (key instanceof Float) {
+					outkey = (KEYOUT) new FloatWritable((Float)key);
+				} else {
+					outkey = (KEYOUT) new Text((String)key);
+				}
+				
+				// Get value and transform it into approximate writable
+				VALUEOUT outvalue;
+				if (IntWritable.class.equals(context.getOutputValueClass())) {
+					outvalue = (VALUEOUT) new ApproximateIntWritable((int) tauhat, tscore*setauhat);
+				} else if (LongWritable.class.equals(context.getOutputValueClass())) {
+					outvalue = (VALUEOUT) new ApproximateLongWritable((long) tauhat, tscore*setauhat);
+				} else {
+					outvalue = (VALUEOUT) new ApproximateLongWritable((long) tauhat, tscore*setauhat);
+				}
+				
+				// Actual output
+				context.write(outkey, outvalue);
+			}
+		}
+		// Info message
+		System.out.format("%s: %.1fs %d/%d max error %s+/-%s (+/-%.2f%%) Keys=%d\n", new Date(), (System.currentTimeMillis()-t0)/1000.0, n, N, toNumberString(maxAbs), toNumberString(maxError), maxErrorRel, clusters.size());
+	}*/
+	
+	
+	/**
+	 * Check if we run the job precisely.
+	 */
+	public void setPrecise() {
+		this.precise = true;
+	}
+	
+	public boolean isPrecise() {
+		return this.precise;
+	}
+}
Index: src/contrib/wikipedia/src/java/org/apache/hadoop/mapreduce/ParameterPartitioner.java
===================================================================
--- src/contrib/wikipedia/src/java/org/apache/hadoop/mapreduce/ParameterPartitioner.java	(revision 0)
+++ src/contrib/wikipedia/src/java/org/apache/hadoop/mapreduce/ParameterPartitioner.java	(working copy)
@@ -0,0 +1,29 @@
+package org.apache.hadoop.mapreduce;
+
+import org.apache.hadoop.io.Text;
+import org.apache.hadoop.io.IntWritable;
+
+import org.apache.hadoop.mapreduce.lib.partition.HashPartitioner;
+
+/**
+ * A partitioner that checks if we have a parameter and send it to the specified one. \0PARAMETER-1-1 -> 1
+ */
+public class ParameterPartitioner<K,V> extends HashPartitioner<K,V> {
+	/**
+	 * Overwrite the partitioner to send to everybody.
+	 */
+	@Override
+	public int getPartition(K key, V value, int numReduceTasks) {
+		if (key instanceof Text) {
+			String aux = ((Text)key).toString();
+			
+			// Check if it's a parameter, it has the \0 first to guarantee is the first when sorting
+			int lastIndex = aux.lastIndexOf('-');
+			if (aux.charAt(0) == '\0' && lastIndex>0) {
+				// We send it to the reducer specified in the parameter
+				return Integer.parseInt(aux.substring(lastIndex+1)) % numReduceTasks;
+			}
+		}
+		return super.getPartition(key, value, numReduceTasks);
+	}
+}
Index: src/contrib/wikipedia/src/java/org/apache/hadoop/mapreduce/SparseArray.java
===================================================================
--- src/contrib/wikipedia/src/java/org/apache/hadoop/mapreduce/SparseArray.java	(revision 0)
+++ src/contrib/wikipedia/src/java/org/apache/hadoop/mapreduce/SparseArray.java	(working copy)
@@ -0,0 +1,216 @@
+package org.apache.hadoop.mapreduce;
+
+import java.util.Collection;
+import java.util.ArrayList;
+import java.util.LinkedList;
+
+import java.io.File;
+import java.io.Serializable;
+import java.io.IOException;
+import java.io.ObjectStreamException;
+import java.io.ObjectOutputStream;
+import java.io.ObjectInputStream;
+import java.io.FileOutputStream;
+import java.io.FileInputStream;
+
+
+/**
+ * Array supporting sparse values (lots of 0/null)
+ */
+public class SparseArray<E> implements Serializable {
+	/**
+	* p -> v
+	*/
+	public class SparseNode<E> implements Serializable {
+		public int p;
+		public E v;
+		
+		public SparseNode(int p, E v) {
+			this.p = p;
+			this.v = v;
+		}
+	}
+	
+	// Size of the array.
+	public int size;
+
+	// Compressed: p->v, p->v
+	public LinkedList<SparseNode<E>> sparse;
+	
+	// Expanded
+	public ArrayList<E> array; // we keep it null until expanding
+	
+	public SparseArray(int size) {
+		this.size = size;
+		// We start sparse
+		this.sparse = new LinkedList<SparseNode<E>>();
+	}
+	
+	/**
+	 * Get element p from the array.
+	 */
+	public E get(int p) {
+		if (p >= size) {
+			throw new IndexOutOfBoundsException();
+		}
+		// Sparse
+		if (array == null) {
+			// Linear search because this is supposed to be small
+			for (int i=0; i<sparse.size(); i++) {
+				if (sparse.get(i).p == p) {
+					return sparse.get(i).v;
+				}
+			}
+			return null;
+		// Complete
+		} else {
+			return array.get(p);
+		}
+	}
+	
+	/**
+	 * Set element p in the array.
+	 */
+	public void set(int p, E v) {
+		if (p >= size) {
+			throw new IndexOutOfBoundsException();
+		}
+		this.checkMaxSize();
+		// Sparse array
+		if (array == null) {
+			// Change it
+			boolean found = false;
+			for (int i=0; i<this.sparse.size(); i++) {
+				if (this.sparse.get(i).p == p) {
+					this.sparse.get(i).v = v;
+					found = true;
+					break;
+				}
+			}
+			// Add it
+			if (!found) {
+				this.sparse.addLast(new SparseNode<E>(p, v));
+			}
+		// Complete array
+		} else {
+			array.set(p, v);
+		}
+	}
+	
+	/**
+	 * Check if the array is actually sparse.
+	 */
+	public boolean isSparse() {
+		return array == null;
+	}
+	
+	/**
+	 * If we have too many elements in the array, we make it complete.
+	 */
+	private void checkMaxSize() {
+		if (array==null && sparse.size() > 0.1*size) { // 10% of size
+			// Create array
+			array = new ArrayList<E>(size);
+			for (int i=0; i<size; i++) {
+				array.add(null);
+			}
+			// Dump sparse values into the array
+			while (sparse.size() > 0) {
+				SparseNode<E> node = sparse.pop();
+				// Add value
+				//array[pos] = val;
+				array.set(node.p, node.v);
+			}
+			// Clear the sparse method
+			sparse = null;
+		}
+	}
+	
+	public Collection<E> values() {
+		LinkedList<E> ret = new LinkedList<E>();
+		for (int i=0; i<size; i++) {
+			E v = get(i);
+			ret.addLast(v);
+		}
+		return ret;
+	}
+	
+	/*public void iterator() {
+		if (array==null) {
+		
+		}
+	}*/
+	
+	/**
+	 * String version of the array.
+	 */
+	public String toString() {
+		String ret = "[";
+		if (this.get(0)==null) {
+			ret += "0";
+		} else {
+			ret += this.get(0);
+		}
+		for (int i=1; i<size; i++) {
+			if (this.get(i)==null) {
+				ret += ",0";
+			} else {
+				ret += "," + this.get(i);
+			}
+		}
+		ret += "]";
+		return ret;
+	}
+	
+	/**
+	 * Tester.
+	 */
+	public static void main(String[] args) {
+		System.out.println("Tester");
+		SparseArray<Long> array = new SparseArray<Long>(60);
+		array.set(5, 11L);
+		array.set(10, 12L);
+		System.out.println(array.isSparse());
+		array.set(15, 13L);
+		System.out.println(array.isSparse());
+		array.set(40, 14L);
+		System.out.println(array.isSparse());
+		array.set(20, 15L);
+		array.set(30, 16L);
+		System.out.println(array.isSparse());
+		array.set(25, 17L);
+		array.set(15, 18L);
+		System.out.println(array.isSparse());
+		array.set(50, 19L);
+		array.set(32, 21L);
+		//array.set(60, 21L);
+		array.set(0, 11L);
+		System.out.println(array);
+		System.out.println(array.isSparse());
+		
+		
+		try {
+			/*FileOutputStream fout = new FileOutputStream("/tmp/mola");
+			ObjectOutputStream oos = new ObjectOutputStream(fout); 
+			oos.writeObject(array);
+			fout.close();*/
+			
+			// Read
+			System.out.println("check file.......");
+			File file = new File("/scratch/hadoop-goiri/mapred/local/taskTracker/goiri/jobcache/job_201405141909_0103/attempt_201405141909_0103_r_000000_0/work/tmp/approx1170333968943084158.tmp");
+			System.out.println(file.length());
+			FileInputStream fin = new FileInputStream(file);
+			
+			// Read
+			ObjectInputStream iis = new ObjectInputStream(fin); 
+			Object o = iis.readObject();
+			//System.out.println(o);
+			
+			System.out.println(o.getClass());
+			
+			File.createTempFile("tmpfile", "tmp");
+		} catch(Exception e) {
+			e.printStackTrace();
+		}
+	}
+}
Index: src/contrib/wikipedia/src/java/org/apache/hadoop/mapreduce/apache/ApacheLogAnalysis.java
===================================================================
--- src/contrib/wikipedia/src/java/org/apache/hadoop/mapreduce/apache/ApacheLogAnalysis.java	(revision 0)
+++ src/contrib/wikipedia/src/java/org/apache/hadoop/mapreduce/apache/ApacheLogAnalysis.java	(working copy)
@@ -0,0 +1,373 @@
+package org.apache.hadoop.mapreduce.apache;
+
+import java.io.IOException;
+
+import java.util.StringTokenizer;
+import java.util.Date;
+
+import org.apache.commons.cli.CommandLine;
+import org.apache.commons.cli.CommandLineParser;
+import org.apache.commons.cli.GnuParser;
+import org.apache.commons.cli.HelpFormatter;
+import org.apache.commons.cli.OptionBuilder;
+import org.apache.commons.cli.Options;
+import org.apache.commons.cli.ParseException;
+
+import java.text.SimpleDateFormat;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.io.IntWritable;
+import org.apache.hadoop.io.LongWritable;
+import org.apache.hadoop.io.Text;
+import org.apache.hadoop.mapreduce.Job;
+import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
+import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
+import org.apache.hadoop.mapreduce.lib.reduce.LongSumReducer;
+import org.apache.hadoop.util.GenericOptionsParser;
+import org.apache.hadoop.util.Tool;
+import org.apache.hadoop.util.ToolRunner;
+
+import org.apache.hadoop.mapreduce.MultistageSamplingMapper;
+import org.apache.hadoop.mapreduce.MultistageSamplingReducer;
+import org.apache.hadoop.mapreduce.MultistageSamplingReducerIncr;
+import org.apache.hadoop.mapreduce.SparseArray;
+import org.apache.hadoop.mapreduce.ApproximateLongWritable;
+
+import org.apache.hadoop.mapreduce.lib.input.ApproximateTextInputFormat;
+
+import org.apache.hadoop.mapreduce.ParameterPartitioner;
+import org.apache.hadoop.mapreduce.MultistageSamplingPartitioner;
+
+/**
+ * Approximate log analysis count.
+ * hack
+ * host
+ * dateweek
+ * size
+ * totalsize
+ * pagesize
+ * page
+ * browser
+ */
+public class ApacheLogAnalysis {
+	//public static final String NUMLINES_SAMPLED = "$NUMLINES-%d$";
+	//public static final String NUMLINES_SAMPLED_MATCH = "\\$NUMLINES-(\\d)+\\$";
+	
+	/**
+	* Mapper that processes apache log lines.
+	*/
+	public static class ApacheLogMapper extends MultistageSamplingMapper<Object, Text, Text, LongWritable> {
+		private final static LongWritable one = new LongWritable(1);
+		private Text word = new Text();
+		private String task = null;
+		
+		// A couple tasks need this variable
+		private long totalSize = 0;
+		
+		// Auxilliary variables to compute standard deviation on the fly
+		private int    num   = 0; // Counter on how many elements
+		private double avg   = 0.0; // Average
+		private double stdev = 0.0;  // Standard deviation
+		
+		private SimpleDateFormat indateformat  = new SimpleDateFormat("dd/MMM/yyyy:HH:mm:ss Z");
+		//private SimpleDateFormat outdateformat = new SimpleDateFormat("yyyy/MM/dd HH:mm:ss");
+		//private SimpleDateFormat outdateformat = new SimpleDateFormat("yyyy/MM/dd HH"); // Day/hour
+		//private SimpleDateFormat outdateformat = new SimpleDateFormat("HH"); // Hour
+		//private SimpleDateFormat outdateformat = new SimpleDateFormat("EEE"); // Day of the week
+		//private SimpleDateFormat outdateformat = new SimpleDateFormat("EEE HH"); // Day of the week
+		
+		public void setup(Context context) {
+			task = context.getConfiguration().get("task");
+		}
+		
+		public void map(Object key, Text value, Context context) throws IOException, InterruptedException {
+			try {
+				// "Inspiration" from: http://www.java2s.com/Code/Java/Regular-Expressions/ParseanApachelogfilewithStringTokenizer.htm
+				StringTokenizer matcher = new StringTokenizer(value.toString());
+				String hostname = matcher.nextToken();
+				matcher.nextToken(); // eat the "-"
+				matcher.nextToken("["); // again
+				String datetime = matcher.nextToken("]").substring(1);
+				matcher.nextToken("\"");
+				String request = matcher.nextToken("\"");
+				matcher.nextToken(" "); // again
+				String response = matcher.nextToken();
+				String byteCount = matcher.nextToken();
+				matcher.nextToken("\"");
+				String referer = matcher.nextToken("\"");
+				matcher.nextToken("\""); // again
+				String userAgent = matcher.nextToken("\"");
+				
+				if (task != null) {
+					// Check who is trying to hack us
+					if (task.equalsIgnoreCase("hack")) {
+						// "POST /cgi-bin/php-cgi?XXX HTTP/1.1"
+						String[] requestSplit = request.split(" ");
+						if (requestSplit.length > 1) {
+							String address = requestSplit[1];
+							String[] keywords = { "/w00tw00t", "/phpMyAdmin", "/pma", "/myadmin", "/MyAdmin", "/phpTest", "/cgi-bin/php", "/cgi-bin/php5", "/cgi-bin/php-cgi" };
+							boolean found = false;
+							for (String keyword : keywords) {
+								if (address.startsWith(keyword)) {
+									found = true;
+									break;
+								}
+							}
+							// We note that that user is pushing us
+							if (found) {
+								word.set(hostname);
+								context.write(word, one);
+							}
+						}
+					// Hosts visiting the web
+					} else if (task.equalsIgnoreCase("host")) {
+						word.set(hostname);
+						context.write(word, one);
+					// Analyze the access time of the visits
+					// 24/Nov/2013:06:25:45 -0500 -> Date
+					} else if (task.equalsIgnoreCase("dateweek")) {
+						Date date = indateformat.parse(datetime);
+						SimpleDateFormat outdateformat = new SimpleDateFormat("EEE HH"); // Day of the week
+						word.set(outdateformat.format(date));
+						context.write(word, one);
+					// Size per object
+					} else if (task.equalsIgnoreCase("size")) {
+						long bytes = (Long.parseLong(byteCount)/100)*100; // Round for histogram
+						word.set(Long.toString(bytes));
+						context.write(word, one);
+					// Total size per object
+					} else if (task.equalsIgnoreCase("totalsize")) {
+						word.set("Total");
+						long bytes = Long.parseLong(byteCount);
+						context.write(word, new LongWritable(bytes));
+						// To send afterwards
+						//totalSize += bytes;
+						addValueforStdev(1.0*bytes);
+					// Page traffic
+					} else if (task.equalsIgnoreCase("pagesize")) {
+						int lastIndex = request.indexOf("?");
+						if (lastIndex > 0) {
+							String aux = request.substring(0, lastIndex);
+							word.set(aux);
+						} else {
+							word.set(request);
+						}
+						long bytes = Long.parseLong(byteCount);
+						context.write(word, new LongWritable(bytes));
+						// To send afterwards
+						//addValueforStdev(1.0*bytes);
+					// Page visit
+					} else if (task.equalsIgnoreCase("page")) {
+						int lastIndex = request.indexOf("?");
+						if (lastIndex > 0) {
+							String aux = request.substring(0, lastIndex);
+							word.set(aux);
+						} else {
+							word.set(request);
+						}
+						context.write(word, one);
+					// Browser
+					} else if (task.equalsIgnoreCase("browser")) {
+						word.set(userAgent);
+						context.write(word, one);
+					// Default
+					} else {
+						System.err.println("Unknown option:" + task);
+					}
+				}
+			} catch (Exception e) {
+				System.err.println("Error processing line for task \""+task+"\": " + value.toString());
+				System.err.println(e);
+				e.printStackTrace();
+			}
+		}
+		
+		/**
+		 * A couple of tasks need to sum everything up.
+		 */
+		public void cleanup(Context context) throws IOException, InterruptedException {
+			// We modify "m", "si2" and "t" parameters
+			if (task.equalsIgnoreCase("totalsize")) {
+				setS((long) getVariance());
+			} else if (task.equalsIgnoreCase("pagesize")) {
+				// Mark to use worst case for variation
+				setS(-3);
+			}
+			// Make the super class take care of sending the rest
+			super.cleanup(context);
+		}
+	
+		/**
+		*A method to calculate the mean and standard deviation of a series of numbers.
+		* http://www.buluschek.com/?p=140
+		* http://www.cs.berkeley.edu/~mhoemmen/cs194/Tutorials/variance.pdf
+		*/
+		private void addValueforStdev(double xk) {
+			num++;
+			double d = xk-avg; // is actually xk - Mk-1, as Mk was not yet updated
+			stdev += (num-1)*d*d/num;
+			avg += d/num;
+		}
+		
+		private double getStdev() {
+			return Math.sqrt(stdev/num);
+		}
+		
+		// si2
+		private double getVariance() {
+			return stdev/num;
+		}
+	}
+	
+	/**
+	 * Reducer that takes into account the approximation factor.
+	 */
+	public static class ApacheLogAnalysisReducer extends MultistageSamplingReducer<Text,LongWritable,Text,LongWritable> {
+		private LongWritable result = new LongWritable();
+		
+		/**
+		 * Reduce function that uses the default collection approach.
+		 */
+		public void reduce(Text key, Iterable<LongWritable> values, Context context) throws IOException, InterruptedException {
+			long sum = 0;
+			for (LongWritable val : values) {
+				sum += val.get();
+			}
+			result.set(sum);
+			context.write(key, result);
+		}
+	}
+	
+	public static class ApacheLogAnalysisReducerIncr extends MultistageSamplingReducerIncr<Text,LongWritable,Text,LongWritable> {
+		private LongWritable result = new LongWritable();
+		
+		/**
+		 * Reduce function that uses the default collection approach.
+		 */
+		public void reduce(Text key, Iterable<LongWritable> values, Context context) throws IOException, InterruptedException {
+			long sum = 0;
+			for (LongWritable val : values) {
+				sum += val.get();
+			}
+			result.set(sum);
+			context.write(key, result);
+		}
+	}
+	
+	/**
+	 * Launch apache log analysis.
+	 */
+	public static void main(String[] args) throws Exception {
+		Configuration conf = new Configuration();
+		String[] otherArgs = new GenericOptionsParser(conf, args).getRemainingArgs();
+		
+		// Options for the rest
+		Options options = new Options();
+		options.addOption("i", "input",    true,  "Input file");
+		options.addOption("o", "output",   true,  "Output file");
+		options.addOption("r", "reduces",  true,  "Number of reducers");
+		options.addOption("c", "incr",     false, "Incremental reducers");
+		options.addOption("d", "drop",     true,  "Percentage of maps to drop");
+		options.addOption("n", "inidrop",  true,  "Percentage of maps to drop initially");
+		options.addOption("p", "precise",  false, "Precise job");
+		options.addOption("s", "sampling", true,  "Sampling rate 1/X");
+		options.addOption("t", "task",     true,  "Task: project, other");
+		
+		try {
+			CommandLine cmdline = new GnuParser().parse(options, otherArgs);
+			// Input output
+			String input  = cmdline.getOptionValue("i");
+			String output = cmdline.getOptionValue("o");
+			if (input == null || output == null) {
+				throw new ParseException("No input/output option");
+			}
+			// Task to perform
+			String optionsStr = "";
+			if (cmdline.hasOption("t")) {
+				conf.set("task", cmdline.getOptionValue("t"));
+				optionsStr+=" "+cmdline.getOptionValue("t");
+			}
+			// Reduces
+			int numReducers = 1;
+			if (cmdline.hasOption("r")) {
+				numReducers = Integer.parseInt(cmdline.getOptionValue("r"));
+			}
+			// Incremental reducers
+			if (cmdline.hasOption("c")) {
+				conf.setBoolean("mapred.tasks.incremental.reduction", true);
+				conf.setBoolean("mapred.tasks.clustering", true); // We arrange the intermediate keys by clusters
+			}
+			// Dropping maps
+			if (cmdline.hasOption("d")) {
+				float dropPercentage = Float.parseFloat(cmdline.getOptionValue("d"));
+				conf.setInt("mapred.map.approximate.drop.extratime", 1);
+				conf.setFloat("mapred.map.approximate.drop.percentage", dropPercentage/100);
+				optionsStr+=String.format(" D:%d%%", (int) dropPercentage);
+			}
+			// Dropping maps
+			if (cmdline.hasOption("n")) {
+				float dropPercentage = Float.parseFloat(cmdline.getOptionValue("n"));
+				conf.setFloat("mapred.map.approximate.drop.ini.percentage", 1-(dropPercentage/100)); // Percentage of maps we initially drop
+			}
+			// Precise job
+			if (cmdline.hasOption("p")) {
+				conf.setBoolean("mapred.job.precise", true);
+				conf.setBoolean("mapred.tasks.incremental.reduction", false);
+			}
+			if (cmdline.hasOption("s")) {
+				int samplingRate = Integer.parseInt(cmdline.getOptionValue("s"));
+				conf.setInt("mapred.input.approximate.skip", samplingRate);
+				optionsStr+=String.format(" S:%d", samplingRate);
+			}
+			if (!optionsStr.equals("")) {
+				optionsStr = " ["+optionsStr.trim()+"]";
+			}
+			
+			// Create job
+			Job job = new Job(conf, "Apache log "+optionsStr);
+			
+			job.setJarByClass(ApacheLogAnalysis.class);
+			
+			job.setNumReduceTasks(numReducers);
+			
+			job.setMapperClass(ApacheLogMapper.class);
+			if (cmdline.hasOption("c")) {
+				job.setReducerClass(ApacheLogAnalysisReducerIncr.class);
+			} else {
+				job.setReducerClass(ApacheLogAnalysisReducer.class);
+			}
+			
+			// For this approach, we can use a combiner that sums the outputs in the maps
+			job.setCombinerClass(LongSumReducer.class);
+			
+			job.setOutputKeyClass(Text.class);
+			job.setOutputValueClass(LongWritable.class);
+			
+			// We need a partitioner that sends clustering information to all reducers
+			if (!cmdline.hasOption("p")) {
+				if (cmdline.hasOption("c")) {
+					job.setPartitionerClass(ParameterPartitioner.class);
+				} else {
+					job.setPartitionerClass(MultistageSamplingPartitioner.class);
+				}
+			}
+			
+			// Approximate Hadoop
+			job.setInputFormatClass(ApproximateTextInputFormat.class);
+			
+			// Input and output
+			FileInputFormat.addInputPath(job,   new Path(input));
+			FileOutputFormat.setOutputPath(job, new Path(output));
+			
+			// Run and exit
+			System.exit(job.waitForCompletion(true) ? 0 : 1);
+		} catch (ParseException exp) {
+			System.err.println("Error parsing command line: " + exp.getMessage());
+			HelpFormatter formatter = new HelpFormatter();
+			formatter.printHelp(ApacheLogAnalysis.class.toString(), options);
+			ToolRunner.printGenericCommandUsage(System.out);
+			System.exit(2);
+		}
+	}
+}
Index: src/contrib/wikipedia/src/java/org/apache/hadoop/mapreduce/wikipedia/ByteMatcher.java
===================================================================
--- src/contrib/wikipedia/src/java/org/apache/hadoop/mapreduce/wikipedia/ByteMatcher.java	(revision 0)
+++ src/contrib/wikipedia/src/java/org/apache/hadoop/mapreduce/wikipedia/ByteMatcher.java	(working copy)
@@ -0,0 +1,86 @@
+/**
+ * Copyright 2011 Yusuke Matsubara
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.mapreduce.wikipedia;
+
+import java.io.*;
+
+import org.apache.hadoop.io.DataOutputBuffer;
+import org.apache.hadoop.fs.Seekable;
+
+public class ByteMatcher {
+  private final InputStream in;
+  private final Seekable pos;
+  private long lastPos;
+  private long currentPos;
+  private long bytes;
+  public ByteMatcher(InputStream in, Seekable pos) throws IOException {
+    this.in = in;
+    this.pos = pos;
+    this.bytes = 0;
+    this.lastPos = -1;
+    this.currentPos = -1;
+  }
+  public ByteMatcher(SeekableInputStream is) throws IOException {
+    this(is, is);
+  }
+  public long getReadBytes() {
+    return this.bytes;
+  }
+  public long getPos() throws IOException {
+    return this.pos.getPos();
+  }
+  public long getLastUnmatchPos() { return this.lastPos; }
+
+  public void skip(long len) throws IOException {
+    this.in.skip(len);
+    this.bytes += len;
+  }
+
+  boolean readUntilMatch(String textPat, DataOutputBuffer outBufOrNull, long end) throws IOException {
+    byte[] match = textPat.getBytes("UTF-8");
+    int i = 0;
+    while (true) {
+      int b = this.in.read();
+      // end of file:
+      if (b == -1) {
+        System.err.println("eof 1");
+        return false;
+      }
+      ++this.bytes;    //! TODO: count up later in batch
+      // save to buffer:
+      if (outBufOrNull != null)
+        outBufOrNull.write(b);
+      
+      // check if we're matching:
+      if (b == match[i]) {
+        i++;
+        if (i >= match.length)
+          return true;
+      } else {
+        i = 0;
+        if ( this.currentPos != this.getPos() ) {
+          this.lastPos = this.currentPos;
+          this.currentPos = this.getPos();
+        }
+      }
+      // see if we've passed the stop point:
+      if (i == 0 && this.pos.getPos() >= end) {
+        System.err.println("eof 2: end=" + end);
+        return false;
+      }
+    }
+  }
+}
\ No newline at end of file
Index: src/contrib/wikipedia/src/java/org/apache/hadoop/mapreduce/wikipedia/CountWikipediaPages.java
===================================================================
--- src/contrib/wikipedia/src/java/org/apache/hadoop/mapreduce/wikipedia/CountWikipediaPages.java	(revision 0)
+++ src/contrib/wikipedia/src/java/org/apache/hadoop/mapreduce/wikipedia/CountWikipediaPages.java	(working copy)
@@ -0,0 +1,160 @@
+/*
+ * Cloud9: A MapReduce Library for Hadoop
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License"); you
+ * may not use this file except in compliance with the License. You may
+ * obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
+ * implied. See the License for the specific language governing
+ * permissions and limitations under the License.
+ */
+
+package org.apache.hadoop.mapreduce.wikipedia;
+
+import java.io.IOException;
+
+import org.apache.commons.cli.CommandLine;
+import org.apache.commons.cli.CommandLineParser;
+import org.apache.commons.cli.GnuParser;
+import org.apache.commons.cli.HelpFormatter;
+import org.apache.commons.cli.OptionBuilder;
+import org.apache.commons.cli.Options;
+import org.apache.commons.cli.ParseException;
+import org.apache.hadoop.conf.Configured;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.io.IntWritable;
+import org.apache.hadoop.io.LongWritable;
+import org.apache.hadoop.io.Text;
+//import org.apache.hadoop.mapreduce.Mapper.Context;
+import org.apache.hadoop.mapreduce.Job;
+import org.apache.hadoop.mapreduce.Mapper;
+import org.apache.hadoop.mapreduce.Reducer;
+import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
+import org.apache.hadoop.mapreduce.lib.output.NullOutputFormat;
+import org.apache.hadoop.util.Tool;
+import org.apache.hadoop.util.ToolRunner;
+import org.apache.log4j.Logger;
+
+/**
+ * Tool for counting the number of pages in a particular Wikipedia XML dump file. This program keeps
+ * track of total number of pages, redirect pages, disambiguation pages, empty pages, actual
+ * articles (including stubs), stubs, and non-articles ("File:", "Category:", "Wikipedia:", etc.).
+ * This also provides a skeleton for MapReduce programs to process the collection. Specify input
+ * path to the Wikipedia XML dump file with the {@code -input} flag.
+ *
+ * @author Jimmy Lin
+ * @author Peter Exner
+ */
+public class CountWikipediaPages extends Configured implements Tool {
+	private static final Logger LOG = Logger.getLogger(CountWikipediaPages.class);
+
+	private static enum PageTypes {
+		TOTAL, REDIRECT, DISAMBIGUATION, EMPTY, ARTICLE, STUB, OTHER
+	};
+
+	/**
+	 * Mapper class that counts the pages
+	 */
+	private static class MyMapper extends Mapper<LongWritable, WikipediaPage, Text, IntWritable> {
+		@Override
+		public void map(LongWritable key, WikipediaPage p, Context context) throws IOException, InterruptedException {
+			context.getCounter(PageTypes.TOTAL).increment(1);
+			if (p.isRedirect()) {
+				context.getCounter(PageTypes.REDIRECT).increment(1);
+			} else if (p.isDisambiguation()) {
+				context.getCounter(PageTypes.DISAMBIGUATION).increment(1);
+			} else if (p.isEmpty()) {
+				context.getCounter(PageTypes.EMPTY).increment(1);
+			} else if (p.isArticle()) {
+				context.getCounter(PageTypes.ARTICLE).increment(1);
+				if (p.isStub()) {
+				context.getCounter(PageTypes.STUB).increment(1);
+				}
+			} else {
+				context.getCounter(PageTypes.OTHER).increment(1);
+			}
+		}
+	}
+
+  private static final String INPUT_OPTION = "input";
+  private static final String LANGUAGE_OPTION = "wiki_language";
+  
+  @SuppressWarnings("static-access")
+  @Override
+  public int run(String[] args) throws Exception {
+    Options options = new Options();
+    options.addOption(OptionBuilder.withArgName("path").hasArg().withDescription("XML dump file").create(INPUT_OPTION));
+    options.addOption(OptionBuilder.withArgName("en|sv|de|cs|es|zh|ar|tr").hasArg().withDescription("two-letter language code").create(LANGUAGE_OPTION));
+    
+    CommandLine cmdline;
+    CommandLineParser parser = new GnuParser();
+    try {
+      cmdline = parser.parse(options, args);
+    } catch (ParseException exp) {
+      System.err.println("Error parsing command line: " + exp.getMessage());
+      return -1;
+    }
+
+    if (!cmdline.hasOption(INPUT_OPTION)) {
+      HelpFormatter formatter = new HelpFormatter();
+      formatter.printHelp(this.getClass().getName(), options);
+      ToolRunner.printGenericCommandUsage(System.out);
+      return -1;
+    }
+    
+    String language = "en"; // Assume 'en' by default.
+    if (cmdline.hasOption(LANGUAGE_OPTION)) {
+      language = cmdline.getOptionValue(LANGUAGE_OPTION);
+      if (language.length() != 2) {
+        System.err.println("Error: \"" + language + "\" unknown language!");
+        return -1;
+      }
+    }
+
+    String inputPath = cmdline.getOptionValue(INPUT_OPTION);
+
+    LOG.info("Tool name: " + this.getClass().getName());
+    LOG.info(" - XML dump file: " + inputPath);
+    LOG.info(" - language: " + language);
+    
+    Configuration conf = getConf();
+    conf.set("xmlinput.start", "<page>");
+    conf.set("xmlinput.end", "</page>");
+    
+    //Job job = Job.getInstance(getConf());
+    Job job = new Job(conf);
+    job.setJarByClass(CountWikipediaPages.class);
+    job.setJobName(String.format("CountWikipediaPages[%s: %s, %s: %s]", INPUT_OPTION, inputPath,
+        LANGUAGE_OPTION, language));
+
+    job.setNumReduceTasks(0);
+
+    FileInputFormat.setInputPaths(job, new Path(inputPath));
+
+    if (language != null) {
+      job.getConfiguration().set("wiki.language", language);
+    }
+    
+    job.setInputFormatClass(WikipediaPageInputFormat.class);
+    job.setOutputFormatClass(NullOutputFormat.class);
+
+    job.setMapperClass(MyMapper.class);
+
+    job.waitForCompletion(true);
+
+    return 0;
+  }
+
+	public CountWikipediaPages() {
+	}
+
+	public static void main(String[] args) throws Exception {
+		ToolRunner.run(new CountWikipediaPages(), args);
+	}
+}
Index: src/contrib/wikipedia/src/java/org/apache/hadoop/mapreduce/wikipedia/Indexable.java
===================================================================
--- src/contrib/wikipedia/src/java/org/apache/hadoop/mapreduce/wikipedia/Indexable.java	(revision 0)
+++ src/contrib/wikipedia/src/java/org/apache/hadoop/mapreduce/wikipedia/Indexable.java	(working copy)
@@ -0,0 +1,58 @@
+/*
+ * Cloud9: A MapReduce Library for Hadoop
+ * 
+ * Licensed under the Apache License, Version 2.0 (the "License"); you
+ * may not use this file except in compliance with the License. You may
+ * obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0 
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
+ * implied. See the License for the specific language governing
+ * permissions and limitations under the License.
+ */
+
+package org.apache.hadoop.mapreduce.wikipedia;
+
+import org.apache.hadoop.io.Writable;
+
+/**
+ * A document that can be indexed.
+ */
+public abstract class Indexable implements Writable {
+
+  /**
+   * Returns the globally-unique String identifier of the document within the collection.
+   *
+   * @return docid of the document
+   */
+  public abstract String getDocid();
+
+  /**
+   * Returns the content of the document.
+   *
+   * @return content of the document
+   */
+  public abstract String getContent();
+
+  /**
+   * Returns the content of the document for display to a human.
+   *
+   * @return displayable content
+   */
+  public String getDisplayContent() {
+    return getContent();
+  }
+
+  /**
+   * Returns the type of the display content, per IANA MIME Media Type (e.g., "text/html").
+   * See {@code http://www.iana.org/assignments/media-types/index.html}
+   *
+   * @return IANA MIME Media Type
+   */
+  public String getDisplayContentType() {
+    return "text/plain";
+  }
+}
Index: src/contrib/wikipedia/src/java/org/apache/hadoop/mapreduce/wikipedia/IndexableFileInputFormat.java
===================================================================
--- src/contrib/wikipedia/src/java/org/apache/hadoop/mapreduce/wikipedia/IndexableFileInputFormat.java	(revision 0)
+++ src/contrib/wikipedia/src/java/org/apache/hadoop/mapreduce/wikipedia/IndexableFileInputFormat.java	(working copy)
@@ -0,0 +1,27 @@
+/*
+ * Cloud9: A MapReduce Library for Hadoop
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License"); you
+ * may not use this file except in compliance with the License. You may
+ * obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
+ * implied. See the License for the specific language governing
+ * permissions and limitations under the License.
+ */
+
+package org.apache.hadoop.mapreduce.wikipedia;
+
+import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
+
+/**
+ * Abstract class representing a {@link FileInputFormat} for {@link Indexable} objects ({@code
+ * org.apache.hadoop.mapreduce} API).
+ */
+public abstract class IndexableFileInputFormat<K, V extends Indexable> extends
+    FileInputFormat<K, V> {
+}
Index: src/contrib/wikipedia/src/java/org/apache/hadoop/mapreduce/wikipedia/PageRankWikipedia.java.bak
===================================================================
--- src/contrib/wikipedia/src/java/org/apache/hadoop/mapreduce/wikipedia/PageRankWikipedia.java.bak	(revision 0)
+++ src/contrib/wikipedia/src/java/org/apache/hadoop/mapreduce/wikipedia/PageRankWikipedia.java.bak	(working copy)
@@ -0,0 +1,498 @@
+/*
+ * Cloud9: A MapReduce Library for Hadoop
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License"); you
+ * may not use this file except in compliance with the License. You may
+ * obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
+ * implied. See the License for the specific language governing
+ * permissions and limitations under the License.
+ */
+
+package org.apache.hadoop.mapred.wikipedia;
+
+import java.io.IOException;
+
+import java.util.Collections;
+import java.util.List;
+import java.util.LinkedList;
+import java.util.Map;
+import java.util.HashMap;
+import java.util.HashSet;
+
+import org.apache.commons.cli.CommandLine;
+import org.apache.commons.cli.CommandLineParser;
+import org.apache.commons.cli.GnuParser;
+import org.apache.commons.cli.HelpFormatter;
+import org.apache.commons.cli.OptionBuilder;
+import org.apache.commons.cli.Options;
+import org.apache.commons.cli.ParseException;
+import org.apache.hadoop.conf.Configured;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.io.IntWritable;
+import org.apache.hadoop.io.LongWritable;
+import org.apache.hadoop.io.Text;
+import org.apache.hadoop.mapreduce.Job;
+import org.apache.hadoop.mapreduce.Mapper;
+import org.apache.hadoop.mapreduce.Reducer;
+import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
+import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
+import org.apache.hadoop.mapreduce.lib.partition.HashPartitioner;
+import org.apache.hadoop.mapreduce.lib.reduce.IntSumReducer;
+import org.apache.hadoop.mapreduce.lib.reduce.LongSumReducer;
+import org.apache.hadoop.util.Tool;
+import org.apache.hadoop.util.ToolRunner;
+import org.apache.log4j.Logger;
+
+import org.apache.commons.math.distribution.TDistribution;
+import org.apache.commons.math.distribution.TDistributionImpl;
+
+import org.apache.hadoop.mapred.JobConf;
+
+
+
+/**
+ * Count the number of links each page gets. It supports 3-stage sampling.
+ */
+public class PageRankWikipedia extends Configured implements Tool {
+	private static final Logger LOG = Logger.getLogger(PageRankWikipedia.class);
+	
+	private static final String NUMPAGES_SAMPLED = "$NUMPAGES-%d$";
+	private static final String NUMLINKS_SAMPLED = "$NUMLINKS-%d$";
+	
+	private static final String NUMPAGES_SAMPLED_MATCH = "\\$NUMPAGES-(\\d)+\\$";
+	private static final String NUMLINKS_SAMPLED_MATCH = "\\$NUMLINKS-(\\d)+\\$";
+	
+	private static enum PageTypes {
+		TOTAL, REDIRECT, DISAMBIGUATION, EMPTY, ARTICLE, STUB, OTHER
+	};
+
+	/**
+	 * Mapper class. It takes a page and parses the links.
+	 */
+	private static class MyMapper extends Mapper<LongWritable, WikipediaPage, Text, IntWritable> {
+		// The number of pages we process in this map
+		private int numPages = 0;
+		private int numLinks = 0;
+	
+		/**
+		 * Map function that collects the links for each wikipedia page.
+		 */
+		public void map(LongWritable key, WikipediaPage p, Context context) throws IOException, InterruptedException {
+			context.getCounter(PageTypes.TOTAL).increment(1);
+			
+			// Check links
+			//if (p.isArticle()) {
+			//}
+			//for (String link : p.extractLinkTargets()) {
+			List<String> links = p.extractLinkTargets();
+			for (String link : new HashSet<String>(links)) {
+				context.write(new Text(link.toLowerCase().replaceAll(" ", "_")), new IntWritable(1));
+				// To do three-stage sampling we need to know how many links there was in this terciary smaple
+				//context.write(new Text(link.toLowerCase().replaceAll(" ", "_")), new IntWritable(links.size()));
+			}
+			
+			// Account for pages and links
+			numPages++;
+			numLinks += links.size();
+		}
+		
+		/**
+		 * Cleanup function that reports how many pages and links have been processed.
+		 */
+		public void cleanup(Context context) throws IOException, InterruptedException {
+			// We send the statistically relevant information to everybody
+			sendParameter(context, NUMPAGES_SAMPLED, numPages);
+			sendParameter(context, NUMLINKS_SAMPLED, numLinks);
+		}
+		
+		/**
+		 * Send a parameter (inside of the data) to all reducers.
+		 */
+		protected void sendParameter(Context context, String param, int value) throws IOException, InterruptedException {
+			int numReducers = context.getConfiguration().getInt("mapred.reduce.tasks", 1);
+			for (int i=0; i<numReducers; i++) {
+				context.write(new Text(String.format(param, i)), new IntWritable(value));
+			}
+		}
+	}
+	
+	/**
+	 * Reducer class. It takes into account the approximation factor.
+	 */
+	public static class Integer3StageSumReducer extends Reducer<Text,IntWritable,Text,IntWritable> {
+		private IntWritable result = new IntWritable();
+
+		// To detect wether we switch between clusters
+		private Text prevKey = new Text();
+		
+		private int curCluster = 0;
+		private int numCluster = 0;
+		
+		private Map<String,int[]> clusters;
+		private int[] clusterPages;
+		private int[] clusterLinks;
+		
+		private long lastCheck = 0;
+		
+		/**
+		 * Initialize the cluster structure
+		 */
+		public void setup(Context context) {
+			numCluster = context.getConfiguration().getInt("mapred.map.tasks", -1);
+			clusters = new HashMap<String,int[]>();
+			//clusters = DBMaker.newTempHashMap(); // Backed up by disk
+			clusterPages = new int[numCluster];
+			clusterLinks = new int[numCluster];
+		}
+		
+		/**
+		 * Reduce function that uses the default collection approach.
+		 */
+		public void reduceOld(Text key, Iterable<IntWritable> values, Context context) throws IOException, InterruptedException {
+			int sum = 0;
+			for (IntWritable val : values) {
+				sum += val.get();
+			}
+			
+			if (key.toString().equals("united_states")) {
+				System.out.println(key + " -> " + sum);
+			}
+			
+			// Just calculate by the sampling factor
+			//int factor = context.getConfiguration().getInt("mapred.input.approximate.skip", 1);
+			//result.set(sum*factor);
+			
+			context.write(key, new IntWritable(sum));
+		}
+		
+		/**
+		 * Reduce function that collects the inputs from the maps and stores them per cluster to approximate the results.
+		 */
+		public void reduce(Text key, Iterable<IntWritable> values, Context context) throws IOException, InterruptedException {
+			// Check if we have a new map output to create a new cluster
+			if (prevKey != null && prevKey.compareTo(key) > 0) {
+				// We see how we are doing
+				if (System.currentTimeMillis()-lastCheck > 10*1000.0) { // More than 10 seconds ago
+					long t0 = System.currentTimeMillis();
+					checkResults(context);
+					System.out.format("%d: %.1fs [%d keys]\n", curCluster+1, (System.currentTimeMillis()-t0)/1000.0, clusters.size());
+					
+					// Reset timing
+					lastCheck = System.currentTimeMillis();
+				}
+				// We have a new cluster
+				curCluster++;
+			}
+			prevKey.set(key);
+			
+			// Calculate aggregated value
+			int sum = 0;
+			for (IntWritable val : values) {
+				sum += val.get();
+			}
+			
+			// Check for the number of pages in the current cluster
+			if (key.toString().matches(NUMPAGES_SAMPLED_MATCH)) {
+				clusterPages[curCluster] = sum;
+			// Check for the number of links in the current cluster
+			} else if (key.toString().matches(NUMLINKS_SAMPLED_MATCH)) {
+				clusterLinks[curCluster] = sum;
+			// Check for the population the cluster
+			} else {
+				if (!clusters.containsKey(key.toString())) {
+					// We create a value for each cluster
+					clusters.put(key.toString(), new int[numCluster]);
+				}
+				clusters.get(key.toString())[curCluster] = sum;
+			}
+		}
+	
+		/**
+		* Calculate the summation of a list of numbers.
+		*/
+		private double sum(int[] arr) {
+			double ret = 0.0;
+			for (int i=0; i<arr.length; i++) {
+				ret += arr[i];
+			}
+			return ret;
+		}
+		private double sum(long[] arr) {
+			double ret = 0.0;
+			for (int i=0; i<arr.length; i++) {
+				ret += arr[i];
+			}
+			return ret;
+		}
+		private double sum(double[] arr) {
+			double ret = 0.0;
+			for (int i=0; i<arr.length; i++) {
+				ret += arr[i];
+			}
+			return ret;
+		}
+		
+		/**
+		* Calculate the variance.
+		*/
+		private double var(double[] arr) {
+			// Calculate the average first
+			double avg = 0.0;
+			for (int i=0; i<arr.length; i++) {
+				avg += 1.0*arr[i]/arr.length;
+			}
+		
+			// Calculate the variance
+			double auxsum = 0.0;
+			for (int i=0; i<arr.length; i++) {
+				double aux = arr[i]-avg;
+				auxsum += aux*aux; //^2
+			}
+			
+			return 1.0*auxsum/(arr.length-1);
+		}
+		
+		/**
+		 * Check the error in the results based on the current sampling.
+		 */
+		 
+		private void checkResults(Context context) throws IOException, InterruptedException {
+			checkResults(context, false);
+		}
+		private void checkResults(Context context, boolean output) throws IOException, InterruptedException {
+			double maxError = 0.0;
+		
+			// Get current paramters
+			int numMaps = context.getConfiguration().getInt("mapred.map.tasks", -1);
+			int skipPages = context.getConfiguration().getInt("mapred.input.approximate.skip", 1);
+			
+			// Multistage sampling
+			int N = numMaps;
+			int n = curCluster+1;
+			
+			// Check if we have done any clustering or everything is together
+			if (n==1 && context.getConfiguration().getFloat("mapred.map.approximate.drop.percentage", 1.0f) >= 1.0) {
+				System.out.println("There is no clusters");
+				N = 1;
+			}
+			
+			// Calculate populations in each cluster (we make it long because the multiplication can cause overflow)
+			long[] mi = new long[n];
+			long[] Mi = new long[n];
+			for (int i=0; i<n; i++) {
+				// The number of pages sampled in each 2-cluster
+				mi[i] = clusterPages[i];
+				Mi[i] = mi[i]*skipPages; // This is an approximation based on the sampling ratio
+			}
+			
+			// Get the t-score for the current distribution
+			double tscore = 1.96; // By default we use the normal distribution
+			try {
+				TDistribution tdist = new TDistributionImpl(n-1);
+				tscore = tdist.inverseCumulativeProbability(0.975); // 95% confidence
+			} catch (Exception e) { }
+			
+			// Go over all the keys
+			for (String key : clusters.keySet()) {
+				// Collect the results for each 2-cluster
+				int[] yi = new int[n];
+				int[] yti = new int[n];
+				for (int i=0; i<n; i++) {
+					// Account on how many pages where linking to this key
+					yi[i] = clusters.get(key)[i];
+					yti[i] = clusterLinks[i];
+				}
+				
+				// Estimate the results for each 2-cluster
+				double[] yhati = new double[n];
+				for (int i=0; i<n; i++) {
+					yhati[i] = (1.0*Mi[i]/mi[i])*yi[i];
+				}
+				
+				// Estimate the total number
+				double tauhat = (1.0*N/n)*sum(yhati);
+				
+				// Calculate the errors
+				// Estimate the global deviation
+				double su2 = var(yhati);
+				
+				// Calculate proportions
+				double[] pi = new double[n];
+				for (int i=0; i<n; i++) {
+					pi[i] = 1.0*yi[i]/yti[i];
+				}
+				
+				// Estimate variance in primary
+				double[] si2 = new double[n];
+				for (int i=0; i<n; i++) {
+					si2[i] = (1.0*mi[i]/(mi[i]-1.0)) * pi[i] * (1.0-pi[i]);
+				}
+				
+				// Calculate total variance
+				double var1 = 1.0*(N*(N-n)*su2)/n;
+				double var2 = 0.0;
+				for (int i=0; i<n; i++) {
+					var2 += 1.0*(Mi[i]*(Mi[i]-mi[i])*si2[i])/mi[i];
+				}
+				var2 = (1.0*N/n)*var2;
+				double var3 = 0.0; // var3 stays 0 because ti=Ti (var ~= Ti*(Ti-ti)*st2/ti)
+				
+				// Variation and standard error for the final result
+				double vartauhat = var1 + var2 + var3;
+				double setauhat = Math.sqrt(vartauhat);
+				
+				// DEBUG
+				if (key.toString().equals("united_states") || key.toString().equals("france")) {
+					System.out.format("%d  -> %s = %.2f +/- %.2f (+/-%.2f%%)\n", n, key, tauhat, tscore*setauhat, 100.0*tscore*setauhat/tauhat);
+				}
+				
+				// Check which is the maximum error
+				if (tscore*setauhat > maxError) {
+					maxError = tscore*setauhat;
+				}
+				if (tscore*setauhat == Double.NaN) {
+					maxError = Double.MAX_VALUE;
+				}
+				
+				// We save the output
+				if (output) {
+					context.write(new Text(key), new IntWritable((int) tauhat));
+				}
+			}
+			
+			// Check if dorpping
+			if (n>1) {
+				System.out.format("The maximum error right now is +/-%.2f\n", maxError);
+				if (maxError < 10*1000) {
+					System.out.format("The maximum error right now is +/-%.2f: we can drop!", maxError);
+					context.setStatus("dropping");
+				}
+			}
+		}
+		
+		/**
+		 * We perform the sampling theory here.
+		 */
+		protected void cleanup(Context context) throws IOException, InterruptedException {
+			// Perform estimation and output
+			long t0 = System.currentTimeMillis();
+			checkResults(context, true);
+			System.out.format("%d: %.1fs [%d keys]\n", curCluster+1, (System.currentTimeMillis()-t0)/1000.0, clusters.size());
+		}
+	}
+	
+	/**
+	 * A partitioner that checks if we have a parameter and send it to the specified one. $PARAMETER-1$ -> 1
+	 */
+	public static class ParameterPartitioner<Text,IntWritable> extends HashPartitioner<Text,IntWritable> {
+		public int getPartition(Text key, IntWritable value, int numReduceTasks) {
+			String aux = key.toString();
+			if (aux.startsWith("$") && aux.endsWith("$") && aux.indexOf("-")>0) {
+				// We send it to the reducer specified in the parameter
+				return Integer.parseInt(aux.substring(aux.indexOf("-")+1, aux.length()-1)) % numReduceTasks;
+			}
+			return super.getPartition(key, value, numReduceTasks);
+		}
+	}
+	
+	// Input parameters
+	private static final String INPUT_OPTION = "input";
+	private static final String OUTPUT_OPTION = "output";
+	private static final String LANGUAGE_OPTION = "wiki_language";
+	private static final String NUM_REDUCES = "r";
+	
+	/**
+	 * Run the wikipedia page rank job.
+	 */
+	public int run(String[] args) throws Exception {
+		Options options = new Options();
+		options.addOption(OptionBuilder.withArgName("path").hasArg().withDescription("XML dump file").create(INPUT_OPTION));
+		options.addOption(OptionBuilder.withArgName("path").hasArg().withDescription("Output file").create(OUTPUT_OPTION));
+		options.addOption(OptionBuilder.withArgName("en|sv|de|cs|es|zh|ar|tr").hasArg().withDescription("two-letter language code").create(LANGUAGE_OPTION));
+		options.addOption(OptionBuilder.withArgName("reducers").hasArg().withDescription("Number of reducers").create(NUM_REDUCES));
+		
+		CommandLine cmdline;
+		CommandLineParser parser = new GnuParser();
+		try {
+			cmdline = parser.parse(options, args);
+		} catch (ParseException exp) {
+			System.err.println("Error parsing command line: " + exp.getMessage());
+			return -1;
+		}
+
+		if (!cmdline.hasOption(INPUT_OPTION) || !cmdline.hasOption(OUTPUT_OPTION)) {
+			HelpFormatter formatter = new HelpFormatter();
+			formatter.printHelp(this.getClass().getName(), options);
+			ToolRunner.printGenericCommandUsage(System.out);
+			return -1;
+		}
+		
+		String language = "en"; // Assume 'en' by default.
+		if (cmdline.hasOption(LANGUAGE_OPTION)) {
+			language = cmdline.getOptionValue(LANGUAGE_OPTION);
+			if (language.length() != 2) {
+				System.err.println("Error: \"" + language + "\" unknown language!");
+				return -1;
+			}
+		}
+		
+		int numReducers = 1;
+		if (cmdline.hasOption(NUM_REDUCES)) {
+			numReducers = Integer.parseInt(cmdline.getOptionValue(NUM_REDUCES));
+		}
+		
+
+		String inputPath = cmdline.getOptionValue(INPUT_OPTION);
+		String outputPath = cmdline.getOptionValue(OUTPUT_OPTION);
+
+		LOG.info("Tool name: " + this.getClass().getName());
+		LOG.info(" - XML dump file: " + inputPath);
+		LOG.info(" - output file: " + outputPath);
+		LOG.info(" - language: " + language);
+		
+		// Configuration for parsing wikipedia XML
+		Configuration conf = getConf();
+		conf.set("xmlinput.start", "<page>");
+		conf.set("xmlinput.end", "</page>");
+		
+		//Job job = Job.getInstance(getConf());
+		Job job = new Job(conf);
+		job.setJarByClass(PageRankWikipedia.class);
+		job.setJobName(String.format("PageRankWikipedia[%s: %s, %s: %s]", INPUT_OPTION, inputPath, LANGUAGE_OPTION, language));
+
+		job.setNumReduceTasks(numReducers);
+
+		FileInputFormat.setInputPaths(job, new Path(inputPath));
+		FileOutputFormat.setOutputPath(job, new Path(outputPath));
+
+		if (language != null) {
+			job.getConfiguration().set("wiki.language", language);
+		}
+		
+		job.setInputFormatClass(WikipediaPageInputFormat.class);
+		
+		job.setOutputKeyClass(Text.class);
+		job.setOutputValueClass(IntWritable.class);
+		
+		job.setPartitionerClass(ParameterPartitioner.class);
+		
+		job.setMapperClass(MyMapper.class);
+		//job.setCombinerClass(LongSumReducer.class);
+		job.setReducerClass(Integer3StageSumReducer.class);
+
+		job.waitForCompletion(true);
+
+		return 0;
+	}
+
+	public static void main(String[] args) throws Exception {
+		ToolRunner.run(new PageRankWikipedia(), args);
+	}
+}
Index: src/contrib/wikipedia/src/java/org/apache/hadoop/mapreduce/wikipedia/SeekableInputStream.java
===================================================================
--- src/contrib/wikipedia/src/java/org/apache/hadoop/mapreduce/wikipedia/SeekableInputStream.java	(revision 0)
+++ src/contrib/wikipedia/src/java/org/apache/hadoop/mapreduce/wikipedia/SeekableInputStream.java	(working copy)
@@ -0,0 +1,81 @@
+/**
+ * Copyright 2011 Yusuke Matsubara
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.mapreduce.wikipedia;
+
+import java.io.*;
+
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.fs.FSDataInputStream;
+import org.apache.hadoop.fs.Seekable;
+import org.apache.hadoop.mapred.FileSplit;
+import org.apache.hadoop.mapred.*;
+import org.apache.hadoop.io.compress.*;
+
+public class SeekableInputStream extends FilterInputStream implements Seekable {
+  private final Seekable seek;
+  private final SplitCompressionInputStream sin;
+  public SeekableInputStream(FSDataInputStream in) {
+    super(in);
+    this.seek = in;
+    this.sin = null;
+  }
+  public SeekableInputStream(SplitCompressionInputStream cin) {
+    super(cin);
+    this.seek = cin;
+    this.sin = cin;
+  }
+  public SeekableInputStream(CompressionInputStream cin, FSDataInputStream in) {
+    super(cin);
+    this.seek = in;
+    this.sin = null;
+  }
+  public static SeekableInputStream getInstance(Path path, long start, long end, FileSystem fs, CompressionCodecFactory compressionCodecs) throws IOException {
+    CompressionCodec codec = compressionCodecs.getCodec(path);
+    FSDataInputStream din = fs.open(path);
+    if (codec != null) {
+      Decompressor decompressor = CodecPool.getDecompressor(codec);
+      if (codec instanceof SplittableCompressionCodec) {
+        SplittableCompressionCodec scodec = (SplittableCompressionCodec)codec;
+        SplitCompressionInputStream cin = scodec.createInputStream
+          (din, decompressor, start, end,
+           SplittableCompressionCodec.READ_MODE.BYBLOCK);
+        return new SeekableInputStream(cin);
+      } else {
+        // non-splittable compression input stream
+        // no seeking or offsetting is needed
+        assert start == 0;
+        CompressionInputStream cin = codec.createInputStream(din, decompressor);
+        return new SeekableInputStream(cin, din);
+      }
+    } else {
+      // non compression input stream
+      // we seek to the start of the split
+      din.seek(start);
+      return new SeekableInputStream(din);
+    }
+  }
+  public static SeekableInputStream getInstance(FileSplit split, FileSystem fs, CompressionCodecFactory compressionCodecs) throws IOException {
+    return getInstance(split.getPath(), split.getStart(), split.getStart() + split.getLength(), fs, compressionCodecs);
+  }
+  public SplitCompressionInputStream getSplitCompressionInputStream() { return this.sin; }
+  public long getPos() throws IOException { return this.seek.getPos(); }
+  public void seek(long pos) throws IOException { this.seek.seek(pos); } 
+  public boolean seekToNewSource(long targetPos) throws IOException { return this.seek.seekToNewSource(targetPos); }
+  @Override public String toString() {
+    return this.in.toString();
+  }
+}
\ No newline at end of file
Index: src/contrib/wikipedia/src/java/org/apache/hadoop/mapreduce/wikipedia/StreamWikiDumpInputFormat.java.bak
===================================================================
--- src/contrib/wikipedia/src/java/org/apache/hadoop/mapreduce/wikipedia/StreamWikiDumpInputFormat.java.bak	(revision 0)
+++ src/contrib/wikipedia/src/java/org/apache/hadoop/mapreduce/wikipedia/StreamWikiDumpInputFormat.java.bak	(working copy)
@@ -0,0 +1,660 @@
+/**
+ * Copyright 2011 Yusuke Matsubara
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.mapred.wikipedia;
+
+import java.io.*;
+import java.util.*;
+
+import org.apache.hadoop.io.DataOutputBuffer;
+import org.apache.hadoop.io.Writable;
+import org.apache.hadoop.io.Text;
+import org.apache.hadoop.io.WritableComparable;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.fs.FSDataInputStream;
+import org.apache.hadoop.io.Text;
+import org.apache.hadoop.fs.Seekable;
+import org.apache.hadoop.fs.FileStatus;
+import org.apache.hadoop.fs.BlockLocation;
+import org.apache.hadoop.net.NetworkTopology;
+/*import org.apache.hadoop.mapred.Reporter;
+import org.apache.hadoop.mapred.FileSplit;
+import org.apache.hadoop.mapred.JobConf;
+import org.apache.hadoop.mapred.RecordReader;
+import org.apache.hadoop.mapred.*;*/
+//import org.apache.hadoop.io.compress.*;
+import java.util.regex.*;
+
+
+
+
+import org.apache.hadoop.io.compress.CompressionCodec;
+import org.apache.hadoop.io.compress.CompressionCodecFactory;
+import org.apache.hadoop.mapreduce.InputFormat;
+import org.apache.hadoop.mapreduce.InputSplit;
+import org.apache.hadoop.mapreduce.JobContext;
+import org.apache.hadoop.mapreduce.RecordReader;
+import org.apache.hadoop.mapreduce.TaskAttemptContext;
+
+import org.apache.hadoop.mapreduce.lib.input.KeyValueTextInputFormat;
+// import org.apache.hadoop.mapreduce.InputFormat;
+// import org.apache.hadoop.mapreduce.InputSplit;
+// import org.apache.hadoop.mapreduce.JobContext;
+// import org.apache.hadoop.mapreduce.RecordReader;
+// import org.apache.hadoop.mapreduce.TaskAttemptContext;
+
+/**
+ * A InputFormat implementation that splits a Wikimedia Dump File into page fragments, and emits them as input records.
+ * The record reader embedded in this input format converts a page into a sequence of page-like elements, each of which contains two consecutive revisions.  Output is given as keys with empty values.
+ *
+ * For example,  Given the following input containing two pages and four revisions,
+ * <pre><code>
+ *  &lt;page&gt;
+ *    &lt;title&gt;ABC&lt;/title&gt;
+ *    &lt;id&gt;123&lt;/id&gt;
+ *    &lt;revision&gt;
+ *      &lt;id&gt;100&lt;/id&gt;
+ *      ....
+ *    &lt;/revision&gt;
+ *    &lt;revision&gt;
+ *      &lt;id&gt;200&lt;/id&gt;
+ *      ....
+ *    &lt;/revision&gt;
+ *    &lt;revision&gt;
+ *      &lt;id&gt;300&lt;/id&gt;
+ *      ....
+ *    &lt;/revision&gt;
+ *  &lt;/page&gt;
+ *  &lt;page&gt;
+ *    &lt;title&gt;DEF&lt;/title&gt;
+ *    &lt;id&gt;456&lt;/id&gt;
+ *    &lt;revision&gt;
+ *      &lt;id&gt;400&lt;/id&gt;
+ *      ....
+ *    &lt;/revision&gt;
+ *  &lt;/page&gt;
+ * </code></pre>
+ * it will produce four keys like this:
+ * <pre><code>
+ *  &lt;page&gt;
+ *    &lt;title&gt;ABC&lt;/title&gt;
+ *    &lt;id&gt;123&lt;/id&gt;
+ *    &lt;revision&gt;&lt;revision beginningofpage="true"&gt;&lt;text xml:space="preserve"&gt;&lt;/text&gt;&lt;/revision&gt;&lt;revision&gt;
+ *      &lt;id&gt;100&lt;/id&gt;
+ *      ....
+ *    &lt;/revision&gt;
+ *  &lt;/page&gt;
+ * </code></pre>
+ * <pre><code>
+ *  &lt;page&gt;
+ *    &lt;title&gt;ABC&lt;/title&gt;
+ *    &lt;id&gt;123&lt;/id&gt;
+ *    &lt;revision&gt;
+ *      &lt;id&gt;100&lt;/id&gt;
+ *      ....
+ *    &lt;/revision&gt;
+ *    &lt;revision&gt;
+ *      &lt;id&gt;200&lt;/id&gt;
+ *      ....
+ *    &lt;/revision&gt;
+ *  &lt;/page&gt;
+ * </code></pre>
+ * <pre><code>
+ *  &lt;page&gt;
+ *    &lt;title&gt;ABC&lt;/title&gt;
+ *    &lt;id&gt;123&lt;/id&gt;
+ *    &lt;revision&gt;
+ *      &lt;id&gt;200&lt;/id&gt;
+ *      ....
+ *    &lt;/revision&gt;
+ *    &lt;revision&gt;
+ *      &lt;id&gt;300&lt;/id&gt;
+ *      ....
+ *    &lt;/revision&gt;
+ *  &lt;/page&gt;
+ * </code></pre>
+ * <pre><code>
+ *  &lt;page&gt;
+ *    &lt;title&gt;DEF&lt;/title&gt;
+ *    &lt;id&gt;456&lt;/id&gt;
+ *    &lt;revision&gt;&lt;revision beginningofpage="true"&gt;&lt;text xml:space="preserve"&gt;&lt;/text&gt;&lt;/revision&gt;&lt;revision&gt;
+ *      &lt;id&gt;400&lt;/id&gt;
+ *      ....
+ *    &lt;/revision&gt;
+ *  &lt;/page&gt;
+ * </code></pre>
+ */
+public class StreamWikiDumpInputFormat extends KeyValueTextInputFormat {
+
+  private static final String KEY_EXCLUDE_PAGE_PATTERN = "org.wikimedia.wikihadoop.excludePagesWith";
+  private static final String KEY_PREVIOUS_REVISION    = "org.wikimedia.wikihadoop.previousRevision";
+  private static final String KEY_SKIP_FACTOR          = "org.wikimedia.wikihadoop.skipFactor";
+  private CompressionCodecFactory compressionCodecs = null;
+   
+   
+	// New API
+	/*@Override
+	public RecordReader<LongWritable, Text> createRecordReader(InputSplit split, TaskAttemptContext context) {
+		return new LineRecordReader();
+	}*/
+
+	@Override
+	protected boolean isSplitable(JobContext context, Path file) {
+		CompressionCodec codec = new CompressionCodecFactory(context.getConfiguration()).getCodec(file);
+		if (null == codec) {
+			return true;
+		}
+		return codec instanceof SplittableCompressionCodec;
+	}
+	
+	/*
+	// Old API
+	public void configure(JobConf conf) {
+		compressionCodecs = new CompressionCodecFactory(conf);
+	}
+	
+	protected boolean isSplitable(FileSystem fs, Path file) {
+		final CompressionCodec codec = compressionCodecs.getCodec(file);
+		if (null == codec) {
+		return true;
+		}
+		return codec instanceof SplittableCompressionCodec;
+	}
+
+	public RecordReader<LongWritable, Text> getRecordReader(InputSplit genericSplit, JobConf job, Reporter reporter)
+	throws IOException {
+	
+		reporter.setStatus(genericSplit.toString());
+		return new LineRecordReader(job, (FileSplit) genericSplit);
+	}
+	*/
+   
+   
+  /*public void configure(JobConf conf) {
+    this.compressionCodecs = new CompressionCodecFactory(conf);
+  }*/
+  
+  /*protected boolean isSplitable(FileSystem fs, Path file) {
+    final CompressionCodec codec = compressionCodecs.getCodec(file);
+    if (null == codec) {
+      return true;
+    }
+    return codec instanceof SplittableCompressionCodec;
+  }*/
+
+  /** 
+   * Generate the list of files and make them into FileSplits.
+   * @param job the job context
+   * @throws IOException
+   */
+  @Override
+  //public InputSplit[] getSplits(JobConf job, int numSplits) throws IOException {
+  public List<InputSplit> getSplits(JobContext job ) throws IOException {
+    //LOG.info("StreamWikiDumpInputFormat.getSplits job=" + job + " n=" + numSplits);
+    //InputSplit[] oldSplits = super.getSplits(job, numSplits);
+    List<InputSplit> oldSplits = super.getSplits(job);
+    List<InputSplit> splits = new ArrayList<InputSplit>();
+    FileStatus[] files = listStatus(job);
+    // Save the number of input files for metrics/loadgen
+    //job.setLong(NUM_INPUT_FILES, files.length);
+    job.setLong("mapreduce.input.num.files", files.length);
+    long totalSize = 0;                           // compute total size
+    for (FileStatus file : files) {                // check we have valid files
+      //if (file.isDirectory()) {              // check we have valid files
+      if (file.isDir()) {
+        throw new IOException("Not a file: "+ file.getPath());
+      }
+      totalSize += file.getLen();
+    }
+    //long minSize = job.getLong(org.apache.hadoop.mapreduce.lib.input.FileInputFormat.SPLIT_MINSIZE, 1);
+    long minSize = job.getLong(org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat.SPLIT_MINSIZE_PERNODE, 1);
+    long goalSize = totalSize / (numSplits == 0 ? 1 : numSplits);
+    for (FileStatus file : files) {
+      //if (file.isDirectory()) {
+      if (file.isDir()) {
+        throw new IOException("Not a file: "+ file.getPath());
+      }
+      long blockSize = file.getBlockSize();
+      long splitSize = computeSplitSize(goalSize, minSize, blockSize);
+      //LOG.info(String.format("goalsize=%d splitsize=%d blocksize=%d", goalSize, splitSize, blockSize));
+      //System.err.println(String.format("goalsize=%d splitsize=%d blocksize=%d", goalSize, splitSize, blockSize));
+      for (InputSplit x: getSplits(job, file, pageBeginPattern, splitSize) ) 
+        splits.add(x);
+    }
+    System.err.println("splits="+splits);
+    return splits.toArray(new InputSplit[splits.size()]);
+  }
+
+
+  /*private FileSplit makeSplit(Path path, long start, long size, NetworkTopology clusterMap, BlockLocation[] blkLocations) throws IOException {
+    return makeSplit(path, start, size, getSplitHosts(blkLocations, start, size, clusterMap));
+  }*/
+
+  //public List<InputSplit> getSplits(JobConf job, FileStatus file, String pattern, long splitSize) throws IOException {
+  public List<InputSplit> getSplits(JobContext job) throws IOException {
+    NetworkTopology clusterMap = new NetworkTopology();
+    List<InputSplit> splits = new ArrayList<InputSplit>();
+    Path path = file.getPath();
+    long length = file.getLen();
+    FileSystem fs = path.getFileSystem(job.getConfiguration());
+    BlockLocation[] blkLocations = fs.getFileBlockLocations(file, 0, length);
+    if ((length != 0) && isSplitable(fs, path)) { 
+      
+      long bytesRemaining = length;
+      SeekableInputStream in = SeekableInputStream.getInstance
+        (path, 0, length, fs, this.compressionCodecs);
+      SplitCompressionInputStream is = in.getSplitCompressionInputStream();
+      long start = 0;
+      long skip = 0;
+      if ( is != null ) {
+        start = is.getAdjustedStart();
+        length = is.getAdjustedEnd();
+        is.close();
+        in = null;
+      }
+      //LOG.info("locations=" + Arrays.asList(blkLocations));
+      FileSplit split = null;
+      Set<Long> processedPageEnds = new HashSet<Long>();
+      float factor = job.getFloat(KEY_SKIP_FACTOR, 1.2F);
+
+      READLOOP:
+      while (((double) bytesRemaining)/splitSize > factor  &&  bytesRemaining > 0) {
+        // prepare matcher
+        ByteMatcher matcher;
+        {
+          long st = Math.min(start + skip + splitSize, length - 1);
+          //split = makeSplit(path, st, Math.min(splitSize, length - st), clusterMap, blkLocations);
+          split = new FileSplit(path, st, Math.min(splitSize, length - st), getSplitHosts(blkLocations, start, Math.min(splitSize, length - st), clusterMap));
+          
+          System.err.println("split move to: " + split);
+          if ( in != null )
+            in.close();
+          if ( split.getLength() <= 1 ) {
+            break;
+          }
+          in = SeekableInputStream.getInstance(split,
+                                               fs, this.compressionCodecs);
+          SplitCompressionInputStream cin = in.getSplitCompressionInputStream();
+        }
+        matcher = new ByteMatcher(in);
+
+        // read until the next page end in the look-ahead split
+        boolean reach = false;
+        while ( !matcher.readUntilMatch(pageEndPattern, null, split.getStart() + split.getLength()) ) {
+          if (matcher.getPos() >= length  ||  split.getLength() == length - split.getStart())
+            break READLOOP;
+          reach = false;
+          //split = makeSplit(path, split.getStart(), Math.min(split.getLength() + splitSize, length - split.getStart()), clusterMap, blkLocations);
+          split = new FileSplit(path, split.getStart(), Math.min(split.getLength() + splitSize, length - split.getStart()), getSplitHosts(blkLocations, start, Math.min(split.getLength() + splitSize, length - split.getStart()), clusterMap));
+          System.err.println("split extend to: " + split);
+        }
+        System.err.println(path + ": #" + splits.size() + " " + pageEndPattern + " found: pos=" + matcher.getPos() + " last=" + matcher.getLastUnmatchPos() + " read=" + matcher.getReadBytes() + " current=" + start + " remaining=" + bytesRemaining + " split=" + split);
+        if ( matcher.getLastUnmatchPos() > 0
+             &&  matcher.getPos() > matcher.getLastUnmatchPos()
+             &&  !processedPageEnds.contains(matcher.getPos()) ) {
+          //splits.add(makeSplit(path, start, matcher.getPos() - start, clusterMap, blkLocations));
+          splits.add(new FileSplit(path, start, matcher.getPos() - start, getSplitHosts(blkLocations, start, matcher.getPos() - start, clusterMap)));
+          
+          processedPageEnds.add(matcher.getPos());
+          long newstart = Math.max(matcher.getLastUnmatchPos(), start);
+          bytesRemaining = length - newstart;
+          start = newstart;
+          skip = 0;
+        } else {
+          skip = matcher.getPos() - start;
+        }
+      }
+      
+      if (bytesRemaining > 0 && !processedPageEnds.contains(length)) {
+        System.err.println(pageEndPattern + " remaining: pos=" + (length-bytesRemaining) + " end=" + length);
+        //splits.add(makeSplit(path, length-bytesRemaining, bytesRemaining, blkLocations[blkLocations.length-1].getHosts()));
+        splits.add(new FileSplit(path, length-bytesRemaining, bytesRemaining, blkLocations[blkLocations.length-1].getHosts()));
+      }
+      if ( in != null )
+        in.close();
+    } else if (length != 0) {
+      //splits.add(makeSplit(path, 0, length, clusterMap, blkLocations));
+      splits.add(new FileSplit(path, 0, length, getSplitHosts(blkLocations, 0, length, clusterMap)));
+    } else { 
+      //Create empty hosts array for zero length files
+      //splits.add(makeSplit(path, 0, length, new String[0]));
+      splits.add(new FileSplit(path, 0, length, new String[0]));
+    }
+    return splits;
+  }
+
+  public RecordReader<Text, Text> getRecordReader(final InputSplit genericSplit,
+                                                  JobConf job, Reporter reporter) throws IOException {
+    // handling non-standard record reader (likely StreamXmlRecordReader) 
+    FileSplit split = (FileSplit) genericSplit;
+    //LOG.info("getRecordReader start.....split=" + split);
+    reporter.setStatus(split.toString());
+
+    // Open the file and seek to the start of the split
+    FileSystem fs = split.getPath().getFileSystem(job);
+    String patt = job.get(KEY_EXCLUDE_PAGE_PATTERN);
+    boolean prev = job.getBoolean(KEY_PREVIOUS_REVISION, true);
+    return new MyRecordReader(split, reporter, job, fs,
+                              patt != null && !"".equals(patt) ? Pattern.compile(patt): null,
+                              prev);
+  }
+
+  private class MyRecordReader implements RecordReader<Text,Text> {
+    
+    public MyRecordReader(FileSplit split, Reporter reporter,
+                          JobConf job, FileSystem fs,
+                          Pattern exclude, boolean prev) throws IOException {
+      this.revisionBeginPattern = "<revision";
+      this.revisionEndPattern   = "</revision>";
+      this.pageHeader   = new DataOutputBuffer();
+      this.prevRevision = new DataOutputBuffer();
+      this.pageFooter = getBuffer("\n</page>\n".getBytes("UTF-8"));
+      this.revHeader  = getBuffer(this.revisionBeginPattern.getBytes("UTF-8"));
+      this.firstDummyRevision = getBuffer(" beginningofpage=\"true\"><text xml:space=\"preserve\"></text></revision>\n".getBytes("UTF-8"));
+      this.bufInRev = new DataOutputBuffer();
+      this.bufBeforeRev = new DataOutputBuffer();
+      this.split = split;
+      this.fs = fs;
+      this.exclude = exclude;
+      this.recordPrevRevision = prev;
+      SeekableInputStream in = SeekableInputStream.getInstance(split, fs, compressionCodecs);
+      SplitCompressionInputStream sin = in.getSplitCompressionInputStream();
+      if ( sin == null ) {
+        this.start = split.getStart();
+        this.end   = split.getStart() + split.getLength();
+      } else {
+        this.start = sin.getAdjustedStart();
+        this.end   = sin.getAdjustedEnd() + 1;
+      }
+      this.reporter = reporter;
+
+      allWrite(this.prevRevision, this.firstDummyRevision);
+      this.currentPageNum = -1;
+      this.pageBytes = getPageBytes(this.split, this.fs, compressionCodecs, this.reporter);
+
+      this.istream = SeekableInputStream.getInstance(this.split, this.fs, compressionCodecs);
+      this.matcher = new ByteMatcher(this.istream, this.istream);
+      this.seekNextRecordBoundary();
+      this.reporter.incrCounter(WikiDumpCounters.WRITTEN_REVISIONS, 0);
+      this.reporter.incrCounter(WikiDumpCounters.WRITTEN_PAGES, 0);
+    }
+    
+    @Override public Text createKey() {
+      return new Text();
+    }
+    
+    @Override public Text createValue() {
+      return new Text();
+    }
+    
+    @Override public void close() throws IOException {
+      this.istream.close();
+    }
+    
+    @Override public float getProgress() throws IOException {
+      float rate = 0.0f;
+      if (this.end == this.start) {
+        rate = 1.0f;
+      } else {
+        rate = ((float)(this.getPos() - this.start)) / ((float)(this.end - this.start));
+      }
+      return rate;
+    }
+    
+    @Override public long getPos() throws IOException {
+      return this.matcher.getPos();
+    }
+    
+    public synchronized long getReadBytes() throws IOException {
+        return this.matcher.getReadBytes();
+      }
+    
+    @Override synchronized public boolean next(Text key, Text value) throws IOException {
+      //LOG.info("StreamWikiDumpInputFormat: split=" + split + " start=" + this.start + " end=" + this.end + " pos=" + this.getPos());
+
+      while (true) {
+        if ( this.nextPageBegin() < 0 ) {
+          return false;
+        }
+        
+        //System.err.println("0.2 check pos="+this.getPos() + " end="+this.end);//!
+        if (this.currentPageNum >= this.pageBytes.size() / 2  ||  this.getReadBytes() >= this.tailPageEnd()) {
+          return false;
+        }
+
+        //System.err.println("2 move to rev from: " + this.getReadBytes());//!
+        if (!readUntilMatch(this.revisionBeginPattern, this.bufBeforeRev)  ||  this.getReadBytes() >= this.tailPageEnd()) { // move to the beginning of the next revision
+          return false;
+        }
+        //System.err.println("2.1 move to rev to: " + this.getReadBytes());//!
+        
+        //System.err.println("4.5 check if exceed: " + this.getReadBytes() + " " + nextPageBegin() + " " + prevPageEnd());//!
+        if ( this.getReadBytes() >= this.nextPageBegin() ) {
+          // int off = (int)(this.nextPageBegin() - this.prevPageEnd());
+          int off = findIndex(pageBeginPattern.getBytes("UTF-8"), this.bufBeforeRev);
+          if ( off >= 0 ) {
+            offsetWrite(this.pageHeader, off, this.bufBeforeRev);
+            allWrite(this.prevRevision, this.firstDummyRevision);
+            this.currentPageNum++;
+            if ( this.exclude != null && this.exclude.matcher(new String(this.pageHeader.getData(), "UTF-8")).find() ) {
+              reporter.incrCounter(WikiDumpCounters.SKIPPED_PAGES, 1);
+              this.seekNextRecordBoundary();
+            } else {
+              reporter.incrCounter(WikiDumpCounters.WRITTEN_PAGES, 1);
+              break;
+            }
+            //System.err.println("4.6 exceed");//!
+          } else {
+            throw new IllegalArgumentException();
+          }
+        } else {
+          break;
+        }
+      }
+      
+      //System.err.println("4 read rev from: " + this.getReadBytes());//!
+      if (!readUntilMatch(this.revisionEndPattern, this.bufInRev)) { // store the revision
+        //System.err.println("no revision end" + this.getReadBytes() + " " + this.end);//!
+          //LOG.info("no revision end");
+          return false;
+        }
+      //System.err.println("4.1 read rev to: " + this.getReadBytes());//!
+        
+      //System.err.println("5 read rev pos " + this.getReadBytes());//!
+      byte[] record = this.recordPrevRevision ?
+        writeInSequence(new DataOutputBuffer[]{ this.pageHeader,
+                                                this.prevRevision,
+                                                this.revHeader,
+                                                this.bufInRev,
+                                                this.pageFooter}):
+        writeInSequence(new DataOutputBuffer[]{ this.pageHeader,
+                                                this.bufInRev,
+                                                this.pageFooter});
+      key.set(record);
+      //System.out.print(key.toString());//!
+      value.set("");
+      this.reporter.setStatus("StreamWikiDumpInputFormat: write new record pos=" + this.getPos() + " bytes=" + this.getReadBytes() + " next=" + this.nextPageBegin() + " prev=" + this.prevPageEnd());
+      reporter.incrCounter(WikiDumpCounters.WRITTEN_REVISIONS, 1);
+      
+      if ( this.recordPrevRevision ) {
+        allWrite(this.prevRevision, this.bufInRev);
+      }
+      
+      return true;
+    }
+    
+    public synchronized void seekNextRecordBoundary() throws IOException {
+      if ( this.getReadBytes() < this.nextPageBegin() ) {
+        long len = this.nextPageBegin() - this.getReadBytes();
+        this.matcher.skip(len);
+      }
+    }
+    private synchronized boolean readUntilMatch(String textPat, DataOutputBuffer outBufOrNull) throws IOException {
+      if ( outBufOrNull != null )
+        outBufOrNull.reset();
+      return this.matcher.readUntilMatch(textPat, outBufOrNull, this.end);
+    }
+    private long tailPageEnd() {
+      if ( this.pageBytes.size() > 0 ) {
+        return this.pageBytes.get(this.pageBytes.size() - 1);
+      } else {
+        return 0;
+      }
+    }
+    private long nextPageBegin() {
+      if ( (this.currentPageNum + 1) * 2 < this.pageBytes.size() ) {
+        return this.pageBytes.get((this.currentPageNum + 1) * 2);
+      } else {
+        return -1;
+      }
+    }
+    private long prevPageEnd() {
+      if ( this.currentPageNum == 0 ) {
+        if ( this.pageBytes.size() > 0 ) {
+          return this.pageBytes.get(0);
+        } else {
+          return 0;
+        }
+      } else if ( this.currentPageNum * 2 - 1 <= this.pageBytes.size() - 1 ) {
+        return this.pageBytes.get(this.currentPageNum * 2 - 1);
+      } else {
+        return this.pageBytes.get(this.pageBytes.size() - 1);
+      }
+    }  
+  
+    private int currentPageNum;
+    private final Pattern exclude;
+    private final boolean recordPrevRevision;
+    private final long start;
+    private final long end;
+    private final List<Long> pageBytes;
+    private final SeekableInputStream  istream;
+    private final String revisionBeginPattern;
+    private final String revisionEndPattern;
+    private final DataOutputBuffer pageHeader;
+    private final DataOutputBuffer revHeader;
+    private final DataOutputBuffer prevRevision;
+    private final DataOutputBuffer pageFooter;
+    private final DataOutputBuffer firstDummyRevision;
+    private final DataOutputBuffer bufInRev;
+    private final DataOutputBuffer bufBeforeRev;
+    private final FileSystem fs;
+    private final FileSplit split;
+    private final Reporter reporter;
+    private final ByteMatcher matcher;
+  }
+
+  private static byte[] writeInSequence(DataOutputBuffer[] array) {
+    int size = 0;
+    for (DataOutputBuffer buf: array) {
+      size += buf.getLength();
+    }
+    byte[] dest = new byte[size];
+    int n = 0;
+    for (DataOutputBuffer buf: array) {
+      System.arraycopy(buf.getData(), 0, dest, n, buf.getLength());
+      n += buf.getLength();
+    }
+    return dest;
+  }
+  
+  private static DataOutputBuffer getBuffer(byte[] bytes) throws IOException {
+    DataOutputBuffer ret = new DataOutputBuffer(bytes.length);
+    ret.write(bytes);
+    return ret;
+  }
+
+  private static List<Long> getPageBytes(FileSplit split, FileSystem fs, CompressionCodecFactory compressionCodecs, Reporter reporter) throws IOException {
+    SeekableInputStream in = null;
+    try {
+      in = SeekableInputStream.getInstance(split, fs, compressionCodecs);
+      long start = split.getStart();
+      long end   = start + split.getLength();
+      SplitCompressionInputStream cin = in.getSplitCompressionInputStream();
+      if ( cin != null ) {
+        start = cin.getAdjustedStart();
+        end   = cin.getAdjustedEnd() + 1;
+      }
+      ByteMatcher matcher = new ByteMatcher(in, in);
+      List<Long> ret = new ArrayList<Long>();
+      while ( true ) {
+        if ( matcher.getPos() >= end || !matcher.readUntilMatch(pageBeginPattern, null, end) ) {
+          break;
+        }
+        ret.add(matcher.getReadBytes() - pageBeginPattern.getBytes("UTF-8").length);
+        if ( matcher.getPos() >= end || !matcher.readUntilMatch(pageEndPattern, null, end) ) {
+          System.err.println("could not find "+pageEndPattern+", page over a split?  pos=" + matcher.getPos() + " bytes=" + matcher.getReadBytes());
+          //ret.add(end);
+          break;
+        }
+        ret.add(matcher.getReadBytes() - pageEndPattern.getBytes("UTF-8").length);
+        String report = String.format("StreamWikiDumpInputFormat: find page %6d start=%d pos=%d end=%d bytes=%d", ret.size(), start, matcher.getPos(), end, matcher.getReadBytes());
+        reporter.setStatus(report);
+        reporter.incrCounter(WikiDumpCounters.FOUND_PAGES, 1);
+        //LOG.info(report);
+      }
+      if ( ret.size() % 2 == 0 ) {
+        ret.add(matcher.getReadBytes());
+      }
+      //System.err.println("getPageBytes " + ret);//!
+      return ret;
+    } finally {
+      if ( in != null ) {
+        in.close();
+      }
+    }
+  }
+
+  private static void offsetWrite(DataOutputBuffer to, int fromOffset, DataOutputBuffer from) throws IOException {
+    if ( from.getLength() <= fromOffset || fromOffset < 0 ) {
+      throw new IllegalArgumentException(String.format("invalid offset: offset=%d length=%d", fromOffset, from.getLength()));
+    }
+    byte[] bytes = new byte[from.getLength() - fromOffset];
+    System.arraycopy(from.getData(), fromOffset, bytes, 0, bytes.length);
+    to.reset();
+    to.write(bytes);
+  }
+  private static void allWrite(DataOutputBuffer to, DataOutputBuffer from) throws IOException {
+    offsetWrite(to, 0, from);
+  }
+
+  private static int findIndex(byte[] match, DataOutputBuffer from_) throws IOException {
+    // TODO: faster string pattern match (KMP etc)
+    int m = 0;
+    int i;
+    byte[] from = from_.getData();
+    for ( i = 0; i < from_.getLength(); ++i ) {
+      if ( from[i] == match[m] ) {
+        ++m;
+      } else {
+        m = 0;
+      }
+      if ( m == match.length ) {
+        return i - m + 1;
+      }
+    }
+    // throw new IllegalArgumentException("pattern not found: " + new String(match) + " in " + new String(from));
+    System.err.println("pattern not found: " + new String(match) + " in " + new String(from, 0, from_.getLength()));//!
+    return -1;
+  }
+
+  private static enum WikiDumpCounters {
+    FOUND_PAGES, WRITTEN_REVISIONS, WRITTEN_PAGES, SKIPPED_PAGES
+  }
+
+  private static final String pageBeginPattern = "<page>";
+  private static final String pageEndPattern   = "</page>";
+}
\ No newline at end of file
Index: src/contrib/wikipedia/src/java/org/apache/hadoop/mapreduce/wikipedia/WikiLengths.java
===================================================================
--- src/contrib/wikipedia/src/java/org/apache/hadoop/mapreduce/wikipedia/WikiLengths.java	(revision 0)
+++ src/contrib/wikipedia/src/java/org/apache/hadoop/mapreduce/wikipedia/WikiLengths.java	(working copy)
@@ -0,0 +1,195 @@
+package org.apache.hadoop.mapreduce.wikipedia;
+
+import java.io.IOException;
+
+import org.apache.commons.cli.CommandLine;
+import org.apache.commons.cli.CommandLineParser;
+import org.apache.commons.cli.GnuParser;
+import org.apache.commons.cli.HelpFormatter;
+import org.apache.commons.cli.OptionBuilder;
+import org.apache.commons.cli.Options;
+import org.apache.commons.cli.ParseException;
+
+import org.apache.hadoop.conf.Configuration;
+
+import org.apache.hadoop.fs.Path;
+
+import org.apache.hadoop.io.IntWritable;
+import org.apache.hadoop.io.LongWritable;
+import org.apache.hadoop.io.Text;
+import org.apache.hadoop.mapreduce.Job;
+import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
+import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
+import org.apache.hadoop.mapreduce.lib.reduce.IntSumReducer;
+import org.apache.hadoop.util.GenericOptionsParser;
+import org.apache.hadoop.util.Tool;
+import org.apache.hadoop.util.ToolRunner;
+
+import org.apache.hadoop.util.GenericOptionsParser;
+
+import org.apache.hadoop.mapreduce.ParameterPartitioner;
+import org.apache.hadoop.mapreduce.MultistageSamplingPartitioner;
+
+/**
+ * This job produces the histogram of the lenghts of wikipedia articles.
+ */
+public class WikiLengths {
+	/**
+	 * Launch wikipedia length histogram.
+	 */
+	public static void main(String[] args) throws Exception {
+		Configuration conf = new Configuration();
+		conf.set("xmlinput.start", "<page>");
+		conf.set("xmlinput.end", "</page>");
+		conf.set("io.serializations", "org.apache.hadoop.io.serializer.JavaSerialization,org.apache.hadoop.io.serializer.WritableSerialization");
+		
+		// Parsing options
+		String[] otherArgs = new GenericOptionsParser(conf, args).getRemainingArgs();
+		
+		// Options for the rest
+		Options options = new Options();
+		options.addOption("i", "input",    true,  "Input file");
+		options.addOption("o", "output",   true,  "Output file");
+		options.addOption("r", "reduces",  true,  "Number of reducers");
+		options.addOption("c", "incr",     false, "Incremental reducers");
+		options.addOption("d", "drop",     true,  "Percentage of maps to drop");
+		options.addOption("n", "inidrop",  true,  "Percentage of maps to drop initially");
+		options.addOption("p", "precise",  false, "Precise job");
+		options.addOption("s", "sampling", true,  "Sampling rate 1/X");
+		options.addOption("t", "task",     true,  "Task: project, other");
+		
+		try {
+			CommandLine cmdline = new GnuParser().parse(options, otherArgs);
+			// Input output
+			String input  = cmdline.getOptionValue("i");
+			String output = cmdline.getOptionValue("o");
+			if (input == null || output == null) {
+				throw new ParseException("No input/output option");
+			}
+			// Task to perform
+			if (cmdline.hasOption("t")) {
+				conf.set("task", cmdline.getOptionValue("t"));
+			}
+			// Reduces
+			int numReducers = 1;
+			if (cmdline.hasOption("r")) {
+				numReducers = Integer.parseInt(cmdline.getOptionValue("r"));
+			}
+			// Incremental reducers
+			if (cmdline.hasOption("c")) {
+				conf.setBoolean("mapred.tasks.incremental.reduction", true);
+				conf.setBoolean("mapred.tasks.clustering", true); // We arrange the intermediate keys by clusters
+			}
+			// Dropping maps
+			if (cmdline.hasOption("d")) {
+				float dropPercentage = Float.parseFloat(cmdline.getOptionValue("d"));
+				conf.setInt("mapred.map.approximate.drop.extratime", 1);
+				conf.setFloat("mapred.map.approximate.drop.percentage", dropPercentage/100);
+			}
+			// Dropping maps
+			if (cmdline.hasOption("n")) {
+				float dropPercentage = Float.parseFloat(cmdline.getOptionValue("n"));
+				conf.setFloat("mapred.map.approximate.drop.ini.percentage", 1-(dropPercentage/100)); // Percentage of maps we initially drop
+			}
+			// Precise job
+			if (cmdline.hasOption("p")) {
+				conf.setBoolean("mapred.job.precise", true);
+				conf.setBoolean("mapred.tasks.incremental.reduction", false);
+			}
+			if (cmdline.hasOption("s")) {
+				int samplingRate = Integer.parseInt(cmdline.getOptionValue("s"));
+				conf.setInt("mapred.input.approximate.skip", samplingRate);
+			}
+			
+			// Create job
+			Job job = new Job(conf, "Wikipedia lengths histogram");
+			
+			job.setJarByClass(WikiLengths.class);
+			
+			job.setNumReduceTasks(numReducers);
+			
+			job.setMapperClass(WikiLengthsMapper.class);
+			if (cmdline.hasOption("c")) {
+				job.setReducerClass(WikiLengthsReducerIncr.class);
+			} else {
+				job.setReducerClass(WikiLengthsReducer.class);
+			}
+			
+			// For this approach, we can use a combiner that sums the outputs in the maps
+			job.setCombinerClass(IntSumReducer.class);
+			
+			job.setMapOutputKeyClass(Text.class);
+			job.setMapOutputValueClass(IntWritable.class);
+			
+			job.setOutputKeyClass(Text.class);
+			job.setOutputValueClass(IntWritable.class);
+			
+			// We need a partitioner that sends clustering information to all reducers
+			if (!cmdline.hasOption("p")) {
+				if (cmdline.hasOption("c")) {
+					job.setPartitionerClass(ParameterPartitioner.class);
+				} else {
+					job.setPartitionerClass(MultistageSamplingPartitioner.class);
+				}
+			}
+			
+			// Approximate Hadoop
+			//job.setInputFormatClass(WikipediaPageInputFormat.class);
+			job.setInputFormatClass(XMLInputFormat.class);
+			
+			// Input and output
+			FileInputFormat.addInputPath(job,   new Path(input));
+			FileOutputFormat.setOutputPath(job, new Path(output));
+			
+			// Run and exit
+			System.exit(job.waitForCompletion(true) ? 0 : 1);
+		} catch (ParseException exp) {
+			System.err.println("Error parsing command line: " + exp.getMessage());
+			HelpFormatter formatter = new HelpFormatter();
+			formatter.printHelp(WikiLengths.class.toString(), options);
+			ToolRunner.printGenericCommandUsage(System.out);
+			System.exit(2);
+		}
+	}
+
+	/*public static void runJob(Configuration conf, String input, String output) throws Exception {
+		conf.set("xmlinput.start", "<page>");
+		conf.set("xmlinput.end", "</page>");
+		conf.set("io.serializations", "org.apache.hadoop.io.serializer.JavaSerialization,org.apache.hadoop.io.serializer.WritableSerialization");
+
+		// These two lines enable bzip output from the reducer
+		//conf.setBoolean("mapred.output.compress", true);
+		//conf.setClass("mapred.output.compression.codec", BZip2Codec.class, CompressionCodec.class);
+		// SimpleDateFormat ymdhms = new SimpleDateFormat("yyyy-MM-ddTHH:mm:ss";
+		SimpleDateFormat ymdhms = new SimpleDateFormat("HH:mm:ss");
+		Job job = new Job(conf, "wikPageLengths " + ymdhms.format(new Date()));
+		
+		FileInputFormat.addInputPath(job, new Path(input));
+		FileOutputFormat.setOutputPath(job, new Path(output));
+		
+		job.setJarByClass(WikiLengths.class);
+		
+		job.setMapperClass(WikiLengthsMapper .class);
+		job.setReducerClass(WikiLengthsReducer.class);
+		
+		job.setInputFormatClass(XMLInputFormat.class);
+		
+		job.setMapOutputKeyClass(LongWritable.class);
+		job.setMapOutputValueClass(LongWritable.class);
+		
+		job.setOutputKeyClass(LongWritable.class);
+		job.setOutputValueClass(LongWritable.class);
+		
+		job.waitForCompletion(true);
+	}
+	
+	public static void main(String[] args) throws Exception {
+		Configuration conf = new Configuration();
+		String[] otherArgs = new GenericOptionsParser(conf, args).getRemainingArgs();
+		if (otherArgs.length != 2) {
+			System.err.println("Usage: wikilengths <in> <out>");
+			System.exit(2);
+		}
+		runJob(conf, otherArgs[0], otherArgs[1]);
+	}*/
+}
\ No newline at end of file
Index: src/contrib/wikipedia/src/java/org/apache/hadoop/mapreduce/wikipedia/WikiLengthsMapper.java
===================================================================
--- src/contrib/wikipedia/src/java/org/apache/hadoop/mapreduce/wikipedia/WikiLengthsMapper.java	(revision 0)
+++ src/contrib/wikipedia/src/java/org/apache/hadoop/mapreduce/wikipedia/WikiLengthsMapper.java	(working copy)
@@ -0,0 +1,34 @@
+package org.apache.hadoop.mapreduce.wikipedia;
+
+import java.io.IOException;
+
+import org.apache.hadoop.io.IntWritable;
+import org.apache.hadoop.io.LongWritable;
+import org.apache.hadoop.io.Text;
+
+import org.apache.hadoop.mapreduce.MultistageSamplingMapper;
+
+/**
+ * Mapper that checks the size of wikipedia articles.
+ */
+public class WikiLengthsMapper extends MultistageSamplingMapper<LongWritable, Text, Text, IntWritable> {
+	private IntWritable one = new IntWritable(1);
+	
+	public static enum mapCounters { NUMPAGES, MAPID }
+	
+	/**
+	 * Map to link size to numer of pages.
+	 */
+	public void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
+		// Size -> One page [We group in 100s]
+		//context.write(new LongWritable((value.getLength()/100)*100), one);
+		//context.write(new Text(Integer.toString((value.getContent().length()/100)*100)), one);
+		context.write(new Text(Integer.toString((value.getLength()/100)*100)), one);
+		
+		// Offset -> Page size
+		//context.write(key, new LongWritable(value.getLength()));
+		
+		// Count the number of pages
+		context.getCounter(mapCounters.NUMPAGES).increment(1);
+	}
+}
Index: src/contrib/wikipedia/src/java/org/apache/hadoop/mapreduce/wikipedia/WikiLengthsReducer.java
===================================================================
--- src/contrib/wikipedia/src/java/org/apache/hadoop/mapreduce/wikipedia/WikiLengthsReducer.java	(revision 0)
+++ src/contrib/wikipedia/src/java/org/apache/hadoop/mapreduce/wikipedia/WikiLengthsReducer.java	(working copy)
@@ -0,0 +1,28 @@
+package org.apache.hadoop.mapreduce.wikipedia;
+
+import java.io.IOException;
+
+import org.apache.hadoop.io.IntWritable;
+import org.apache.hadoop.io.LongWritable;
+import org.apache.hadoop.io.Text;
+
+import org.apache.hadoop.mapreduce.MultistageSamplingReducer;
+
+/**
+ * Aggregate the lenghts of the wikipedia pages using multistage sampling.
+ */
+public class WikiLengthsReducer extends MultistageSamplingReducer<Text, IntWritable, Text, LongWritable> {
+	private LongWritable result = new LongWritable();
+	
+	/**
+	 * Precise version that collects everybody and outputs it.
+	 */
+	public void reduce(Text key, Iterable<IntWritable> values, Context context) throws IOException, InterruptedException {
+		long sum = 0;
+		for (IntWritable val : values) {
+			sum += val.get();
+		}
+		result.set(sum);
+		context.write(key, result);
+	}
+}
Index: src/contrib/wikipedia/src/java/org/apache/hadoop/mapreduce/wikipedia/WikiLengthsReducerIncr.java
===================================================================
--- src/contrib/wikipedia/src/java/org/apache/hadoop/mapreduce/wikipedia/WikiLengthsReducerIncr.java	(revision 0)
+++ src/contrib/wikipedia/src/java/org/apache/hadoop/mapreduce/wikipedia/WikiLengthsReducerIncr.java	(working copy)
@@ -0,0 +1,166 @@
+package org.apache.hadoop.mapreduce.wikipedia;
+
+import java.io.IOException;
+
+import org.apache.hadoop.io.IntWritable;
+import org.apache.hadoop.io.LongWritable;
+import org.apache.hadoop.io.Text;
+
+import org.apache.hadoop.mapreduce.MultistageSamplingReducerIncr;
+
+/**
+ * Aggregate the lenghts of the wikipedia pages using multistage sampling.
+ */
+public class WikiLengthsReducerIncr extends MultistageSamplingReducerIncr<Text, IntWritable, Text, LongWritable> {
+	private LongWritable result = new LongWritable();
+	
+	/**
+	 * Precise version that collects everybody and outputs it.
+	 */
+	public void reduce(Text key, Iterable<IntWritable> values, Context context) throws IOException, InterruptedException {
+		long sum = 0;
+		for (IntWritable val : values) {
+			sum += val.get();
+		}
+		result.set(sum);
+		context.write(key, result);
+	}
+}
+
+
+// OLD STUFF
+//private LinkedList<HashMap<Long,Long>> clusters;
+//HashMap<Long,Long> totalCluster;
+
+/*public void setup(Context context) {
+	super.setup(context);
+	if (!isPrecise()) {
+		clusters = new LinkedList<HashMap<Long,Long>>();
+		clusters.addLast(new HashMap<Long,Long>());
+		totalCluster = new HashMap<Long,Long>();
+	}
+}*/
+
+/*public void reduce(LongWritable key, Iterable<LongWritable> values, Context context) throws IOException, InterruptedException {
+	// Calculate aggregated value
+	long counter = 0;
+	for(LongWritable v : values){
+		counter++;
+		//counter += v.get();
+	}
+	
+	// Add to the proper cluster
+	synchronized(this) {
+		clusters.getLast().put(key.get(), counter);
+		if (!totalCluster.containsKey(key.get())) {
+			totalCluster.put(key.get(), 0L);
+		}
+	}
+}*/
+
+// TODO I need to check that this transformation is proper
+/*protected synchronized void checkQuality(Context context, boolean output) throws IOException, InterruptedException {
+	long maxAbs= 0;
+	double maxError = 0.0;
+	double maxErrorRel = 0.0;
+	
+	// Get current paramters
+	int numMaps   = context.getConfiguration().getInt("mapred.map.tasks", -1);
+	int skipPages = context.getConfiguration().getInt("mapred.input.approximate.skip", 1);
+	
+	// Multistage sampling
+	int N = numMaps;
+	int n = clusters.size();
+	
+	// Calculate populations in each cluster
+	long[] mi = new long[n];
+	long[] Mi = new long[n];
+	for (int i=0; i<n; i++) {
+		//mi[i] = clusters.get(i).size();
+		mi[i] = 0;
+		for (Long v : clusters.get(i).values()) {
+			mi[i] += v;
+		}
+		Mi[i] = mi[i]*skipPages; // This is an approximation based on the sampling ratio
+	}
+	
+	// Get the t-score for the current distribution
+	double tscore = MultistageSamplingReducer.getTScore(n-1, 0.95);
+	
+	// Go over all the keys and estimate their values
+	for (Long key : totalCluster.keySet()) {
+		// Collect the results for each cluster
+		long[] yi = new long[n];
+		long[] yti = new long[n];
+		for (int i=0; i<n; i++) {
+			yi[i] = 0;
+			HashMap<Long,Long> cluster = clusters.get(i);
+			if (cluster.containsKey(key)) {
+				yi[i] = cluster.get(key);
+			}
+			yti[i] = mi[i];
+		}
+
+		// Estimate the results in each cluster
+		double[] yhati = new double[n];
+		for (int i=0; i<n; i++) {
+			yhati[i] = (1.0*Mi[i]/mi[i])* yi[i];
+		}
+		
+		// Estimate the total number
+		double tauhat = (1.0*N/n)*sum(yhati);
+		
+		// Calculate the errors
+		// Estimate the global deviation
+		double su2 = var(yhati);
+	
+		// Calculate proportions
+		double[] pi = new double[n];
+		for (int i=0; i<n; i++) {
+			pi[i] = 1.0*yi[i]/yti[i];
+		}
+		
+		// Estimate variance in primary
+		double[] si2 = new double[n];
+		for (int i=0; i<n; i++) {
+			si2[i] = (1.0*mi[i]/(mi[i]-1.0)) * pi[i] * (1.0-pi[i]);
+		}
+		
+		// Calculate total variance
+		double var1 = 1.0*(N*(N-n)*su2)/n;
+		double var2 = 0.0;
+		for (int i=0; i<n; i++) {
+			var2 += 1.0*(Mi[i]*(Mi[i]-mi[i])*si2[i])/mi[i];
+		}
+		var2 = (1.0*N/n)*var2;
+		
+		double vartauhat = var1 + var2;
+		Double setauhat = Math.sqrt(vartauhat);
+		if (setauhat.isNaN()) { 
+			setauhat = Double.MAX_VALUE;
+		}
+		
+		// Check which is the maximum error
+		if (tscore*setauhat > maxError) {
+			maxAbs = (long) tauhat;
+			maxError = tscore*setauhat;
+			maxErrorRel = 100.0*tscore*setauhat/tauhat;
+		}
+		
+		// Output the estimation
+		if (output) {
+			LongWritable outkey = new LongWritable(key);
+			ApproximateLongWritable outval = new ApproximateLongWritable((long) tauhat, tscore*setauhat);
+			context.write(outkey, outval);
+		}
+	}
+	
+	// Check if we should start dropping
+	if (n>1) {
+		System.out.format(new Date() + ": %d/%d maps error is %d+/-%.2f (+/-%.2f%%)\n", n, N, maxAbs, maxError, maxErrorRel);
+		if (maxErrorRel < 10.0) { // >10%
+			System.out.format("With this error, we can drop!\n");
+			context.setStatus("dropping");
+		}
+	}
+}*/
Index: src/contrib/wikipedia/src/java/org/apache/hadoop/mapreduce/wikipedia/WikiPageRank.java
===================================================================
--- src/contrib/wikipedia/src/java/org/apache/hadoop/mapreduce/wikipedia/WikiPageRank.java	(revision 0)
+++ src/contrib/wikipedia/src/java/org/apache/hadoop/mapreduce/wikipedia/WikiPageRank.java	(working copy)
@@ -0,0 +1,170 @@
+/*
+ * Cloud9: A MapReduce Library for Hadoop
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License"); you
+ * may not use this file except in compliance with the License. You may
+ * obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
+ * implied. See the License for the specific language governing
+ * permissions and limitations under the License.
+ */
+
+package org.apache.hadoop.mapreduce.wikipedia;
+
+import org.apache.commons.cli.CommandLine;
+import org.apache.commons.cli.CommandLineParser;
+import org.apache.commons.cli.GnuParser;
+import org.apache.commons.cli.HelpFormatter;
+import org.apache.commons.cli.OptionBuilder;
+import org.apache.commons.cli.Options;
+import org.apache.commons.cli.ParseException;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.io.IntWritable;
+import org.apache.hadoop.io.LongWritable;
+import org.apache.hadoop.io.Text;
+import org.apache.hadoop.mapreduce.Job;
+import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
+import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
+import org.apache.hadoop.mapreduce.lib.reduce.IntSumReducer;
+import org.apache.hadoop.util.GenericOptionsParser;
+import org.apache.hadoop.util.Tool;
+import org.apache.hadoop.util.ToolRunner;
+
+import org.apache.hadoop.mapreduce.ParameterPartitioner;
+import org.apache.hadoop.mapreduce.MultistageSamplingPartitioner;
+
+/**
+ * Count the number of links each page gets. It supports 3-stage sampling.
+ */
+public class WikiPageRank {
+	/**
+	 * Launch wikipedia page rank.
+	 */
+	public static void main(String[] args) throws Exception {
+		Configuration conf = new Configuration();
+		conf.set("xmlinput.start", "<page>");
+		conf.set("xmlinput.end", "</page>");
+		conf.set("io.serializations", "org.apache.hadoop.io.serializer.JavaSerialization,org.apache.hadoop.io.serializer.WritableSerialization");
+		
+		// Parsing options
+		String[] otherArgs = new GenericOptionsParser(conf, args).getRemainingArgs();
+		
+		// Options for the rest
+		Options options = new Options();
+		options.addOption("i", "input",    true,  "Input file");
+		options.addOption("o", "output",   true,  "Output file");
+		options.addOption("r", "reduces",  true,  "Number of reducers");
+		options.addOption("c", "incr",     false, "Incremental reducers");
+		options.addOption("d", "drop",     true,  "Percentage of maps to drop");
+		options.addOption("n", "inidrop",  true,  "Percentage of maps to drop initially");
+		options.addOption("p", "precise",  false, "Precise job");
+		options.addOption("s", "sampling", true,  "Sampling rate 1/X");
+		options.addOption("t", "task",     true,  "Task: project, other");
+		options.addOption("e", "error",    true,  "Maximum error");
+		
+		try {
+			CommandLine cmdline = new GnuParser().parse(options, otherArgs);
+			// Input output
+			String input  = cmdline.getOptionValue("i");
+			String output = cmdline.getOptionValue("o");
+			if (input == null || output == null) {
+				throw new ParseException("No input/output option");
+			}
+			// Task to perform
+			if (cmdline.hasOption("t")) {
+				conf.set("task", cmdline.getOptionValue("t"));
+			}
+			// Reduces
+			int numReducers = 1;
+			if (cmdline.hasOption("r")) {
+				numReducers = Integer.parseInt(cmdline.getOptionValue("r"));
+			}
+			// Incremental reducers
+			if (cmdline.hasOption("c")) {
+				conf.setBoolean("mapred.tasks.incremental.reduction", true);
+				conf.setBoolean("mapred.tasks.clustering", true); // We arrange the intermediate keys by clusters
+			}
+			// Dropping maps
+			if (cmdline.hasOption("d")) {
+				float dropPercentage = Float.parseFloat(cmdline.getOptionValue("d"));
+				conf.setInt("mapred.map.approximate.drop.extratime", 1);
+				conf.setFloat("mapred.map.approximate.drop.percentage", dropPercentage/100);
+			}
+			// Dropping maps
+			if (cmdline.hasOption("n")) {
+				float dropPercentage = Float.parseFloat(cmdline.getOptionValue("n"));
+				conf.setFloat("mapred.map.approximate.drop.ini.percentage", 1-(dropPercentage/100)); // Percentage of maps we initially drop
+			}
+			// Precise job
+			if (cmdline.hasOption("p")) {
+				conf.setBoolean("mapred.job.precise", true);
+				conf.setBoolean("mapred.tasks.incremental.reduction", false);
+			}
+			// Sampling ratio
+			if (cmdline.hasOption("s")) {
+				int samplingRate = Integer.parseInt(cmdline.getOptionValue("s"));
+				conf.setInt("mapred.input.approximate.skip", samplingRate);
+			}
+			// Target goal
+			if (cmdline.hasOption("e")) {
+				int targetError = Integer.parseInt(cmdline.getOptionValue("e"));
+				conf.setBoolean("mapred.input.approximate.skip.adaptive", true); // We allow dynamic sampling rate
+				conf.setBoolean("mapred.map.approximate.drop.adaptive", true); // We allow dynamic sampling rate
+				conf.setFloat("mapred.approximate.error.target", targetError/100f);
+				conf.setBoolean("mapred.map.approximate.drop.hard", false);
+			}
+			
+			// Create job
+			Job job = new Job(conf, "Wikipedia PageRank");
+			
+			job.setJarByClass(WikiPageRank.class);
+			
+			job.setNumReduceTasks(numReducers);
+			
+			job.setMapperClass(WikiPageRankMapper.class);
+			if (cmdline.hasOption("c")) {
+				job.setReducerClass(WikiPageRankReducerIncr.class);
+			} else {
+				job.setReducerClass(WikiPageRankReducer.class);
+			}
+			
+			// For this approach, we can use a combiner that sums the outputs in the maps
+			job.setCombinerClass(IntSumReducer.class);
+			
+			job.setOutputKeyClass(Text.class);
+			job.setOutputValueClass(IntWritable.class);
+			
+			// We need a partitioner that sends clustering information to all reducers
+			if (!cmdline.hasOption("p")) {
+				if (cmdline.hasOption("c")) {
+					job.setPartitionerClass(ParameterPartitioner.class);
+				} else {
+					job.setPartitionerClass(MultistageSamplingPartitioner.class);
+				}
+			}
+			
+			// Approximate Hadoop
+			job.setInputFormatClass(WikipediaPageInputFormat.class);
+			
+			// Input and output
+			FileInputFormat.addInputPath(job,   new Path(input));
+			FileOutputFormat.setOutputPath(job, new Path(output));
+			
+			// Run and exit
+			System.exit(job.waitForCompletion(true) ? 0 : 1);
+		} catch (ParseException exp) {
+			System.err.println("Error parsing command line: " + exp.getMessage());
+			HelpFormatter formatter = new HelpFormatter();
+			formatter.printHelp(WikiPageRank.class.toString(), options);
+			ToolRunner.printGenericCommandUsage(System.out);
+			System.exit(2);
+		}
+	}
+}
Index: src/contrib/wikipedia/src/java/org/apache/hadoop/mapreduce/wikipedia/WikiPageRankMapper.java
===================================================================
--- src/contrib/wikipedia/src/java/org/apache/hadoop/mapreduce/wikipedia/WikiPageRankMapper.java	(revision 0)
+++ src/contrib/wikipedia/src/java/org/apache/hadoop/mapreduce/wikipedia/WikiPageRankMapper.java	(working copy)
@@ -0,0 +1,58 @@
+package org.apache.hadoop.mapreduce.wikipedia;
+
+import java.io.IOException;
+
+import java.util.List;
+import java.util.HashSet;
+
+import org.apache.hadoop.io.Text;
+import org.apache.hadoop.io.IntWritable;
+import org.apache.hadoop.io.LongWritable;
+
+import org.apache.hadoop.mapreduce.MultistageSamplingMapper;
+
+/**
+ * Mapper class. It takes a page and parses the links.
+ */
+public class WikiPageRankMapper extends MultistageSamplingMapper<LongWritable, WikipediaPage, Text, IntWritable> {
+	private static enum PageTypes {
+		TOTAL, REDIRECT, DISAMBIGUATION, EMPTY, ARTICLE, STUB, OTHER
+	};
+	
+	IntWritable one = new IntWritable(1);
+	
+	// The number of pages and links we have processed in this map
+	private long numPages = 0;
+	// private long numLinks = 0;
+
+	/**
+	 * Map function that collects the links for each wikipedia page.
+	 */
+	public void map(LongWritable key, WikipediaPage p, Context context) throws IOException, InterruptedException {
+		context.getCounter(PageTypes.TOTAL).increment(1);
+		
+		// Check links
+		List<String> links = p.extractLinkTargets();
+		for (String link : new HashSet<String>(links)) {
+			context.write(new Text(link.toLowerCase().replaceAll(" ", "_")), one);
+			// To do three-stage sampling we need to know how many links there was in this terciary sample
+			//context.write(new Text(link.toLowerCase().replaceAll(" ", "_")), new IntWritable(links.size()));
+		}
+		
+		// Account for pages and links
+		numPages++;
+// 		numLinks += links.size();
+	}
+	
+	/**
+	 * Cleanup function that reports how many pages and links have been processed.
+	 * We need to overwrite this to send the terciary sampling.
+	 */
+	/*public void cleanup(Context context) throws IOException, InterruptedException {
+		// We modify "m" and "t" parameters
+		setM(numPages);
+		setT(numLinks);
+		// Make the super class take care of sending the rest
+		super.cleanup(context);
+	}*/
+}
Index: src/contrib/wikipedia/src/java/org/apache/hadoop/mapreduce/wikipedia/WikiPageRankReducer.java
===================================================================
--- src/contrib/wikipedia/src/java/org/apache/hadoop/mapreduce/wikipedia/WikiPageRankReducer.java	(revision 0)
+++ src/contrib/wikipedia/src/java/org/apache/hadoop/mapreduce/wikipedia/WikiPageRankReducer.java	(working copy)
@@ -0,0 +1,28 @@
+package org.apache.hadoop.mapreduce.wikipedia;
+
+import java.io.IOException;
+
+import org.apache.hadoop.io.Text;
+import org.apache.hadoop.io.IntWritable;
+import org.apache.hadoop.mapreduce.Reducer;
+
+import org.apache.hadoop.mapreduce.MultistageSamplingReducer;
+
+/**
+ * Reducer class. It takes into account the approximation factor.
+ */
+public class WikiPageRankReducer extends MultistageSamplingReducer<Text,IntWritable,Text,IntWritable> {
+	private IntWritable result = new IntWritable();
+	
+	/**
+	 * Reduce function that uses the default collection approach.
+	 */
+	public void reduce(Text key, Iterable<IntWritable> values, Context context) throws IOException, InterruptedException {
+		int sum = 0;
+		for (IntWritable val : values) {
+			sum += val.get();
+		}
+		result.set(sum);
+		context.write(key, result);
+	}
+}
Index: src/contrib/wikipedia/src/java/org/apache/hadoop/mapreduce/wikipedia/WikiPageRankReducerIncr.java
===================================================================
--- src/contrib/wikipedia/src/java/org/apache/hadoop/mapreduce/wikipedia/WikiPageRankReducerIncr.java	(revision 0)
+++ src/contrib/wikipedia/src/java/org/apache/hadoop/mapreduce/wikipedia/WikiPageRankReducerIncr.java	(working copy)
@@ -0,0 +1,201 @@
+package org.apache.hadoop.mapreduce.wikipedia;
+
+import java.io.IOException;
+
+import org.apache.hadoop.io.Text;
+import org.apache.hadoop.io.IntWritable;
+import org.apache.hadoop.mapreduce.Reducer;
+
+import org.apache.hadoop.mapreduce.MultistageSamplingReducerIncr;
+
+/**
+ * Reducer class. It takes into account the approximation factor.
+ */
+public class WikiPageRankReducerIncr extends MultistageSamplingReducerIncr<Text,IntWritable,Text,IntWritable> {
+	private IntWritable result = new IntWritable();
+	
+	/**
+	 * Reduce function that uses the default collection approach.
+	 */
+	public void reduce(Text key, Iterable<IntWritable> values, Context context) throws IOException, InterruptedException {
+		int sum = 0;
+		for (IntWritable val : values) {
+			sum += val.get();
+		}
+		result.set(sum);
+		context.write(key, result);
+	}
+}
+
+/*public void reduce(Text key, Iterable<IntWritable> values, Context context) throws IOException, InterruptedException {
+	// Calculate aggregated value
+	int sum = 0;
+	for (IntWritable val : values) {
+		sum += val.get();
+	}
+	
+	// Check for the number of pages in the current cluster
+	if (key.toString().matches(WikiPageRank.NUMPAGES_SAMPLED_MATCH)) {
+		clusterPages[curCluster] = sum;
+	// Check for the number of links in the current cluster
+	} else if (key.toString().matches(WikiPageRank.NUMLINKS_SAMPLED_MATCH)) {
+		clusterLinks[curCluster] = sum;
+	// Add to the proper cluster
+	} else {
+		synchronized(this) {
+			// We have to create the cluster structure for new keys
+			if (!clusters.containsKey(key.toString())) {
+				clusters.put(key.toString(), new int[numCluster]);
+			}
+			clusters.get(key.toString())[curCluster] = sum;
+		}
+	}
+}*/
+
+/*protected synchronized void checkQuality(Context context, boolean output) throws IOException, InterruptedException {
+	double maxError = 0.0;
+	double maxErrorRel = 0.0;
+
+	// Get current paramters
+	int numMaps = context.getConfiguration().getInt("mapred.map.tasks", -1);
+	int skipPages = context.getConfiguration().getInt("mapred.input.approximate.skip", 1);
+	
+	// Multistage sampling
+	int N = numMaps;
+	int n = curCluster+1;
+	
+	// Check if we have done any clustering or everything is together
+	if (n==1 && context.getConfiguration().getFloat("mapred.map.approximate.drop.percentage", 1.0f) >= 1.0) {
+		System.out.println("There is no clusters");
+		N = 1;
+	}
+	
+	// Calculate populations in each cluster (we make it long because the multiplication can cause overflow)
+	/*long[] mi = new long[n];
+	long[] Mi = new long[n];
+	for (int i=0; i<n; i++) {
+		// The number of pages sampled in each 2-cluster
+		mi[i] = clusterPages[i];
+		Mi[i] = mi[i]*skipPages; // This is an approximation based on the sampling ratio
+	}* /
+	double[] auxmi1 = new double[n]; // Mi/mi
+	double[] auxmi2 = new double[n]; // mi/(mi-1)
+	for (int i=0; i<n; i++) {
+		long mi = clusterPages[i];
+		long Mi = mi*skipPages;
+		// We use precomputed values
+		auxmi1[i] = 1.0*Mi/mi;
+		auxmi2[i] = 1.0*Mi*(Mi-mi)/(mi-1.0);
+	}
+	
+	// Get the t-score for the current distribution
+	double tscore = 1.96; // By default we use the normal distribution
+	try {
+		TDistribution tdist = new TDistributionImpl(n-1);
+		double confidence = 0.95; // 95% confidence => 0.975
+		tscore = tdist.inverseCumulativeProbability(1.0-((1.0-confidence)/2.0)); // 95% confidence 1-alpha
+	} catch (Exception e) { }
+	
+	// Go over all the keys
+	Collection<String> keys = clusters.keySet();
+	/*if (output) {
+		// We sort the keys if we try to output them
+		keys = new LinkedList<String>(keys);
+		Collections.sort((List<String>)keys);
+	}* /
+	for (String key : keys) {
+		// This comment is the long and understandable version of what we do afterwards
+		/*
+		// Collect the results for each 2-cluster
+		int[] yi  = new int[n];
+		int[] yti = new int[n];
+		for (int i=0; i<n; i++) {
+			// Account on how many pages where linking to this key
+			yi[i] = clusters.get(key)[i];
+			yti[i] = clusterLinks[i];
+		}
+		
+		// Estimate the results for each 2-cluster
+		double[] yhati = new double[n];
+		for (int i=0; i<n; i++) {
+			yhati[i] = (1.0*Mi[i]/mi[i])*yi[i];
+		}
+		
+		// Estimate the total number
+		double tauhat = (1.0*N/n)*sum(yhati);
+		
+		// Calculate the errors
+		// Estimate the global deviation
+		double su2 = var(yhati);
+		
+		// Calculate proportions
+		double[] pi = new double[n];
+		for (int i=0; i<n; i++) {
+			pi[i] = 1.0*yi[i]/yti[i];
+		}
+		
+		// Estimate variance in primary
+		double[] si2 = new double[n];
+		for (int i=0; i<n; i++) {
+			si2[i] = (1.0*mi[i]/(mi[i]-1.0)) * pi[i] * (1.0-pi[i]);
+		}
+		
+		// Calculate total variance
+		double var1 = 1.0*(N*(N-n)*su2)/n;
+		double var2 = 0.0;
+		for (int i=0; i<n; i++) {
+			var2 += 1.0*(Mi[i]*(Mi[i]-mi[i])*si2[i])/mi[i];
+		}
+		var2 = (1.0*N/n)*var2;
+		double var3 = 0.0; // var3 stays 0 because ti=Ti (var ~= Ti*(Ti-ti)*st2/ti)
+		
+		// Variation and standard error for the final result
+		double vartauhat = var1 + var2 + var3;
+		double setauhat = Math.sqrt(vartauhat);
+		* /
+		
+		// This is the dirty and more efficient version
+		double[] yhati = new double[n];
+		double var2 = 0.0;
+		double tauhat = 0.0;
+		for (int i=0; i<n; i++) {
+			int yi = clusters.get(key)[i];
+			int yti = clusterLinks[i];
+			yhati[i] = auxmi1[i]*yi;
+			tauhat += (1.0*N/n)*yhati[i];
+			double pi = 1.0*yi/yti;
+			var2 += auxmi2[i] * pi * (1.0-pi);
+		}
+		// Standard error for the final result
+		double setauhat = Math.sqrt(1.0*(N*(N-n)*var(yhati))/n + (1.0*N/n)*var2);
+		
+		// DEBUG
+		/*if (key.toString().equals("united_states") || key.toString().equals("france")) {
+			System.out.format("%d  -> %s = %.2f +/- %.2f (+/-%.2f%%)\n", n, key, tauhat, tscore*setauhat, 100.0*tscore*setauhat/tauhat);
+		}* /
+		
+		// Check which is the maximum error
+		if (tscore*setauhat == Double.NaN) {
+			maxError = Double.MAX_VALUE;
+		} else if (tscore*setauhat > maxError) {
+			maxError = tscore*setauhat;
+			maxErrorRel = 100.0*tscore*setauhat/tauhat;
+		}
+		
+		// We save the output
+		if (output) {
+			//context.write(new Text(key), new IntWritable((int) tauhat));
+			context.write(new Text(key), new ApproximateIntWritable((int) tauhat, tscore*setauhat));
+		}
+	}
+	
+	// Check if we can start dropping
+	if (n>1) {
+		System.out.format(new Date() + ": Max error with %d maps is +/-%.2f (+/-%.2f)\n", n, maxError, maxErrorRel);
+		//if (maxError < 10*1000) {
+		if (maxErrorRel < 10.0) { // >10%
+			System.out.format("With this error, we can drop!\n");
+			context.setStatus("dropping");
+		}
+	}
+}*/
Index: src/contrib/wikipedia/src/java/org/apache/hadoop/mapreduce/wikipedia/WikiPopularity.java
===================================================================
--- src/contrib/wikipedia/src/java/org/apache/hadoop/mapreduce/wikipedia/WikiPopularity.java	(revision 0)
+++ src/contrib/wikipedia/src/java/org/apache/hadoop/mapreduce/wikipedia/WikiPopularity.java	(working copy)
@@ -0,0 +1,378 @@
+package org.apache.hadoop.mapreduce.wikipedia;
+
+import java.io.IOException;
+
+import java.util.Date;
+import java.util.StringTokenizer;
+import java.util.HashMap;
+import java.util.TreeMap;
+import java.util.LinkedList;
+import java.util.Map;
+
+import org.apache.commons.cli.CommandLine;
+import org.apache.commons.cli.CommandLineParser;
+import org.apache.commons.cli.GnuParser;
+import org.apache.commons.cli.HelpFormatter;
+import org.apache.commons.cli.OptionBuilder;
+import org.apache.commons.cli.Options;
+import org.apache.commons.cli.ParseException;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.io.IntWritable;
+import org.apache.hadoop.io.LongWritable;
+import org.apache.hadoop.io.Writable;
+import org.apache.hadoop.io.Text;
+import org.apache.hadoop.mapreduce.Job;
+import org.apache.hadoop.mapreduce.Mapper;
+import org.apache.hadoop.mapreduce.Reducer;
+import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
+import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
+import org.apache.hadoop.mapreduce.lib.reduce.LongSumReducer;
+import org.apache.hadoop.util.GenericOptionsParser;
+import org.apache.hadoop.util.Tool;
+import org.apache.hadoop.util.ToolRunner;
+
+import org.apache.hadoop.mapreduce.MultistageSamplingMapper;
+import org.apache.hadoop.mapreduce.MultistageSamplingReducer;
+import org.apache.hadoop.mapreduce.MultistageSamplingReducerIncr;
+import org.apache.hadoop.mapreduce.SparseArray;
+import org.apache.hadoop.mapreduce.ParameterPartitioner;
+import org.apache.hadoop.mapreduce.MultistageSamplingPartitioner;
+import org.apache.hadoop.mapreduce.ApproximateLongWritable;
+
+import org.apache.hadoop.mapreduce.lib.input.ApproximateTextInputFormat;
+
+/**
+ * Check the popularity of wikipedia pages.
+ * pages
+ * project
+ */
+public class WikiPopularity {
+	/**
+	 * Mapper that processes wikpedia log lines.
+	 */
+	public static class WikiPopularityMapper extends MultistageSamplingMapper<LongWritable,Text,Text,LongWritable> {
+		//LongWritable result = new LongWritable();
+		private final static LongWritable one = new LongWritable(1);
+		private Text word = new Text();
+		
+		/**
+		 * Gets a log line from wikipedia parses it and produces an output.
+		 */
+		public void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
+			try {
+				/*
+				StringTokenizer itr = new StringTokenizer(value.toString());
+				
+				// Parse line
+				String date    = itr.nextToken();
+				String hour    = itr.nextToken();
+				
+				String project = itr.nextToken(); // language
+				String page    = itr.nextToken();
+				
+				String size    = itr.nextToken();
+				*/
+				
+				String line = value.toString().replaceAll("\t", " ");
+				String[] lineSplit = line.split(" ");
+				if (lineSplit.length >= 4) {
+					String date    = lineSplit[0];
+					String hour    = lineSplit[1];
+					
+					String project = lineSplit[2]; // language
+					String page    = lineSplit[3];
+					
+					if (lineSplit.length >= 5) {
+						String size = lineSplit[4];
+					}
+					
+					// Perform actual task
+					String task = context.getConfiguration().get("task");
+					if ("project".equalsIgnoreCase(task)) {
+						word.set(project);
+						context.write(word, one);
+					} else {
+						word.set(project+" "+page);
+						context.write(word, one);
+					}
+				} else {
+					System.out.println("Cannot split: " + value.toString());
+				}
+			} catch (Exception e) {
+				System.out.println("Error processing line: " + value.toString());
+				System.out.println(e);
+			}
+		}
+	}
+	
+	/**
+	 * Reducer that takes into account the approximation factor.
+	 */
+	public static class WikiPopularityReducer extends MultistageSamplingReducer<Text,LongWritable,Text,LongWritable> {
+		LongWritable result = new LongWritable();
+		/**
+		 * Reduce function that uses the default collection approach.
+		 */
+		public void reduce(Text key, Iterable<LongWritable> values, Context context) throws IOException, InterruptedException {
+			long sum = 0;
+			for (LongWritable val : values) {
+				sum += val.get();
+			}
+			result.set(sum);
+			context.write(key, result);
+		}
+	}
+	
+	public static class WikiPopularityReducerIncr extends MultistageSamplingReducerIncr<Text,LongWritable,Text,LongWritable> {
+		LongWritable result = new LongWritable();
+		/**
+		 * Reduce function that uses the default collection approach.
+		 */
+		public void reduce(Text key, Iterable<LongWritable> values, Context context) throws IOException, InterruptedException {
+			long sum = 0;
+			for (LongWritable val : values) {
+				sum += val.get();
+			}
+			result.set(sum);
+			context.write(key, result);
+		}
+
+		/*public void reduce(Text key, Iterable<LongWritable> values, Context context) throws IOException, InterruptedException {
+			// Calculate aggregated value
+			long sum = 0;
+			for (LongWritable val : values) {
+				sum += val.get();
+			}
+			
+			// Check for the number of pages in the current cluster
+			if (key.toString().matches(WikiPopularity.NUMLINES_SAMPLED_MATCH)) {
+				clusterLines[curCluster] = sum;
+			// Add to the proper cluster
+			} else {
+				synchronized(this) {
+					// We have to create the cluster structure for new keys
+					if (!clusters.containsKey(key.toString())) {
+						//clusters.put(key.toString(), new long[numCluster]);
+						clusters.put(key.toString(), new SparseArray<Long>(numCluster));
+					}
+					//clusters.get(key.toString())[curCluster] = sum;
+					clusters.get(key.toString()).set(curCluster, sum);
+				}
+			}
+			
+			// Debugging memory problem
+			/ *if (clusters.size() > prevsize+(10*1000)) {
+				System.out.println("heapMaxSize  = " + Runtime.getRuntime().maxMemory()/(1024*1024) + " MB");
+				System.out.println("heapSize     = " + Runtime.getRuntime().totalMemory()/(1024*1024) + " MB");
+				System.out.println("heapFreeSize = " + Runtime.getRuntime().freeMemory()/(1024*1024) + " MB");
+				int auxS = 0;
+				for (SparseArray<Long> array : clusters.values()) {
+					if (array.isSparse()) {
+						auxS++;
+					}
+				}
+				System.out.println("Clusters: " + auxS + "/" + clusters.size());
+				
+				try {
+					File tmpFile = File.createTempFile("approx", ".tmp");
+					FileOutputStream fout = new FileOutputStream(tmpFile);
+					ObjectOutputStream oos = new ObjectOutputStream(fout); 
+					oos.writeObject(clusters);
+					oos.close();
+					fout.close();
+					
+					System.out.format("%s: File: %s %.2fMB\n", new Date(), tmpFile, tmpFile.length()/(1024.0*1024.0));
+				} catch (Exception e) {
+					e.printStackTrace();
+				}
+				
+				// We create a new map
+				clusters.clear();
+				
+				prevsize = clusters.size();
+			}* /
+		}*/
+		
+		/*protected synchronized void checkQuality(Context context, boolean output) throws IOException, InterruptedException {
+			long t0 = System.currentTimeMillis();
+		
+			double maxAbs = 0.0;
+			double maxError = 0.0;
+			double maxErrorRel = 0.0;
+		
+			// Get current paramters
+			int numMaps = context.getConfiguration().getInt("mapred.map.tasks", -1);
+			int skipPages = context.getConfiguration().getInt("mapred.input.approximate.skip", 1);
+			
+			// Multistage sampling
+			int N = numMaps;
+			int n = curCluster+1;
+			
+			// Calculations populatons in each cluster
+			long[] mi = new long[n];
+			double[] auxmi1 = new double[n];
+			double[] auxmi2 = new double[n];
+			double[] auxmi3 = new double[n];
+			for (int i=0; i<n; i++) {
+				mi[i] = clusterLines[i];
+				double Mi = mi[i]*skipPages;
+				auxmi1[i] = 1.0*Mi/mi[i];
+				auxmi2[i] = 1.0*Mi*(Mi-mi[i])/(mi[i]-1.0);
+			}
+			
+			// Get the t-score for the current distribution
+			double tscore = MultistageSamplingReducer.getTScore(n-1, 0.95);
+			
+			// Go over all the keys
+			for (Object key : clusters.keySet()) {
+				// Calculate total estimation
+				double[] yhati = new double[n];
+				double var2 = 0.0;
+				double tauhat = 0.0;
+				for (int i=0; i<n; i++) {
+					//double yi = clusters.get(key)[i];
+					double yi = 0.0;
+					try {
+						yi = clusters.get(key).get(i);
+					} catch (Exception e) { }
+					yhati[i] = auxmi1[i] * yi;
+					tauhat += (1.0*N/n) * yhati[i];
+					double pi = 1.0 * yi / mi[i];
+					var2 += auxmi2[i] * pi * (1.0-pi);
+				}
+				// Calculate total variance
+				Double setauhat = Math.sqrt(1.0*(N*(N-n)*var(yhati))/n + (1.0*N/n)*var2);
+				if (setauhat.isNaN()) { 
+					setauhat = Double.MAX_VALUE;
+				}
+				
+				// Calculate the maximum relative error
+				if (tscore*setauhat > maxError) {
+					maxAbs = tauhat;
+					maxError = tscore*setauhat;
+					maxErrorRel = 100.0*tscore*setauhat/tauhat;
+				}
+				
+				// Output the estimation
+				if (output) {
+					Text outkey = new Text((String)key);
+					ApproximateLongWritable outval = new ApproximateLongWritable((long) tauhat, tscore*setauhat);
+					context.write(outkey, outval);
+				}
+			}
+			System.out.format("%s: %.1fs %d/%d max error %.1fM+/-%.1fM (+/-%.2f%%) Keys=%d\n", new Date(), (System.currentTimeMillis()-t0)/1000.0, n, N, maxAbs/1000000, maxError/1000000, maxErrorRel, clusters.size());
+		}*/
+	}
+	
+	/**
+	 * Launch wikipedia log analysis.
+	 */
+	public static void main(String[] args) throws Exception {
+		// Get Hadoop options first
+		Configuration conf = new Configuration();
+		String[] otherArgs = new GenericOptionsParser(conf, args).getRemainingArgs();
+		
+		// Options for the rest
+		Options options = new Options();
+		options.addOption("i", "input",    true,  "Input file");
+		options.addOption("o", "output",   true,  "Output file");
+		options.addOption("r", "reduces",  true,  "Number of reducers");
+		options.addOption("c", "incr",     false, "Incremental reducers");
+		options.addOption("d", "drop",     true,  "Percentage of maps to drop");
+		options.addOption("n", "inidrop",  true,  "Percentage of maps to drop initially");
+		options.addOption("p", "precise",  false, "Precise job");
+		options.addOption("s", "sampling", true,  "Sampling rate 1/X");
+		options.addOption("t", "task",     true,  "Task: project, other");
+		
+		try {
+			CommandLine cmdline = new GnuParser().parse(options, otherArgs);
+			// Input output
+			String input  = cmdline.getOptionValue("i");
+			String output = cmdline.getOptionValue("o");
+			if (input == null || output == null) {
+				throw new ParseException("No input/output option");
+			}
+			// Task to perform
+			if (cmdline.hasOption("t")) {
+				conf.set("task", cmdline.getOptionValue("t"));
+			}
+			// Reduces
+			int numReducers = 1;
+			if (cmdline.hasOption("r")) {
+				numReducers = Integer.parseInt(cmdline.getOptionValue("r"));
+			}
+			// Incremental reducers
+			if (cmdline.hasOption("c")) {
+				conf.setBoolean("mapred.tasks.incremental.reduction", true);
+				conf.setBoolean("mapred.tasks.clustering", true); // We arrange the intermediate keys by clusters
+			}
+			// Dropping maps
+			if (cmdline.hasOption("d")) {
+				float dropPercentage = Float.parseFloat(cmdline.getOptionValue("d"));
+				conf.setInt("mapred.map.approximate.drop.extratime", 1);
+				conf.setFloat("mapred.map.approximate.drop.percentage", dropPercentage/100);
+			}
+			// Dropping maps
+			if (cmdline.hasOption("n")) {
+				float dropPercentage = Float.parseFloat(cmdline.getOptionValue("n"));
+				conf.setFloat("mapred.map.approximate.drop.ini.percentage", 1-(dropPercentage/100)); // Percentage of maps we initially drop
+			}
+			// Precise job
+			if (cmdline.hasOption("p")) {
+				conf.setBoolean("mapred.job.precise", true);
+				conf.setBoolean("mapred.tasks.incremental.reduction", false);
+			}
+			if (cmdline.hasOption("s")) {
+				int samplingRate = Integer.parseInt(cmdline.getOptionValue("s"));
+				conf.setInt("mapred.input.approximate.skip", samplingRate);
+			}
+			
+			// Create job
+			Job job = new Job(conf, "Wikipedia access analysis");
+			
+			job.setJarByClass(WikiPopularity.class);
+			
+			job.setNumReduceTasks(numReducers);
+			
+			job.setMapperClass(WikiPopularityMapper.class);
+			if (cmdline.hasOption("c")) {
+				job.setReducerClass(WikiPopularityReducerIncr.class);
+			} else {
+				job.setReducerClass(WikiPopularityReducer.class);
+			}
+			
+			// For this approach, we can use a combiner that sums the outputs in the maps
+			job.setCombinerClass(LongSumReducer.class);
+			
+			job.setOutputKeyClass(Text.class);
+			job.setOutputValueClass(LongWritable.class);
+			
+			// We need a partitioner that sends clustering information to all reducers
+			if (!cmdline.hasOption("p")) {
+				if (cmdline.hasOption("c")) {
+					job.setPartitionerClass(ParameterPartitioner.class);
+				} else {
+					job.setPartitionerClass(MultistageSamplingPartitioner.class);
+				}
+			}
+			
+			// Approximate Hadoop
+			job.setInputFormatClass(ApproximateTextInputFormat.class);
+			
+			// Input and output
+			FileInputFormat.addInputPath(job,   new Path(input));
+			FileOutputFormat.setOutputPath(job, new Path(output));
+			
+			// Run and exit
+			System.exit(job.waitForCompletion(true) ? 0 : 1);
+		} catch (ParseException exp) {
+			System.err.println("Error parsing command line: " + exp.getMessage());
+			HelpFormatter formatter = new HelpFormatter();
+			formatter.printHelp(WikiPopularity.class.toString(), options);
+			ToolRunner.printGenericCommandUsage(System.out);
+			System.exit(2);
+		}
+	}
+}
Index: src/contrib/wikipedia/src/java/org/apache/hadoop/mapreduce/wikipedia/WikipediaPage.java
===================================================================
--- src/contrib/wikipedia/src/java/org/apache/hadoop/mapreduce/wikipedia/WikipediaPage.java	(revision 0)
+++ src/contrib/wikipedia/src/java/org/apache/hadoop/mapreduce/wikipedia/WikipediaPage.java	(working copy)
@@ -0,0 +1,432 @@
+/*
+ * Cloud9: A MapReduce Library for Hadoop
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License"); you
+ * may not use this file except in compliance with the License. You may
+ * obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
+ * implied. See the License for the specific language governing
+ * permissions and limitations under the License.
+ */
+
+package org.apache.hadoop.mapreduce.wikipedia;
+
+import info.bliki.wiki.filter.PlainTextConverter;
+import info.bliki.wiki.model.WikiModel;
+
+import java.io.DataInput;
+import java.io.DataOutput;
+import java.io.IOException;
+import java.util.List;
+import java.util.regex.Pattern;
+
+import javax.annotation.Nullable;
+
+import org.apache.commons.lang.StringEscapeUtils;
+import org.apache.hadoop.io.WritableUtils;
+
+import com.google.common.base.Function;
+import com.google.common.collect.Lists;
+
+/**
+ * A page from Wikipedia.
+ * 
+ * @author Jimmy Lin
+ * @author Peter Exner
+ */
+public abstract class WikipediaPage extends Indexable {
+  /**
+   * Start delimiter of the page, which is &lt;<code>page</code>&gt;.
+   */
+  public static final String XML_START_TAG = "<page>";
+
+  /**
+   * End delimiter of the page, which is &lt;<code>/page</code>&gt;.
+   */
+  public static final String XML_END_TAG = "</page>";
+
+  /**
+   * Start delimiter of the title, which is &lt;<code>title</code>&gt;.
+   */
+  protected static final String XML_START_TAG_TITLE = "<title>";
+
+  /**
+   * End delimiter of the title, which is &lt;<code>/title</code>&gt;.
+   */
+  protected static final String XML_END_TAG_TITLE = "</title>";
+
+  /**
+   * Start delimiter of the namespace, which is &lt;<code>ns</code>&gt;.
+   */
+  protected static final String XML_START_TAG_NAMESPACE = "<ns>";
+
+  /**
+   * End delimiter of the namespace, which is &lt;<code>/ns</code>&gt;.
+   */
+  protected static final String XML_END_TAG_NAMESPACE = "</ns>";
+
+  /**
+   * Start delimiter of the id, which is &lt;<code>id</code>&gt;.
+   */
+  protected static final String XML_START_TAG_ID = "<id>";
+
+  /**
+   * End delimiter of the id, which is &lt;<code>/id</code>&gt;.
+   */
+  protected static final String XML_END_TAG_ID = "</id>";
+
+  /**
+   * Start delimiter of the text, which is &lt;<code>text xml:space=\"preserve\"</code>&gt;.
+   */
+  protected static final String XML_START_TAG_TEXT = "<text xml:space=\"preserve\">";
+
+  /**
+   * End delimiter of the text, which is &lt;<code>/text</code>&gt;.
+   */
+  protected static final String XML_END_TAG_TEXT = "</text>";
+
+  protected String page;
+  protected String title;
+  protected String mId;
+  protected int textStart;
+  protected int textEnd;
+  protected boolean isRedirect;
+  protected boolean isDisambig;
+  protected boolean isStub;
+  protected boolean isArticle;
+  protected String language;
+
+  private WikiModel wikiModel;
+  private PlainTextConverter textConverter;
+
+  /**
+   * Creates an empty <code>WikipediaPage</code> object.
+   */
+  public WikipediaPage() {
+    wikiModel = new WikiModel("", "");
+    textConverter = new PlainTextConverter();
+  }
+
+  /**
+   * Deserializes this object.
+   */
+  public void write(DataOutput out) throws IOException {
+    byte[] bytes = page.getBytes("UTF-8");
+    WritableUtils.writeVInt(out, bytes.length);
+    out.write(bytes, 0, bytes.length);
+    out.writeUTF(language == null ? "unk" : language);
+  }
+
+  /**
+   * Serializes this object.
+   */
+  public void readFields(DataInput in) throws IOException {
+    int length = WritableUtils.readVInt(in);
+    byte[] bytes = new byte[length];
+    in.readFully(bytes, 0, length);
+    WikipediaPage.readPage(this, new String(bytes, "UTF-8"));
+    language = in.readUTF();
+  }
+
+  /**
+   * Returns the article title (i.e., the docid).
+   */
+  public String getDocid() {
+    return mId;
+  }
+
+  @Deprecated
+  public void setLanguage(String language) {
+    this.language = language;
+  }
+
+  public String getLanguage() {
+    return this.language;
+  }
+
+  // Explictly remove <ref>...</ref>, because there are screwy things like this:
+  // <ref>[http://www.interieur.org/<!-- Bot generated title -->]</ref>
+  // where "http://www.interieur.org/<!--" gets interpreted as the URL by
+  // Bliki in conversion to text
+  private static final Pattern REF = Pattern.compile("<ref>.*?</ref>");
+
+  private static final Pattern LANG_LINKS = Pattern.compile("\\[\\[[a-z\\-]+:[^\\]]+\\]\\]");
+  private static final Pattern REGULAR_LINKS = Pattern.compile("\\[\\[[a-z\\-]+\\]\\]");
+  private static final Pattern DOUBLE_CURLY = Pattern.compile("\\{\\{.*?\\}\\}");
+
+  private static final Pattern URL = Pattern.compile("http://[^ <]+"); // Note, don't capture
+                                                                       // possible HTML tag
+
+  private static final Pattern HTML_TAG = Pattern.compile("<[^!][^>]*>"); // Note, don't capture
+                                                                          // comments
+  private static final Pattern HTML_COMMENT = Pattern.compile("<!--.*?-->", Pattern.DOTALL);
+
+  /**
+   * Returns the contents of this page (title + text).
+   */
+  public String getContent() {
+    String s = getWikiMarkup();
+
+    // Bliki doesn't seem to properly handle inter-language links, so remove manually.
+    s = LANG_LINKS.matcher(s).replaceAll(" ");
+
+    wikiModel.setUp();
+    s = getTitle() + "\n" + wikiModel.render(textConverter, s);
+    wikiModel.tearDown();
+
+    // The way the some entities are encoded, we have to unescape twice.
+    s = StringEscapeUtils.unescapeHtml(StringEscapeUtils.unescapeHtml(s));
+
+    s = REF.matcher(s).replaceAll(" ");
+    s = HTML_COMMENT.matcher(s).replaceAll(" ");
+
+    // Sometimes, URL bumps up against comments e.g., <!-- http://foo.com/-->
+    // Therefore, we want to remove the comment first; otherwise the URL pattern might eat up
+    // the comment terminator.
+    s = URL.matcher(s).replaceAll(" ");
+    s = DOUBLE_CURLY.matcher(s).replaceAll(" ");
+    s = HTML_TAG.matcher(s).replaceAll(" ");
+
+    return s;
+  }
+
+  public String getDisplayContent() {
+    wikiModel.setUp();
+    String s = "<h1>" + getTitle() + "</h1>\n" + wikiModel.render(getWikiMarkup());
+    wikiModel.tearDown();
+
+    s = DOUBLE_CURLY.matcher(s).replaceAll(" ");
+
+    return s;
+  }
+
+  @Override
+  public String getDisplayContentType() {
+    return "text/html";
+  }
+
+  /**
+   * Returns the raw XML of this page.
+   */
+  public String getRawXML() {
+    return page;
+  }
+
+  /**
+   * Returns the text of this page.
+   */
+  public String getWikiMarkup() {
+    if (textStart == -1)
+      return null;
+
+    return page.substring(textStart + 27, textEnd);
+  }
+
+  /**
+   * Returns the title of this page.
+   */
+  public String getTitle() {
+    return title;
+  }
+
+  /**
+   * Checks to see if this page is a disambiguation page. A <code>WikipediaPage</code> is either an
+   * article, a disambiguation page, a redirect page, or an empty page.
+   * 
+   * @return <code>true</code> if this page is a disambiguation page
+   */
+  public boolean isDisambiguation() {
+    return isDisambig;
+  }
+
+  /**
+   * Checks to see if this page is a redirect page. A <code>WikipediaPage</code> is either an
+   * article, a disambiguation page, a redirect page, or an empty page.
+   * 
+   * @return <code>true</code> if this page is a redirect page
+   */
+  public boolean isRedirect() {
+    return isRedirect;
+  }
+
+  /**
+   * Checks to see if this page is an empty page. A <code>WikipediaPage</code> is either an article,
+   * a disambiguation page, a redirect page, or an empty page.
+   * 
+   * @return <code>true</code> if this page is an empty page
+   */
+  public boolean isEmpty() {
+    return textStart == -1;
+  }
+
+  /**
+   * Checks to see if this article is a stub. Return value is only meaningful if this page isn't a
+   * disambiguation page, a redirect page, or an empty page.
+   * 
+   * @return <code>true</code> if this article is a stub
+   */
+  public boolean isStub() {
+    return isStub;
+  }
+
+  /**
+   * Checks to see if this page lives in the main/article namespace, and not, for example, "File:",
+   * "Category:", "Wikipedia:", etc.
+   * 
+   * @return <code>true</code> if this page is an actual article
+   */
+  public boolean isArticle() {
+    return isArticle;
+  }
+
+  /**
+   * Returns the inter-language link to a specific language (if any).
+   * 
+   * @param lang language
+   * @return title of the article in the foreign language if link exists, <code>null</code>
+   *         otherwise
+   */
+  public String findInterlanguageLink(String lang) {
+    int start = page.indexOf("[[" + lang + ":");
+
+    if (start < 0)
+      return null;
+
+    int end = page.indexOf("]]", start);
+
+    if (end < 0)
+      return null;
+
+    // Some pages have malformed links. For example, "[[de:Frances Willard]"
+    // in enwiki-20081008-pages-articles.xml.bz2 has only one closing square
+    // bracket. Temporary solution is to ignore malformed links (instead of
+    // trying to hack around them).
+    String link = page.substring(start + 3 + lang.length(), end);
+
+    // If a newline is found, it probably means that the link is malformed
+    // (see above comment). Abort in this case.
+    if (link.indexOf("\n") != -1) {
+      return null;
+    }
+
+    if (link.length() == 0)
+      return null;
+
+    return link;
+  }
+
+  public static class Link {
+    private String anchor;
+    private String target;
+
+    private Link(String anchor, String target) {
+      this.anchor = anchor;
+      this.target = target;
+    }
+
+    public String getAnchorText() {
+      return anchor;
+    }
+
+    public String getTarget() {
+      return target;
+    }
+
+    public String toString() {
+      return String.format("[target: %s, anchor: %s]", target, anchor);
+    }
+  }
+
+  public List<Link> extractLinks() {
+    int start = 0;
+    List<Link> links = Lists.newArrayList();
+
+    while (true) {
+      start = page.indexOf("[[", start);
+
+      if (start < 0) {
+        break;
+      }
+
+      int end = page.indexOf("]]", start);
+
+      if (end < 0) {
+        break;
+      }
+
+      String text = page.substring(start + 2, end);
+      String anchor = null;
+
+      // skip empty links
+      if (text.length() == 0) {
+        start = end + 1;
+        continue;
+      }
+
+      // skip special links
+      if (text.indexOf(":") != -1) {
+        start = end + 1;
+        continue;
+      }
+
+      // if there is anchor text, get only article title
+      int a;
+      if ((a = text.indexOf("|")) != -1) {
+        anchor = text.substring(a + 1, text.length());
+        text = text.substring(0, a);
+      }
+
+      if ((a = text.indexOf("#")) != -1) {
+        text = text.substring(0, a);
+      }
+
+      // ignore article-internal links, e.g., [[#section|here]]
+      if (text.length() == 0) {
+        start = end + 1;
+        continue;
+      }
+
+      if (anchor == null) {
+        anchor = text;
+      }
+      links.add(new Link(anchor, text));
+
+      start = end + 1;
+    }
+
+    return links;
+  }
+
+  public List<String> extractLinkTargets() {
+    return Lists.transform(extractLinks(), new Function<Link, String>() {
+      @Override @Nullable
+      public String apply(@Nullable Link link) {
+        return link.getTarget();
+      }
+    });
+  }
+
+  /**
+   * Reads a raw XML string into a <code>WikipediaPage</code> object.
+   * 
+   * @param page the <code>WikipediaPage</code> object
+   * @param s raw XML string
+   */
+  public static void readPage(WikipediaPage page, String s) {
+    page.page = s;
+    page.processPage(s);
+  }
+
+  /**
+   * Reads a raw XML string into a <code>WikipediaPage</code> object. Added for backwards
+   * compability.
+   * 
+   * @param s raw XML string
+   */
+  protected abstract void processPage(String s);
+}
Index: src/contrib/wikipedia/src/java/org/apache/hadoop/mapreduce/wikipedia/WikipediaPageInputFormat.java
===================================================================
--- src/contrib/wikipedia/src/java/org/apache/hadoop/mapreduce/wikipedia/WikipediaPageInputFormat.java	(revision 0)
+++ src/contrib/wikipedia/src/java/org/apache/hadoop/mapreduce/wikipedia/WikipediaPageInputFormat.java	(working copy)
@@ -0,0 +1,95 @@
+/*
+ * Cloud9: A MapReduce Library for Hadoop
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License"); you
+ * may not use this file except in compliance with the License. You may
+ * obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
+ * implied. See the License for the specific language governing
+ * permissions and limitations under the License.
+ */
+
+package org.apache.hadoop.mapreduce.wikipedia;
+
+import java.io.IOException;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.io.LongWritable;
+import org.apache.hadoop.mapreduce.InputSplit;
+import org.apache.hadoop.mapreduce.RecordReader;
+import org.apache.hadoop.mapreduce.TaskAttemptContext;
+import org.apache.hadoop.mapreduce.SamplingRecordReader;
+
+import org.apache.hadoop.mapreduce.wikipedia.IndexableFileInputFormat;
+import org.apache.hadoop.mapreduce.wikipedia.XMLInputFormat;
+import org.apache.hadoop.mapreduce.wikipedia.XMLInputFormat.XMLRecordReader;
+import org.apache.hadoop.mapreduce.wikipedia.language.WikipediaPageFactory;
+
+/**
+ * Hadoop {@code InputFormat} for processing Wikipedia pages from the XML dumps.
+ *
+ * @author Jimmy Lin
+ * @author Peter Exner
+ */
+public class WikipediaPageInputFormat extends IndexableFileInputFormat<LongWritable, WikipediaPage> {
+	@Override
+	public RecordReader<LongWritable, WikipediaPage> createRecordReader(InputSplit split, TaskAttemptContext context) throws IOException, InterruptedException {
+		return new WikipediaPageRecordReader();
+	}
+
+	public static class WikipediaPageRecordReader extends RecordReader<LongWritable, WikipediaPage> implements SamplingRecordReader {
+		private XMLRecordReader reader = new XMLRecordReader();
+		private WikipediaPage page;
+		private String language;
+		
+		@Override
+		public void initialize(InputSplit split, TaskAttemptContext context) throws IOException, InterruptedException {
+			Configuration conf = context.getConfiguration();
+			conf.set(XMLInputFormat.START_TAG_KEY, WikipediaPage.XML_START_TAG);
+			conf.set(XMLInputFormat.END_TAG_KEY, WikipediaPage.XML_END_TAG);
+			
+			language = conf.get("wiki.language", "en"); // Assume 'en' by default.
+			page = WikipediaPageFactory.createWikipediaPage(language);
+
+			reader.initialize(split, context);
+		}
+
+		@Override
+		public LongWritable getCurrentKey() throws IOException, InterruptedException {
+			return reader.getCurrentKey();
+		}
+
+		@Override
+		public WikipediaPage getCurrentValue() throws IOException, InterruptedException {
+			WikipediaPage.readPage(page, reader.getCurrentValue().toString());
+			return page;
+		}
+
+		@Override
+		public boolean nextKeyValue() throws IOException, InterruptedException {
+			return reader.nextKeyValue();
+		}
+
+		@Override
+		public void close() throws IOException {
+			reader.close();
+		}
+
+		@Override
+		public float getProgress() throws IOException, InterruptedException {
+			return reader.getProgress();
+		}
+		
+		/**
+		* Return the sampling ratio.
+		*/
+		public int getSamplingRatio() {
+			return reader.getSamplingRatio();
+		}
+	}
+}
Index: src/contrib/wikipedia/src/java/org/apache/hadoop/mapreduce/wikipedia/XMLInputFormat.java
===================================================================
--- src/contrib/wikipedia/src/java/org/apache/hadoop/mapreduce/wikipedia/XMLInputFormat.java	(revision 0)
+++ src/contrib/wikipedia/src/java/org/apache/hadoop/mapreduce/wikipedia/XMLInputFormat.java	(working copy)
@@ -0,0 +1,366 @@
+/*
+ * Cloud9: A MapReduce Library for Hadoop
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License"); you
+ * may not use this file except in compliance with the License. You may
+ * obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
+ * implied. See the License for the specific language governing
+ * permissions and limitations under the License.
+ */
+
+package org.apache.hadoop.mapreduce.wikipedia;
+
+import java.io.InputStream;
+import java.io.DataInputStream;
+import java.io.IOException;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.FSDataInputStream;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.fs.Seekable;
+import org.apache.hadoop.io.DataOutputBuffer;
+import org.apache.hadoop.io.LongWritable;
+import org.apache.hadoop.io.Text;
+import org.apache.hadoop.io.compress.CodecPool;
+import org.apache.hadoop.io.compress.CompressionCodec;
+import org.apache.hadoop.io.compress.CompressionCodecFactory;
+import org.apache.hadoop.io.compress.Decompressor;
+import org.apache.hadoop.io.compress.SplitCompressionInputStream;
+import org.apache.hadoop.io.compress.SplittableCompressionCodec;
+import org.apache.hadoop.mapreduce.InputSplit;
+import org.apache.hadoop.mapreduce.RecordReader;
+import org.apache.hadoop.mapreduce.TaskAttemptContext;
+import org.apache.hadoop.mapreduce.lib.input.FileSplit;
+import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
+import org.apache.log4j.Logger;
+
+import org.apache.hadoop.mapreduce.lib.input.ApproximateLineRecordReader;
+import org.apache.hadoop.mapreduce.SamplingRecordReader;
+
+import org.apache.hadoop.mapred.JobClient;
+import org.apache.hadoop.mapred.RunningJob;
+import org.apache.hadoop.mapred.JobConf;
+import org.apache.hadoop.mapred.Counters.Counter;
+
+import java.util.Random;
+
+/**
+ * A simple {@link org.apache.hadoop.mapreduce.InputFormat} for XML documents ({@code
+ * org.apache.hadoop.mapreduce} API). The class recognizes begin-of-document and end-of-document
+ * tags only: everything between those delimiting tags is returned in an uninterpreted {@code Text}
+ * object.
+ *
+ * @author Jimmy Lin
+ */
+public class XMLInputFormat extends TextInputFormat {
+  public static final String START_TAG_KEY = "xmlinput.start";
+  public static final String END_TAG_KEY = "xmlinput.end";
+
+  /**
+   * Create a record reader for a given split. The framework will call
+   * {@link RecordReader#initialize(InputSplit, TaskAttemptContext)} before
+   * the split is used.
+   *
+   * @param split the split to be read
+   * @param context the information about the task
+   * @return a new record reader
+   * @throws IOException
+   * @throws InterruptedException
+   */
+  @Override
+  public RecordReader<LongWritable, Text> createRecordReader(InputSplit split,
+      TaskAttemptContext context) {
+    return new XMLRecordReader();
+  }
+
+  /**
+   * Simple {@link org.apache.hadoop.mapreduce.RecordReader} for XML documents ({@code
+   * org.apache.hadoop.mapreduce} API). Recognizes begin-of-document and end-of-document tags only:
+   * everything between those delimiting tags is returned in a {@link Text} object.
+   *
+   * @author Jimmy Lin
+   */
+  public static class XMLRecordReader extends RecordReader<LongWritable, Text> implements SamplingRecordReader {
+    private static final Logger LOG = Logger.getLogger(XMLRecordReader.class);
+
+    private byte[] startTag;
+    private byte[] endTag;
+    private long start;
+    private long end;
+    private long pos;
+    private InputStream fsin = null;
+    private DataOutputBuffer buffer = new DataOutputBuffer();
+
+    private CompressionCodec codec = null;
+    private Decompressor decompressor = null;
+    
+    // To perform random sampling for this file
+    private int skipPages = 0;
+    private int prevSkipPages = 0;
+    private Random rnd = new Random();
+    
+    private long recordStartPos;
+
+    private final LongWritable key = new LongWritable();
+    private final Text value = new Text();
+
+    /**
+     * Return the sampling ratio.
+     */
+    public int getSamplingRatio() {
+      return skipPages;
+    }
+    
+    /**
+     * Called once at initialization.
+     *
+     * @param input the split that defines the range of records to read
+     * @param context the information about the task
+     * @throws IOException
+     * @throws InterruptedException
+     */
+    @Override
+    public void initialize(InputSplit input, TaskAttemptContext context)
+        throws IOException, InterruptedException {
+      Configuration conf = context.getConfiguration();
+      if (conf.get(START_TAG_KEY) == null || conf.get(END_TAG_KEY) == null)
+        throw new RuntimeException("Error! XML start and end tags unspecified!");
+
+      startTag = conf.get(START_TAG_KEY).getBytes("utf-8");
+      endTag = conf.get(END_TAG_KEY).getBytes("utf-8");
+      
+      // Initial approximation ratio
+      skipPages = conf.getInt("mapred.input.approximate.skip", 1);
+      
+      // Use adaptive sampling ratio
+      if (conf.getBoolean("mapred.input.approximate.skip.adaptive", false)) {
+        try {
+          // Connect to the JobTracker to get the sampling ratio required by the reducers
+          // We use a hybrid between the old and the new APIs, in the newest one we should "Cluster()"
+          JobClient client = new JobClient(new JobConf(conf));
+          RunningJob parentJob = client.getJob(context.getJobID().toString());
+          
+          // Check the values for every reducer
+          int numReducers = conf.getInt("mapred.reduce.tasks", 1);
+          int minSamplingRatio = Integer.MAX_VALUE;
+          org.apache.hadoop.mapred.Counters counters = parentJob.getCounters();
+          // Get the required sampling ratio for each reducer
+          for (int reducer=0; reducer<numReducers; reducer++) {
+            org.apache.hadoop.mapred.Counters.Counter counter = counters.findCounter("SamplingRatio", Integer.toString(reducer));
+            int samplingRatio = skipPages;
+            if (counter.getValue() > 0) {
+              samplingRatio = (int) counter.getValue();
+            }
+            // Check which is the minimum sampling ratio we should follow
+            if (samplingRatio < minSamplingRatio) {
+              minSamplingRatio = samplingRatio;
+            }
+          }
+          skipPages = minSamplingRatio;
+        } catch (Exception e) {
+          System.err.println("Error getting the sampling ratio: " + e);
+        }
+      }
+      
+      // Negative ratios don't mean anything
+      if (skipPages < 1) {
+        skipPages = 1;
+      }
+      
+      FileSplit split = (FileSplit) input;
+      start = split.getStart();
+      end = start + split.getLength();
+      Path file = split.getPath();
+
+      CompressionCodecFactory compressionCodecs = new CompressionCodecFactory(conf);
+      codec = compressionCodecs.getCodec(file);
+
+      FileSystem fs = file.getFileSystem(conf);
+
+      if (isCompressedInput()) {
+        LOG.info("Reading compressed file " + file + "...");
+        FSDataInputStream fileIn = fs.open(file);
+        decompressor = CodecPool.getDecompressor(codec);
+        if (codec instanceof SplittableCompressionCodec) {
+          // We can read blocks
+          final SplitCompressionInputStream cIn = ((SplittableCompressionCodec)codec).createInputStream(fileIn, decompressor, start, end, SplittableCompressionCodec.READ_MODE.BYBLOCK);
+          fsin = cIn;
+          start = cIn.getAdjustedStart();
+          end = cIn.getAdjustedEnd();
+        } else {
+          // We cannot read blocks, we have to read everything
+          fsin = new DataInputStream(codec.createInputStream(fileIn, decompressor));
+          
+          end = Long.MAX_VALUE;
+        }
+      } else {
+        LOG.info("Reading uncompressed file " + file + "...");
+        FSDataInputStream fileIn = fs.open(file);
+
+        fileIn.seek(start);
+        fsin = fileIn;
+
+        end = start + split.getLength();
+      }
+
+      recordStartPos = start;
+
+      // Because input streams of gzipped files are not seekable, we need to keep track of bytes
+      // consumed ourselves.
+      pos = start;
+    }
+
+    /**
+     * Read the next key, value pair.
+     *
+     * @return {@code true} if a key/value pair was read
+     * @throws IOException
+     * @throws InterruptedException
+     */
+    @Override
+    public boolean nextKeyValue() throws IOException, InterruptedException {
+      if (getFilePosition() < end) {
+        // We skip a few pages and remember how many we skipped before
+        int rndSkipPages = rnd.nextInt(skipPages);
+        int currentSkipPages = prevSkipPages + rndSkipPages;
+        prevSkipPages = skipPages-rndSkipPages-1;
+        for (int i=0; i<currentSkipPages; i++) {
+          if(!readUntilMatch(startTag, false)) {
+            return false;
+          }
+        }
+      
+        if (readUntilMatch(startTag, false)) {
+          recordStartPos = pos - startTag.length;
+
+          try {
+            buffer.write(startTag);
+            if (readUntilMatch(endTag, true)) {
+              key.set(recordStartPos);
+              value.set(buffer.getData(), 0, buffer.getLength());
+              return true;
+            }
+          } finally {
+            // Because input streams of gzipped files are not seekable, we need to keep track of
+            // bytes consumed ourselves.
+
+            // This is a sanity check to make sure our internal computation of bytes consumed is
+            // accurate. This should be removed later for efficiency once we confirm that this code
+            // works correctly.
+
+            if (fsin instanceof Seekable) {
+              // The position for compressed inputs is weird
+              if (!isCompressedInput()) {
+                if (pos != ((Seekable) fsin).getPos()) {
+                  throw new RuntimeException("bytes consumed error!");
+                }
+              }
+            }
+
+            buffer.reset();
+          }
+        }
+      }
+      return false;
+    }
+
+    /**
+     * Returns the current key.
+     *
+     * @return the current key or {@code null} if there is no current key
+     * @throws IOException
+     * @throws InterruptedException
+     */
+    @Override
+    public LongWritable getCurrentKey() throws IOException, InterruptedException {
+      return key;
+    }
+
+    /**
+     * Returns the current value.
+     *
+     * @return current value
+     * @throws IOException
+     * @throws InterruptedException
+     */
+    @Override
+    public Text getCurrentValue() throws IOException, InterruptedException {
+      return value;
+    }
+
+    /**
+     * Closes the record reader.
+     */
+    @Override
+    public void close() throws IOException {
+      fsin.close();
+    }
+
+    /**
+     * The current progress of the record reader through its data.
+     *
+     * @return a number between 0.0 and 1.0 that is the fraction of the data read
+     * @throws IOException
+     * @throws InterruptedException
+     */
+    @Override
+    public float getProgress() throws IOException {
+      if (start == end) {
+        return 0.0f;
+      } else {
+        return Math.min(1.0f, (getFilePosition() - start) / (float)(end - start));
+      }
+    }
+    
+    private boolean isCompressedInput() {
+      return (codec != null);
+    }
+    
+    protected long getFilePosition() throws IOException {
+      long retVal;
+      if (isCompressedInput() && null != fsin && fsin instanceof Seekable) {
+        retVal = ((Seekable)fsin).getPos();
+      } else {
+        retVal = pos;
+      }
+      return retVal;
+    }
+
+    private boolean readUntilMatch(byte[] match, boolean withinBlock)
+        throws IOException {
+      int i = 0;
+      while (true) {
+        int b = fsin.read();
+        
+        // end of file:
+        if (b == -1)
+          return false;
+            
+        // increment position (bytes consumed)
+        pos++;
+        
+        // save to buffer:
+        if (withinBlock)
+          buffer.write(b);
+
+        // check if we're matching:
+        if (b == match[i]) {
+          i++;
+          if (i >= match.length)
+            return true;
+        } else
+          i = 0;
+        // see if we've passed the stop point:
+        if (!withinBlock && i == 0 && getFilePosition() >= end)
+          return false;
+      }
+    }
+  }
+}
Index: src/contrib/wikipedia/src/java/org/apache/hadoop/mapreduce/wikipedia/XmlInputFormat.java.bak
===================================================================
--- src/contrib/wikipedia/src/java/org/apache/hadoop/mapreduce/wikipedia/XmlInputFormat.java.bak	(revision 0)
+++ src/contrib/wikipedia/src/java/org/apache/hadoop/mapreduce/wikipedia/XmlInputFormat.java.bak	(working copy)
@@ -0,0 +1,147 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.mapred.wikipedia;
+
+import java.io.IOException;
+
+import org.apache.hadoop.fs.FSDataInputStream;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.io.DataOutputBuffer;
+import org.apache.hadoop.io.LongWritable;
+import org.apache.hadoop.io.Text;
+import org.apache.hadoop.mapred.FileSplit;
+import org.apache.hadoop.mapred.InputSplit;
+import org.apache.hadoop.mapred.JobConf;
+import org.apache.hadoop.mapred.RecordReader;
+import org.apache.hadoop.mapred.Reporter;
+import org.apache.hadoop.mapred.TextInputFormat;
+
+/**
+ * Reads records that are delimited by a specifc begin/end tag.
+ */
+public class XmlInputFormat extends TextInputFormat {
+	public static final String START_TAG_KEY = "xmlinput.start";
+	public static final String END_TAG_KEY = "xmlinput.end";
+	
+	@Override
+	public RecordReader<LongWritable,Text> getRecordReader(InputSplit inputSplit,
+								JobConf jobConf,
+								Reporter reporter) throws IOException {
+		return new XmlRecordReader((FileSplit) inputSplit, jobConf);
+	}
+	
+	/**
+	 * XMLRecordReader class to read through a given xml document to output xml
+	 * blocks as records as specified by the start tag and end tag
+	 */
+	public static class XmlRecordReader implements RecordReader<LongWritable,Text> {
+		private final byte[] startTag;
+		private final byte[] endTag;
+		private final long start;
+		private final long end;
+		private final FSDataInputStream fsin;
+		private final DataOutputBuffer buffer = new DataOutputBuffer();
+		
+		private long skipLines = 0;
+		
+		public XmlRecordReader(FileSplit split, JobConf jobConf) throws IOException {
+			startTag = jobConf.get(START_TAG_KEY).getBytes("utf-8");
+			endTag = jobConf.get(END_TAG_KEY).getBytes("utf-8");
+			
+			this.skipLines = jobConf.getInt("mapred.input.approximate.skip", 1);
+			
+			// open the file and seek to the start of the split
+			start = split.getStart();
+			end = start + split.getLength();
+			Path file = split.getPath();
+			FileSystem fs = file.getFileSystem(jobConf);
+			fsin = fs.open(split.getPath());
+			fsin.seek(start);
+		}
+		
+		@Override
+		public boolean next(LongWritable key, Text value) throws IOException {
+			if (fsin.getPos() < end) {
+				if (readUntilMatch(startTag, false)) {
+					try {
+						buffer.write(startTag);
+						if (readUntilMatch(endTag, true)) {
+							key.set(fsin.getPos());
+							value.set(buffer.getData(), 0, buffer.getLength());
+							return true;
+						}
+					} finally {
+						buffer.reset();
+					}
+				}
+			}
+			return false;
+		}
+	
+		@Override
+		public LongWritable createKey() {
+			return new LongWritable();
+		}
+		
+		@Override
+		public Text createValue() {
+			return new Text();
+		}
+		
+		@Override
+		public long getPos() throws IOException {
+			return fsin.getPos();
+		}
+		
+		@Override
+		public void close() throws IOException {
+			fsin.close();
+		}
+		
+		@Override
+		public float getProgress() throws IOException {
+			return (fsin.getPos() - start) / (float) (end - start);
+		}
+		
+		private boolean readUntilMatch(byte[] match, boolean withinBlock) throws IOException {
+			int i = 0;
+			while (true) {
+				int b = fsin.read();
+				// end of file:
+				if (b == -1)
+					return false;
+				// save to buffer:
+				if (withinBlock)
+					buffer.write(b);
+				
+				// check if we're matching:
+				if (b == match[i]) {
+					i++;
+					if (i >= match.length)
+						return true;
+				} else {
+					i = 0;
+				}
+				// see if we've passed the stop point:
+				if (!withinBlock && i == 0 && fsin.getPos() >= end)
+					return false;
+			}
+		}
+	}
+}
\ No newline at end of file
Index: src/contrib/wikipedia/src/java/org/apache/hadoop/mapreduce/wikipedia/XmlInputFormat2.java.bak
===================================================================
--- src/contrib/wikipedia/src/java/org/apache/hadoop/mapreduce/wikipedia/XmlInputFormat2.java.bak	(revision 0)
+++ src/contrib/wikipedia/src/java/org/apache/hadoop/mapreduce/wikipedia/XmlInputFormat2.java.bak	(working copy)
@@ -0,0 +1,426 @@
+/*
+ * Cloud9: A MapReduce Library for Hadoop
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License"); you
+ * may not use this file except in compliance with the License. You may
+ * obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
+ * implied. See the License for the specific language governing
+ * permissions and limitations under the License.
+ */
+
+package org.apache.hadoop.mapred.wikipedia;
+
+import java.io.DataInputStream;
+import java.io.InputStream;
+import java.io.IOException;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.FSDataInputStream;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.fs.Seekable;
+import org.apache.hadoop.io.DataOutputBuffer;
+import org.apache.hadoop.io.LongWritable;
+import org.apache.hadoop.io.Text;
+
+import org.apache.hadoop.io.compress.CodecPool;
+import org.apache.hadoop.io.compress.CompressionCodec;
+import org.apache.hadoop.io.compress.CompressionCodecFactory;
+import org.apache.hadoop.io.compress.Decompressor;
+import org.apache.hadoop.io.compress.SplitCompressionInputStream;
+import org.apache.hadoop.io.compress.SplittableCompressionCodec;
+
+import org.apache.hadoop.mapreduce.InputSplit;
+import org.apache.hadoop.mapreduce.RecordReader;
+import org.apache.hadoop.mapreduce.TaskAttemptContext;
+import org.apache.hadoop.mapreduce.lib.input.FileSplit;
+import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
+import org.apache.log4j.Logger;
+
+/**
+ * A simple {@link org.apache.hadoop.mapreduce.InputFormat} for XML documents ({@code
+ * org.apache.hadoop.mapreduce} API). The class recognizes begin-of-document and end-of-document
+ * tags only: everything between those delimiting tags is returned in an uninterpreted {@code Text}
+ * object.
+ *
+ * @author Jimmy Lin
+ */
+public class XmlInputFormat2 extends TextInputFormat {
+	public static final String START_TAG_KEY = "xmlinput.start";
+	public static final String END_TAG_KEY = "xmlinput.end";
+
+	/**
+	* Create a record reader for a given split. The framework will call
+	* {@link RecordReader#initialize(InputSplit, TaskAttemptContext)} before
+	* the split is used.
+	*
+	* @param split the split to be read
+	* @param context the information about the task
+	* @return a new record reader
+	* @throws IOException
+	* @throws InterruptedException
+	*/
+	@Override
+	public RecordReader<LongWritable, Text> createRecordReader(InputSplit split, TaskAttemptContext context) {
+		return new XMLRecordReader();
+	}
+
+	/**
+	 * Simple {@link org.apache.hadoop.mapreduce.RecordReader} for XML documents ({@code
+	 * org.apache.hadoop.mapreduce} API). Recognizes begin-of-document and end-of-document tags only:
+	 * everything between those delimiting tags is returned in a {@link Text} object.
+	 * @author Jimmy Lin
+	*/
+	public static class XMLRecordReader extends RecordReader<LongWritable, Text> {
+		private static final Logger LOG = Logger.getLogger(XMLRecordReader.class);
+
+		private static final int READ_BLOCK = 1024;
+		
+		private byte[] startTag;
+		private byte[] endTag;
+		private long start;
+		private long end;
+		private long pos;
+		//private DataInputStream fsin = null;
+		private InputStream fsin = null;
+		private DataOutputBuffer buffer = new DataOutputBuffer();
+
+		private long skipPages = 0;
+		
+		private long recordStartPos;
+
+		private final LongWritable key = new LongWritable();
+		private final Text value = new Text();
+		
+		private CompressionCodec codec = null;
+		private Decompressor decompressor = null;
+		
+		// Read buffer
+		private byte[] readBuffer = new byte[READ_BLOCK];
+		private int readBufferIni = -1;
+		private int readBufferFin = -1;
+		
+		/**
+		* Called once at initialization.
+		*
+		* @param input the split that defines the range of records to read
+		* @param context the information about the task
+		* @throws IOException
+		* @throws InterruptedException
+		*/
+		@Override
+		public void initialize(InputSplit input, TaskAttemptContext context) throws IOException, InterruptedException {
+			Configuration conf = context.getConfiguration();
+			if (conf.get(START_TAG_KEY) == null || conf.get(END_TAG_KEY) == null)
+				throw new RuntimeException("Error! XML start and end tags unspecified!");
+
+			startTag = conf.get(START_TAG_KEY).getBytes("utf-8");
+			endTag = conf.get(END_TAG_KEY).getBytes("utf-8");
+			
+			skipPages = conf.getInt("mapred.input.approximate.skip", 1);
+			
+			FileSplit split = (FileSplit) input;
+			start = split.getStart();
+			end = start + split.getLength();
+			Path file = split.getPath();
+
+			CompressionCodecFactory compressionCodecs = new CompressionCodecFactory(conf);
+			codec = compressionCodecs.getCodec(file);
+
+			FileSystem fs = file.getFileSystem(conf);
+
+			// This was the original code that didn't use the splittable codec
+			/*if (codec != null) {
+				LOG.info("Reading compressed file " + file + "...");
+				fsin = new DataInputStream(codec.createInputStream(fs.open(file)));
+
+				end = Long.MAX_VALUE;
+			} else {
+				LOG.info("Reading uncompressed file " + file + "...");
+				FSDataInputStream fileIn = fs.open(file);
+
+				fileIn.seek(start);
+				fsin = fileIn;
+
+				end = start + split.getLength();
+			}*/
+			
+			if (isCompressedInput()) {
+				LOG.info("Reading compressed file " + file + "...");
+				/*fsin = new DataInputStream(codec.createInputStream(fs.open(file)));
+
+				end = Long.MAX_VALUE;*/
+				
+				FSDataInputStream fileIn = fs.open(file);
+				decompressor = CodecPool.getDecompressor(codec);
+				if (codec instanceof SplittableCompressionCodec) {
+					System.out.println("We originally read from " + start + " to " + end);
+					final SplitCompressionInputStream cIn = ((SplittableCompressionCodec)codec).createInputStream(fileIn, decompressor, start, end, SplittableCompressionCodec.READ_MODE.BYBLOCK);
+					fsin = cIn;
+					start = cIn.getAdjustedStart();
+					end = cIn.getAdjustedEnd();
+					//filePosition = cIn;
+					System.out.println("This is compressed so we read from " + start + " to " + end);
+				} else {
+					fsin = new DataInputStream(codec.createInputStream(fileIn, decompressor));
+					
+					end = Long.MAX_VALUE;
+				}
+			} else {
+				LOG.info("Reading uncompressed file " + file + "...");
+				FSDataInputStream fileIn = fs.open(file);
+
+				fileIn.seek(start);
+				fsin = fileIn;
+
+				end = start + split.getLength();
+			}
+			
+			recordStartPos = start;
+
+			// Because input streams of gzipped files are not seekable, we need to keep track of bytes
+			// consumed ourselves.
+			pos = start;
+		}
+
+		/**
+		 * Read the next key, value pair.
+		 * @return {@code true} if a key/value pair was read
+		 * @throws IOException
+		 * @throws InterruptedException
+		 */
+		@Override
+		public boolean nextKeyValue() throws IOException, InterruptedException {
+			if (getFilePosition() <= end) {
+				// We skip a few pages
+				long currentSkipPages = skipPages;
+				for (long i=0; i<currentSkipPages-1; i++) {
+					if(!readUntilMatch(startTag, false)) {
+						return false;
+					}
+				}
+				
+				if (readUntilMatch(startTag, false)) {
+					recordStartPos = pos - startTag.length;
+					try {
+						buffer.write(startTag);
+						if (readUntilMatch(endTag, true)) {
+							key.set(recordStartPos);
+							value.set(buffer.getData(), 0, buffer.getLength());
+							return true;
+						}
+						
+					} finally {
+						// Because input streams of gzipped files are not seekable, we need to keep track of
+						// bytes consumed ourselves.
+
+						// This is a sanity check to make sure our internal computation of bytes consumed is
+						// accurate. This should be removed later for efficiency once we confirm that this code
+						// works correctly.
+						
+						// With the splittable compression codec this is not good anymore
+						/*if (fsin instanceof Seekable) {
+							LOG.info(pos + " -> " + ((Seekable) fsin).getPos());
+							if (Math.abs(pos - ((Seekable) fsin).getPos()) > 5) {
+							//if (pos != ((Seekable) fsin).getPos()) {
+								System.out.println(pos + " != " + ((Seekable) fsin).getPos());
+								throw new RuntimeException("bytes consumed error!");
+							}
+						}*/
+						buffer.reset();
+					}
+				}
+			}
+			return false;
+		}
+
+		/**
+		 * Returns the current key.
+		 * @return the current key or {@code null} if there is no current key
+		 * @throws IOException
+		 * @throws InterruptedException
+		 */
+		@Override
+		public LongWritable getCurrentKey() throws IOException, InterruptedException {
+			return key;
+		}
+
+		/**
+		* Returns the current value.
+		*
+		* @return current value
+		* @throws IOException
+		* @throws InterruptedException
+		*/
+		@Override
+		public Text getCurrentValue() throws IOException, InterruptedException {
+			return value;
+		}
+
+		/**
+		* Closes the record reader.
+		*/
+		@Override
+		public void close() throws IOException {
+			try {
+				if (fsin != null) {
+					fsin.close();
+				}
+			} finally {
+				if (decompressor != null) {
+					CodecPool.returnDecompressor(decompressor);
+				}
+			}
+		}
+
+		/**
+		* The current progress of the record reader through its data.
+		*
+		* @return a number between 0.0 and 1.0 that is the fraction of the data read
+		* @throws IOException
+		* @throws InterruptedException
+		*/
+		@Override
+		public float getProgress() throws IOException {
+			if (start == end) {
+				return 0.0f;
+			} else {
+				//return ((float) (pos - start)) / ((float) (end - start));
+				return Math.min(1.0f, (getFilePosition() - start) / (float)(end - start));
+			}
+		}
+		
+		private boolean isCompressedInput() {
+			return (codec != null);
+		}
+		
+		protected long getFilePosition() throws IOException {
+			long retVal;
+			if (isCompressedInput() && null != fsin && fsin instanceof Seekable) {
+				retVal = ((Seekable)fsin).getPos();
+			} else {
+				retVal = pos;
+			}
+			return retVal;
+		}
+		
+		/*private boolean readUntilMatch(byte[] match, boolean withinBlock) throws IOException {
+			int i = 0;
+			while (true) {
+				int b = fsin.read();
+				// increment position (bytes consumed)
+				pos++;
+
+				// end of file:
+				if (b == -1) {
+					return false;
+				}
+				// save to buffer:
+				if (withinBlock) {
+					buffer.write(b);
+				}
+
+				// check if we're matching:
+				if (b == match[i]) {
+					i++;
+					if (i >= match.length) {
+						return true;
+					}
+				} else {
+					i = 0;
+				}
+				// see if we've passed the stop point:
+				if (!withinBlock && i == 0 && getFilePosition() >= end) {
+					return false;
+				}
+			}
+		}*/
+		
+		private boolean readUntilMatch(byte[] match, boolean withinBlock) throws IOException {
+			int i = 0;
+			while (true) {
+				/*
+				// We need to read more, otherwise we just use the previous buffer from the point we left it
+				if (readBufferIni < 0) {
+					readBufferIni = 0;
+					readBufferFin = fsin.read(readBuffer, 0, READ_BLOCK);
+					
+					// end of file:
+					if (readBufferFin == -1) {
+						return false;
+					}
+					
+					// Increment position (bytes consumed)
+					pos += readBufferFin;
+				}
+				
+				// Search for the match in the read buffer
+				for (int j=readBufferIni; j<readBufferFin; j++) {
+					if (readBuffer[j] == match[i]) {
+						i++;
+						if (match.length <= i) {
+							// We copy everything until this point
+							if (withinBlock) {
+								buffer.write(readBuffer, 0, j);
+							}
+							
+							// Set it for the next iteration to start from this point
+							readBufferIni = j;
+						
+							// We found the match
+							return true;
+						}
+					} else {
+						i =0;
+					}
+				}
+				
+				// We finished the block, copy everything
+				if (withinBlock) {
+					buffer.write(readBuffer, 0, readBufferFin);
+				}
+				
+				// We finished the read buffer
+				readBufferIni = -1;
+				*/
+				
+				int b = fsin.read();
+				
+				// end of file:
+				if (b == -1) {
+					return false;
+				}
+				
+				// increment position (bytes consumed)
+				pos++;
+				
+				// save to buffer:
+				if (withinBlock) {
+					buffer.write(b);
+				}
+
+				// check if we're matching:
+				if (match[i] == b) {
+					i++;
+					// We matched!
+					if (match.length <= i) {
+						return true;
+					}
+				} else {
+					i = 0;
+				}
+				
+				// see if we've passed the stop point:
+				if (!withinBlock && i == 0 && getFilePosition() >= end) {
+					return false;
+				}
+			}
+		}
+	}
+}
\ No newline at end of file
Index: src/contrib/wikipedia/src/java/org/apache/hadoop/mapreduce/wikipedia/languages/ArabicWikipediaPage.java
===================================================================
--- src/contrib/wikipedia/src/java/org/apache/hadoop/mapreduce/wikipedia/languages/ArabicWikipediaPage.java	(revision 0)
+++ src/contrib/wikipedia/src/java/org/apache/hadoop/mapreduce/wikipedia/languages/ArabicWikipediaPage.java	(working copy)
@@ -0,0 +1,79 @@
+/*
+ * Cloud9: A MapReduce Library for Hadoop
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License"); you
+ * may not use this file except in compliance with the License. You may
+ * obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
+ * implied. See the License for the specific language governing
+ * permissions and limitations under the License.
+ */
+
+package org.apache.hadoop.mapreduce.wikipedia.language;
+
+import java.util.regex.Matcher;
+import java.util.regex.Pattern;
+
+import org.apache.commons.lang.StringEscapeUtils;
+import org.apache.hadoop.mapreduce.wikipedia.WikipediaPage;
+
+/**
+ * An Arabic page from Wikipedia.
+ * 
+ * @author Ferhan Ture
+ */
+public class ArabicWikipediaPage extends WikipediaPage {
+  /**
+   * Language dependent identifiers of disambiguation, redirection, and stub pages.
+   */
+  private static final String IDENTIFIER_REDIRECTION_UPPERCASE = "#REDIRECT";
+  private static final String IDENTIFIER_REDIRECTION_LOWERCASE = "#redirect";
+  private static final String IDENTIFIER_STUB_TEMPLATE = "stub}}";
+  private static final String IDENTIFIER_STUB_WIKIPEDIA_NAMESPACE = "Wikipedia:Stub";
+  private static final Pattern disambPattern = Pattern.compile("\\{\\{\u062A\u0648\u0636\u064A\u062D\\}\\}", Pattern.CASE_INSENSITIVE);
+  private static final String LANGUAGE_CODE = "ar";
+
+  /**
+   * Creates an empty <code>ArabicWikipediaPage</code> object.
+   */
+  public ArabicWikipediaPage() {
+    super();
+  }
+
+  @Override
+  protected void processPage(String s) {
+    this.language = LANGUAGE_CODE;
+
+    // parse out title
+    int start = s.indexOf(XML_START_TAG_TITLE);
+    int end = s.indexOf(XML_END_TAG_TITLE, start);
+    this.title = StringEscapeUtils.unescapeHtml(s.substring(start + 7, end));
+
+    // determine if article belongs to the article namespace
+    start = s.indexOf(XML_START_TAG_NAMESPACE);
+    end = s.indexOf(XML_END_TAG_NAMESPACE);
+    this.isArticle = s.substring(start + 4, end).trim().equals("0");
+    
+    // parse out the document id
+    start = s.indexOf(XML_START_TAG_ID);
+    end = s.indexOf(XML_END_TAG_ID);
+    this.mId = s.substring(start + 4, end);
+
+    // parse out actual text of article
+    this.textStart = s.indexOf(XML_START_TAG_TEXT);
+    this.textEnd = s.indexOf(XML_END_TAG_TEXT, this.textStart);
+
+    // determine if article is a disambiguation, redirection, and/or stub page.
+    Matcher matcher = disambPattern.matcher(page);
+    this.isDisambig = matcher.find();
+    this.isRedirect = s.substring(this.textStart + XML_START_TAG_TEXT.length(), this.textStart + XML_START_TAG_TEXT.length() + IDENTIFIER_REDIRECTION_UPPERCASE.length()).compareTo(IDENTIFIER_REDIRECTION_UPPERCASE) == 0 ||
+                      s.substring(this.textStart + XML_START_TAG_TEXT.length(), this.textStart + XML_START_TAG_TEXT.length() + IDENTIFIER_REDIRECTION_LOWERCASE.length()).compareTo(IDENTIFIER_REDIRECTION_LOWERCASE) == 0;
+    this.isStub = s.indexOf(IDENTIFIER_STUB_TEMPLATE, this.textStart) != -1 || 
+                  s.indexOf(IDENTIFIER_STUB_WIKIPEDIA_NAMESPACE) != -1;
+  }
+}
Index: src/contrib/wikipedia/src/java/org/apache/hadoop/mapreduce/wikipedia/languages/ChineseWikipediaPage.java
===================================================================
--- src/contrib/wikipedia/src/java/org/apache/hadoop/mapreduce/wikipedia/languages/ChineseWikipediaPage.java	(revision 0)
+++ src/contrib/wikipedia/src/java/org/apache/hadoop/mapreduce/wikipedia/languages/ChineseWikipediaPage.java	(working copy)
@@ -0,0 +1,79 @@
+/*
+ * Cloud9: A MapReduce Library for Hadoop
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License"); you
+ * may not use this file except in compliance with the License. You may
+ * obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
+ * implied. See the License for the specific language governing
+ * permissions and limitations under the License.
+ */
+
+package org.apache.hadoop.mapreduce.wikipedia.language;
+
+import java.util.regex.Matcher;
+import java.util.regex.Pattern;
+
+import org.apache.commons.lang.StringEscapeUtils;
+import org.apache.hadoop.mapreduce.wikipedia.WikipediaPage;
+
+/**
+ * An Chinese page from Wikipedia.
+ * 
+ * @author Ferhan Ture
+ */
+public class ChineseWikipediaPage extends WikipediaPage {
+  /**
+   * Language dependent identifiers of disambiguation, redirection, and stub pages.
+   */
+  private static final String IDENTIFIER_REDIRECTION_UPPERCASE = "#REDIRECT";
+  private static final String IDENTIFIER_REDIRECTION_LOWERCASE = "#redirect";
+  private static final String IDENTIFIER_STUB_TEMPLATE = "stub}}";
+  private static final String IDENTIFIER_STUB_WIKIPEDIA_NAMESPACE = "Wikipedia:Stub";
+  private static final Pattern disambPattern = Pattern.compile("\\{\\{disambig.+Cat=.+\\}\\}", Pattern.CASE_INSENSITIVE);
+  private static final String LANGUAGE_CODE = "zh";
+
+  /**
+   * Creates an empty <code>ChineseWikipediaPage</code> object.
+   */
+  public ChineseWikipediaPage() {
+    super();
+  }
+
+  @Override
+  protected void processPage(String s) {
+    this.language = LANGUAGE_CODE;
+
+    // parse out title
+    int start = s.indexOf(XML_START_TAG_TITLE);
+    int end = s.indexOf(XML_END_TAG_TITLE, start);
+    this.title = StringEscapeUtils.unescapeHtml(s.substring(start + 7, end));
+
+    // determine if article belongs to the article namespace
+    start = s.indexOf(XML_START_TAG_NAMESPACE);
+    end = s.indexOf(XML_END_TAG_NAMESPACE);
+    this.isArticle = s.substring(start + 4, end).trim().equals("0");
+    
+    // parse out the document id
+    start = s.indexOf(XML_START_TAG_ID);
+    end = s.indexOf(XML_END_TAG_ID);
+    this.mId = s.substring(start + 4, end);
+
+    // parse out actual text of article
+    this.textStart = s.indexOf(XML_START_TAG_TEXT);
+    this.textEnd = s.indexOf(XML_END_TAG_TEXT, this.textStart);
+
+    // determine if article is a disambiguation, redirection, and/or stub page.
+    Matcher matcher = disambPattern.matcher(page);
+    this.isDisambig = matcher.find();
+    this.isRedirect = s.substring(this.textStart + XML_START_TAG_TEXT.length(), this.textStart + XML_START_TAG_TEXT.length() + IDENTIFIER_REDIRECTION_UPPERCASE.length()).compareTo(IDENTIFIER_REDIRECTION_UPPERCASE) == 0 ||
+                      s.substring(this.textStart + XML_START_TAG_TEXT.length(), this.textStart + XML_START_TAG_TEXT.length() + IDENTIFIER_REDIRECTION_LOWERCASE.length()).compareTo(IDENTIFIER_REDIRECTION_LOWERCASE) == 0;
+    this.isStub = s.indexOf(IDENTIFIER_STUB_TEMPLATE, this.textStart) != -1 || 
+                  s.indexOf(IDENTIFIER_STUB_WIKIPEDIA_NAMESPACE) != -1;
+  }
+}
Index: src/contrib/wikipedia/src/java/org/apache/hadoop/mapreduce/wikipedia/languages/CzechWikipediaPage.java
===================================================================
--- src/contrib/wikipedia/src/java/org/apache/hadoop/mapreduce/wikipedia/languages/CzechWikipediaPage.java	(revision 0)
+++ src/contrib/wikipedia/src/java/org/apache/hadoop/mapreduce/wikipedia/languages/CzechWikipediaPage.java	(working copy)
@@ -0,0 +1,79 @@
+/*
+ * Cloud9: A MapReduce Library for Hadoop
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License"); you
+ * may not use this file except in compliance with the License. You may
+ * obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
+ * implied. See the License for the specific language governing
+ * permissions and limitations under the License.
+ */
+
+package org.apache.hadoop.mapreduce.wikipedia.language;
+
+import java.util.regex.Matcher;
+import java.util.regex.Pattern;
+
+import org.apache.commons.lang.StringEscapeUtils;
+import org.apache.hadoop.mapreduce.wikipedia.WikipediaPage;
+
+/**
+ * An Czech page from Wikipedia.
+ * 
+ * @author Ferhan Ture
+ */
+public class CzechWikipediaPage extends WikipediaPage {
+  /**
+   * Language dependent identifiers of disambiguation, redirection, and stub pages.
+   */
+  private static final String IDENTIFIER_REDIRECTION_UPPERCASE = "#REDIRECT";
+  private static final String IDENTIFIER_REDIRECTION_LOWERCASE = "#redirect";
+  private static final String IDENTIFIER_STUB_TEMPLATE = "stub}}";
+  private static final String IDENTIFIER_STUB_WIKIPEDIA_NAMESPACE = "Wikipedia:Stub";
+  private static final Pattern disambPattern = Pattern.compile("\\{\\{rozcestn\u00EDk\\}\\}", Pattern.CASE_INSENSITIVE);
+  private static final String LANGUAGE_CODE = "cs";
+
+  /**
+   * Creates an empty <code>CzechWikipediaPage</code> object.
+   */
+  public CzechWikipediaPage() {
+    super();
+  }
+
+  @Override
+  protected void processPage(String s) {
+    this.language = LANGUAGE_CODE;
+
+    // parse out title
+    int start = s.indexOf(XML_START_TAG_TITLE);
+    int end = s.indexOf(XML_END_TAG_TITLE, start);
+    this.title = StringEscapeUtils.unescapeHtml(s.substring(start + 7, end));
+
+    // determine if article belongs to the article namespace
+    start = s.indexOf(XML_START_TAG_NAMESPACE);
+    end = s.indexOf(XML_END_TAG_NAMESPACE);
+    this.isArticle = s.substring(start + 4, end).trim().equals("0");
+    
+    // parse out the document id
+    start = s.indexOf(XML_START_TAG_ID);
+    end = s.indexOf(XML_END_TAG_ID);
+    this.mId = s.substring(start + 4, end);
+
+    // parse out actual text of article
+    this.textStart = s.indexOf(XML_START_TAG_TEXT);
+    this.textEnd = s.indexOf(XML_END_TAG_TEXT, this.textStart);
+
+    // determine if article is a disambiguation, redirection, and/or stub page.
+    Matcher matcher = disambPattern.matcher(page);
+    this.isDisambig = matcher.find();
+    this.isRedirect = s.substring(this.textStart + XML_START_TAG_TEXT.length(), this.textStart + XML_START_TAG_TEXT.length() + IDENTIFIER_REDIRECTION_UPPERCASE.length()).compareTo(IDENTIFIER_REDIRECTION_UPPERCASE) == 0 ||
+                      s.substring(this.textStart + XML_START_TAG_TEXT.length(), this.textStart + XML_START_TAG_TEXT.length() + IDENTIFIER_REDIRECTION_LOWERCASE.length()).compareTo(IDENTIFIER_REDIRECTION_LOWERCASE) == 0;
+    this.isStub = s.indexOf(IDENTIFIER_STUB_TEMPLATE, this.textStart) != -1 || 
+                  s.indexOf(IDENTIFIER_STUB_WIKIPEDIA_NAMESPACE) != -1;
+  }
+}
Index: src/contrib/wikipedia/src/java/org/apache/hadoop/mapreduce/wikipedia/languages/EnglishWikipediaPage.java
===================================================================
--- src/contrib/wikipedia/src/java/org/apache/hadoop/mapreduce/wikipedia/languages/EnglishWikipediaPage.java	(revision 0)
+++ src/contrib/wikipedia/src/java/org/apache/hadoop/mapreduce/wikipedia/languages/EnglishWikipediaPage.java	(working copy)
@@ -0,0 +1,81 @@
+/*
+ * Cloud9: A MapReduce Library for Hadoop
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License"); you
+ * may not use this file except in compliance with the License. You may
+ * obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
+ * implied. See the License for the specific language governing
+ * permissions and limitations under the License.
+ */
+
+package org.apache.hadoop.mapreduce.wikipedia.language;
+
+import java.util.regex.Matcher;
+import java.util.regex.Pattern;
+
+import org.apache.commons.lang.StringEscapeUtils;
+import org.apache.hadoop.mapreduce.wikipedia.WikipediaPage;
+
+/**
+ * An English page from Wikipedia.
+ * 
+ * @author Peter Exner
+ * @author Ferhan Ture
+ */
+public class EnglishWikipediaPage extends WikipediaPage {
+  /**
+   * Language dependent identifiers of disambiguation, redirection, and stub pages.
+   */
+  private static final String IDENTIFIER_REDIRECTION_UPPERCASE = "#REDIRECT";
+  private static final String IDENTIFIER_REDIRECTION_LOWERCASE = "#redirect";
+  private static final String IDENTIFIER_STUB_TEMPLATE = "stub}}";
+  private static final String IDENTIFIER_STUB_WIKIPEDIA_NAMESPACE = "Wikipedia:Stub";
+  private static final Pattern disambPattern = Pattern.compile("\\{\\{disambig\\w*\\}\\}", Pattern.CASE_INSENSITIVE);
+  private static final String LANGUAGE_CODE = "en";
+
+  /**
+   * Creates an empty <code>EnglishWikipediaPage</code> object.
+   */
+  public EnglishWikipediaPage() {
+    super();
+  }
+
+  @Override
+  protected void processPage(String s) {
+    this.language = LANGUAGE_CODE;
+    
+    // parse out title
+    int start = s.indexOf(XML_START_TAG_TITLE);
+    int end = s.indexOf(XML_END_TAG_TITLE, start);
+    this.title = StringEscapeUtils.unescapeHtml(s.substring(start + 7, end));
+
+    // determine if article belongs to the article namespace
+    start = s.indexOf(XML_START_TAG_NAMESPACE);
+    end = s.indexOf(XML_END_TAG_NAMESPACE);
+    this.isArticle = start == -1 ? true : s.substring(start + 4, end).trim().equals("0");
+    // add check because namespace tag not present in older dumps
+    
+    // parse out the document id
+    start = s.indexOf(XML_START_TAG_ID);
+    end = s.indexOf(XML_END_TAG_ID);
+    this.mId = s.substring(start + 4, end);
+
+    // parse out actual text of article
+    this.textStart = s.indexOf(XML_START_TAG_TEXT);
+    this.textEnd = s.indexOf(XML_END_TAG_TEXT, this.textStart);
+
+    // determine if article is a disambiguation, redirection, and/or stub page.
+    Matcher matcher = disambPattern.matcher(page);
+    this.isDisambig = matcher.find();
+    this.isRedirect = s.substring(this.textStart + XML_START_TAG_TEXT.length(), this.textStart + XML_START_TAG_TEXT.length() + IDENTIFIER_REDIRECTION_UPPERCASE.length()).compareTo(IDENTIFIER_REDIRECTION_UPPERCASE) == 0 ||
+                      s.substring(this.textStart + XML_START_TAG_TEXT.length(), this.textStart + XML_START_TAG_TEXT.length() + IDENTIFIER_REDIRECTION_LOWERCASE.length()).compareTo(IDENTIFIER_REDIRECTION_LOWERCASE) == 0;
+    this.isStub = s.indexOf(IDENTIFIER_STUB_TEMPLATE, this.textStart) != -1 || 
+                  s.indexOf(IDENTIFIER_STUB_WIKIPEDIA_NAMESPACE) != -1;
+  }
+}
Index: src/contrib/wikipedia/src/java/org/apache/hadoop/mapreduce/wikipedia/languages/GermanWikipediaPage.java
===================================================================
--- src/contrib/wikipedia/src/java/org/apache/hadoop/mapreduce/wikipedia/languages/GermanWikipediaPage.java	(revision 0)
+++ src/contrib/wikipedia/src/java/org/apache/hadoop/mapreduce/wikipedia/languages/GermanWikipediaPage.java	(working copy)
@@ -0,0 +1,86 @@
+/*
+ * Cloud9: A MapReduce Library for Hadoop
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License"); you
+ * may not use this file except in compliance with the License. You may
+ * obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
+ * implied. See the License for the specific language governing
+ * permissions and limitations under the License.
+ */
+
+package org.apache.hadoop.mapreduce.wikipedia.language;
+
+import java.util.regex.Matcher;
+import java.util.regex.Pattern;
+
+import org.apache.commons.lang.StringEscapeUtils;
+import org.apache.hadoop.mapreduce.wikipedia.WikipediaPage;
+
+/**
+ * An German page from Wikipedia.
+ * 
+ * @author Peter Exner
+ * @author Ferhan Ture
+ */
+public class GermanWikipediaPage extends WikipediaPage {
+  /**
+   * Language dependent identifiers of disambiguation, redirection, and stub pages.
+   */
+  private static final String IDENTIFIER_REDIRECTION_UPPERCASE = "#REDIRECT";
+  private static final String IDENTIFIER_REDIRECTION_LOWERCASE = "#redirect";
+  private static final String IDENTIFIER_REDIRECTION_UPPERCASE_DE = "#WEITERLEITUNG";
+  private static final String IDENTIFIER_REDIRECTION_LOWERCASE_DE = "#weiterleitung";
+  private static final String IDENTIFIER_REDIRECTION_CAPITALIZED_DE = "#Weiterleitung";
+  private static final String IDENTIFIER_STUB_TEMPLATE = "stub}}";
+  private static final String IDENTIFIER_STUB_WIKIPEDIA_NAMESPACE = "Wikipedia:Stub";
+  private static final Pattern disambPattern = Pattern.compile("\\{\\{begriffskl\u00E4rung\\}\\}", Pattern.CASE_INSENSITIVE);
+  private static final String LANGUAGE_CODE = "de";
+
+  /**
+   * Creates an empty <code>GermanWikipediaPage</code> object.
+   */
+  public GermanWikipediaPage() {
+    super();
+  }
+
+  @Override
+  protected void processPage(String s) {
+    this.language = LANGUAGE_CODE;
+
+    // parse out title
+    int start = s.indexOf(XML_START_TAG_TITLE);
+    int end = s.indexOf(XML_END_TAG_TITLE, start);
+    this.title = StringEscapeUtils.unescapeHtml(s.substring(start + 7, end));
+
+    // determine if article belongs to the article namespace
+    start = s.indexOf(XML_START_TAG_NAMESPACE);
+    end = s.indexOf(XML_END_TAG_NAMESPACE);
+    this.isArticle = s.substring(start + 4, end).trim().equals("0");
+    
+    // parse out the document id
+    start = s.indexOf(XML_START_TAG_ID);
+    end = s.indexOf(XML_END_TAG_ID);
+    this.mId = s.substring(start + 4, end);
+
+    // parse out actual text of article
+    this.textStart = s.indexOf(XML_START_TAG_TEXT);
+    this.textEnd = s.indexOf(XML_END_TAG_TEXT, this.textStart);
+
+    // determine if article is a disambiguation, redirection, and/or stub page.
+    Matcher matcher = disambPattern.matcher(page);
+    this.isDisambig = matcher.find();
+    this.isRedirect = s.substring(this.textStart + XML_START_TAG_TEXT.length(), this.textStart + XML_START_TAG_TEXT.length() + IDENTIFIER_REDIRECTION_UPPERCASE.length()).compareTo(IDENTIFIER_REDIRECTION_UPPERCASE) == 0 ||
+                      s.substring(this.textStart + XML_START_TAG_TEXT.length(), this.textStart + XML_START_TAG_TEXT.length() + IDENTIFIER_REDIRECTION_LOWERCASE.length()).compareTo(IDENTIFIER_REDIRECTION_LOWERCASE) == 0 ||
+                      s.substring(this.textStart + XML_START_TAG_TEXT.length(), this.textStart + XML_START_TAG_TEXT.length() + IDENTIFIER_REDIRECTION_UPPERCASE_DE.length()).compareTo(IDENTIFIER_REDIRECTION_UPPERCASE_DE) == 0 ||
+                      s.substring(this.textStart + XML_START_TAG_TEXT.length(), this.textStart + XML_START_TAG_TEXT.length() + IDENTIFIER_REDIRECTION_LOWERCASE_DE.length()).compareTo(IDENTIFIER_REDIRECTION_LOWERCASE_DE) == 0 ||
+                      s.substring(this.textStart + XML_START_TAG_TEXT.length(), this.textStart + XML_START_TAG_TEXT.length() + IDENTIFIER_REDIRECTION_CAPITALIZED_DE.length()).compareTo(IDENTIFIER_REDIRECTION_CAPITALIZED_DE) == 0;
+    this.isStub = s.indexOf(IDENTIFIER_STUB_TEMPLATE, this.textStart) != -1 || 
+                  s.indexOf(IDENTIFIER_STUB_WIKIPEDIA_NAMESPACE) != -1;
+  }
+}
Index: src/contrib/wikipedia/src/java/org/apache/hadoop/mapreduce/wikipedia/languages/SpanishWikipediaPage.java
===================================================================
--- src/contrib/wikipedia/src/java/org/apache/hadoop/mapreduce/wikipedia/languages/SpanishWikipediaPage.java	(revision 0)
+++ src/contrib/wikipedia/src/java/org/apache/hadoop/mapreduce/wikipedia/languages/SpanishWikipediaPage.java	(working copy)
@@ -0,0 +1,79 @@
+/*
+ * Cloud9: A MapReduce Library for Hadoop
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License"); you
+ * may not use this file except in compliance with the License. You may
+ * obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
+ * implied. See the License for the specific language governing
+ * permissions and limitations under the License.
+ */
+
+package org.apache.hadoop.mapreduce.wikipedia.language;
+
+import java.util.regex.Matcher;
+import java.util.regex.Pattern;
+
+import org.apache.commons.lang.StringEscapeUtils;
+import org.apache.hadoop.mapreduce.wikipedia.WikipediaPage;
+
+/**
+ * An Spanish page from Wikipedia.
+ * 
+ * @author Ferhan Ture
+ */
+public class SpanishWikipediaPage extends WikipediaPage {
+  /**
+   * Language dependent identifiers of disambiguation, redirection, and stub pages.
+   */
+  private static final String IDENTIFIER_REDIRECTION_UPPERCASE = "#REDIRECT";
+  private static final String IDENTIFIER_REDIRECTION_LOWERCASE = "#redirect";
+  private static final String IDENTIFIER_STUB_TEMPLATE = "stub}}";
+  private static final String IDENTIFIER_STUB_WIKIPEDIA_NAMESPACE = "Wikipedia:Stub";
+  private static final Pattern disambPattern = Pattern.compile("\\{\\{desambiguaci\u00F3n\\}\\}", Pattern.CASE_INSENSITIVE);
+  private static final String LANGUAGE_CODE = "es";
+
+  /**
+   * Creates an empty <code>SpanishWikipediaPage</code> object.
+   */
+  public SpanishWikipediaPage() {
+    super();
+  }
+
+  @Override
+  protected void processPage(String s) {
+    this.language = LANGUAGE_CODE;
+    
+    // parse out title
+    int start = s.indexOf(XML_START_TAG_TITLE);
+    int end = s.indexOf(XML_END_TAG_TITLE, start);
+    this.title = StringEscapeUtils.unescapeHtml(s.substring(start + 7, end));
+
+    // determine if article belongs to the article namespace
+    start = s.indexOf(XML_START_TAG_NAMESPACE);
+    end = s.indexOf(XML_END_TAG_NAMESPACE);
+    this.isArticle = s.substring(start + 4, end).trim().equals("0");
+    
+    // parse out the document id
+    start = s.indexOf(XML_START_TAG_ID);
+    end = s.indexOf(XML_END_TAG_ID);
+    this.mId = s.substring(start + 4, end);
+
+    // parse out actual text of article
+    this.textStart = s.indexOf(XML_START_TAG_TEXT);
+    this.textEnd = s.indexOf(XML_END_TAG_TEXT, this.textStart);
+
+    // determine if article is a disambiguation, redirection, and/or stub page.
+    Matcher matcher = disambPattern.matcher(page);
+    this.isDisambig = matcher.find();
+    this.isRedirect = s.substring(this.textStart + XML_START_TAG_TEXT.length(), this.textStart + XML_START_TAG_TEXT.length() + IDENTIFIER_REDIRECTION_UPPERCASE.length()).compareTo(IDENTIFIER_REDIRECTION_UPPERCASE) == 0 ||
+                      s.substring(this.textStart + XML_START_TAG_TEXT.length(), this.textStart + XML_START_TAG_TEXT.length() + IDENTIFIER_REDIRECTION_LOWERCASE.length()).compareTo(IDENTIFIER_REDIRECTION_LOWERCASE) == 0;
+    this.isStub = s.indexOf(IDENTIFIER_STUB_TEMPLATE, this.textStart) != -1 || 
+                  s.indexOf(IDENTIFIER_STUB_WIKIPEDIA_NAMESPACE) != -1;
+  }
+}
Index: src/contrib/wikipedia/src/java/org/apache/hadoop/mapreduce/wikipedia/languages/SwedishWikipediaPage.java
===================================================================
--- src/contrib/wikipedia/src/java/org/apache/hadoop/mapreduce/wikipedia/languages/SwedishWikipediaPage.java	(revision 0)
+++ src/contrib/wikipedia/src/java/org/apache/hadoop/mapreduce/wikipedia/languages/SwedishWikipediaPage.java	(working copy)
@@ -0,0 +1,83 @@
+/*
+ * Cloud9: A MapReduce Library for Hadoop
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License"); you
+ * may not use this file except in compliance with the License. You may
+ * obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
+ * implied. See the License for the specific language governing
+ * permissions and limitations under the License.
+ */
+
+package org.apache.hadoop.mapreduce.wikipedia.language;
+
+import org.apache.commons.lang.StringEscapeUtils;
+import org.apache.hadoop.mapreduce.wikipedia.WikipediaPage;
+
+/**
+ * A Swedish page from Wikipedia.
+ * 
+ * @author Peter Exner
+ */
+public class SwedishWikipediaPage extends WikipediaPage {
+  /**
+   * Language dependent identifiers of disambiguation, redirection, and stub pages.
+   */
+  private static final String IDENTIFIER_DISAMBIGUATION_UPPERCASE_SV = "{{F\u00F6rgrening}}";
+  private static final String IDENTIFIER_DISAMBIGUATION_LOWERCASE_SV = "{{f\u00F6rgrening}}";
+  private static final String IDENTIFIER_REDIRECTION_UPPERCASE = "#REDIRECT";
+  private static final String IDENTIFIER_REDIRECTION_LOWERCASE = "#redirect";
+  private static final String IDENTIFIER_REDIRECTION_UPPERCASE_SV = "#OMDIRIGERING";
+  private static final String IDENTIFIER_REDIRECTION_LOWERCASE_SV = "#omdirigering";
+  private static final String IDENTIFIER_REDIRECTION_CAPITALIZED_SV = "#Omdirigering";
+  private static final String IDENTIFIER_STUB_TEMPLATE = "stub}}";
+  private static final String IDENTIFIER_STUB_WIKIPEDIA_NAMESPACE = "Wikipedia:Stub";
+  private static final String LANGUAGE_CODE = "sv";
+
+  /**
+   * Creates an empty <code>EnglishWikipediaPage</code> object.
+   */
+  public SwedishWikipediaPage() {
+    super();
+  }
+
+  @Override
+  protected void processPage(String s) {
+    this.language = LANGUAGE_CODE;
+    
+    // parse out title
+    int start = s.indexOf(XML_START_TAG_TITLE);
+    int end = s.indexOf(XML_END_TAG_TITLE, start);
+    this.title = StringEscapeUtils.unescapeHtml(s.substring(start + 7, end));
+
+    // determine if article belongs to the article namespace
+    start = s.indexOf(XML_START_TAG_NAMESPACE);
+    end = s.indexOf(XML_END_TAG_NAMESPACE);
+    this.isArticle = s.substring(start + 4, end).trim().equals("0");
+    
+    // parse out the document id
+    start = s.indexOf(XML_START_TAG_ID);
+    end = s.indexOf(XML_END_TAG_ID);
+    this.mId = s.substring(start + 4, end);
+
+    // parse out actual text of article
+    this.textStart = s.indexOf(XML_START_TAG_TEXT);
+    this.textEnd = s.indexOf(XML_END_TAG_TEXT, this.textStart);
+
+    // determine if article is a disambiguation, redirection, and/or stub page.
+    this.isDisambig = s.indexOf(IDENTIFIER_DISAMBIGUATION_LOWERCASE_SV, this.textStart) != -1 || 
+                      s.indexOf(IDENTIFIER_DISAMBIGUATION_UPPERCASE_SV, this.textStart) != -1;
+    this.isRedirect = s.substring(this.textStart + XML_START_TAG_TEXT.length(), this.textStart + XML_START_TAG_TEXT.length() + IDENTIFIER_REDIRECTION_UPPERCASE.length()).compareTo(IDENTIFIER_REDIRECTION_UPPERCASE) == 0 ||
+                      s.substring(this.textStart + XML_START_TAG_TEXT.length(), this.textStart + XML_START_TAG_TEXT.length() + IDENTIFIER_REDIRECTION_LOWERCASE.length()).compareTo(IDENTIFIER_REDIRECTION_LOWERCASE) == 0 ||
+                      s.substring(this.textStart + XML_START_TAG_TEXT.length(), this.textStart + XML_START_TAG_TEXT.length() + IDENTIFIER_REDIRECTION_UPPERCASE_SV.length()).compareTo(IDENTIFIER_REDIRECTION_UPPERCASE_SV) == 0 ||
+                      s.substring(this.textStart + XML_START_TAG_TEXT.length(), this.textStart + XML_START_TAG_TEXT.length() + IDENTIFIER_REDIRECTION_LOWERCASE_SV.length()).compareTo(IDENTIFIER_REDIRECTION_LOWERCASE_SV) == 0 ||
+                      s.substring(this.textStart + XML_START_TAG_TEXT.length(), this.textStart + XML_START_TAG_TEXT.length() + IDENTIFIER_REDIRECTION_CAPITALIZED_SV.length()).compareTo(IDENTIFIER_REDIRECTION_CAPITALIZED_SV) == 0;
+    this.isStub = s.indexOf(IDENTIFIER_STUB_TEMPLATE, this.textStart) != -1 || 
+                  s.indexOf(IDENTIFIER_STUB_WIKIPEDIA_NAMESPACE) != -1;
+  }
+}
Index: src/contrib/wikipedia/src/java/org/apache/hadoop/mapreduce/wikipedia/languages/TurkishWikipediaPage.java
===================================================================
--- src/contrib/wikipedia/src/java/org/apache/hadoop/mapreduce/wikipedia/languages/TurkishWikipediaPage.java	(revision 0)
+++ src/contrib/wikipedia/src/java/org/apache/hadoop/mapreduce/wikipedia/languages/TurkishWikipediaPage.java	(working copy)
@@ -0,0 +1,79 @@
+/*
+ * Cloud9: A MapReduce Library for Hadoop
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License"); you
+ * may not use this file except in compliance with the License. You may
+ * obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
+ * implied. See the License for the specific language governing
+ * permissions and limitations under the License.
+ */
+
+package org.apache.hadoop.mapreduce.wikipedia.language;
+
+import java.util.regex.Matcher;
+import java.util.regex.Pattern;
+
+import org.apache.commons.lang.StringEscapeUtils;
+import org.apache.hadoop.mapreduce.wikipedia.WikipediaPage;
+
+/**
+ * An Turkish page from Wikipedia.
+ * 
+ * @author Ferhan Ture
+ */
+public class TurkishWikipediaPage extends WikipediaPage {
+  /**
+   * Language dependent identifiers of disambiguation, redirection, and stub pages.
+   */
+  private static final String IDENTIFIER_REDIRECTION_UPPERCASE = "#REDIRECT";
+  private static final String IDENTIFIER_REDIRECTION_LOWERCASE = "#redirect";
+  private static final String IDENTIFIER_STUB_TEMPLATE = "stub}}";
+  private static final String IDENTIFIER_STUB_WIKIPEDIA_NAMESPACE = "Wikipedia:Stub";
+  private static final Pattern disambPattern = Pattern.compile("\\{\\{anlam ayr\u0131m\u0131\\}\\}", Pattern.CASE_INSENSITIVE);
+  private static final String LANGUAGE_CODE = "tr";
+
+  /**
+   * Creates an empty <code>TurkishWikipediaPage</code> object.
+   */
+  public TurkishWikipediaPage() {
+    super();
+  }
+
+  @Override
+  protected void processPage(String s) {
+    this.language = LANGUAGE_CODE;
+
+    // parse out title
+    int start = s.indexOf(XML_START_TAG_TITLE);
+    int end = s.indexOf(XML_END_TAG_TITLE, start);
+    this.title = StringEscapeUtils.unescapeHtml(s.substring(start + 7, end));
+
+    // determine if article belongs to the article namespace
+    start = s.indexOf(XML_START_TAG_NAMESPACE);
+    end = s.indexOf(XML_END_TAG_NAMESPACE);
+    this.isArticle = s.substring(start + 4, end).trim().equals("0");
+    
+    // parse out the document id
+    start = s.indexOf(XML_START_TAG_ID);
+    end = s.indexOf(XML_END_TAG_ID);
+    this.mId = s.substring(start + 4, end);
+
+    // parse out actual text of article
+    this.textStart = s.indexOf(XML_START_TAG_TEXT);
+    this.textEnd = s.indexOf(XML_END_TAG_TEXT, this.textStart);
+
+    // determine if article is a disambiguation, redirection, and/or stub page.
+    Matcher matcher = disambPattern.matcher(page);
+    this.isDisambig = matcher.find();
+    this.isRedirect = s.substring(this.textStart + XML_START_TAG_TEXT.length(), this.textStart + XML_START_TAG_TEXT.length() + IDENTIFIER_REDIRECTION_UPPERCASE.length()).compareTo(IDENTIFIER_REDIRECTION_UPPERCASE) == 0 ||
+                      s.substring(this.textStart + XML_START_TAG_TEXT.length(), this.textStart + XML_START_TAG_TEXT.length() + IDENTIFIER_REDIRECTION_LOWERCASE.length()).compareTo(IDENTIFIER_REDIRECTION_LOWERCASE) == 0;
+    this.isStub = s.indexOf(IDENTIFIER_STUB_TEMPLATE, this.textStart) != -1 || 
+                  s.indexOf(IDENTIFIER_STUB_WIKIPEDIA_NAMESPACE) != -1;
+  }
+}
Index: src/contrib/wikipedia/src/java/org/apache/hadoop/mapreduce/wikipedia/languages/WikipediaPageFactory.java
===================================================================
--- src/contrib/wikipedia/src/java/org/apache/hadoop/mapreduce/wikipedia/languages/WikipediaPageFactory.java	(revision 0)
+++ src/contrib/wikipedia/src/java/org/apache/hadoop/mapreduce/wikipedia/languages/WikipediaPageFactory.java	(working copy)
@@ -0,0 +1,73 @@
+/*
+ * Cloud9: A MapReduce Library for Hadoop
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License"); you
+ * may not use this file except in compliance with the License. You may
+ * obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
+ * implied. See the License for the specific language governing
+ * permissions and limitations under the License.
+ */
+
+package org.apache.hadoop.mapreduce.wikipedia.language;
+
+import org.apache.hadoop.mapreduce.wikipedia.WikipediaPage;
+
+/**
+ * Hadoop {@code WikipediaPageFactory} for creating language dependent WikipediaPage Objects.
+ *
+ * @author Peter Exner
+ * @author Ferhan Ture
+ */
+public class WikipediaPageFactory {
+
+  /**
+   * Returns a {@code WikipediaPage} for this {@code language}.
+   */
+  public static WikipediaPage createWikipediaPage(String language) {
+    if (language == null) {
+      return new EnglishWikipediaPage();
+    }
+
+    if (language.equalsIgnoreCase("en")) {
+      return new EnglishWikipediaPage();
+    } else if (language.equalsIgnoreCase("sv")) {
+      return new SwedishWikipediaPage();
+    } else if (language.equalsIgnoreCase("de")) {
+      return new GermanWikipediaPage();
+    } else if (language.equalsIgnoreCase("cs")) {
+      return new CzechWikipediaPage();
+    } else if (language.equalsIgnoreCase("es")) {
+      return new SpanishWikipediaPage();
+    } else if (language.equalsIgnoreCase("ar")) {
+      return new ArabicWikipediaPage();
+    } else if (language.equalsIgnoreCase("tr")) {
+      return new TurkishWikipediaPage();
+    } else if (language.equalsIgnoreCase("zh")) {
+      return new ChineseWikipediaPage();
+    } else {
+      return new EnglishWikipediaPage();
+    }
+  }
+
+  public static Class<? extends WikipediaPage> getWikipediaPageClass(String language) {
+    if (language == null) {
+      return EnglishWikipediaPage.class;
+    }
+
+    if (language.equalsIgnoreCase("en")) {
+      return EnglishWikipediaPage.class;
+    } else if (language.equalsIgnoreCase("sv")) {
+      return SwedishWikipediaPage.class;
+    } else if (language.equalsIgnoreCase("de")) {
+      return GermanWikipediaPage.class;
+    } else {
+      return EnglishWikipediaPage.class;
+    }
+  }
+}
Index: src/core/org/apache/hadoop/io/DataInputBuffer.java
===================================================================
--- src/core/org/apache/hadoop/io/DataInputBuffer.java	(revision 1596572)
+++ src/core/org/apache/hadoop/io/DataInputBuffer.java	(working copy)
@@ -56,6 +56,17 @@
     public int getLength() { return count; }
   }
 
+  // Approximate Hadoop: we should know where the key comes from
+  private String originId = null;
+  
+  public void setOriginId(String originId) {
+    this.originId = originId;
+  }
+  
+  public String getOriginId() {
+    return this.originId;
+  }
+  
   private Buffer buffer;
   
   /** Constructs a new empty buffer. */
Index: src/examples/org/apache/hadoop/examples/ApproximateGrep.java
===================================================================
--- src/examples/org/apache/hadoop/examples/ApproximateGrep.java	(revision 0)
+++ src/examples/org/apache/hadoop/examples/ApproximateGrep.java	(working copy)
@@ -0,0 +1,100 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.examples;
+
+import java.util.Random;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.conf.Configured;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.io.LongWritable;
+import org.apache.hadoop.io.Text;
+import org.apache.hadoop.mapred.*;
+import org.apache.hadoop.mapred.lib.*;
+import org.apache.hadoop.util.Tool;
+import org.apache.hadoop.util.ToolRunner;
+
+/* Extracts matching regexs from input files and counts them. */
+public class ApproximateGrep extends Configured implements Tool {
+  private ApproximateGrep() {}                               // singleton
+
+  public int run(String[] args) throws Exception {
+    if (args.length < 3) {
+      System.out.println("Grep <inDir> <outDir> <regex> [<group>]");
+      ToolRunner.printGenericCommandUsage(System.out);
+      return -1;
+    }
+
+    Path tempDir =
+      new Path("grep-temp-"+
+          Integer.toString(new Random().nextInt(Integer.MAX_VALUE)));
+
+    JobConf grepJob = new JobConf(getConf(), ApproximateGrep.class);
+    
+    try {
+      
+      grepJob.setJobName("grep-search");
+
+      FileInputFormat.setInputPaths(grepJob, args[0]);
+
+      // Approximate Hadoop
+      grepJob.setInputFormat(ApproximateTextInputFormat.class);
+      
+      grepJob.setMapperClass(RegexMapper.class);
+      grepJob.set("mapred.mapper.regex", args[2]);
+      if (args.length == 4)
+        grepJob.set("mapred.mapper.regex.group", args[3]);
+
+      grepJob.setCombinerClass(LongSumReducer.class);
+      grepJob.setReducerClass(LongSumReducer.class);
+
+      FileOutputFormat.setOutputPath(grepJob, tempDir);
+      grepJob.setOutputFormat(SequenceFileOutputFormat.class);
+      grepJob.setOutputKeyClass(Text.class);
+      grepJob.setOutputValueClass(LongWritable.class);
+
+      JobClient.runJob(grepJob);
+
+      JobConf sortJob = new JobConf(getConf(), ApproximateGrep.class);
+      sortJob.setJobName("grep-sort");
+
+      FileInputFormat.setInputPaths(sortJob, tempDir);
+      sortJob.setInputFormat(SequenceFileInputFormat.class);
+
+      sortJob.setMapperClass(InverseMapper.class);
+
+      sortJob.setNumReduceTasks(1);                 // write a single file
+      FileOutputFormat.setOutputPath(sortJob, new Path(args[1]));
+      sortJob.setOutputKeyComparatorClass           // sort by decreasing freq
+      (LongWritable.DecreasingComparator.class);
+
+      JobClient.runJob(sortJob);
+    }
+    finally {
+      FileSystem.get(grepJob).delete(tempDir, true);
+    }
+    return 0;
+  }
+
+  public static void main(String[] args) throws Exception {
+    int res = ToolRunner.run(new Configuration(), new ApproximateGrep(), args);
+    System.exit(res);
+  }
+
+}
Index: src/examples/org/apache/hadoop/examples/ApproximateLogApacheAnalysis.java
===================================================================
--- src/examples/org/apache/hadoop/examples/ApproximateLogApacheAnalysis.java	(revision 0)
+++ src/examples/org/apache/hadoop/examples/ApproximateLogApacheAnalysis.java	(working copy)
@@ -0,0 +1,182 @@
+package org.apache.hadoop.examples;
+
+import java.io.IOException;
+
+import java.util.StringTokenizer;
+import java.util.Date;
+
+import java.text.SimpleDateFormat;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.io.IntWritable;
+import org.apache.hadoop.io.LongWritable;
+import org.apache.hadoop.io.Text;
+import org.apache.hadoop.mapreduce.Job;
+import org.apache.hadoop.mapreduce.Mapper;
+import org.apache.hadoop.mapreduce.Reducer;
+import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
+import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
+import org.apache.hadoop.util.GenericOptionsParser;
+
+// Input format
+import org.apache.hadoop.mapreduce.lib.input.ApproximateTextInputFormat;
+import org.apache.hadoop.mapreduce.lib.reduce.LongSumReducer;
+
+/**
+ * Approximate log analysis count.
+ */
+public class ApproximateLogApacheAnalysis {
+	/**
+	* Map that processes each line.
+	*/
+	public static class ApacheLogMapper extends Mapper<Object, Text, Text, LongWritable> {
+		private final static LongWritable one = new LongWritable(1);
+		private Text word = new Text();
+		private SimpleDateFormat indateformat  = new SimpleDateFormat("dd/MMM/yyyy:HH:mm:ss Z");
+		
+// 		private SimpleDateFormat outdateformat = new SimpleDateFormat("yyyy/MM/dd HH:mm:ss");
+// 		private SimpleDateFormat outdateformat = new SimpleDateFormat("yyyy/MM/dd HH"); // Day/hour
+// 		private SimpleDateFormat outdateformat = new SimpleDateFormat("HH"); // Hour
+// 		private SimpleDateFormat outdateformat = new SimpleDateFormat("EEE"); // Day of the week
+// 		private SimpleDateFormat outdateformat = new SimpleDateFormat("EEE HH"); // Day of the week
+		
+		public void map(Object key, Text value, Context context) throws IOException, InterruptedException {
+			try {
+				// "Inspiration" from: http://www.java2s.com/Code/Java/Regular-Expressions/ParseanApachelogfilewithStringTokenizer.htm
+				StringTokenizer matcher = new StringTokenizer(value.toString());
+				String hostname = matcher.nextToken();
+				matcher.nextToken(); // eat the "-"
+				matcher.nextToken("["); // again
+				String datetime = matcher.nextToken("]").substring(1);
+				matcher.nextToken("\"");
+				String request = matcher.nextToken("\"");
+				matcher.nextToken(" "); // again
+				String response = matcher.nextToken();
+				String byteCount = matcher.nextToken();
+				matcher.nextToken("\"");
+				String referer = matcher.nextToken("\"");
+				matcher.nextToken("\""); // again
+				String userAgent = matcher.nextToken("\"");
+				
+				String factor = context.getConfiguration().get("task");
+				if (factor != null) {
+					// Check who is trying to hack us
+					if (factor.equalsIgnoreCase("hack")) {
+						// "POST /cgi-bin/php-cgi?XXX HTTP/1.1"
+						String[] requestSplit = request.split(" ");
+						if (requestSplit.length > 1) {
+							String address = requestSplit[1];
+							String[] keywords = { "/w00tw00t", "/phpMyAdmin", "/pma", "/myadmin", "/MyAdmin", "/phpTest", "/cgi-bin/php", "/cgi-bin/php5", "/cgi-bin/php-cgi" };
+							boolean found = false;
+							for (String keyword : keywords) {
+								if (address.startsWith(keyword)) {
+									found = true;
+									break;
+								}
+							}
+							// We note that that user is pushing us
+							if (found) {
+								word.set(hostname);
+								context.write(word, one);
+							}
+						}
+					// Hosts visiting the web
+					} else if (factor.equalsIgnoreCase("host")) {
+						word.set(hostname);
+						context.write(word, one);
+					// Analyze the access time of the visits
+					// 24/Nov/2013:06:25:45 -0500 -> Date
+					} else if (factor.equalsIgnoreCase("dateweek")) {
+						Date date = indateformat.parse(datetime);
+						SimpleDateFormat outdateformat = new SimpleDateFormat("EEE HH"); // Day of the week
+						word.set(outdateformat.format(date));
+						context.write(word, one);
+					// Size per object
+					} else if (factor.equalsIgnoreCase("size")) {
+						long bytes = (Long.parseLong(byteCount)/100)*100; // Round for histogram
+						word.set(Long.toString(bytes));
+						context.write(word, one);
+					// Total size per object
+					} else if (factor.equalsIgnoreCase("totalsize")) {
+						word.set("Total");
+						context.write(word, new LongWritable(Long.parseLong(byteCount)));
+					// Page traffic
+					} else if (factor.equalsIgnoreCase("pagesize")) {
+						String aux = request.substring(0, request.indexOf("?"));
+						word.set(aux);
+						context.write(word, new LongWritable(Long.parseLong(byteCount)));
+					// Page visit
+					} else if (factor.equalsIgnoreCase("page")) {
+						String aux = request.substring(0, request.indexOf("?"));
+						word.set(aux);
+						context.write(word, one);
+					// Default
+					} else {
+						System.err.println("Unknown option:" + factor);
+					}
+				}
+			} catch (Exception ee) {
+				// ee.printStackTrace();
+				System.err.println(ee);
+			}
+		}
+	}
+	
+	/**
+	* Reduce that takes into account the approximation factor.
+	*/
+	public static class LongApproxSumReducer extends Reducer<Text,LongWritable,Text,LongWritable> {
+		private LongWritable result = new LongWritable();
+
+		public void reduce(Text key, Iterable<LongWritable> values, Context context) throws IOException, InterruptedException {
+			long sum = 0;
+			for (LongWritable val : values) {
+				sum += val.get();
+			}
+			// Get the approximation factor
+			int factor = context.getConfiguration().getInt("mapred.input.approximate.skip", 1);
+			/*int factor = 1;
+			if (sum > 1) { // If it is one, then this is zipfian
+				factor = context.getConfiguration().getInt("mapred.input.approximate.skip", 1);
+				if (factor < 1) {
+					factor = 1;
+				}
+			}*/
+			// Correct factor
+			result.set(sum*factor);
+			context.write(key, result);
+		}
+	}
+	
+	/**
+	 * Launch apache log analysis.
+	 */
+	public static void main(String[] args) throws Exception {
+		Configuration conf = new Configuration();
+		String[] otherArgs = new GenericOptionsParser(conf, args).getRemainingArgs();
+		if (otherArgs.length != 2) {
+			System.err.println("Usage: approximatelogapache <in> <out>");
+			System.exit(2);
+		}
+		Job job = new Job(conf, "Apache log analysis");
+		
+		job.setJarByClass(ApproximateLogApacheAnalysis.class);
+		
+		job.setMapperClass(ApacheLogMapper.class);
+		job.setCombinerClass(LongSumReducer.class);
+		job.setReducerClass(LongApproxSumReducer.class);
+		
+		job.setOutputKeyClass(Text.class);
+		job.setOutputValueClass(LongWritable.class);
+		
+		// Approximate Hadoop
+		job.setInputFormatClass(ApproximateTextInputFormat.class);
+		//job.setInputFormatClass(TextInputFormat.class);
+		
+		FileInputFormat.addInputPath(job, new Path(otherArgs[0]));
+		FileOutputFormat.setOutputPath(job, new Path(otherArgs[1]));
+		
+		System.exit(job.waitForCompletion(true) ? 0 : 1);
+	}
+}
Index: src/examples/org/apache/hadoop/examples/ApproximateLogWirelessAnalysis.java
===================================================================
--- src/examples/org/apache/hadoop/examples/ApproximateLogWirelessAnalysis.java	(revision 0)
+++ src/examples/org/apache/hadoop/examples/ApproximateLogWirelessAnalysis.java	(working copy)
@@ -0,0 +1,134 @@
+package org.apache.hadoop.examples;
+
+import java.io.IOException;
+import java.util.StringTokenizer;
+import java.util.Date;
+import java.text.SimpleDateFormat;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.io.IntWritable;
+import org.apache.hadoop.io.Text;
+import org.apache.hadoop.mapreduce.Job;
+import org.apache.hadoop.mapreduce.Mapper;
+import org.apache.hadoop.mapreduce.Reducer;
+import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
+import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
+import org.apache.hadoop.util.GenericOptionsParser;
+
+// Input format
+import org.apache.hadoop.mapreduce.lib.input.ApproximateTextInputFormat;
+
+import org.apache.hadoop.mapreduce.lib.reduce.IntSumReducer;
+
+/**
+ * Approximate log analysis count.
+ */
+public class ApproximateLogWirelessAnalysis {
+	/**
+	* Map.
+	*/
+	public static class LogMapper extends Mapper<Object, Text, Text, IntWritable> {
+		private final static IntWritable one = new IntWritable(1);
+		private Text word = new Text();
+// 		private SimpleDateFormat indateformat  = new SimpleDateFormat("dd/MMM/yyyy:HH:mm:ss Z");
+// 		private SimpleDateFormat outdateformat = new SimpleDateFormat("yyyy/MM/dd HH:mm:ss");
+// 		private SimpleDateFormat outdateformat = new SimpleDateFormat("yyyy/MM/dd HH"); // Day/hour
+// 		private SimpleDateFormat outdateformat = new SimpleDateFormat("HH"); // Hour
+// 		private SimpleDateFormat outdateformat = new SimpleDateFormat("EEE"); // Day of the week
+// 		private SimpleDateFormat outdateformat = new SimpleDateFormat("EEE HH"); // Day of the week
+		
+		public void map(Object key, Text value, Context context) throws IOException, InterruptedException {
+			// "Offset (ms)","Receiver Timestamp (ms)","Physical Layer","Device ID","Receiver ID","RSSI","Received Data"
+			StringTokenizer matcher = new StringTokenizer(value.toString());
+			try {
+				int offset = Integer.parseInt(matcher.nextToken(","));
+				// Date time  = new Date(Long.parseLong(matcher.nextToken(",")));
+				String time  = matcher.nextToken(",");
+				String layer = matcher.nextToken(",");
+				String sndId = matcher.nextToken(",");
+				String rcvId = matcher.nextToken(",");
+				//String rssi =  matcher.nextToken(",");
+				//String data =  matcher.nextToken(","); // .length()
+				
+				// Analyze the senders
+				/*try {
+					word.set(sndId);
+					context.write(word, one);
+				} catch (Exception e) {}
+				*/
+				
+				// Analyze the receivers
+				/*
+				try {
+					word.set(rcvId);
+					context.write(word, one);
+				} catch (Exception e) {}
+				*/
+				
+				// Analyze senders and receivers
+				/*try {
+					word.set(rcvId);
+					context.write(word, one);
+					word.set(sndId);
+					context.write(word, one);
+				} catch (Exception e) {}
+				*/
+				
+				// Analyze communications
+				try {
+					if (sndId.compareTo(rcvId) > 0) {
+						word.set(sndId+"->"+rcvId);
+					} else {
+						word.set(rcvId+"->"+sndId);
+					}
+					context.write(word, one);
+				} catch (Exception e) {}
+			} catch (Exception ee) {
+				System.err.println("Cannot parse line \"" + value.toString() + "\": " + ee.getMessage() + " -> " + ee.toString() + " -> " + ee.getCause());
+			}
+		}
+	}
+	
+	/**
+	* Reduce. It takes into account the approximation factor.
+	*/
+	public static class IntApproxSumReducer extends Reducer<Text,IntWritable,Text,IntWritable> {
+		private IntWritable result = new IntWritable();
+
+		public void reduce(Text key, Iterable<IntWritable> values, Context context) throws IOException, InterruptedException {
+			int sum = 0;
+			for (IntWritable val : values) {
+				sum += val.get();
+			}
+			// Get the approximation factor
+			int factor = context.getConfiguration().getInt("mapred.input.approximate.skip", 1);
+			if (factor < 1) factor = 1;
+			// Correct factor
+			result.set(sum*factor);
+			context.write(key, result);
+		}
+	}
+	
+	public static void main(String[] args) throws Exception {
+		Configuration conf = new Configuration();
+		String[] otherArgs = new GenericOptionsParser(conf, args).getRemainingArgs();
+		if (otherArgs.length != 2) {
+			System.err.println("Usage: approximatewireless <in> <out>");
+			System.exit(2);
+		}
+		Job job = new Job(conf, "log analysis");
+		job.setJarByClass(ApproximateLogWirelessAnalysis.class);
+		job.setMapperClass(LogMapper.class);
+		job.setCombinerClass(IntSumReducer.class);
+		job.setReducerClass(IntApproxSumReducer.class);
+		job.setOutputKeyClass(Text.class);
+		job.setOutputValueClass(IntWritable.class);
+		// Approximate Hadoop
+		job.setInputFormatClass(ApproximateTextInputFormat.class);
+		//job.setInputFormatClass(TextInputFormat.class);
+		FileInputFormat.addInputPath(job, new Path(otherArgs[0]));
+		FileOutputFormat.setOutputPath(job, new Path(otherArgs[1]));
+		System.exit(job.waitForCompletion(true) ? 0 : 1);
+	}
+}
Index: src/examples/org/apache/hadoop/examples/ApproximateSleepJob.java
===================================================================
--- src/examples/org/apache/hadoop/examples/ApproximateSleepJob.java	(revision 0)
+++ src/examples/org/apache/hadoop/examples/ApproximateSleepJob.java	(working copy)
@@ -0,0 +1,262 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.examples;
+
+import java.io.IOException;
+import java.io.DataInput;
+import java.io.DataOutput;
+import java.util.Iterator;
+import java.util.Random;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.conf.Configured;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.io.IntWritable;
+import org.apache.hadoop.io.NullWritable;
+import org.apache.hadoop.io.SequenceFile;
+import org.apache.hadoop.mapred.*;
+import org.apache.hadoop.mapred.lib.NullOutputFormat;
+import org.apache.hadoop.util.Tool;
+import org.apache.hadoop.util.ToolRunner;
+
+/**
+ * Dummy class for testing MR framefork. Sleeps for a defined period 
+ * of time in mapper and reducer. Generates fake input for map / reduce 
+ * jobs. Note that generated number of input pairs is in the order 
+ * of <code>numMappers * mapSleepTime / 100</code>, so the job uses
+ * some disk space.
+ */
+public class ApproximateSleepJob extends Configured implements Tool, 
+             ApproximateMapper<IntWritable, IntWritable, IntWritable, NullWritable>,
+             ApproximateReducer<IntWritable, NullWritable, NullWritable, NullWritable>,
+             Partitioner<IntWritable,NullWritable> {
+
+  private long mapSleepDuration = 100;
+  private long reduceSleepDuration = 100;
+  private int mapSleepCount = 1;
+  private int reduceSleepCount = 1;
+  private int count = 0;
+
+  public int getPartition(IntWritable k, NullWritable v, int numPartitions) {
+    return k.get() % numPartitions;
+  }
+  
+  public static class EmptySplit implements InputSplit {
+    public void write(DataOutput out) throws IOException { }
+    public void readFields(DataInput in) throws IOException { }
+    public long getLength() { return 0L; }
+    public String[] getLocations() { return new String[0]; }
+  }
+
+  public static class SleepInputFormat extends Configured
+      implements InputFormat<IntWritable,IntWritable> {
+    public InputSplit[] getSplits(JobConf conf, int numSplits) {
+      InputSplit[] ret = new InputSplit[numSplits];
+      for (int i = 0; i < numSplits; ++i) {
+        ret[i] = new EmptySplit();
+      }
+      return ret;
+    }
+    public RecordReader<IntWritable,IntWritable> getRecordReader(
+        InputSplit ignored, JobConf conf, Reporter reporter)
+        throws IOException {
+      final int count = conf.getInt("sleep.job.map.sleep.count", 1);
+      if (count < 0) throw new IOException("Invalid map count: " + count);
+      final int redcount = conf.getInt("sleep.job.reduce.sleep.count", 1);
+      if (redcount < 0)
+        throw new IOException("Invalid reduce count: " + redcount);
+      final int emitPerMapTask = (redcount * conf.getNumReduceTasks());
+    return new RecordReader<IntWritable,IntWritable>() {
+        private int records = 0;
+        private int emitCount = 0;
+
+        public boolean next(IntWritable key, IntWritable value)
+            throws IOException {
+          key.set(emitCount);
+          int emit = emitPerMapTask / count;
+          if ((emitPerMapTask) % count > records) {
+            ++emit;
+          }
+          emitCount += emit;
+          value.set(emit);
+          return records++ < count;
+        }
+        public IntWritable createKey() { return new IntWritable(); }
+        public IntWritable createValue() { return new IntWritable(); }
+        public long getPos() throws IOException { return records; }
+        public void close() throws IOException { }
+        public float getProgress() throws IOException {
+          return records / ((float)count);
+        }
+      };
+    }
+  }
+
+  public void map(IntWritable key, IntWritable value, OutputCollector<IntWritable, NullWritable> output, Reporter reporter) throws IOException {
+    //it is expected that every map processes mapSleepCount number of records. 
+    try {
+      reporter.setStatus("Precise sleeping... (" + (mapSleepDuration * (mapSleepCount - count)) + ") ms left");
+      Thread.sleep(mapSleepDuration);
+    }
+    catch (InterruptedException ex) {
+      throw (IOException)new IOException("Interrupted while sleeping").initCause(ex);
+    }
+    ++count;
+    // output reduceSleepCount * numReduce number of random values, so that
+    // each reducer will get reduceSleepCount number of keys.
+    int k = key.get();
+    for (int i = 0; i < value.get(); ++i) {
+      output.collect(new IntWritable(k + i), NullWritable.get());
+    }
+  }
+
+  public void mapApproximate(IntWritable key, IntWritable value, OutputCollector<IntWritable, NullWritable> output, Reporter reporter) throws IOException {
+    //it is expected that every map processes mapSleepCount number of records. 
+    try {
+      reporter.setStatus("Approximate sleeping... (" + (mapSleepDuration/5.0 * (mapSleepCount - count)) + ") ms left");
+      Thread.sleep((int) (mapSleepDuration/5.0));
+    }
+    catch (InterruptedException ex) {
+      throw (IOException)new IOException("Interrupted while sleeping").initCause(ex);
+    }
+    ++count;
+    // output reduceSleepCount * numReduce number of random values, so that
+    // each reducer will get reduceSleepCount number of keys.
+    int k = key.get();
+    for (int i = 0; i < value.get(); ++i) {
+      output.collect(new IntWritable(k + i), NullWritable.get());
+    }
+  }
+
+  public void reduce(IntWritable key, Iterator<NullWritable> values, OutputCollector<NullWritable, NullWritable> output, Reporter reporter) throws IOException {
+    try {
+      reporter.setStatus("Sleeping... (" + (reduceSleepDuration * (reduceSleepCount - count)) + ") ms left");
+      Thread.sleep(reduceSleepDuration);
+    }
+    catch (InterruptedException ex) {
+      throw (IOException)new IOException("Interrupted while sleeping").initCause(ex);
+    }
+    count++;
+  }
+
+  public void reduceApproximate(IntWritable key, Iterator<NullWritable> values, OutputCollector<NullWritable, NullWritable> output, Reporter reporter) throws IOException {
+    try {
+      reporter.setStatus("Approximate sleeping... (" + (reduceSleepDuration/5.0 * (reduceSleepCount - count)) + ") ms left");
+      Thread.sleep((int) (reduceSleepDuration/5.0));
+    }
+    catch (InterruptedException ex) {
+      throw (IOException)new IOException("Interrupted while sleeping").initCause(ex);
+    }
+    count++;
+  }
+
+  public void configure(JobConf job) {
+    this.mapSleepCount =
+      job.getInt("sleep.job.map.sleep.count", mapSleepCount);
+    this.reduceSleepCount =
+      job.getInt("sleep.job.reduce.sleep.count", reduceSleepCount);
+    this.mapSleepDuration =
+      job.getLong("sleep.job.map.sleep.time" , 100) / mapSleepCount;
+    this.reduceSleepDuration =
+      job.getLong("sleep.job.reduce.sleep.time" , 100) / reduceSleepCount;
+  }
+
+  public void close() throws IOException {
+  }
+
+  public static void main(String[] args) throws Exception{
+    int res = ToolRunner.run(new Configuration(), new ApproximateSleepJob(), args);
+    System.exit(res);
+  }
+
+  public int run(int numMapper, int numReducer, long mapSleepTime,
+      int mapSleepCount, long reduceSleepTime,
+      int reduceSleepCount) throws IOException {
+    JobConf job = setupJobConf(numMapper, numReducer, mapSleepTime, 
+                  mapSleepCount, reduceSleepTime, reduceSleepCount);
+    JobClient.runJob(job);
+    return 0;
+  }
+
+  public JobConf setupJobConf(int numMapper, int numReducer, 
+                                long mapSleepTime, int mapSleepCount, 
+                                long reduceSleepTime, int reduceSleepCount) {
+    JobConf job = new JobConf(getConf(), ApproximateSleepJob.class);
+    job.setNumMapTasks(numMapper);
+    job.setNumReduceTasks(numReducer);
+    job.setMapperClass(ApproximateSleepJob.class);
+    job.setMapOutputKeyClass(IntWritable.class);
+    job.setMapOutputValueClass(NullWritable.class);
+    job.setReducerClass(ApproximateSleepJob.class);
+    job.setOutputFormat(NullOutputFormat.class);
+    job.setInputFormat(SleepInputFormat.class);
+    job.setPartitionerClass(ApproximateSleepJob.class);
+    //job.setSpeculativeExecution(false);
+    job.setJobName("Sleep job");
+    FileInputFormat.addInputPath(job, new Path("ignored"));
+    job.setLong("sleep.job.map.sleep.time", mapSleepTime);
+    job.setLong("sleep.job.reduce.sleep.time", reduceSleepTime);
+    job.setInt("sleep.job.map.sleep.count", mapSleepCount);
+    job.setInt("sleep.job.reduce.sleep.count", reduceSleepCount);
+    // Approximate Hadoop extension: approximation level
+    //job.setFloat("mapred.map.approximate", 0.8);
+    return job;
+  }
+
+  public int run(String[] args) throws Exception {
+
+    if(args.length < 1) {
+      System.err.println("ApproximateSleepJob [-m numMapper] [-r numReducer]" +
+          " [-mt mapSleepTime (msec)] [-rt reduceSleepTime (msec)]" +
+          " [-recordt recordSleepTime (msec)]");
+      ToolRunner.printGenericCommandUsage(System.err);
+      return -1;
+    }
+
+    int numMapper = 1, numReducer = 1;
+    long mapSleepTime = 100, reduceSleepTime = 100, recSleepTime = 100;
+    int mapSleepCount = 1, reduceSleepCount = 1;
+
+    for(int i=0; i < args.length; i++ ) {
+      if(args[i].equals("-m")) {
+        numMapper = Integer.parseInt(args[++i]);
+      }
+      else if(args[i].equals("-r")) {
+        numReducer = Integer.parseInt(args[++i]);
+      }
+      else if(args[i].equals("-mt")) {
+        mapSleepTime = Long.parseLong(args[++i]);
+      }
+      else if(args[i].equals("-rt")) {
+        reduceSleepTime = Long.parseLong(args[++i]);
+      }
+      else if (args[i].equals("-recordt")) {
+        recSleepTime = Long.parseLong(args[++i]);
+      }
+    }
+    
+    // sleep for *SleepTime duration in Task by recSleepTime per record
+    mapSleepCount = (int)Math.ceil(mapSleepTime / ((double)recSleepTime));
+    reduceSleepCount = (int)Math.ceil(reduceSleepTime / ((double)recSleepTime));
+    
+    return run(numMapper, numReducer, mapSleepTime, mapSleepCount,
+        reduceSleepTime, reduceSleepCount);
+  }
+
+}
Index: src/examples/org/apache/hadoop/examples/ApproximateWordCount.java
===================================================================
--- src/examples/org/apache/hadoop/examples/ApproximateWordCount.java	(revision 0)
+++ src/examples/org/apache/hadoop/examples/ApproximateWordCount.java	(working copy)
@@ -0,0 +1,170 @@
+package org.apache.hadoop.examples;
+
+import java.io.IOException;
+import java.util.StringTokenizer;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.io.IntWritable;
+import org.apache.hadoop.io.Text;
+import org.apache.hadoop.mapreduce.Job;
+import org.apache.hadoop.mapreduce.ApproximateMapper;
+import org.apache.hadoop.mapreduce.ApproximateReducer;
+import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
+import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
+import org.apache.hadoop.util.GenericOptionsParser;
+
+// Input format
+import org.apache.hadoop.mapreduce.lib.input.ApproximateTextInputFormat;
+/*import org.apache.hadoop.io.LongWritable;
+import org.apache.hadoop.io.Text;
+import org.apache.hadoop.mapred.TextInputFormat;
+import org.apache.hadoop.mapred.RecordReader;
+import org.apache.hadoop.mapred.LineRecordReader;
+import org.apache.hadoop.mapred.Reporter;
+import org.apache.hadoop.mapred.JobConf;
+import org.apache.hadoop.mapred.FileSplit;
+import org.apache.hadoop.mapred.InputSplit;
+import org.apache.hadoop.mapreduce.InputFormat;*/
+
+/**
+ * Approximate word count.
+ */
+public class ApproximateWordCount {
+  /*public class ApproximateLineRecordReader extends LineRecordReader {
+    private int lineNumber = 0;
+    public ApproximateLineRecordReader(Configuration job, FileSplit split) throws IOException {
+      super(job, split);
+    }
+    public synchronized boolean next(LongWritable key, Text value) throws IOException {
+      // We always read one extra line, which lies outside the upper
+      // split limit i.e. (end - 1)
+      while (getFilePosition() <= end) {
+        // Check if it is a line we should skip
+        lineNumber++;
+        if ((lineNumber-1)%2==1) {
+        System.out.println("Skip a line");
+          continue;
+        }
+        
+        key.set(pos);
+        int newSize = in.readLine(value, maxLineLength, Math.max(maxBytesToConsume(pos), maxLineLength));
+        if (newSize == 0) {
+          return false;
+        }
+        pos += newSize;
+        if (newSize < maxLineLength) {
+          return true;
+        }
+      }
+      return false;
+    }
+  }*/
+  
+    /*public boolean nextKeyValue() throws IOException {
+    if (key == null) {
+      key = new LongWritable();
+    }
+    key.set(pos);
+    if (value == null) {
+      value = new Text();
+    }
+    int newSize = 0;
+    // We always read one extra line, which lies outside the upper
+    // split limit i.e. (end - 1)
+    while (getFilePosition() <= end) {
+      newSize = in.readLine(value, maxLineLength,
+          Math.max(maxBytesToConsume(pos), maxLineLength));
+      if (newSize == 0) {
+        break;
+      }
+      pos += newSize;
+      if (newSize < maxLineLength) {
+        break;
+      }
+
+      // line too long. try again
+      LOG.info("Skipped line of size " + newSize + " at pos " + 
+               (pos - newSize));
+    }
+    if (newSize == 0) {
+      key = null;
+      value = null;
+      return false;
+    } else {
+      return true;
+    }
+  }*/
+  
+  
+  /**
+   * Map.
+   */
+  public static class TokenizerMapper extends ApproximateMapper<Object, Text, Text, IntWritable>{
+    private final static IntWritable one = new IntWritable(1);
+    private Text word = new Text();
+      
+    public void map(Object key, Text value, Context context) throws IOException, InterruptedException {
+      StringTokenizer itr = new StringTokenizer(value.toString());
+      while (itr.hasMoreTokens()) {
+        word.set(itr.nextToken());
+        context.write(word, one);
+      }
+    }
+    
+    public void mapApproximate(Object key, Text value, Context context) throws IOException, InterruptedException {
+      StringTokenizer itr = new StringTokenizer(value.toString());
+      while (itr.hasMoreTokens()) {
+        word.set(itr.nextToken());
+        context.write(word, one);
+      }
+    }
+  }
+  
+  /**
+   * Reduce.
+   */
+  public static class IntSumReducer extends ApproximateReducer<Text,IntWritable,Text,IntWritable> {
+    private IntWritable result = new IntWritable();
+
+    public void reduce(Text key, Iterable<IntWritable> values, Context context) throws IOException, InterruptedException {
+      int sum = 0;
+      for (IntWritable val : values) {
+        sum += val.get();
+      }
+      result.set(sum);
+      context.write(key, result);
+    }
+    
+    public void reduceApproximate(Text key, Iterable<IntWritable> values, Context context) throws IOException, InterruptedException {
+      int sum = 0;
+      for (IntWritable val : values) {
+        sum += val.get();
+      }
+      result.set(sum);
+      context.write(key, result);
+    }
+  }
+
+  public static void main(String[] args) throws Exception {
+    Configuration conf = new Configuration();
+    String[] otherArgs = new GenericOptionsParser(conf, args).getRemainingArgs();
+    if (otherArgs.length != 2) {
+      System.err.println("Usage: wordcount <in> <out>");
+      System.exit(2);
+    }
+    Job job = new Job(conf, "word count");
+    job.setJarByClass(WordCount.class);
+    job.setMapperClass(TokenizerMapper.class);
+    job.setCombinerClass(IntSumReducer.class);
+    job.setReducerClass(IntSumReducer.class);
+    job.setOutputKeyClass(Text.class);
+    job.setOutputValueClass(IntWritable.class);
+    // Approximate Hadoop
+    job.setInputFormatClass(ApproximateTextInputFormat.class);
+    //job.setInputFormatClass(TextInputFormat.class);
+    FileInputFormat.addInputPath(job, new Path(otherArgs[0]));
+    FileOutputFormat.setOutputPath(job, new Path(otherArgs[1]));
+    System.exit(job.waitForCompletion(true) ? 0 : 1);
+  }
+}
Index: src/examples/org/apache/hadoop/examples/ExampleDriver.java
===================================================================
--- src/examples/org/apache/hadoop/examples/ExampleDriver.java	(revision 1596572)
+++ src/examples/org/apache/hadoop/examples/ExampleDriver.java	(working copy)
@@ -35,8 +35,9 @@
     int exitCode = -1;
     ProgramDriver pgd = new ProgramDriver();
     try {
-      pgd.addClass("wordcount", WordCount.class, 
-                   "A map/reduce program that counts the words in the input files.");
+      pgd.addClass("wordcount", WordCount.class, "A map/reduce program that counts the words in the input files.");
+      pgd.addClass("wordcountincr", WordCountIncremental.class, "A map/reduce program that counts the words in the input files.");
+      pgd.addClass("approximatewordcount", ApproximateWordCount.class, "A map/reduce program that counts the words in the input files.");
       pgd.addClass("aggregatewordcount", AggregateWordCount.class, 
                    "An Aggregate based map/reduce program that counts the words in the input files.");
       pgd.addClass("aggregatewordhist", AggregateWordHistogram.class, 
@@ -43,6 +44,8 @@
                    "An Aggregate based map/reduce program that computes the histogram of the words in the input files.");
       pgd.addClass("grep", Grep.class, 
                    "A map/reduce program that counts the matches of a regex in the input.");
+      pgd.addClass("approximategrep", ApproximateGrep.class, 
+                   "A map/reduce program that counts the matches of a regex in the input.");
       pgd.addClass("randomwriter", RandomWriter.class, 
                    "A map/reduce program that writes 10GB of random data per node.");
       pgd.addClass("randomtextwriter", RandomTextWriter.class, 
@@ -55,6 +58,7 @@
                    "An example defining a secondary sort to the reduce.");
       pgd.addClass("sudoku", Sudoku.class, "A sudoku solver.");
       pgd.addClass("sleep", SleepJob.class, "A job that sleeps at each map and reduce task.");
+      pgd.addClass("approximatesleep", ApproximateSleepJob.class, "A job that sleeps at each map and reduce task.");
       pgd.addClass("join", Join.class, "A job that effects a join over sorted, equally partitioned datasets");
       pgd.addClass("multifilewc", MultiFileWordCount.class, "A job that counts words from several files.");
       pgd.addClass("dbcount", DBCountPageView.class, "An example job that count the pageview counts from a database.");
@@ -61,6 +65,10 @@
       pgd.addClass("teragen", TeraGen.class, "Generate data for the terasort");
       pgd.addClass("terasort", TeraSort.class, "Run the terasort");
       pgd.addClass("teravalidate", TeraValidate.class, "Checking results of terasort");
+      
+      pgd.addClass("approximatelogapache", ApproximateLogApacheAnalysis.class, "A map/reduce program to analyze Apache logs.");
+      pgd.addClass("approximatelogwireless", ApproximateLogWirelessAnalysis.class, "A map/reduce program to analyze Wireless logs.");
+      
       pgd.driver(argv);
       
       // Success
Index: src/examples/org/apache/hadoop/examples/WordCountIncremental.java
===================================================================
--- src/examples/org/apache/hadoop/examples/WordCountIncremental.java	(revision 0)
+++ src/examples/org/apache/hadoop/examples/WordCountIncremental.java	(working copy)
@@ -0,0 +1,305 @@
+package org.apache.hadoop.examples;
+
+import java.io.IOException;
+import java.util.StringTokenizer;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.io.IntWritable;
+import org.apache.hadoop.io.Text;
+import org.apache.hadoop.mapreduce.Job;
+import org.apache.hadoop.mapreduce.Mapper;
+import org.apache.hadoop.mapreduce.Reducer;
+import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
+import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
+import org.apache.hadoop.util.GenericOptionsParser;
+
+// Incremental
+import java.util.TreeMap;
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.Vector;
+
+import java.io.File;
+import java.io.FileOutputStream;
+import java.io.FileInputStream;
+import java.io.ObjectOutputStream;
+import java.io.ObjectInputStream;
+import java.io.EOFException;
+import java.io.FileNotFoundException;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+
+/**
+ * Based on Abhishek Verma code.
+ */
+public class WordCountIncremental {
+	// Incremental
+	private static final Log LOG = LogFactory.getLog(WordCountIncremental.class.getName());
+
+	public static class TokenizerMapper extends Mapper<Object, Text, Text, IntWritable> {
+		private final static IntWritable one = new IntWritable(1);
+		private Text word = new Text();
+		
+		public void map(Object key, Text value, Context context) throws IOException, InterruptedException {
+			StringTokenizer itr = new StringTokenizer(value.toString());
+			while (itr.hasMoreTokens()) {
+				word.set(itr.nextToken());
+				context.write(word, one);
+			}
+		}
+		
+		/**
+		 * Incremental.
+		 */
+		public void run(Context context) throws IOException, InterruptedException {
+			setup(context);
+			while (context.nextKeyValue()) {
+				map(context.getCurrentKey(), context.getCurrentValue(), context);
+			}
+			cleanup(context);
+		}
+	}
+  
+	public static class IntSumReducer extends Reducer<Text,IntWritable,Text,IntWritable> {
+		private IntWritable result = new IntWritable();
+		
+		// Incremental (I have the feeling that most of this could go to the reducer part)
+		TreeMap<Text, IntWritable> hm = new TreeMap<Text, IntWritable>();	
+		Vector<File> flist = new Vector<File>();
+		int mcount = 0;
+		int cname = 0;
+		boolean spilled = false;
+		int M_THRESH = 0;
+		
+		/**
+		 * Incremental
+		 */
+		protected void setup(Context context) {
+			try{
+				M_THRESH = Integer.parseInt(context.getConfiguration().get("mem.m_thresh"));
+				LOG.info("INCRED: m_thresh set to " + M_THRESH);
+			} catch(Exception e) {
+				M_THRESH = 0;
+				LOG.info("INCRED: running without m_thresh");            
+			}
+		}
+		
+		/**
+		 * Incremental
+		 */
+		public void spill() throws IOException{
+			spilled = true;
+			// Opens a new file, spills hm to disk, and appends file handler to flist
+			LOG.info("INCRED: spilling to " + cname);
+			try{
+				File newf = new File("wcInc" + cname++ + ".tmp");
+				FileOutputStream of = new FileOutputStream(newf);
+				ObjectOutputStream os = new ObjectOutputStream(of);
+				for (Object key: hm.keySet()) {
+					Text curKey = (Text)key;
+					IntWritable curVal = ((IntWritable)hm.get(key));
+					curKey.write(os);
+					curVal.write(os);
+				}
+				os.close();
+				of.close();
+				flist.add(newf);
+			} catch (FileNotFoundException e) {
+				LOG.info("INCRED: file not found");
+			}
+		}
+		
+		/**
+		 * With incremental, the reduce now actually sums everything in a tree before writing
+		 */
+		public void reduce(Text key, Iterable<IntWritable> values, Context context) throws IOException, InterruptedException {
+			int sum = 0;
+			for (IntWritable val : values) {
+				sum += val.get();
+			}
+			// Incremental replaces this by the tree
+			// result.set(sum);
+			// context.write(key, result);
+			hm.put(new Text(key), new IntWritable(sum));
+		}
+		
+		/**
+		 * Incremental merge after reduce
+		 */
+		public void merge(Context context, Text key, Vector<IntWritable> vals) throws IOException, InterruptedException {
+			int sum = 0;
+			for (IntWritable val: vals) {
+				sum += val.get();
+			}
+			// This time we actually merge
+			result.set(sum);
+			context.write(key, result);
+		}
+
+		/**
+		 * Incremental run
+		 */
+		public void run(Context context) throws IOException, InterruptedException {
+			setup(context);
+			while (context.nextKey()) {
+				Text key = context.getCurrentKey();
+				if (!hm.containsKey(key)) {
+					hm.put(new Text(key), new IntWritable(0));
+					mcount += key.getLength() + 12;
+				}
+				reduce(key, context.getValues(), context);
+				if (M_THRESH != 0 && mcount > M_THRESH) {
+					// spill to disk
+					spill();
+					mcount = 0;
+					hm.clear();	
+				}
+			}
+			
+			if (spilled) {
+				// make sure to spill anything still in memory
+				spill();
+				hm.clear();
+			
+				// need to perform merge
+				LOG.info("INCRED: merge");
+			
+				// oss contains all the objectinput streams
+				Vector<ObjectInputStream> oss = new Vector<ObjectInputStream>();
+			
+				// bMap contains all the values for keys seen so far
+				HashMap<Text, Vector<IntWritable>> bMap = new HashMap<Text, Vector<IntWritable>>();
+			
+				// keys is a sorted map of all keys, and their corresponding files that are known to hold it
+				// (files are stored as integer index into oss)
+				TreeMap<Text, HashSet<Integer>> keys = new TreeMap<Text, HashSet<Integer>>();
+					
+				// create objectinputstreams
+				for (File f : flist) {
+					FileInputStream iff = new FileInputStream(f);
+					oss.add(new ObjectInputStream(iff));
+				}
+
+				// get first key/values from each files
+				int i=0;
+				for (i=0; i<oss.size(); i++) {
+					ObjectInputStream curOS = oss.get(i);
+					Text curKey = new Text();
+					IntWritable curVal = new IntWritable();
+					curKey.readFields(curOS);
+					curVal.readFields(curOS);
+					if (keys.containsKey(curKey)) {
+						keys.get(curKey).add((Integer)i);
+					} else {
+						keys.put(curKey, new HashSet<Integer>());
+						keys.get(curKey).add((Integer)i);
+					}
+					if (bMap.containsKey(curKey)) {
+						bMap.get(curKey).add(curVal);
+					} else {
+						bMap.put(curKey, new Vector<IntWritable>());
+						bMap.get(curKey).add(curVal);
+					}
+				}
+
+				Boolean changed = false;
+				int done = oss.size(); // keep track of all files that we finish going through
+				while (done > 0) {
+					// get lowest key
+					Text curKey = keys.keySet().iterator().next();
+			
+					// iterate through all of the OSS's that contain this key
+					HashSet<Integer> ossInds = keys.get(curKey);
+					for (Integer I : ossInds) {
+						changed = false;
+						i = I.intValue();
+						while (!changed) {
+							ObjectInputStream curOS = oss.get(i);
+							Text newKey = new Text();
+							IntWritable newVal = new IntWritable();
+							try {                                
+								newKey.readFields(curOS);
+								newVal.readFields(curOS);
+								// LOG.info("INCRED: found k/v " + newKey.toString() + '/' + newVal.get());
+							} catch (EOFException e) {
+								// LOG.info("Reached EOF for " + i);
+								done--;
+								break;
+							}
+							if (newKey == curKey) {
+								// add values to hashmap's entry for this key
+								bMap.get(curKey).add(newVal);   
+							} else {
+								if (keys.containsKey(newKey)) {
+									// we've seen this key, add index for this OSS
+									keys.get(newKey).add(I);    
+								} else {
+									// new key, create a hashset to store OSS entries for this key, and add the current
+									keys.put(newKey, new HashSet<Integer>());
+									keys.get(newKey).add(I);
+								}
+								
+								// @TODO: if this overflows heap (ie too many values)
+								//        then need to catch exception, do temporary merge,
+								//        get rest of values, and remerge
+								if (bMap.containsKey(newKey)) {
+									// Already have entry for this key, simply append new value
+									bMap.get(newKey).add(newVal);
+								} else {
+									// Create new vector of values for this key, and append new value
+									bMap.put(newKey, new Vector<IntWritable>());
+									bMap.get(newKey).add(newVal);
+								}
+								changed = true;
+							}
+						}
+					}
+		
+					merge(context, curKey, bMap.get(curKey));
+				
+					// we're done with this key, remove it and move on
+					keys.remove(curKey);
+					bMap.remove(curKey);
+				}
+			}
+			
+			LOG.info("INCRED: Reducer starting output to file");
+			try{
+				for (Object key: hm.keySet()) {
+					context.write((Text)(key), ((IntWritable)hm.get(key)));
+				}
+			} catch (IOException e) {
+				e.printStackTrace();
+			}
+			LOG.info("INCRED: Reducer done writing to output");
+			
+			cleanup(context);
+		}
+	}
+
+	public static void main(String[] args) throws Exception {
+		Configuration conf = new Configuration();
+		String[] otherArgs = new GenericOptionsParser(conf, args).getRemainingArgs();
+		if (otherArgs.length != 2) {
+			System.err.println("Usage: wordcount <in> <out>");
+			System.exit(2);
+		}
+		// Incremental
+		conf.setBoolean("mapred.tasks.incremental.reduction", true);
+		//conf.getBoolean("mapred.tasks.incremental.reduction", false);
+		conf.setInt("mem.m_thresh", 32*1024*1024); // 32 MB
+		
+		Job job = new Job(conf, "word count incr");
+		job.setJarByClass(WordCountIncremental.class);
+		job.setMapperClass(TokenizerMapper.class);
+		job.setCombinerClass(IntSumReducer.class);
+		job.setReducerClass(IntSumReducer.class);
+		job.setOutputKeyClass(Text.class);
+		job.setOutputValueClass(IntWritable.class);
+		FileInputFormat.addInputPath(job, new Path(otherArgs[0]));
+		FileOutputFormat.setOutputPath(job, new Path(otherArgs[1]));
+		System.exit(job.waitForCompletion(true) ? 0 : 1);
+	}
+}
Index: src/hdfs/org/apache/hadoop/hdfs/DFSClient.java
===================================================================
--- src/hdfs/org/apache/hadoop/hdfs/DFSClient.java	(revision 1596572)
+++ src/hdfs/org/apache/hadoop/hdfs/DFSClient.java	(working copy)
@@ -3262,10 +3262,84 @@
           LOG.warn("Error Recovery for block " + block +
                    " in pipeline " + pipelineMsg + 
                    ": bad datanode " + nodes[errorIndex].getName());
+        
+	// Agile Hadoop. Get the list of nodes from the namenode.
+	// When sending nodes to sleep, we may have out of date data.
+	DatanodeInfo failingDatanode = nodes[errorIndex];
+	LOG.info("We failed to recover block "+block+" of file "+src+", from "+failingDatanode.getName()+". It might be located in a sleeping node.");
+	// Output
+	pipelineMsg = new StringBuilder();
+	for (int j = 0; j < nodes.length; j++) {
+		pipelineMsg.append(nodes[j].getName());
+		if (j < nodes.length - 1) {
+			pipelineMsg.append(", ");
+		}
+	}
+
+	// Looking for the locations again
+	try {
+		LocatedBlocks blockLocations = callGetBlockLocations(namenode, src, 0, Long.MAX_VALUE);
+		List<LocatedBlock> locatedblocks = blockLocations.getLocatedBlocks();
+		for (LocatedBlock auxblock : locatedblocks) {
+			if (auxblock.getBlock().getBlockId() == block.getBlockId()) {
+				nodes = auxblock.getLocations();
+				break;
+			}
+		}
+		
+		// Check if we have any location available
+		if (nodes.length <= 0) {
+			LOG.warn("There are no locations for "+block);
+			lastException = new IOException("There are no locations for "+block);
+			closed = true;
+			if (streamer != null) streamer.close();
+			return false;
+		} else {
+			// Update block locations
+			pipelineMsg = new StringBuilder();
+			for (int j = 0; j < nodes.length; j++) {
+				pipelineMsg.append(nodes[j].getName());
+				if (j < nodes.length - 1) {
+					pipelineMsg.append(", ");
+				}
+			}
+			LOG.info("The new locations for "+block+" are "+pipelineMsg);
+			if (nodes.length > 1) {
+				LOG.info("Removing faulty " + failingDatanode.getName() + " from results");
+				int searchIndex = -1;
+				for (int j=0; j<nodes.length; j++) {
+					DatanodeInfo searchNode = nodes[j];
+					if (searchNode.getName().equals(failingDatanode.getName())) {
+						searchIndex = j;
+						break;
+					}
+				}
+				if (searchIndex>0) {
+					newnodes = new DatanodeInfo[nodes.length-1];
+					System.arraycopy(nodes, 0, newnodes, 0, searchIndex);
+					System.arraycopy(nodes, searchIndex+1, newnodes, searchIndex, newnodes.length-searchIndex);
+				} else {
+					LOG.info("The faulty datanode is not in the list anymore.");
+					newnodes = nodes;
+				}
+			} else {
+				LOG.info("There is nobody else, use the new results");
+				newnodes = nodes;
+			}
+		}
+	} catch (IOException e) {
+		LOG.error("Could not locate the block, use old method");
+		newnodes =  new DatanodeInfo[nodes.length-1];
+		System.arraycopy(nodes, 0, newnodes, 0, errorIndex);
+		System.arraycopy(nodes, errorIndex+1, newnodes, errorIndex, newnodes.length-errorIndex);
+	}
+	// Old code
+	/*
           newnodes =  new DatanodeInfo[nodes.length-1];
           System.arraycopy(nodes, 0, newnodes, 0, errorIndex);
           System.arraycopy(nodes, errorIndex+1, newnodes, errorIndex,
               newnodes.length-errorIndex);
+        */
         }
 
         // Tell the primary datanode to do error recovery 
@@ -3481,6 +3555,7 @@
                                  bytesPerChecksum);
         }
 
+
         // setup pipeline to append to the last block
         nodes = lastBlock.getLocations();
         errorIndex = -1;   // no errors yet.
@@ -3496,6 +3571,7 @@
           } catch (InterruptedException  e) {
           }
         }
+
         if (lastException != null) {
           throw lastException;
         }
@@ -3958,6 +4034,7 @@
     }
 
     private void waitForAckedSeqno(long seqnumToWaitFor) throws IOException {
+    /*
       synchronized (ackQueue) {
         while (!closed) {
           isClosed();
@@ -3964,12 +4041,28 @@
           if (lastAckedSeqno >= seqnumToWaitFor) {
             break;
           }
-          try {
-            ackQueue.wait();
-          } catch (InterruptedException ie) {}
+          ackQueue.wait(timeout);
         }
       }
       isClosed();
+    */
+    
+	// Agile Hadoop. This wasn't needed before but now we get stuck here.
+	while (!closed) {
+		isClosed();
+		if (lastAckedSeqno >= seqnumToWaitFor) {
+			break;
+		}
+		synchronized (ackQueue) {
+			try {
+				long timeout = 20*1000; // 20 seconds
+				ackQueue.wait(timeout);
+			} catch (InterruptedException ie) {
+				LOG.error("We got timed out.");
+			}
+		}
+	}
+	isClosed();
     }
  
     /**
@@ -4083,7 +4176,9 @@
 
         long localstart = System.currentTimeMillis();
         boolean fileComplete = false;
-        while (!fileComplete) {
+        // Agile Hadoop. We make a maximum of 100 retries.
+        // while (!fileComplete) {
+        for (int i=0; !fileComplete && i<100; i++) {
           fileComplete = namenode.complete(src, clientName);
           if (!fileComplete) {
             if (!clientRunning ||
@@ -4097,14 +4192,20 @@
                 throw new IOException(msg);
             }
             try {
-              Thread.sleep(400);
+              Thread.sleep(1000);
               if (System.currentTimeMillis() - localstart > 5000) {
-                LOG.info("Could not complete file " + src + " retrying...");
+                //LOG.info("Could not complete file " + src + " retrying...");
+                LOG.info("Could not complete file " + src + " retrying... Time: " + (System.currentTimeMillis() - localstart)+" Timeout:"+hdfsTimeout);
               }
             } catch (InterruptedException ie) {
             }
           }
         }
+        
+        if (!fileComplete) {
+		throw new IOException("Unable to close file because dfsclient was unable to contact the HDFS servers.");
+        }
+        
       } finally {
         closed = true;
       }
Index: src/hdfs/org/apache/hadoop/hdfs/protocol/DatanodeInfo.java
===================================================================
--- src/hdfs/org/apache/hadoop/hdfs/protocol/DatanodeInfo.java	(revision 1596572)
+++ src/hdfs/org/apache/hadoop/hdfs/protocol/DatanodeInfo.java	(working copy)
@@ -42,6 +42,7 @@
   protected long dfsUsed;
   protected long remaining;
   protected long lastUpdate;
+  protected boolean sleep; // Agile Hadoop
   protected int xceiverCount;
   protected String location = NetworkTopology.DEFAULT_RACK;
 
@@ -66,6 +67,7 @@
     this.dfsUsed = from.getDfsUsed();
     this.remaining = from.getRemaining();
     this.lastUpdate = from.getLastUpdate();
+    this.sleep = from.isSleep(); // Agile Hadoop
     this.xceiverCount = from.getXceiverCount();
     this.location = from.getNetworkLocation();
     this.adminState = from.adminState;
@@ -78,6 +80,7 @@
     this.dfsUsed = 0L;
     this.remaining = 0L;
     this.lastUpdate = 0L;
+    this.sleep = false;
     this.xceiverCount = 0;
     this.adminState = null;    
   }
@@ -101,6 +104,7 @@
     this.dfsUsed = dfsUsed;
     this.remaining = remaining;
     this.lastUpdate = lastUpdate;
+    this.sleep = false; // Agile Hadoop
     this.xceiverCount = xceiverCount;
     this.location = networkLocation;
     this.hostName = hostName;
@@ -160,6 +164,18 @@
   public void setLastUpdate(long lastUpdate) { 
     this.lastUpdate = lastUpdate; 
   }
+  
+  /**
+   * Agile Hadoop.
+   * Set/get if the node is sleeping.
+   */
+  public boolean isSleep() {
+    return this.sleep && (System.currentTimeMillis() - getLastUpdate()) > 5*1000; // 5 seconds
+  }
+  
+  public void setSleep(boolean sleep) {
+    this.sleep = sleep;
+  }
 
   /** Sets number of active connections */
   public void setXceiverCount(int xceiverCount) { 
Index: src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java
===================================================================
--- src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java	(revision 1596572)
+++ src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java	(working copy)
@@ -350,6 +350,14 @@
   ReplicationTargetChooser replicator;
 
   private HostsFileReader hostsReader; 
+  ///////////////////////////////////////////////
+  // Agile Hadoop
+  // File with the sleeping hosts
+  ///////////////////////////////////////////////  // Agile Hadoop
+  private static String SLEEP_NODES_FILE = "conf/sleep_nodes";
+  private HostsFileReader sleepHostsReader; 
+  private Long sleepHostsReaderLastModified;
+  ///////////////////////////////////////////////
   private Daemon dnthread = null;
 
   private long maxFsObjects = 0;          // maximum number of fs objects
@@ -432,6 +440,13 @@
 
     this.hostsReader = new HostsFileReader(conf.get("dfs.hosts",""),
                                            conf.get("dfs.hosts.exclude",""));
+	////////////////////////////////////////////////////////////////
+	// Agile Hadoop
+	// File with the sleeping nodes
+	////////////////////////////////////////////////////////////////
+	this.sleepHostsReader = new HostsFileReader(SLEEP_NODES_FILE, SLEEP_NODES_FILE);
+	this.sleepHostsReaderLastModified = new File(SLEEP_NODES_FILE).lastModified();
+	////////////////////////////////////////////////////////////////
     this.dnthread = new Daemon(new DecommissionManager(this).new Monitor(
         conf.getInt("dfs.namenode.decommission.interval", 30),
         conf.getInt("dfs.namenode.decommission.nodes.per.interval", 5)));
@@ -1108,6 +1123,32 @@
     } while (curPos < endOff 
           && curBlk < blocks.length 
           && results.size() < nrBlocksToReturn);
+
+	/////////////////////////////////////////////////////
+	// Agile Hadoop
+	// Remove sleeping nodes from location results
+	/////////////////////////////////////////////////////
+	checkSleepingHosts();
+	// Get list of sleeping nodes
+	Set<String> sleepingHosts = sleepHostsReader.getHosts();
+	// Filter results
+	for (int i=0; i<results.size(); i++) {
+		LocatedBlock b = results.get(i);
+		// Remove sleeping nodes from the locations
+		LinkedList<DatanodeInfo> locationsFiltered = new LinkedList<DatanodeInfo>();
+		for (DatanodeInfo dn : b.getLocations()) {
+			if (!sleepingHosts.contains(dn.getHostName())) {
+				locationsFiltered.add(dn);
+			}
+		}
+		// Generate block again
+		DatanodeInfo[] locationsFilteredArray = new DatanodeInfo[locationsFiltered.size()];
+		for (int j=0; j<locationsFiltered.size(); j++) {
+			locationsFilteredArray[j] = locationsFiltered.get(j);
+		}
+		results.set(i, new LocatedBlock(b.getBlock(), locationsFilteredArray, b.getStartOffset(), b.isCorrupt()));
+	}
+	/////////////////////////////////////////////////////
     
     return inode.createLocatedBlocks(results);
   }
@@ -1609,6 +1650,24 @@
     NameNode.stateChangeLog.debug("BLOCK* NameSystem.getAdditionalBlock: file "
                                   +src+" for "+clientName);
 
+	///////////////////////////////////////////////////
+	// Agile Hadoop
+	// Consider which nodes are sleeping and exclude those
+	///////////////////////////////////////////////////
+	// Check when the sleeping hosts were modified
+	checkSleepingHosts();
+	// Get the list and exclude it
+	Set<String> sleepingHosts = sleepHostsReader.getHosts();
+	if (excludedNodes == null && sleepingHosts.size()>0) {
+		excludedNodes = new LinkedList<Node>();
+	}
+	for(DatanodeDescriptor node : datanodeMap.values()) {
+		if (sleepingHosts.contains(node.getHostName())) {
+			excludedNodes.add(node);
+		}
+	}
+	///////////////////////////////////////////////////
+                                  
     synchronized (this) {
       if (isInSafeMode()) {//check safemode first for failing-fast
         throw new SafeModeException("Cannot add block to " + src, safeMode);
@@ -1638,7 +1697,8 @@
     if (targets.length < this.minReplication) {
       throw new IOException("File " + src + " could only be replicated to " +
                             targets.length + " nodes, instead of " +
-                            minReplication);
+                            minReplication + " -> sleep " + sleepingHosts + " -> exclude" + excludedNodes + "-> avail " + datanodeMap.keySet());
+                            //minReplication);
     }
 
     // Allocate a new block and record it in the INode. 
@@ -2697,10 +2757,10 @@
     }
     return newID;
   }
-    
+  
   private boolean isDatanodeDead(DatanodeDescriptor node) {
     return (node.getLastUpdate() <
-            (now() - heartbeatExpireInterval));
+            (now() - heartbeatExpireInterval)) && !node.isSleep(); // Agile Hadoop
   }
     
   private void setDatanodeDead(DatanodeDescriptor node) throws IOException {
@@ -2829,6 +2889,8 @@
     public void run() {
       while (fsRunning) {
         try {
+          // Agile Hadoop. We check if there are 
+          checkSleepingHosts();
           long now = now();
           if (lastHeartbeatCheck + heartbeatRecheckInterval < now) {
             heartbeatCheck();
@@ -4694,6 +4756,14 @@
     hostsReader.updateFileNames(conf.get("dfs.hosts",""), 
                                 conf.get("dfs.hosts.exclude", ""));
     hostsReader.refresh();
+    
+	////////////////////////////////////////////////////////////////
+	// Agile Hadoop
+	// Update the list of files
+	////////////////////////////////////////////////////////////////
+	checkSleepingHosts(true);
+	////////////////////////////////////////////////////////////////
+    
     synchronized (this) {
       for (Iterator<DatanodeDescriptor> it = datanodeMap.values().iterator();
            it.hasNext();) {
@@ -5733,10 +5803,26 @@
     return decommissioningNodes;
   }
 
+  /**
+   * Agile Hadoop.
+   * List of sleeping nodes.
+   */
+  public synchronized ArrayList<DatanodeDescriptor> getSleepingNodes() {
+    ArrayList<DatanodeDescriptor> sleepingNodes = new ArrayList<DatanodeDescriptor>();
+    ArrayList<DatanodeDescriptor> results = getDatanodeListForReport(DatanodeReportType.LIVE);
+    for (Iterator<DatanodeDescriptor> it = results.iterator(); it.hasNext();) {
+      DatanodeDescriptor node = it.next();
+      if (node.isSleep()) {
+        sleepingNodes.add(node);
+      }
+    }
+    return sleepingNodes;
+  }
+  
+  
   /*
    * Delegation Token
    */
-  
   private DelegationTokenSecretManager createDelegationTokenSecretManager(
       Configuration conf) {
     return new DelegationTokenSecretManager(conf.getLong(
@@ -6223,4 +6309,55 @@
     checkSuperuserPrivilege();
     return getCorruptFileBlocks();
   }
+  
+	/**
+	 * Agile Hadoop.
+	 * Refresh sleeping hosts list.
+	 */
+	private void checkSleepingHosts() {
+		checkSleepingHosts(false);
+	}
+	
+	private void checkSleepingHosts(boolean force) {
+		boolean change = false;
+		try {
+			if (force || sleepHostsReaderLastModified == null || (new File(SLEEP_NODES_FILE).lastModified() > sleepHostsReaderLastModified)) {
+				// Update the file
+				if (sleepHostsReader == null) {
+					sleepHostsReader = new HostsFileReader(SLEEP_NODES_FILE, SLEEP_NODES_FILE);
+					sleepHostsReaderLastModified = new File(SLEEP_NODES_FILE).lastModified();
+				}
+				sleepHostsReader.updateFileNames(SLEEP_NODES_FILE, SLEEP_NODES_FILE);
+				sleepHostsReader.refresh();
+				sleepHostsReaderLastModified = new File(SLEEP_NODES_FILE).lastModified();
+				change = true;
+			}
+			if (force || change) {
+				synchronized (this) {
+					synchronized(heartbeats) {
+						synchronized (datanodeMap) {
+							// Check the heartbeats
+							//checkSleepingHosts();
+							Set<String> sleepingHosts = sleepHostsReader.getHosts();
+							for (DatanodeDescriptor nodeInfo : heartbeats) {
+								if (sleepingHosts.contains(nodeInfo.getHostName())) {
+									//nodeInfo.setLastUpdate(now());
+									nodeInfo.setSleep(true);
+								} else {
+									nodeInfo.setSleep(false);
+								}
+							}
+							// Datanode list
+							// TODO I don't know why this was refreshing everybody all the time...
+							/*for(DatanodeDescriptor nodeInfo : datanodeMap.values()) {
+								nodeInfo.setLastUpdate(now());
+							}*/
+						}
+					}
+				}
+			}
+		} catch (IOException e) {
+			LOG.error("Updating the list of sleeping hosts.");
+		}
+	}
 }
Index: src/hdfs/org/apache/hadoop/hdfs/server/namenode/ReplicationTargetChooser.java
===================================================================
--- src/hdfs/org/apache/hadoop/hdfs/server/namenode/ReplicationTargetChooser.java	(revision 1596572)
+++ src/hdfs/org/apache/hadoop/hdfs/server/namenode/ReplicationTargetChooser.java	(working copy)
@@ -119,7 +119,7 @@
     if (!clusterMap.contains(writer)) {
       writer=null;
     }
-      
+    
     DatanodeDescriptor localNode = chooseTarget(numOfReplicas, writer, 
                                                 excludedNodes, blocksize, maxNodesPerRack, results);
       
Index: src/mapred/org/apache/hadoop/mapred/ApproximateLineRecordReader.java
===================================================================
--- src/mapred/org/apache/hadoop/mapred/ApproximateLineRecordReader.java	(revision 0)
+++ src/mapred/org/apache/hadoop/mapred/ApproximateLineRecordReader.java	(working copy)
@@ -0,0 +1,114 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.mapred;
+
+import java.io.IOException;
+import java.io.InputStream;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.io.LongWritable;
+import org.apache.hadoop.io.Text;
+
+import java.util.Random;
+
+/**
+ * Approximate Hadoop: Reads lines skipping some.
+ */
+public class ApproximateLineRecordReader extends LineRecordReader {
+  private long lineNumber = 0;
+  private long skipLines = 0;
+  private long prevSkipLines = 0;
+  private long maxLines = -1;
+
+  private Random rnd;
+  
+  public ApproximateLineRecordReader(Configuration job, FileSplit split) throws IOException {
+    super(job, split);
+    // Configure how many lines to approximate
+    this.skipLines = job.getInt("mapred.input.approximate.skip", 1);
+    if (this.skipLines < 1) {
+      this.skipLines = 1;
+    }
+    rnd = new Random();
+    // A maximum set of lines
+    this.maxLines = job.getInt("mapred.input.approximate.max", -1);
+  }
+
+  public ApproximateLineRecordReader(InputStream in, long offset, long endOffset, Configuration job) throws IOException {
+    super(in, offset, endOffset, job);
+    // Configure how many lines to approximate
+    this.skipLines = job.getInt("mapred.input.approximate.skip", 1);
+    if (this.skipLines < 1) {
+      this.skipLines = 1;
+    }
+    rnd = new Random();
+    // A maximum set of lines
+    this.maxLines = job.getInt("mapred.input.approximate.max", -1);
+  }
+  
+  /** 
+   * Read a line skipping a few.
+   */
+  public synchronized boolean next(LongWritable key, Text value) throws IOException {
+    // Select a random number to skip
+    int randomNumber = rnd.nextInt((int)this.skipLines);
+    
+    // We always read one extra line, which lies outside the upper split limit i.e. (end - 1)
+    while (getFilePosition() <= end) {
+      // We just read the first few lines
+      if (maxLines > 0 && lineNumber > maxLines) {
+         return false;
+      }
+    
+      key.set(pos);
+
+      int newSize = in.readLine(value, maxLineLength, Math.max(maxBytesToConsume(pos), maxLineLength));
+      if (newSize == 0) {
+        return false;
+      }
+      
+      // We read one every X lines
+      lineNumber++;
+      //if ((lineNumber-1)%this.skipLines != 0) {
+      //  continue;
+      //}
+      
+      // We first skip the number of lines from the previous run
+      if (prevSkipLines > 0) {
+        prevSkipLines--;
+        continue;
+      }
+      
+      // Now we skip the selected random
+      if (randomNumber > 0) {
+        randomNumber--;
+        prevSkipLines--;
+        continue;
+      }
+      
+      pos += newSize;
+      if (newSize < maxLineLength) {
+        // We now account for the lines we need to skip the next time
+        prevSkipLines = skipLines + prevSkipLines-1;
+        return true;
+      }
+    }
+    return false;
+  }
+}
Index: src/mapred/org/apache/hadoop/mapred/ApproximateMapper.java
===================================================================
--- src/mapred/org/apache/hadoop/mapred/ApproximateMapper.java	(revision 0)
+++ src/mapred/org/apache/hadoop/mapred/ApproximateMapper.java	(working copy)
@@ -0,0 +1,35 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.mapred;
+
+import java.io.IOException;
+
+/** 
+ * Approximate mapper.
+ */
+public interface ApproximateMapper<K1, V1, K2, V2> extends Mapper<K1, V1, K2, V2> {
+  /**
+   * Approximate Hadoop: approximate map.
+   */
+  void mapApproximate(K1 key, V1 value, OutputCollector<K2, V2> output, Reporter reporter) throws IOException;
+  
+  // public void setApproximate(boolean approximate);
+  
+  // public boolean isApproximate();
+}
Index: src/mapred/org/apache/hadoop/mapred/ApproximateReducer.java
===================================================================
--- src/mapred/org/apache/hadoop/mapred/ApproximateReducer.java	(revision 0)
+++ src/mapred/org/apache/hadoop/mapred/ApproximateReducer.java	(working copy)
@@ -0,0 +1,30 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.mapred;
+
+import java.io.IOException;
+
+import java.util.Iterator;
+
+/** 
+ * Approximate reducer.
+ */
+public interface ApproximateReducer<K2, V2, K3, V3> extends Reducer<K2, V2, K3, V3> {
+    void reduceApproximate(K2 key, Iterator<V2> values, OutputCollector<K3, V3> output, Reporter reporter) throws IOException;
+}
Index: src/mapred/org/apache/hadoop/mapred/ApproximateTextInputFormat.java
===================================================================
--- src/mapred/org/apache/hadoop/mapred/ApproximateTextInputFormat.java	(revision 0)
+++ src/mapred/org/apache/hadoop/mapred/ApproximateTextInputFormat.java	(working copy)
@@ -0,0 +1,35 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.mapred;
+
+import java.io.*;
+
+import org.apache.hadoop.fs.*;
+import org.apache.hadoop.io.LongWritable;
+import org.apache.hadoop.io.Text;
+
+/** 
+ * Approximate
+ */
+public class ApproximateTextInputFormat extends TextInputFormat {
+  public RecordReader<LongWritable, Text> getRecordReader(InputSplit genericSplit, JobConf job, Reporter reporter) throws IOException {
+    reporter.setStatus(genericSplit.toString());
+    return new ApproximateLineRecordReader(job, (FileSplit) genericSplit);
+  }
+}
Index: src/mapred/org/apache/hadoop/mapred/JSPUtil.java
===================================================================
--- src/mapred/org/apache/hadoop/mapred/JSPUtil.java	(revision 1596572)
+++ src/mapred/org/apache/hadoop/mapred/JSPUtil.java	(working copy)
@@ -26,6 +26,7 @@
 import java.util.Iterator;
 import java.util.LinkedHashMap;
 import java.util.Map;
+import java.util.List;
 
 import javax.servlet.RequestDispatcher;
 import javax.servlet.ServletException;
@@ -40,6 +41,7 @@
 import org.apache.hadoop.http.HtmlQuoting;
 import org.apache.hadoop.mapred.JobHistory.JobInfo;
 import org.apache.hadoop.mapred.JobHistory.Keys;
+import org.apache.hadoop.mapred.JobHistory.Values;
 import org.apache.hadoop.mapred.JobTracker.RetireJobInfo;
 import org.apache.hadoop.mapreduce.JobACL;
 import org.apache.hadoop.security.AccessControlException;
@@ -48,6 +50,7 @@
 import org.apache.hadoop.util.ServletUtil;
 import org.apache.hadoop.util.StringUtils;
 
+
 class JSPUtil {
   static final String PRIVATE_ACTIONS_KEY = "webinterface.private.actions";
 
@@ -301,11 +304,13 @@
       sb.append("<td><b>User</b></td>");
       sb.append("<td><b>Name</b></td>");
       sb.append("<td><b>Map % Complete</b></td>");
-      sb.append("<td><b>Map Total</b></td>");
-      sb.append("<td><b>Maps Completed</b></td>");
+      //sb.append("<td><b>Map Total</b></td>");
+      //sb.append("<td><b>Maps Completed</b></td>");
+      sb.append("<td><b>Maps</b></td>");
       sb.append("<td><b>Reduce % Complete</b></td>");
-      sb.append("<td><b>Reduce Total</b></td>");
-      sb.append("<td><b>Reduces Completed</b></td>");
+      //sb.append("<td><b>Reduce Total</b></td>");
+      //sb.append("<td><b>Reduces Completed</b></td>");
+      sb.append("<td><b>Reduces</b></td>");
       sb.append("<td><b>Job Scheduling Information</b></td>");
       sb.append("<td><b>Diagnostic Info </b></td>");
       sb.append("</tr>\n");
@@ -330,37 +335,33 @@
         if (isModifiable) {
           sb.append("<tr><td><input TYPE=\"checkbox\" " +
           		"onclick=\"checkButtonVerbage()\" " +
-          		"name=\"jobCheckBox\" value="
-                  + jobid + "></td>");
+          		"name=\"jobCheckBox\" value=" + jobid + "></td>");
         } else {
           sb.append("<tr>");
         }
 
-        sb.append("<td id=\"job_" + rowId
-            + "\"><a href=\"jobdetails.jsp?jobid=" + jobid + "&refresh="
-            + refresh + "\">" + jobid + "</a></td>"
-            + "<td id=\"started_" + rowId + "\">" + time + "</td>"
-            + "<td id=\"priority_" + rowId + "\">"
-            + jobpri + "</td>" + "<td id=\"user_" + rowId
-            + "\">" + HtmlQuoting.quoteHtmlChars(profile.getUser()) +
-              "</td>" + "<td id=\"name_" + rowId
-            + "\">" + ("".equals(name) ? "&nbsp;" : name) + "</td>" + "<td>"
-            + StringUtils.formatPercent(status.mapProgress(), 2)
+        sb.append("<td id=\"job_" + rowId + "\"><a href=\"jobdetails.jsp?jobid=" + jobid + "&refresh=" + refresh + "\">" + jobid + "</a></td>");
+        sb.append("<td id=\"started_" + rowId + "\">" + time + "</td>");
+        sb.append("<td id=\"priority_" + rowId + "\">" + jobpri + "</td>");
+        sb.append("<td id=\"user_" + rowId + "\">" + HtmlQuoting.quoteHtmlChars(profile.getUser()) + "</td>");
+        sb.append("<td id=\"name_" + rowId + "\">" + ("".equals(name) ? "&nbsp;" : name) + "</td>");
+        sb.append("<td>" + StringUtils.formatPercent(status.mapProgress(), 2)
             + ServletUtil.percentageGraph(status.mapProgress() * 100, 80)
-            + "</td><td>" + desiredMaps + "</td><td>" + completedMaps
-            + "</td><td>"
-            + StringUtils.formatPercent(status.reduceProgress(), 2)
-            + ServletUtil.percentageGraph(status.reduceProgress() * 100, 80)
-            + "</td><td>" + desiredReduces + "</td><td> " + completedReduces 
-            + "</td><td>" + schedulingInfo
-            + "</td><td>" + diagnosticInfo + "</td></tr>\n");
+            //+ "</td><td>" + desiredMaps + "</td><td>" + completedMaps
+            + "</td>");
+        sb.append("<td>" + completedMaps + "/" + desiredMaps + "</td>");
+        sb.append("<td>" + StringUtils.formatPercent(status.reduceProgress(), 2)
+            + ServletUtil.percentageGraph(status.reduceProgress() * 100, 80) + "</td>");
+            //+ "</td><td>" + desiredReduces + "</td><td> " + completedReduces + "</td>");
+        sb.append("<td>" + completedReduces + "/" + desiredReduces+ "</td>");
+        sb.append("<td>" + schedulingInfo+ "</td>");
+        sb.append("<td>" + diagnosticInfo + "</td></tr>\n");
       }
       if (isModifiable) {
         sb.append("</form>\n");
       }
     } else {
-      sb.append("<tr><td align=\"center\" colspan=\"8\"><i>none</i>" +
-      		"</td></tr>\n");
+      sb.append("<tr><td align=\"center\" colspan=\"8\"><i>none</i></td></tr>\n");
     }
     sb.append("</table>\n");
     
@@ -452,8 +453,8 @@
   }
 
   /**
-   * Read a job-history log file and construct the corresponding {@link JobInfo}
-   * . Also cache the {@link JobInfo} for quick serving further requests.
+   * Read a job-history log file and construct the corresponding {@link JobInfo}.
+   * Also cache the {@link JobInfo} for quick serving further requests.
    * 
    * @param logFile
    * @param fs
@@ -468,7 +469,7 @@
       jobInfo = jobHistoryCache.remove(jobid);
       if (jobInfo == null) {
         jobInfo = new JobHistory.JobInfo(jobid);
-        LOG.info("Loading Job History file "+jobid + ".   Cache size is " +
+        LOG.debug("Loading Job History file "+jobid + ".   Cache size is " +
             jobHistoryCache.size());
         DefaultJobHistoryParser.parseJobTasks(logFile.toUri().getPath(),
             jobInfo, fs);
@@ -475,13 +476,13 @@
       }
       jobHistoryCache.put(jobid, jobInfo);
       int CACHE_SIZE = 
-        jobConf.getInt("mapred.job.tracker.jobhistory.lru.cache.size", 5);
+        jobConf.getInt("mapred.job.tracker.jobhistory.lru.cache.size", 100);
       if (jobHistoryCache.size() > CACHE_SIZE) {
         Iterator<Map.Entry<String, JobInfo>> it = 
           jobHistoryCache.entrySet().iterator();
         String removeJobId = it.next().getKey();
         it.remove();
-        LOG.info("Job History file removed form cache "+removeJobId);
+        LOG.debug("Job History file removed form cache "+removeJobId);
       }
     }
 
@@ -605,4 +606,91 @@
   static boolean privateActionsAllowed(JobConf conf) {
     return conf.getBoolean(PRIVATE_ACTIONS_KEY, false);
   }
+  
+	/**
+	 * Hadoop viewer.
+	 */
+	static class TaskExecution {
+		long start;
+		long end;
+		String color;
+		String content;
+		
+		public TaskExecution(long start, long end, String color, String content) {
+			this.start = start;
+			this.end = end;
+			this.color = color;
+			this.content = content;
+		}
+	}
+
+	static String getExecutionGraph(long start, long end) {
+		return getExecutionGraph(start, end, "#0000FF");
+	}
+	
+	static String getExecutionGraph(long start, long end, String color) {
+		return getExecutionGraph(start, end, color, "");
+	}
+	
+	static String getExecutionGraph(long start, long end, String color, String content) {
+		return getExecutionGraph(start, end, color, content, 2);
+	}
+	
+	static String getExecutionGraph(long start, long end, String color, String content, float zoom) {
+		String out = "";
+		out += "<table  border=\"0\" cellspacing=\"0\" cellpadding=\"0\">";
+		out += "<tr height=\"5px\">\n";
+		out += "<td width=\"" + ((int) Math.floor(1.0*     start *zoom)) + "px\" bgcolor=\"white\"/>\n";
+		out += "<td width=\"" + ((int) Math.floor(1.0*(end-start)*zoom)) + "px\" style=\"background-color:"+color+"; color:#FFFFFF; font-size:small\" align=\"left\">"+content+"</td>\n";
+		out += "</tr>";
+		out += "</table>\n";
+		return out;
+	}
+	
+	static String getExecutionGraph(List<JSPUtil.TaskExecution> executions) {
+		return getExecutionGraph(executions, 2);
+	}
+	
+	static String getExecutionGraph(List<JSPUtil.TaskExecution> executions, float zoom) {
+		String out = "";
+		out += "<table  border=\"0\" cellspacing=\"0\" cellpadding=\"0\">";
+		out += "<tr height=\"5px\">\n";
+		for (int i=0; i<executions.size(); i++) {
+			long prev = 0;
+			if (i>0)
+				prev = executions.get(i-1).end;
+			JSPUtil.TaskExecution exec = executions.get(i);
+			out += "<td width=\"" + ((int) Math.floor(1.0*(exec.start -   prev)*zoom)) + "px\" bgcolor=\"white\"/>\n";
+			out += "<td width=\"" + ((int) Math.floor(1.0*(exec.end-exec.start)*zoom)) + "px\" style=\"background-color:"+exec.color+"; color:#FFFFFF; font-size:small\" align=\"left\">"+exec.content+"</td>\n";
+		}
+		out += "</tr>";
+		out += "</table>\n";
+		return out;
+	}
+	
+	static String getTaskColor(JobHistory.Task task) {
+		String color = "yellow";
+		
+		String taskStatus = task.get(Keys.TASK_STATUS);
+		boolean approximated = Boolean.parseBoolean(task.get(Keys.APPROXIMATED));
+		boolean isMap = Values.MAP.name().equals(task.get(Keys.TASK_TYPE));
+		boolean isRed = Values.REDUCE.name().equals(task.get(Keys.TASK_TYPE));
+		
+		if (taskStatus.equalsIgnoreCase("DROPPED")) {
+			color = "#000000";
+		} else if (isMap && approximated) {
+			color = "#0000FF";
+		} else if (isMap) {
+			color = "#000080";
+		} else if (isRed && approximated) {
+			color = "#FF0000";
+		} else if (isRed) {
+			color = "#800000";
+		} else if (approximated) {
+			color = "#00FF00";
+		} else {
+			color = "#008000";
+		}
+		return color;
+	}
 }
Index: src/mapred/org/apache/hadoop/mapred/JobClient.java
===================================================================
--- src/mapred/org/apache/hadoop/mapred/JobClient.java	(revision 1596572)
+++ src/mapred/org/apache/hadoop/mapred/JobClient.java	(working copy)
@@ -178,7 +178,7 @@
  */
 public class JobClient extends Configured implements MRConstants, Tool  {
   private static final Log LOG = LogFactory.getLog(JobClient.class);
-  public static enum TaskStatusFilter { NONE, KILLED, FAILED, SUCCEEDED, ALL }
+  public static enum TaskStatusFilter { NONE, KILLED, FAILED, DROPPED, SUCCEEDED, ALL }
   private TaskStatusFilter taskOutputFilter = TaskStatusFilter.FAILED; 
   private static final long MAX_JOBPROFILE_AGE = 1000 * 2;
 
@@ -385,6 +385,20 @@
     }
     
     /**
+     * Approximate Hadoop.
+     * Interface to drop tasks.
+     */
+    public synchronized void dropTask(TaskAttemptID taskId) throws IOException {
+      jobSubmitClient.dropTask(taskId);
+    }
+    
+    @Deprecated
+    public synchronized void dropTask(String taskId) throws IOException {
+      dropTask(TaskAttemptID.forName(taskId));
+    }
+    
+    
+    /**
      * Fetch task completion events from jobtracker for this job. 
      */
     public synchronized TaskCompletionEvent[] getTaskCompletionEvents(
@@ -539,6 +553,8 @@
         new HashMap<String,RetryPolicy>();
     methodNameToPolicyMap.put("killJob", RetryPolicies.TRY_ONCE_THEN_FAIL);
     methodNameToPolicyMap.put("killTask", RetryPolicies.TRY_ONCE_THEN_FAIL);
+    // Approximate Hadoop
+    methodNameToPolicyMap.put("dropTask", RetryPolicies.TRY_ONCE_THEN_FAIL);
     
     return (JobSubmissionProtocol) RetryProxy.create(JobSubmissionProtocol.class,
         rpcJobSubmitClient, defaultPolicy, methodNameToPolicyMap);
@@ -883,6 +899,9 @@
   public RunningJob submitJob(JobConf job) throws FileNotFoundException,
                                                   IOException {
     try {
+      // Approximate Hadoop
+      checkOldJobOutputs(job);
+    
       return submitJobInternal(job);
     } catch (InterruptedException ie) {
       throw new IOException("interrupted", ie);
@@ -892,6 +911,23 @@
   }
 
   /**
+   * Approximate Hadoop.
+   * Check if we are using an old job.
+   */
+  public static void checkOldJobOutputs(JobConf job) {
+    String prevJob = job.get("mapred.job.previous");
+    if (prevJob != null) {
+      JobHistoryReader jobHistory = new JobHistoryReader(prevJob, job);
+      Map<Integer, String> jobPreciseOutputs = jobHistory.getPreciseOutputs();
+      for (Integer taskIdNum : jobPreciseOutputs.keySet()) {
+        String mapOutput = jobPreciseOutputs.get(taskIdNum);
+        job.set("mapred.map."+taskIdNum+".previous.output", mapOutput);
+        LOG.info("Adding old output for map " + taskIdNum + " at " + mapOutput);
+      }
+    }
+  }
+  
+  /**
    * Internal method for submitting jobs to the system.
    * @param job the configuration to submit
    * @return a proxy object for the running job
@@ -1320,6 +1356,10 @@
    */
   public static RunningJob runJob(JobConf job) throws IOException {
     JobClient jc = new JobClient(job);
+    
+    // Approximate Hadoop
+    checkOldJobOutputs(job);
+    
     RunningJob rj = jc.submitJob(job);
     try {
       if (!jc.monitorAndPrintJob(job, rj)) {
@@ -1408,6 +1448,11 @@
             LOG.info(event.toString());
           }
           break; 
+        case DROPPED:
+          if (event.getTaskStatus() == TaskCompletionEvent.Status.DROPPED){
+            LOG.info(event.toString());
+          }
+          break; 
         case ALL:
           LOG.info(event.toString());
           displayTaskLogs(event.getTaskAttemptId(), event.getTaskTrackerHttp());
@@ -1630,6 +1675,7 @@
     boolean displayTasks = false;
     boolean killTask = false;
     boolean failTask = false;
+    boolean dropTask = false; // Approximate Hadoop
     boolean setJobPriority = false;
 
     if ("-submit".equals(cmd)) {
@@ -1720,6 +1766,14 @@
       }
       failTask = true;
       taskid = argv[1];
+    // Approximate Hadoop
+    } else if("-drop-task".equals(cmd)) {
+      if(argv.length != 2) {
+        displayUsage(cmd);
+        return exitCode;
+      }
+      dropTask = true;
+      taskid = argv[1];
     } else if ("-list-active-trackers".equals(cmd)) {
       if (argv.length != 1) {
         displayUsage(cmd);
@@ -1868,6 +1922,15 @@
           System.out.println("Could not fail task " + taskid);
           exitCode = -1;
         }
+      // Drop task
+      } else if(dropTask) {
+        if(jobSubmitClient.dropTask(TaskAttemptID.forName(taskid))) {
+          System.out.println("Dropped task " + taskid + " by failing it");
+          exitCode = 0;
+        } else {
+          System.out.println("Could not drop task " + taskid);
+          exitCode = -1;
+        }
       }
     } catch (RemoteException re){
       IOException unwrappedException = re.unwrapRemoteException();
@@ -1930,7 +1993,7 @@
     if (jobs == null)
       jobs = new JobStatus[0];
     System.out.printf("%d jobs submitted\n", jobs.length);
-    System.out.printf("States are:\n\tRunning : 1\tSucceded : 2" +
+    System.out.printf("States are:\n\tRunning : 1\tSucceeded : 2" +
     "\tFailed : 3\tPrep : 4\n");
     displayJobList(jobs);
   }
Index: src/mapred/org/apache/hadoop/mapred/JobConf.java
===================================================================
--- src/mapred/org/apache/hadoop/mapred/JobConf.java	(revision 1596572)
+++ src/mapred/org/apache/hadoop/mapred/JobConf.java	(working copy)
@@ -976,7 +976,28 @@
   public void setMapRunnerClass(Class<? extends MapRunnable> theClass) {
     setClass("mapred.map.runner.class", theClass, MapRunnable.class);
   }
+  
+  /**
+   * Approximate Hadoop
+   */
+  public Class<? extends Mapper> getApproximateMapperClass() {
+    return getClass("mapred.mapper.approximate.class", IdentityMapper.class, Mapper.class);
+  }
+  
+  public void setApproximateMapperClass(Class<? extends Mapper> theClass) {
+    setClass("mapred.mapper.approximate.class", theClass, Mapper.class);
+  }
 
+  public Class<? extends MapRunnable> getApproximateMapRunnerClass() {
+    return getClass("mapred.map.approximate.runner.class",
+                    MapRunner.class, MapRunnable.class);
+  }
+  
+  public void setApproximateMapRunnerClass(Class<? extends MapRunnable> theClass) {
+    setClass("mapred.map.approximate.runner.class", theClass, MapRunnable.class);
+  }
+  
+  
   /**
    * Get the {@link Partitioner} used to partition {@link Mapper}-outputs 
    * to be sent to the {@link Reducer}s.
Index: src/mapred/org/apache/hadoop/mapred/JobHistory.java
===================================================================
--- src/mapred/org/apache/hadoop/mapred/JobHistory.java	(revision 1596572)
+++ src/mapred/org/apache/hadoop/mapred/JobHistory.java	(working copy)
@@ -103,7 +103,7 @@
   static final String UNDERSCORE_ESCAPE = "%5F";
 
   public static final Log LOG = LogFactory.getLog(JobHistory.class);
-  private static final char DELIMITER = ' ';
+  static final char DELIMITER = ' ';
   static final char LINE_DELIMITER_CHAR = '.';
   static final char[] charsToEscape = new char[] {'"', '=', 
                                                 LINE_DELIMITER_CHAR};
@@ -476,7 +476,9 @@
     ERROR, TASK_ATTEMPT_ID, TASK_STATUS, COPY_PHASE, SORT_PHASE, REDUCE_PHASE, 
     SHUFFLE_FINISHED, SORT_FINISHED, COUNTERS, SPLITS, JOB_PRIORITY, HTTP_PORT, 
     TRACKER_NAME, STATE_STRING, VERSION, MAP_COUNTERS, REDUCE_COUNTERS,
-    VIEW_JOB, MODIFY_JOB, JOB_QUEUE, FAIL_REASON
+    VIEW_JOB, MODIFY_JOB, JOB_QUEUE, FAIL_REASON,
+    // Approximate Hadoop
+    APPROXIMATED
   }
 
   /**
@@ -485,7 +487,7 @@
    * most places in history file. 
    */
   public static enum Values {
-    SUCCESS, FAILED, KILLED, MAP, REDUCE, CLEANUP, RUNNING, PREP, SETUP
+    SUCCESS, FAILED, KILLED, MAP, REDUCE, CLEANUP, RUNNING, PREP, SETUP, DROPPED
   }
 
   /**
@@ -2044,7 +2046,7 @@
      * @param finishTime finish timeof task in ms
      */
     public static void logFinished(TaskID taskId, String taskType, 
-                                   long finishTime, Counters counters){
+                                   long finishTime, boolean dropped, Counters counters){
       JobID id = taskId.getJobID();
       ArrayList<PrintWriter> writer = fileManager.getWriters(id);
 
@@ -2053,7 +2055,7 @@
                         new Keys[]{Keys.TASKID, Keys.TASK_TYPE, 
                                    Keys.TASK_STATUS, Keys.FINISH_TIME,
                                    Keys.COUNTERS}, 
-                        new String[]{ taskId.toString(), taskType, Values.SUCCESS.name(), 
+                        new String[]{ taskId.toString(), taskType, dropped ? Values.DROPPED.name() : Values.SUCCESS.name(), 
                                       String.valueOf(finishTime),
                                       counters.makeEscapedCompactString()}, id);
       }
@@ -2137,8 +2139,8 @@
      *             {@link #logStarted(TaskAttemptID, long, String, int, String)}
      */
     @Deprecated
-    public static void logStarted(TaskAttemptID taskAttemptId, long startTime, String hostName){
-      logStarted(taskAttemptId, startTime, hostName, -1, Values.MAP.name());
+    public static void logStarted(TaskAttemptID taskAttemptId, long startTime, boolean approximated, String hostName){
+      logStarted(taskAttemptId, startTime, approximated, hostName, -1, Values.MAP.name());
     }
     
     /**
@@ -2151,7 +2153,9 @@
      * @param taskType Whether the attempt is cleanup or setup or map 
      */
     public static void logStarted(TaskAttemptID taskAttemptId, long startTime,
-                                  String trackerName, int httpPort, 
+                                  // Approximated
+                                  boolean approximated,
+                                  String trackerName, int httpPort,
                                   String taskType) {
       JobID id = taskAttemptId.getJobID();
       ArrayList<PrintWriter> writer = fileManager.getWriters(id); 
@@ -2160,13 +2164,17 @@
         JobHistory.log(writer, RecordTypes.MapAttempt, 
                        new Keys[]{ Keys.TASK_TYPE, Keys.TASKID, 
                                    Keys.TASK_ATTEMPT_ID, Keys.START_TIME, 
-                                   Keys.TRACKER_NAME, Keys.HTTP_PORT},
+                                   Keys.TRACKER_NAME, Keys.HTTP_PORT,
+                                   // Approximated
+                                   Keys.APPROXIMATED},
                        new String[]{taskType,
                                     taskAttemptId.getTaskID().toString(), 
                                     taskAttemptId.toString(), 
                                     String.valueOf(startTime), trackerName,
                                     httpPort == -1 ? "" : 
-                                      String.valueOf(httpPort)}, id); 
+                                      String.valueOf(httpPort),
+                                    // Approximated
+                                    String.valueOf(approximated)}, id); 
       }
     }
     
@@ -2179,10 +2187,11 @@
      * {@link #logFinished(TaskAttemptID, long, String, String, String, Counters)}
      */
     @Deprecated
-    public static void logFinished(TaskAttemptID taskAttemptId, long finishTime, 
+    public static void logFinished(TaskAttemptID taskAttemptId, long finishTime,
+                                   // Approximate Hadoop
+                                   boolean approximated,
                                    String hostName){
-      logFinished(taskAttemptId, finishTime, hostName, Values.MAP.name(), "", 
-                  new Counters());
+      logFinished(taskAttemptId, finishTime, approximated, false, hostName, Values.MAP.name(), "", new Counters());
     }
 
     /**
@@ -2197,6 +2206,9 @@
      */
     public static void logFinished(TaskAttemptID taskAttemptId, 
                                    long finishTime, 
+                                   // Approximate Hadoop
+                                   boolean approximated,
+                                   boolean dropped,
                                    String hostName,
                                    String taskType,
                                    String stateString, 
@@ -2209,13 +2221,13 @@
                        new Keys[]{ Keys.TASK_TYPE, Keys.TASKID, 
                                    Keys.TASK_ATTEMPT_ID, Keys.TASK_STATUS, 
                                    Keys.FINISH_TIME, Keys.HOSTNAME, 
-                                   Keys.STATE_STRING, Keys.COUNTERS},
+                                   Keys.STATE_STRING, Keys.APPROXIMATED, Keys.COUNTERS},
                        new String[]{taskType, 
                                     taskAttemptId.getTaskID().toString(),
                                     taskAttemptId.toString(), 
-                                    Values.SUCCESS.name(),  
+                                    dropped ? Values.DROPPED.name() : Values.SUCCESS.name(), // Approximate Hadoop dropped
                                     String.valueOf(finishTime), hostName, 
-                                    stateString, 
+                                    stateString, String.valueOf(approximated),
                                     counter.makeEscapedCompactString()}, id); 
       }
     }
Index: src/mapred/org/apache/hadoop/mapred/JobHistoryReader.java
===================================================================
--- src/mapred/org/apache/hadoop/mapred/JobHistoryReader.java	(revision 0)
+++ src/mapred/org/apache/hadoop/mapred/JobHistoryReader.java	(working copy)
@@ -0,0 +1,221 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.mapred;
+
+import java.net.URI;
+
+import java.util.List;
+import java.util.Map;
+import java.util.LinkedList;
+import java.util.HashMap;
+
+import java.io.FileInputStream;
+import java.io.InputStreamReader;
+import java.io.BufferedReader;
+import java.io.IOException;
+import java.io.FileNotFoundException;
+
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.fs.FSDataInputStream;
+
+import org.apache.hadoop.conf.Configuration;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+
+/**
+ * Approximate Hadoop.
+ * Reads the history information from a host.
+ * After implementing this, I found HistoryViewer and DefaultJobHistoryParser.
+ */
+public class JobHistoryReader {
+  // taskId => <attemptId => attempt>
+  private Map<TaskID, Map<TaskAttemptID, AttemptInfo>> tasks = new HashMap<TaskID, Map<TaskAttemptID, AttemptInfo>>();
+  // nodeId => attempt
+  private Map<String, Map<TaskAttemptID, AttemptInfo>> nodes = new HashMap<String, Map<TaskAttemptID, AttemptInfo>>();
+
+  public static final Log LOG = LogFactory.getLog(JobHistoryReader.class);
+  
+  class AttemptInfo {
+    public TaskID taskId;
+    public String taskType;
+    public TaskAttemptID attemptId;
+    public String host;
+    public Integer port;
+    public String taskStatus;
+    public Boolean approximated;
+    
+    /**
+     * Copy from another attempt.
+     */
+    public void update(AttemptInfo other) {
+      if (taskId == null)
+        taskId = other.taskId;
+      if (attemptId == null)
+        attemptId = other.attemptId;
+      if (taskType == null)
+        taskType = other.taskType;
+      if (host == null)
+        host = other.host;
+      if (port == null)
+        port = other.port;
+      if (taskStatus == null)
+        taskStatus = other.taskStatus;
+      if (approximated == null)
+        approximated = other.approximated;
+    }
+    
+    /**
+     * The host is usually messy, clean it.
+     */
+    public String getHost() {
+      String ret = host;
+      
+      if (ret.startsWith("tracker_")) {
+        ret = ret.substring("tracker_".length());
+      }
+      if (ret.indexOf(":") >= 0) {
+        ret = ret.substring(0, ret.indexOf(":"));
+      }
+      if (ret.indexOf("/") >= 0) {
+        ret = ret.substring(ret.lastIndexOf("/")+1);
+      }
+    
+      return ret;
+    }
+    
+    public JobID getJobID() {
+      return taskId.getJobID();
+    }
+    
+    // http://sol021:50060/mapOutput?job=job_201308221433_0001&map=attempt_201308221433_0001_m_000000_0&reduce=0
+    public String getOutputLocation() {
+      return "http://"+getHost()+":"+port+"/mapOutput?job="+getJobID()+"&map="+attemptId+"&reduce=0";
+    }
+  }
+  
+  public JobHistoryReader(String filename, Configuration conf) {
+    String line = "";
+    try {
+      LOG.info("Read previous job information from: " + filename);
+    
+      Path path = new Path(filename);
+      FileSystem fs = path.getFileSystem(conf);
+      FSDataInputStream istream = fs.open(path);
+    
+      // Open the file
+      BufferedReader br = new BufferedReader(new InputStreamReader(istream));
+    
+      // Read history line by line
+      while ((line = br.readLine()) != null) {
+        // Get type and content
+        int index0 = line.indexOf(JobHistory.DELIMITER);
+        String contenttype = line.substring(0, index0);
+        String content = line.substring(index0+1);
+      
+        // Check Maps
+        if (contenttype.equals("MapAttempt")) {
+          AttemptInfo attempt = new AttemptInfo();
+          while (content.indexOf("=") >= 0) {
+            String key = content.substring(0, content.indexOf("="));
+            int i0 = content.indexOf("\"");
+            int i1 = content.indexOf("\"", i0+1);
+            String value = content.substring(i0+1, i1);
+            
+            if (key.equalsIgnoreCase("TASKID")) {
+              attempt.taskId = TaskID.forName(value);
+            } else if (key.equalsIgnoreCase("TASK_TYPE")) {
+              attempt.taskType = value;
+            } else if (key.equalsIgnoreCase("TASK_ATTEMPT_ID")) {
+              attempt.attemptId = TaskAttemptID.forName(value);
+            } else if (key.equalsIgnoreCase("HOSTNAME")) {
+              attempt.host = value;
+            } else if (key.equalsIgnoreCase("TRACKER_NAME")) {
+              attempt.host = value;
+            } else if (key.equalsIgnoreCase("HTTP_PORT")) {
+              try {
+                attempt.port = Integer.parseInt(value);
+              } catch (NumberFormatException ee) {
+                attempt.port = -1;
+              }
+            } else if (key.equalsIgnoreCase("TASK_STATUS")) {
+              attempt.taskStatus = value;
+            } else if (key.equalsIgnoreCase("APPROXIMATED")) {
+              attempt.approximated = Boolean.parseBoolean(value);
+            }
+            // Keep searching
+            content = content.substring(i1+1).trim();
+          }
+          
+          // Store the attempt information
+          if (!tasks.containsKey(attempt.taskId)) {
+            tasks.put(attempt.taskId, new HashMap<TaskAttemptID, AttemptInfo>());
+          }
+          if (!tasks.get(attempt.taskId).containsKey(attempt.attemptId)) {
+            tasks.get(attempt.taskId).put(attempt.attemptId, attempt);
+          }
+          tasks.get(attempt.taskId).get(attempt.attemptId).update(attempt);
+          
+          // Store the nodes
+          if (!nodes.containsKey(attempt.getHost())) {
+            nodes.put(attempt.getHost(), new HashMap<TaskAttemptID, AttemptInfo>());
+          }
+          nodes.get(attempt.getHost()).put(attempt.attemptId, tasks.get(attempt.taskId).get(attempt.attemptId));
+        }
+      }
+      // Close the input stream
+      br.close();
+    } catch (Exception e) {
+      LOG.error("Error reading " + filename + ": " + e);
+      LOG.error("Line:" + line);
+      e.printStackTrace();
+    }
+    
+    // Output
+    /*
+    LOG.info("Output..........");
+    for (TaskID taskId : tasks.keySet()) {
+      System.out.println(taskId);
+      Map<TaskAttemptID, AttemptInfo> attempts = tasks.get(taskId);
+      for (AttemptInfo attempt : attempts.values()) {
+        System.out.println("   " + attempt.attemptId + " " + attempt.approximated + " " + attempt.getOutputLocation());
+      }
+    }
+    */
+  }
+  
+  /**
+   * Get the address of the outputs that are not approximated.
+   */
+  public Map<Integer, String> getPreciseOutputs() {
+    Map<Integer, String> ret = new HashMap<Integer, String>();
+    for (TaskID taskId : tasks.keySet()) {
+      Integer taskIdNum = taskId.getId();
+      Map<TaskAttemptID, AttemptInfo> attempts = tasks.get(taskId);
+      for (AttemptInfo attempt : attempts.values()) {
+        // We only report precise maps that had succeeded
+        if (!attempt.approximated && attempt.taskType.equalsIgnoreCase("MAP") && attempt.taskStatus.equalsIgnoreCase("SUCCESS")) {
+          ret.put(taskIdNum, attempt.getOutputLocation());
+        }
+      }
+    }
+    return ret;
+  }
+}
Index: src/mapred/org/apache/hadoop/mapred/JobInProgress.java
===================================================================
--- src/mapred/org/apache/hadoop/mapred/JobInProgress.java	(revision 1596572)
+++ src/mapred/org/apache/hadoop/mapred/JobInProgress.java	(working copy)
@@ -21,6 +21,7 @@
 import java.security.PrivilegedExceptionAction;
 import java.util.ArrayList;
 import java.util.Collection;
+import java.util.Collections; // Approximate hadoop: shuffle map order
 import java.util.Comparator;
 import java.util.EnumMap;
 import java.util.HashMap;
@@ -113,6 +114,9 @@
   int finishedReduceTasks = 0;
   int failedMapTasks = 0; 
   int failedReduceTasks = 0;
+  // Approximate Hadoop
+  int droppedMapTasks = 0;
+  int droppedReduceTasks = 0;
   private static long DEFAULT_REDUCE_INPUT_LIMIT = -1L;
   long reduce_input_limit = -1L;
   private static float DEFAULT_COMPLETED_MAPS_PERCENT_FOR_REDUCE_SLOWSTART = 0.05f;
@@ -233,6 +237,9 @@
   long startTime;
   long launchTime;
   long finishTime;
+  
+  // Approximate Hadoop. We use this value to know when to start dropping tasks.
+  long startDropTime = -1;
 
   // First *task launch time
   final Map<TaskType, Long> firstTaskLaunchTimes =
@@ -538,7 +545,14 @@
       new IdentityHashMap<Node, List<TaskInProgress>>(maxLevel);
     
     Set<String> uniqueHosts = new TreeSet<String>();
+    // Approximate Hadoop: Dropping should be random order
+    ArrayList<Integer> shuffleIs = new ArrayList<Integer>();
     for (int i = 0; i < splits.length; i++) {
+	shuffleIs.add(i);
+    }
+    Collections.shuffle(shuffleIs);
+    for (int i : shuffleIs) {
+    //for (int i = 0; i < splits.length; i++) {
       String[] splitLocations = splits[i].getLocations();
       if (splitLocations == null || splitLocations.length == 0) {
         nonLocalMaps.add(maps[i]);
@@ -838,15 +852,18 @@
   public int desiredMaps() {
     return numMapTasks;
   }
+  public synchronized int runningMaps() {
+    return runningMapTasks;
+  }
   public synchronized int finishedMaps() {
     return finishedMapTasks;
   }
+  public synchronized int droppedMaps() {
+    return droppedMapTasks;
+  }
   public int desiredReduces() {
     return numReduceTasks;
   }
-  public synchronized int runningMaps() {
-    return runningMapTasks;
-  }
   public synchronized int runningReduces() {
     return runningReduceTasks;
   }
@@ -853,13 +870,16 @@
   public synchronized int finishedReduces() {
     return finishedReduceTasks;
   }
+  public synchronized int droppedReduces() {
+    return droppedMapTasks;
+  }
   public synchronized int pendingMaps() {
     return numMapTasks - runningMapTasks - failedMapTIPs - 
-    finishedMapTasks + speculativeMapTasks;
+    finishedMapTasks + droppedMapTasks + speculativeMapTasks;
   }
   public synchronized int pendingReduces() {
     return numReduceTasks - runningReduceTasks - failedReduceTIPs - 
-    finishedReduceTasks + speculativeReduceTasks;
+    finishedReduceTasks + droppedReduceTasks + speculativeReduceTasks;
   }
   
   /**
@@ -1057,7 +1077,6 @@
    */
   public synchronized void updateTaskStatus(TaskInProgress tip, 
                                             TaskStatus status) {
-
     double oldProgress = tip.getProgress();   // save old progress
     boolean wasRunning = tip.isRunning();
     boolean wasComplete = tip.isComplete();
@@ -1072,7 +1091,9 @@
     // User has requested to kill the task, but TT reported SUCCEEDED, 
     // mark the task KILLED.
     if ((wasComplete || tip.wasKilled(taskid)) && 
-        (status.getRunState() == TaskStatus.State.SUCCEEDED)) {
+        // Approximate Hadoop. we drop this task, so don't take it into account.
+        (status.getRunState() == TaskStatus.State.SUCCEEDED || status.getRunState() == TaskStatus.State.DROPPED) &&
+        !tip.wasDropped()) {
       status.setRunState(TaskStatus.State.KILLED);
     }
     
@@ -1090,6 +1111,7 @@
       }
     }
     
+    
     boolean change = tip.updateStatus(status);
     if (change) {
       TaskStatus.State state = status.getRunState();
@@ -1112,7 +1134,8 @@
       }
 
       TaskCompletionEvent taskEvent = null;
-      if (state == TaskStatus.State.SUCCEEDED) {
+      // Approximate Hadoop
+      if (state == TaskStatus.State.SUCCEEDED || state == TaskStatus.State.DROPPED) {
         taskEvent = new TaskCompletionEvent(
                                             taskCompletionEventTracker, 
                                             taskid,
@@ -1120,12 +1143,12 @@
                                             status.getIsMap() &&
                                             !tip.isJobCleanupTask() &&
                                             !tip.isJobSetupTask(),
-                                            TaskCompletionEvent.Status.SUCCEEDED,
+                                            state == TaskStatus.State.SUCCEEDED ? TaskCompletionEvent.Status.SUCCEEDED : TaskCompletionEvent.Status.DROPPED, // Approximate Hadoop
                                             httpTaskLogLocation 
                                            );
         taskEvent.setTaskRunTime((int)(status.getFinishTime() 
                                        - status.getStartTime()));
-        tip.setSuccessEventNumber(taskCompletionEventTracker); 
+        tip.setSuccessEventNumber(taskCompletionEventTracker);
       } else if (state == TaskStatus.State.COMMIT_PENDING) {
         // If it is the first attempt reporting COMMIT_PENDING
         // ask the task to commit.
@@ -1144,9 +1167,11 @@
         }
         // Remove the task entry from jobtracker
         jobtracker.removeTaskEntry(taskid);
-      }
+        
+        // Approximate Hadoop. We log this event in the job history.
+        droppedTask(tip, status);
       //For a failed task update the JT datastructures. 
-      else if (state == TaskStatus.State.FAILED ||
+      } else if (state == TaskStatus.State.FAILED ||
                state == TaskStatus.State.KILLED) {
         // Get the event number for the (possibly) previously successful
         // task. If there exists one, then set that status to OBSOLETE 
@@ -1195,7 +1220,7 @@
         if(ttStat != null) { // ttStat can be null in case of lost tracker
           ttStat.incrTotalTasks();
         }
-        if (state == TaskStatus.State.SUCCEEDED) {
+        if (state == TaskStatus.State.SUCCEEDED || state == TaskStatus.State.DROPPED) {
           completedTask(tip, status);
           if(ttStat != null) {
             ttStat.incrSucceededTasks();
@@ -1565,10 +1590,10 @@
     }
     // Check if all maps and reducers have finished.
     boolean launchCleanupTask = 
-        ((finishedMapTasks + failedMapTIPs) == (numMapTasks));
+        ((finishedMapTasks + droppedMapTasks + failedMapTIPs) == (numMapTasks));
     if (launchCleanupTask) {
       launchCleanupTask = 
-        ((finishedReduceTasks + failedReduceTIPs) == numReduceTasks);
+        ((finishedReduceTasks + droppedReduceTasks + failedReduceTIPs) == numReduceTasks);
     }
     return launchCleanupTask;
   }
@@ -1619,7 +1644,7 @@
   }
   
   public synchronized boolean scheduleReduces() {
-    return finishedMapTasks + failedMapTIPs >= completedMapsForReduceSlowstart;
+    return finishedMapTasks + droppedMapTasks + failedMapTIPs >= completedMapsForReduceSlowstart;
   }
   
   /**
@@ -2551,6 +2576,52 @@
   }
   
   /**
+   * Approximate Hadoop.
+   * We drop a task. This is just for the job history.
+   */
+  public synchronized boolean droppedTask(TaskInProgress tip, TaskStatus status) {
+    TaskAttemptID taskid = status.getTaskID();
+    // The problem is that Hadoop repeats the attempt id for the cleanup.
+    TaskAttemptID taskidDropped = new TaskAttemptID(taskid.getTaskID(), taskid.getId()+5);
+    if (tip.wasDropped() || tip.wasKilled()) {
+      String taskType = getTaskType(tip);
+      TaskTrackerStatus ttStatus = this.jobtracker.getTaskTrackerStatus(status.getTaskTracker());
+      String trackerHostname = jobtracker.getNode(ttStatus.getHost()).toString();
+      int trackerPort = ttStatus.getHttpPort();
+    
+      // Update jobhistory 
+      if (status.getIsMap()) {
+        JobHistory.MapAttempt.logStarted(taskidDropped, status.getStartTime(), 
+                                       tip.isApproximated(taskid), // Approximate Hadoop. Mark if this is approximated
+                                       status.getTaskTracker(),
+                                       trackerPort, 
+                                       taskType); 
+        JobHistory.MapAttempt.logFinished(taskidDropped, status.getFinishTime(), 
+                                        tip.isApproximated(taskid),  // Approximate Hadoop. Mark if this is approximated
+                                        true, // Approximate Hadoop. Mark if dropped.
+                                        trackerHostname, taskType,
+                                        status.getStateString(), 
+                                        status.getCounters()); 
+      } else {
+        JobHistory.ReduceAttempt.logStarted(taskidDropped, status.getStartTime(), 
+                                          status.getTaskTracker(),
+                                          trackerPort, 
+                                          taskType); 
+        JobHistory.ReduceAttempt.logFinished(taskidDropped, status.getShuffleFinishTime(),
+                                           status.getSortFinishTime(), status.getFinishTime(), 
+                                           trackerHostname, 
+                                           taskType,
+                                           status.getStateString(), 
+                                           status.getCounters()); 
+      }
+    } else {
+      LOG.error("We are dropping " + taskid + " but it's not true.");
+      return false;
+    }
+    return true;
+  }
+  
+  /**
    * A taskid assigned to this JobInProgress has reported in successfully.
    */
   public synchronized boolean completedTask(TaskInProgress tip, 
@@ -2559,7 +2630,7 @@
     TaskAttemptID taskid = status.getTaskID();
     int oldNumAttempts = tip.getActiveTasks().size();
     final JobTrackerInstrumentation metrics = jobtracker.getInstrumentation();
-        
+    
     // Metering
     meterTaskAttempt(tip, status);
     
@@ -2581,30 +2652,43 @@
     } 
 
     LOG.info("Task '" + taskid + "' has completed " + tip.getTIPId() + 
-             " successfully.");          
+             " successfully.");
+    
+//long oldStartime = tip.getTaskStatus(taskid).getStartTime();
+    
+    // Mark the TIP as complete
+    // tip.completed(taskid);
+    tip.completed(taskid, status.getRunState()); // Approximate Hadoop. Sometimes we need to specify that is dropped.
 
-    // Mark the TIP as complete
-    tip.completed(taskid);
     resourceEstimator.updateWithCompletedTask(status, tip);
+    
+    // Approximate Hadoop. If it is dropped we put dummy information.
+    String taskType = getTaskType(tip);
+    String trackerHostname = "dropped";
+    int trackerPort = -1;
+    if (!status.getTaskTracker().equals("dropped")) { 
+      TaskTrackerStatus ttStatus = this.jobtracker.getTaskTrackerStatus(status.getTaskTracker());
+      trackerHostname = jobtracker.getNode(ttStatus.getHost()).toString();
+      trackerPort = ttStatus.getHttpPort();
+    }
 
     // Update jobhistory 
-    TaskTrackerStatus ttStatus = 
-      this.jobtracker.getTaskTrackerStatus(status.getTaskTracker());
-    String trackerHostname = jobtracker.getNode(ttStatus.getHost()).toString();
-    String taskType = getTaskType(tip);
-    if (status.getIsMap()){
+    if (status.getIsMap()) {
       JobHistory.MapAttempt.logStarted(status.getTaskID(), status.getStartTime(), 
-                                       status.getTaskTracker(), 
-                                       ttStatus.getHttpPort(), 
+                                       tip.isApproximated(taskid), // Approximate Hadoop. Mark if this is approximated
+                                       status.getTaskTracker(),
+                                       trackerPort, 
                                        taskType); 
       JobHistory.MapAttempt.logFinished(status.getTaskID(), status.getFinishTime(), 
+                                        tip.isApproximated(taskid),  // Approximate Hadoop. Mark if this is approximated
+                                        status.getRunState() == TaskStatus.State.DROPPED, // Approximate Hadoop. Mark if dropped.
                                         trackerHostname, taskType,
                                         status.getStateString(), 
                                         status.getCounters()); 
-    }else{
-      JobHistory.ReduceAttempt.logStarted( status.getTaskID(), status.getStartTime(), 
+    } else {
+      JobHistory.ReduceAttempt.logStarted(status.getTaskID(), status.getStartTime(), 
                                           status.getTaskTracker(),
-                                          ttStatus.getHttpPort(), 
+                                          trackerPort, 
                                           taskType); 
       JobHistory.ReduceAttempt.logFinished(status.getTaskID(), status.getShuffleFinishTime(),
                                            status.getSortFinishTime(), status.getFinishTime(), 
@@ -2616,8 +2700,9 @@
     JobHistory.Task.logFinished(tip.getTIPId(), 
                                 taskType,
                                 tip.getExecFinishTime(),
+                                tip.wasDropped(), // Approximate Hadoop. Mark if dropped.
                                 status.getCounters()); 
-        
+
     int newNumAttempts = tip.getActiveTasks().size();
     if (tip.isJobSetupTask()) {
       // setup task has finished. kill the extra setup tip
@@ -2659,12 +2744,16 @@
       if (oldNumAttempts > 1) {
         speculativeMapTasks -= (oldNumAttempts - newNumAttempts);
       }
-      finishedMapTasks += 1;
+      if (status.getRunState() == TaskStatus.State.DROPPED) {
+        droppedMapTasks += 1;
+      } else {
+        finishedMapTasks += 1;
+      }
       metrics.completeMap(taskid);
       this.queueMetrics.completeMap(taskid);
       // remove the completed map from the resp running caches
       retireMap(tip);
-      if ((finishedMapTasks + failedMapTIPs) == (numMapTasks)) {
+      if ((finishedMapTasks + droppedMapTasks + failedMapTIPs) == (numMapTasks)) {
         this.status.setMapProgress(1.0f);
         if (canLaunchJobCleanupTask()) {
           checkCounterLimitsAndFail();
@@ -2675,12 +2764,16 @@
       if (oldNumAttempts > 1) {
         speculativeReduceTasks -= (oldNumAttempts - newNumAttempts);
       }
-      finishedReduceTasks += 1;
+      if (status.getRunState() == TaskStatus.State.DROPPED) {
+        finishedReduceTasks += 1;
+      } else {
+        finishedReduceTasks += 1;
+      }
       metrics.completeReduce(taskid);
       this.queueMetrics.completeReduce(taskid);
       // remove the completed reduces from the running reducers set
       retireReduce(tip);
-      if ((finishedReduceTasks + failedReduceTIPs) == (numReduceTasks)) {
+      if ((finishedReduceTasks + droppedReduceTasks + failedReduceTIPs) == (numReduceTasks)) {
         this.status.setReduceProgress(1.0f);
         if (canLaunchJobCleanupTask()) {
           checkCounterLimitsAndFail();
@@ -2760,7 +2853,7 @@
       this.finishTime = jobtracker.getClock().getTime();
       LOG.info("Job " + this.status.getJobID() + 
       " has completed successfully.");
-
+      
       // Log the job summary (this should be done prior to logging to 
       // job-history to ensure job-counters are in-sync 
       JobSummary.logJobSummary(this, jobtracker.getClusterStatus(false));
@@ -2782,7 +2875,6 @@
                                      failedReduceTasks, mapCounters,
                                      reduceCounters, jobCounters);
       
-      
       // Note that finalize will close the job history handles which garbage collect
       // might try to finalize
       garbageCollect();
@@ -3031,7 +3123,11 @@
         // (since they might have been removed from the cache of other 
         // racks/switches, if the input split blocks were present there too)
         failMap(tip);
-        finishedMapTasks -= 1;
+        if (tip.wasDropped()) {
+          droppedMapTasks -= 1;
+        } else {
+          finishedMapTasks -= 1;
+        }
       }
     }
         
@@ -3054,6 +3150,7 @@
     String taskType = getTaskType(tip);
     if (taskStatus.getIsMap()) {
       JobHistory.MapAttempt.logStarted(taskid, startTime, 
+        tip.isApproximated(taskid),
         taskTrackerName, taskTrackerPort, taskType);
       if (taskStatus.getRunState() == TaskStatus.State.FAILED) {
         JobHistory.MapAttempt.logFailed(taskid, finishTime,
@@ -3205,6 +3302,7 @@
     long startTime = oldStatus == null
                      ? jobtracker.getClock().getTime()
                      : oldStatus.getStartTime();
+                     
     status.setStartTime(startTime);
     status.setFinishTime(jobtracker.getClock().getTime());
     boolean wasComplete = tip.isComplete();
@@ -3320,7 +3418,7 @@
     if (tip.isComplete()) {
       TaskStatus[] statuses = tip.getTaskStatuses();
       for(int i=0; i < statuses.length; i++) {
-        if (statuses[i].getRunState() == TaskStatus.State.SUCCEEDED) {
+        if (statuses[i].getRunState() == TaskStatus.State.SUCCEEDED || statuses[i].getRunState() == TaskStatus.State.DROPPED) {
           return statuses[i];
         }
       }
Index: src/mapred/org/apache/hadoop/mapred/JobSubmissionProtocol.java
===================================================================
--- src/mapred/org/apache/hadoop/mapred/JobSubmissionProtocol.java	(revision 1596572)
+++ src/mapred/org/apache/hadoop/mapred/JobSubmissionProtocol.java	(working copy)
@@ -138,6 +138,13 @@
   public boolean killTask(TaskAttemptID taskId, boolean shouldFail) throws IOException;
   
   /**
+   * Approximate Hadoop.
+   * Drop indicated task attempt.
+   * @param taskId the id of the task to drop.  
+   */ 
+  public boolean dropTask(TaskAttemptID taskId) throws IOException;
+  
+  /**
    * Grab a handle to a job that is already known to the JobTracker.
    * @return Profile of the job, or null if not found. 
    */
Index: src/mapred/org/apache/hadoop/mapred/JobTracker.java
===================================================================
--- src/mapred/org/apache/hadoop/mapred/JobTracker.java	(revision 1596572)
+++ src/mapred/org/apache/hadoop/mapred/JobTracker.java	(working copy)
@@ -18,6 +18,7 @@
 package org.apache.hadoop.mapred;
 
 
+import java.io.File;
 import java.io.FileNotFoundException;
 import java.io.IOException;
 import java.io.PrintWriter;
@@ -51,6 +52,12 @@
 import java.util.concurrent.CopyOnWriteArrayList;
 import java.util.concurrent.atomic.AtomicBoolean;
 
+// Approximate Hadoop
+import java.util.Random;
+
+// Sleeping nodes
+import java.util.Scanner;
+
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.conf.Configuration;
@@ -473,7 +480,11 @@
           // for a task tracker.
           //
           Thread.sleep(TASKTRACKER_EXPIRY_INTERVAL / 3);
-
+          
+          // Agile Hadoop.
+          // Check the nodes that are sleeping before anything.
+          manageSleepingNodes();
+          
           //
           // Loop through all expired items in the queue
           //
@@ -488,17 +499,38 @@
             synchronized (taskTrackers) {
               synchronized (trackerExpiryQueue) {
                 long now = clock.getTime();
+ 
                 TaskTrackerStatus leastRecent = null;
                 while ((trackerExpiryQueue.size() > 0) &&
                        (leastRecent = trackerExpiryQueue.first()) != null &&
                        ((now - leastRecent.getLastSeen()) > TASKTRACKER_EXPIRY_INTERVAL)) {
-
                         
-                  // Remove profile from head of queue
-                  trackerExpiryQueue.remove(leastRecent);
-                  String trackerName = leastRecent.getTrackerName();
+			// Remove profile from head of queue and start again
+			trackerExpiryQueue.remove(leastRecent);
+			//boolean ok = trackerExpiryQueue.remove(leastRecent);
+			////////////////////////////////////////////////////////////////////
+			// Agile Hadoop
+			// This was a bug introduced by the sleeping nodes management
+			// The reason is we modify the values inside of the queue.
+			// The fix was to create a new status in manageSleepingNodes()
+			////////////////////////////////////////////////////////////////////
+			/*if (!ok) {
+				// This usually means the set is unsorted, we need to fix it
+				// Move all the list to a temporary location
+				LinkedList<TaskTrackerStatus> tmpTrackerExpiryQueue = new LinkedList<TaskTrackerStatus>();
+				while (trackerExpiryQueue.size() > 0) {
+					tmpTrackerExpiryQueue.add(trackerExpiryQueue.pollFirst());
+				}
+				// Put them back into the real queue
+				while (tmpTrackerExpiryQueue.size() > 0) {
+					trackerExpiryQueue.add(tmpTrackerExpiryQueue.pop());
+				}
+				continue;
+			}*/
+			////////////////////////////////////////////////////////////////////
                         
                   // Figure out if last-seen time should be updated, or if tracker is dead
+                  String trackerName = leastRecent.getTrackerName();
                   TaskTracker current = getTaskTracker(trackerName);
                   TaskTrackerStatus newProfile = 
                     (current == null ) ? null : current.getStatus();
@@ -507,15 +539,24 @@
                   // tracker has already been destroyed.
                   if (newProfile != null) {
                     if ((now - newProfile.getLastSeen()) > TASKTRACKER_EXPIRY_INTERVAL) {
-                      removeTracker(current);
-                      // remove the mapping from the hosts list
-                      String hostname = newProfile.getHost();
-                      hostnameToTaskTracker.get(hostname).remove(trackerName);
+                      // Agile Hadoop. If it's sleeping we keep it.
+                      if (!newProfile.isSleep()) {
+                        removeTracker(current);
+                        // remove the mapping from the hosts list
+                        String hostname = newProfile.getHost();
+                        hostnameToTaskTracker.get(hostname).remove(trackerName);
+                      } else {
+                        // Agile Hadoop. The node is sleeping, add a dummy one.
+                        newProfile = new TaskTrackerStatus(newProfile.getTrackerName(), newProfile.getHost());
+                        newProfile.setLastSeen(clock.getTime());
+                        trackerExpiryQueue.add(newProfile);
+                      }
                     } else {
                       // Update time by inserting latest profile
                       trackerExpiryQueue.add(newProfile);
                     }
                   }
+                  now = clock.getTime();
                 }
               }
             }
@@ -1709,6 +1750,13 @@
   private int totalMapTaskCapacity;
   private int totalReduceTaskCapacity;
   private HostsFileReader hostsReader;
+  ////////////////////////////////////////////////////////////////
+  // Agile Hadoop. File with the sleeping nodes.
+  ////////////////////////////////////////////////////////////////
+  private static String SLEEP_NODES_FILE = "conf/sleep_nodes";
+  private HostsFileReader sleepHostsReader;
+  private Long sleepHostsReaderLastModified;
+  ////////////////////////////////////////////////////////////////
   
   // JobTracker recovery variables
   private volatile boolean hasRestarted = false;
@@ -1778,6 +1826,13 @@
   // Number of resolved entries
   int numResolved;
 
+  /**
+   * Approximate Hadoop.
+   * This is the list of tasks that we have dropped.
+   */
+  Map<TaskID,List<TaskAttemptID>> droppedTasks = new HashMap<TaskID,List<TaskAttemptID>>();
+  Set<JobID> droppedJobs = new HashSet<JobID>();
+  
   // statistics about TaskTrackers with faults; may lead to graylisting
   private FaultyTrackersInfo faultyTrackers = new FaultyTrackersInfo();
   
@@ -2155,6 +2210,15 @@
     // Read the hosts/exclude files to restrict access to the jobtracker.
     this.hostsReader = new HostsFileReader(conf.get("mapred.hosts", ""),
                                            conf.get("mapred.hosts.exclude", ""));
+                                           
+    ////////////////////////////////////////////////////////////////
+    // Agile Hadoop
+    // File with the list of sleeping nodes
+    ////////////////////////////////////////////////////////////////
+    this.sleepHostsReader = new HostsFileReader(SLEEP_NODES_FILE,
+                                                SLEEP_NODES_FILE);
+    ////////////////////////////////////////////////////////////////
+    
     aclsManager = new ACLsManager(conf, new JobACLsManager(conf), queueManager);
 
     LOG.info("Starting jobtracker with owner as " +
@@ -3012,6 +3076,10 @@
    * Return the Node in the network topology that corresponds to the hostname
    */
   public Node getNode(String name) {
+    // Approximate Hadoop: node that doesn't exist
+    if (name.equals("dropped")) {
+      return new NodeBase("dropped", "/dropped");
+    }
     return hostnameToNodeMap.get(name);
   }
   public int getNumTaskCacheLevels() {
@@ -3148,7 +3216,7 @@
       return new HeartbeatResponse(newResponseId, 
                    new TaskTrackerAction[] {new ReinitTrackerAction()});
     }
-      
+    
     // Initialize the response to be sent for the heartbeat
     HeartbeatResponse response = new HeartbeatResponse(newResponseId, null);
     List<TaskTrackerAction> actions = new ArrayList<TaskTrackerAction>();
@@ -3162,6 +3230,163 @@
         List<Task> tasks = getSetupAndCleanupTasks(taskTrackerStatus);
         if (tasks == null ) {
           tasks = taskScheduler.assignTasks(taskTrackers.get(trackerName));
+          
+          /**
+           * Approximate Hadoop. Main block for Approximate Hadoop.
+           * 1) Don't resubmit dropped tasks.
+           * 2) Select maps to approximate.
+           * 3) Make reduce know about dropped maps.
+           */
+          // If we have killed a map, don't resubmit it
+          for (Iterator<Task> it = tasks.iterator(); it.hasNext(); ) {
+            Task task = it.next();
+            TaskAttemptID attemptId = task.getTaskID();
+            TaskID taskId = attemptId.getTaskID();
+            if (droppedTasks.containsKey(taskId)) {
+              LOG.error("We dropped task '" + taskId + "'. We don't resubmit it.");
+              it.remove();
+            }
+          }
+          // Select maps to approximate to fulfill the constraint
+          for (Task task : tasks) {
+            if (task.isMapOrReduce() && task.isMapTask()) { // TODO right now everything is maps
+              // Check if we approximate the map
+              JobID jobId = task.getJobID();
+              JobInProgress job = getJob(jobId);
+              JobConf jobConf = job.getJobConf();
+              float approxLevel = jobConf.getFloat("mapred.map.approximate", 0);
+              // Check how many approximations we have right now for the job
+              double auxApprox  = 0.0;
+              double auxPending = 0.0;
+              double auxTotal   = 0.0;
+              for (TaskInProgress tip : job.getTasks(TaskType.MAP)) {
+                auxTotal++;
+                if (tip.isApproximated()) {
+                  auxApprox++;
+		}
+		if (!tip.isRunning() && !tip.isComplete()) {
+		  auxPending++;
+		}
+              }
+              // The ones currently being scheduled also count as pending
+              for (Task schedtask : tasks) {
+                if (schedtask.getTaskID().getJobID().equals(jobId)) {
+                  auxPending++;
+                }
+              }
+              // We discard the ones we have already scheduled
+              auxPending -= tasks.indexOf(task); // This has the problem of multiple jobs being scheduled...
+              // Calculate how much we should approximate
+              double numMapApprox = Math.ceil(auxTotal * approxLevel);
+              double approxLeft = 0.0;
+              if (auxPending > 0) {
+                approxLeft = (numMapApprox-auxApprox)/(auxPending);
+              }
+              
+              // Approximate with a X% probability
+              Random randomGenerator = new Random();
+              if (randomGenerator.nextDouble() >= (1.0 - approxLeft)) {
+                TaskAttemptID attemptId = task.getTaskID();
+                TaskInProgress tip = taskidToTIPMap.get(attemptId);
+                LOG.info("We decided to approximate map " + attemptId);
+                task.setApproximate(true);
+                tip.addApproximation(attemptId);
+                tip.addDiagnosticInfo(attemptId, "Approximation.");
+              }
+            }
+          }
+          
+          // Check if this task is a speculation, we always approximate it
+          for (Task task : tasks) {
+            if (task.isMapOrReduce() && task.isMapTask()) {
+              JobID jobId = task.getJobID();
+              JobConf jobConf = getJob(jobId).getJobConf();
+              boolean approxSpeculative = jobConf.getBoolean("mapred.map.tasks.speculative.execution.approximate", false);
+              if (approxSpeculative && jobConf.getMapSpeculativeExecution()) {
+                TaskAttemptID attemptId = task.getTaskID();
+                TaskInProgress tip = taskidToTIPMap.get(attemptId);
+                TreeMap<TaskAttemptID, String> otherAttempts = tip.getActiveTasks();
+                if (otherAttempts.size() > 1) {
+                  // TODO we may want to improve the way we select tasks to speculate for later approximation
+                  LOG.error("Launch a speculative task '"+attemptId+"' for task "+attemptId.getTaskID());
+                  for (TaskAttemptID otherAttemptId : otherAttempts.keySet()) {
+                    LOG.error("  The other attempts are: " + otherAttemptId);
+                  }
+                  task.setApproximate(true);
+                  tip.addApproximation(attemptId);
+                }
+              }
+            }
+          }
+          
+          // Reduce tasks need to know if we dropped maps
+          /*for (Task task : tasks) {
+            if (task.isMapOrReduce() && task instanceof ReduceTask) {
+              boolean dropped = false;
+              // Check if we have dropped tasks
+              ReduceTask redTask = (ReduceTask) task;
+              JobID jobId = task.getJobID();
+              JobInProgress job = getJob(jobId);
+              for (TaskInProgress tip : job.maps) {
+                TaskID taskId = tip.getTIPId();
+                if (droppedTasks.containsKey(taskId)) {
+                  // Add dropped tasks to reduce
+                  for (TaskAttemptID attemptId : droppedTasks.get(taskId)) {
+                    LOG.info("Reduce '" + redTask.getTaskID() + "' has a dropped map: " + attemptId);
+                    // This makes the reduce know that it shouldn't look for this map output
+                    // It also reduces the number of maps to wait for
+                    //redTask.addDroppedMap(attemptId);
+                    dropped = true;
+                  }
+                }
+              }
+              if (!dropped) {
+                LOG.info("Reduce '" + redTask.getTaskID() + "' has no dropped maps");
+              }
+              
+              // Check if we can make the reducer skip some of the maps
+              JobConf jobConf = getJob(jobId).getJobConf();
+              int dropMaps = jobConf.getInt("mapred.reduce.approximate.drop.maps", 0);
+              if (dropMaps > 0) {
+                int newNumMaps = redTask.getNumMaps() - dropMaps;
+                if (newNumMaps < 0) {
+                  newNumMaps = 0;
+                }
+                redTask.setNumMaps(newNumMaps);
+              }
+            }
+          }*/
+          
+          // Check if we will use an old output for this map
+          for (Iterator<Task> it = tasks.iterator(); it.hasNext(); ) {
+            Task task = it.next();
+            if (task.isMapOrReduce() && task.isMapTask()) {
+              TaskAttemptID attemptId = task.getTaskID();
+              TaskInProgress tip = taskidToTIPMap.get(attemptId);
+              TaskID taskId = tip.getTIPId();
+              JobID jobId = task.getJobID();
+              JobInProgress job = getJob(jobId);
+              JobConf jobConf = getJob(jobId).getJobConf();
+              String previousOutput = jobConf.get("mapred.map."+taskId.getId()+".previous.output");
+              if (previousOutput != null) {
+                LOG.info(taskId + " will use previous output. Don't start it.");
+                // Mark as dropped
+                tip.setDropped();
+                
+                // We change the task status to succeeded without even starting it
+                TaskStatus completedStatus = TaskStatus.createTaskStatus(tip.isMapTask(), attemptId, 1.0f, 0, TaskStatus.State.DROPPED,
+                                                "Output already available. Task dropped before starting.", "not started", "dropped", TaskStatus.Phase.MAP, new Counters());
+                
+                completedStatus.setStartTime(clock.getTime());
+                completedStatus.setFinishTime(clock.getTime());
+                job.updateTaskStatus(tip, completedStatus);
+                  
+                // Don't submit the task
+                it.remove();
+              }
+            }
+          }
+          /** Approximate Hadoop. Main block finished. */
         }
         if (tasks != null) {
           for (Task task : tasks) {
@@ -3169,12 +3394,22 @@
             if(LOG.isDebugEnabled()) {
               LOG.debug(trackerName + " -> LaunchTask: " + task.getTaskID());
             }
+            
             actions.add(new LaunchTaskAction(task));
           }
         }
       }
     }
-      
+    
+    // Approximate Hadoop.
+    // Check for tasks to be dropped.
+    // This is always empty, it just triggers the mechanisms for the killer.
+    /*List<TaskTrackerAction> dropTasksList = getTasksToDrop(trackerName);
+    if (dropTasksList != null) {
+      actions.addAll(dropTasksList);
+    }*/
+    updateTasksToDrop(trackerName);
+    
     // Check for tasks to be killed
     List<TaskTrackerAction> killTasksList = getTasksToKill(trackerName);
     if (killTasksList != null) {
@@ -3196,8 +3431,7 @@
     // calculate next heartbeat interval and put in heartbeat response
     int nextInterval = getNextHeartbeatInterval();
     response.setHeartbeatInterval(nextInterval);
-    response.setActions(
-                        actions.toArray(new TaskTrackerAction[actions.size()]));
+    response.setActions(actions.toArray(new TaskTrackerAction[actions.size()]));
     
     // check if the restart info is req
     if (addRestartInfo) {
@@ -3408,7 +3642,6 @@
                                  TaskTrackerStatus trackerStatus, 
                                  boolean initialContact,
                                  long timeStamp) throws UnknownHostException {
-
     getInstrumentation().heartbeat();
 
     String trackerName = trackerStatus.getTrackerName();
@@ -3494,8 +3727,190 @@
     }
     return killList;
   }
-
+  
   /**
+   * Approximate Hadoop.
+   * Check if there is any task that should be dropped.
+   */
+  private synchronized void updateTasksToDrop(String taskTracker) {
+    // Check the running jobs
+    for (JobInProgress job : jobs.values()) {
+      if (job.getStatus().getRunState() == JobStatus.RUNNING && !droppedJobs.contains(job.getJobID())) {
+        boolean startDropping = false;
+      
+        // Check if the job can start dropping tasks
+        JobConf jobConf = job.getJobConf();
+        float   approxDropPerc = jobConf.getFloat("mapred.map.approximate.drop.percentage", 1.0f); // Percentage of tasks to start dropping: 0%
+        int     approxDropTime = jobConf.getInt("mapred.map.approximate.drop.extratime", 0); // Extra seconds to wait: 10s
+        boolean approxDropHard = jobConf.getBoolean("mapred.map.approximate.drop.hard", false); // Do we kill the running jobs? By default we don't kill running tasks
+        
+        if (approxDropPerc < 1) {
+          // Check how many tasks are already completed
+          float completedTasks = 0;
+          if (job.desiredMaps() > 0) {
+            completedTasks = ((float) job.finishedMaps()) / ((float) job.desiredMaps());
+          }
+          // If we have enough tasks to start dropping, store the actual dropping time
+          if (completedTasks < 1 && completedTasks > approxDropPerc && job.startDropTime < 0) {
+            // We can start dropping tasks after a period
+            job.startDropTime = clock.getTime() + approxDropTime*1000;
+          }
+          // Check if the time to start dropping has come
+          if (job.startDropTime > 0 && clock.getTime() > job.startDropTime) {
+            LOG.error("Job " + job.getJobID() + " has crossed the maps threshold. Start dropping!");
+            startDropping = true;
+          }
+        }
+        
+        // Check if the reducers can drop maps. If they can and they are done, we can drop the maps
+        int dropMaps =       jobConf.getInt("mapred.reduce.approximate.drop.maps", 0);
+        float dropMapsPerc = jobConf.getFloat("mapred.reduce.approximate.drop.maps.percentage", 1.0f);
+        if ((dropMaps > 0 || dropMapsPerc < 1) && !startDropping && job.reduces.length > 0) {
+          boolean allComplete = true;
+          for (TaskInProgress tip : job.reduces) {
+            if (!tip.isComplete()) {
+              allComplete = false;
+              break;
+            }
+          }
+          if (allComplete) {
+            LOG.error("All the reducers in " + job.getJobID() + " are done. We can drop all the tasks!");
+            startDropping = true;
+          }
+        }
+        
+        // Check if all the reduces say that we can start dropping
+        if (!startDropping && job.reduces.length > 0) {
+          boolean allReducesDropping = true;
+          for (TaskInProgress tip : job.reduces) {
+            boolean isReduceDropping = false;
+            // Check all the attempts of the reducer
+            for (TaskStatus status : tip.getTaskStatuses()) {
+              // The reducer sets the state string to dropping
+              if (status.getStateString().startsWith("dropping")) {
+                isReduceDropping = true;
+                break;
+              }
+            }
+            if (!isReduceDropping) {
+              allReducesDropping = false;
+            }
+          }
+          // If all the reduces for this job say that we can drop, we drop
+          if (allReducesDropping) {
+            startDropping = true;
+          }
+        }
+        
+        // If the job should already start dropping, let's do it!
+        if (startDropping) {
+          // Mark it as dropped
+          droppedJobs.add(job.getJobID());
+          // Check each one of the maps
+          for (TaskInProgress tip : job.maps) {
+            // If the task is not complete and is not already dropped
+            if (!tip.isComplete() && !droppedTasks.containsKey(tip.getTIPId())) {
+              // Check if killing currently running maps
+              if (approxDropHard) {
+                // Check the attempts status
+                for (TaskAttemptID attemptId : tip.getAllTaskAttemptIDs()) {
+                  try {
+                    // The task is running so we have to drop it
+                    LOG.info(attemptId + " was running... drop it!");
+                    dropTask(attemptId);
+                  } catch (Exception e) {
+                    LOG.error("Cannot drop task: " + StringUtils.stringifyException(e));
+                  }
+                }
+              }
+              // A task with no attempts, drop it before starting
+              if (tip.getAllTaskAttemptIDs().length == 0) {
+                try {
+                  LOG.error("Task " + tip.getTIPId() + " had no attempts. Mark it as completed.");
+                  dropNonStartedTask(job, tip);
+                  /*TaskAttemptID attemptId = new TaskAttemptID(tip.getTIPId(), 0);
+                  tip.addRunningTask(attemptId, "dropped", false);
+                  tip.setDropped();
+                 
+                  // Add task to the list of dropped tasks.
+                  if (!droppedTasks.containsKey(attemptId.getTaskID())) {
+                    droppedTasks.put(attemptId.getTaskID(), new LinkedList<TaskAttemptID>());
+                  }
+                  // If the attempt wasn't already marked as dropped, do it
+                  if (!droppedTasks.get(attemptId.getTaskID()).contains(attemptId)) {
+                    droppedTasks.get(attemptId.getTaskID()).add(attemptId);
+                  }
+                  // We change the task status to succeeded without even starting it
+                  TaskStatus completedStatus = TaskStatus.createTaskStatus(tip.isMapTask(), attemptId, 1.0f, 0, TaskStatus.State.DROPPED,
+                                                  "Task dropped before starting.", "not started", "dropped", TaskStatus.Phase.MAP, new Counters());
+                  long now = clock.getTime();
+                  completedStatus.setStartTime(now);
+                  completedStatus.setFinishTime(now);
+                  job.updateTaskStatus(tip, completedStatus);*/
+                } catch (Exception e) {
+                  LOG.error("Trying to drop task "+tip.getTIPId()+" before starting.");
+                }
+              }
+            }
+          }
+        }
+        
+        // Check if we should drop before at the beginning
+        float approxDropIniPerc = jobConf.getFloat("mapred.map.approximate.drop.ini.percentage", 0.0f); // Percentage of tasks to start dropping: 0%
+        if (approxDropIniPerc > 0) {
+          int numMapstoDrop = (int) Math.ceil(1.0*approxDropIniPerc*job.maps.length);
+         // Check how many are already dropped
+          for (TaskInProgress tip : job.maps) {
+            if (droppedTasks.containsKey(tip.getTIPId())) {
+              numMapstoDrop--;
+            }
+          }
+          // We still have maps to drop, let's go
+          for (int i=numMapstoDrop*3+1; numMapstoDrop>0 && i>0; i++) {
+            LOG.info("We need to initially drop " + numMapstoDrop + " maps");
+            Random randomGenerator = new Random();
+            while (numMapstoDrop > 0) {
+              // Pick a random map
+              int numMap = randomGenerator.nextInt(job.maps.length);
+              TaskInProgress tip = job.maps[numMap];
+              if (!tip.wasDropped()) {
+                LOG.info("Initially dropping "+ tip.getTIPId());
+                dropNonStartedTask(job, tip);
+                numMapstoDrop--;
+              }
+            }
+          }
+        }
+      }
+    }
+  }
+  
+  /**
+   * Approximate Hadoop. Function to drop a task that haven't started.
+   */
+  private void dropNonStartedTask(JobInProgress job, TaskInProgress tip) {
+    TaskAttemptID attemptId = new TaskAttemptID(tip.getTIPId(), 0);
+    tip.addRunningTask(attemptId, "dropped", false);
+    tip.setDropped();
+                 
+    // Add task to the list of dropped tasks.
+    if (!droppedTasks.containsKey(attemptId.getTaskID())) {
+      droppedTasks.put(attemptId.getTaskID(), new LinkedList<TaskAttemptID>());
+    }
+    // If the attempt wasn't already marked as dropped, do it
+    if (!droppedTasks.get(attemptId.getTaskID()).contains(attemptId)) {
+      droppedTasks.get(attemptId.getTaskID()).add(attemptId);
+    }
+    // We change the task status to succeeded without even starting it
+    TaskStatus completedStatus = TaskStatus.createTaskStatus(tip.isMapTask(), attemptId, 1.0f, 0, TaskStatus.State.DROPPED,
+                  "Task dropped before starting.", "not started", "dropped", TaskStatus.Phase.MAP, new Counters());
+    long now = clock.getTime();
+    completedStatus.setStartTime(now);
+    completedStatus.setFinishTime(now);
+    job.updateTaskStatus(tip, completedStatus);
+  }
+  
+  /**
    * Add a job to cleanup for the tracker.
    */
   private void addJobForCleanup(JobID id) {
@@ -3605,8 +4020,17 @@
         for (Iterator<JobInProgress> it = jobs.values().iterator();
              it.hasNext();) {
           JobInProgress job = it.next();
-          t = job.obtainJobSetupTask(taskTracker, numTaskTrackers,
-                                  numUniqueHosts, true);
+		////////////////////////////////////////////////////////////
+		// Agile Hadoop
+		// Check if we can actually run this job here
+		////////////////////////////////////////////////////////////
+		if (taskScheduler.runJobInTracker(job, taskTracker.getHost())) {
+			t = job.obtainJobSetupTask(taskTracker, numTaskTrackers, numUniqueHosts, true);
+		}
+		////////////////////////////////////////////////////////////
+		//t = job.obtainJobSetupTask(taskTracker, numTaskTrackers,
+		//                        numUniqueHosts, true);
+		////////////////////////////////////////////////////////////
           if (t != null) {
             return Collections.singletonList(t);
           }
@@ -3633,8 +4057,16 @@
         for (Iterator<JobInProgress> it = jobs.values().iterator();
              it.hasNext();) {
           JobInProgress job = it.next();
-          t = job.obtainJobSetupTask(taskTracker, numTaskTrackers,
-                                    numUniqueHosts, false);
+		////////////////////////////////////////////////////////////
+		// Agile Hadoop
+		// Check if we can actually run this job here
+		////////////////////////////////////////////////////////////
+		if (taskScheduler.runJobInTracker(job, taskTracker.getHost())) {
+			t = job.obtainJobSetupTask(taskTracker, numTaskTrackers, numUniqueHosts, false);
+		}
+		////////////////////////////////////////////////////////////
+          //t = job.obtainJobSetupTask(taskTracker, numTaskTrackers,
+          //                          numUniqueHosts, false);
           if (t != null) {
             return Collections.singletonList(t);
           }
@@ -3785,6 +4217,7 @@
         failJob(job);
         throw ioe;
       }
+      
       return status;
     }
   }
@@ -4029,7 +4462,7 @@
       job.getStatus().setFailureInfo(failureInfo);
       failJob(job);
     }
-	 }
+  }
 
   /**
    * Fail a job and inform the listeners. Other components in the framework 
@@ -4379,7 +4812,7 @@
       aclsManager.checkAccess(tip.getJob(),
           UserGroupInformation.getCurrentUser(),
           shouldFail ? Operation.FAIL_TASK : Operation.KILL_TASK);
-
+      
       return tip.killTask(taskid, shouldFail);
     }
     else {
@@ -4389,6 +4822,37 @@
   }
   
   /**
+   * Approximate Hadoop.
+   * Extension to offer the interface to drop tasks. This is an extension to killTask.
+   */
+  public synchronized boolean dropTask(TaskAttemptID attemptId) throws IOException {
+    // No 'dropTask' in safe-mode
+    checkSafeMode();
+
+    TaskInProgress tip = taskidToTIPMap.get(attemptId);
+    if(tip != null) {
+      // Check both queue-level and job-level access
+      aclsManager.checkAccess(tip.getJob(), UserGroupInformation.getCurrentUser(), Operation.KILL_TASK);
+      
+      // Add task to the list of dropped tasks.
+      if (!droppedTasks.containsKey(attemptId.getTaskID())) {
+        droppedTasks.put(attemptId.getTaskID(), new LinkedList<TaskAttemptID>());
+      }
+      // If the attempt wasn't already dropped
+      if (!droppedTasks.get(attemptId.getTaskID()).contains(attemptId)) {
+        droppedTasks.get(attemptId.getTaskID()).add(attemptId);
+      }
+      
+      // Actually dropping the task.
+      return tip.dropTask(attemptId);
+    }
+    else {
+      LOG.info("Drop task attempt failed since task " + attemptId + " was not found");
+      return false;
+    }
+  }
+  
+  /**
    * Get tracker name for a given task id.
    * @param taskId the name of the task
    * @return The name of the task tracker
@@ -4543,6 +5007,31 @@
           job.addRunningTaskToTIP(tip, taskId, status, false);
         }
         
+        // Approximate Hadoop
+        // If we have dropped a task, we have to take care about its status updates
+        if (droppedTasks.containsKey(taskId.getTaskID())) {
+          if (report.getRunState() == TaskStatus.State.RUNNING) {
+            try {
+              LOG.info("Task " + taskId + " should be dropped but it's still running. Re-drop it!");
+              dropTask(taskId);
+            } catch (IOException e) {
+              LOG.error("Error trying to drop a task that was supposed to be dropped.");
+            }
+          } else if (report.getRunState() == TaskStatus.State.SUCCEEDED) {
+            tip.addDiagnosticInfo(taskId, "Task succeeded before dropping. Drop it anyway.");
+            report.setRunState(TaskStatus.State.DROPPED);
+            report.setStateString("dropped");
+          } else if (report.getRunState() == TaskStatus.State.KILLED) {
+            // KILLED -> DROPPED
+            report.setRunState(TaskStatus.State.DROPPED);
+            report.setStateString("dropped");
+            // Add a message to notify is completely dropped.
+            tip.addDiagnosticInfo(taskId, "Task dropped successfully.");
+          } else if (report.getRunState() != TaskStatus.State.KILLED_UNCLEAN) {
+            LOG.error("A dropped task "+taskId+" is reporting an unknown state: " + report.getRunState());
+          }
+        }
+        
         // Update the job and inform the listeners if necessary
         JobStatus prevStatus = (JobStatus)job.getStatus().clone();
         // Clone TaskStatus object here, because JobInProgress
@@ -4709,6 +5198,9 @@
       }
     }
     decommissionNodes(excludeSet);
+    
+    // Agile Hadoop. Force the refresh.
+    manageSleepingNodes();
   }
 
   // Assumes JobTracker, taskTrackers and trackerExpiryQueue is locked on entry
@@ -5275,4 +5767,83 @@
       }
     }
   }
+  
+	/**
+	 * Agile Hadoop.
+	 * Update the information for nodes that are sleeping.
+	 * Sometimes we restart sleeping nodes, so may keep an old duplicated copy of a tascktracker.
+	 * We remove the old one.
+	 */
+	private synchronized void manageSleepingNodes() throws IOException {
+		synchronized (JobTracker.this) {
+			synchronized (taskTrackers) {
+				synchronized (trackerExpiryQueue) {
+					// Check if we have duplicated trackers
+					List<String> allTrackers = new LinkedList<String>();
+					List<String> duplicatedTrackers = new LinkedList<String>();
+					for (String taskTrackerName : taskTrackers.keySet()) {
+						TaskTracker taskTracker = getTaskTracker(taskTrackerName);
+						String trackerHostname = taskTracker.getStatus().getHost();
+						if (!allTrackers.contains(trackerHostname)) {
+							allTrackers.add(trackerHostname);
+						} else if (!duplicatedTrackers.contains(trackerHostname)) {
+							// If we have a duplicated host, we don't update and we let it expire
+							LOG.error("We have a duplicated tracker: " + trackerHostname);
+							duplicatedTrackers.add(trackerHostname);
+						}
+					}
+					
+					// Check old sleeping nodes
+					checkSleepingHosts();
+					Set<String> sleepingTrackers = sleepHostsReader.getHosts();
+					
+					// Keep updating the heartbeat of sleeping hosts		
+					if (sleepingTrackers != null) {
+						// Check all the nodes
+						for (String taskTrackerName : taskTrackers.keySet()) {
+							TaskTracker taskTracker = getTaskTracker(taskTrackerName);
+							String trackerHostname = taskTracker.getStatus().getHost();
+							if (sleepingTrackers.contains(trackerHostname) && !duplicatedTrackers.contains(trackerHostname)) {
+								// This host is sleeping
+								taskTracker.getStatus().setSleep(true);
+							} else {
+								// If this is a duplicated host, we let it expire
+								taskTracker.getStatus().setSleep(false);
+							}
+						}
+					}
+				}
+			}
+		}
+	}
+	
+	/**
+	 * Check the file with the sleeping hosts.
+	 */
+	private void checkSleepingHosts() {
+		checkSleepingHosts(false);
+	}
+	
+	/**
+	 * Check if the file with the sleeping hosts has changed.
+	 * We can also force it using @force.
+	 */
+	private void checkSleepingHosts(boolean force) {
+		boolean change = false;
+		try {
+			if (force || sleepHostsReaderLastModified == null || (new File(SLEEP_NODES_FILE).lastModified() > sleepHostsReaderLastModified)) {
+				// Update the file
+				if (sleepHostsReader == null) {
+					sleepHostsReader = new HostsFileReader(SLEEP_NODES_FILE, SLEEP_NODES_FILE);
+					sleepHostsReaderLastModified = new File(SLEEP_NODES_FILE).lastModified();
+				}
+				sleepHostsReader.updateFileNames(SLEEP_NODES_FILE, SLEEP_NODES_FILE);
+				sleepHostsReader.refresh();
+				sleepHostsReaderLastModified = new File(SLEEP_NODES_FILE).lastModified();
+				change = true;
+			}
+		} catch (IOException e) {
+			LOG.error("Updating the list of sleeping hosts.");
+		}
+	}
 }
Index: src/mapred/org/apache/hadoop/mapred/LineRecordReader.java
===================================================================
--- src/mapred/org/apache/hadoop/mapred/LineRecordReader.java	(revision 1596572)
+++ src/mapred/org/apache/hadoop/mapred/LineRecordReader.java	(working copy)
@@ -45,11 +45,11 @@
     = LogFactory.getLog(LineRecordReader.class.getName());
 
   private CompressionCodecFactory compressionCodecs = null;
-  private long start;
-  private long pos;
-  private long end;
-  private LineReader in;
-  int maxLineLength;
+  protected long start;
+  protected long pos;
+  protected long end;
+  protected LineReader in;
+  protected int maxLineLength;
   private Seekable filePosition;
   private CompressionCodec codec;
   private Decompressor decompressor;
@@ -118,13 +118,13 @@
     return (codec != null);
   }
 
-  private int maxBytesToConsume(long pos) {
+  protected int maxBytesToConsume(long pos) {
     return isCompressedInput()
       ? Integer.MAX_VALUE
       : (int) Math.min(Integer.MAX_VALUE, end - pos);
   }
 
-  private long getFilePosition() throws IOException {
+  protected long getFilePosition() throws IOException {
     long retVal;
     if (isCompressedInput() && null != filePosition) {
       retVal = filePosition.getPos();
Index: src/mapred/org/apache/hadoop/mapred/LocalJobRunner.java
===================================================================
--- src/mapred/org/apache/hadoop/mapred/LocalJobRunner.java	(revision 1596572)
+++ src/mapred/org/apache/hadoop/mapred/LocalJobRunner.java	(working copy)
@@ -454,6 +454,11 @@
     throw new UnsupportedOperationException("Killing tasks in " +
     "LocalJobRunner is not supported");
   }
+  
+  /** Approximate Hadoop. Throws {@link UnsupportedOperationException} */
+  public boolean dropTask(TaskAttemptID taskId) throws IOException {
+    throw new UnsupportedOperationException("Dropping tasks in LocalJobRunner is not supported");
+  }
 
   public JobProfile getJobProfile(JobID id) {
     Job job = jobs.get(id);
Index: src/mapred/org/apache/hadoop/mapred/MapRunnable.java
===================================================================
--- src/mapred/org/apache/hadoop/mapred/MapRunnable.java	(revision 1596572)
+++ src/mapred/org/apache/hadoop/mapred/MapRunnable.java	(working copy)
@@ -45,4 +45,9 @@
   void run(RecordReader<K1, V1> input, OutputCollector<K2, V2> output,
            Reporter reporter)
     throws IOException;
+  
+  /**
+   * Approximate Hadoop.
+   */
+  void setApproximate(boolean approximate);
 }
Index: src/mapred/org/apache/hadoop/mapred/MapRunner.java
===================================================================
--- src/mapred/org/apache/hadoop/mapred/MapRunner.java	(revision 1596572)
+++ src/mapred/org/apache/hadoop/mapred/MapRunner.java	(working copy)
@@ -20,6 +20,9 @@
 
 import java.io.IOException;
 
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+
 import org.apache.hadoop.util.ReflectionUtils;
 
 /** Default {@link MapRunnable} implementation.*/
@@ -28,7 +31,12 @@
   
   private Mapper<K1, V1, K2, V2> mapper;
   private boolean incrProcCount;
+  
+  // Approximate Hadoop
+  private boolean approximate = false;
 
+  public static final Log LOG = LogFactory.getLog(MapRunner.class);
+  
   @SuppressWarnings("unchecked")
   public void configure(JobConf job) {
     this.mapper = ReflectionUtils.newInstance(job.getMapperClass(), job);
@@ -35,8 +43,21 @@
     //increment processed counter only if skipping feature is enabled
     this.incrProcCount = SkipBadRecords.getMapperMaxSkipRecords(job)>0 && 
       SkipBadRecords.getAutoIncrMapperProcCount(job);
+    // Approximate Hadoop
+    LOG.info(job);
   }
 
+  /**
+   * Approximate Hadoop
+   */
+  public void setApproximate(boolean approximate) {
+    this.approximate = approximate;
+  }
+  
+  public boolean isApproximate() {
+    return this.approximate;
+  }
+  
   public void run(RecordReader<K1, V1> input, OutputCollector<K2, V2> output,
                   Reporter reporter)
     throws IOException {
@@ -47,7 +68,15 @@
       
       while (input.next(key, value)) {
         // map pair to output
-        mapper.map(key, value, output, reporter);
+        // Approximate Hadoop: if we have to approximate and we can, do it
+        if (this.approximate && mapper instanceof ApproximateMapper) {
+          ((ApproximateMapper<K1, V1, K2, V2>) mapper).mapApproximate(key, value, output, reporter);
+        } else {
+          /*if (this.approximate) {
+            ((PipeMapper) mapper).setApproximate(true);
+          }*/
+          mapper.map(key, value, output, reporter);
+        }
         if(incrProcCount) {
           reporter.incrCounter(SkipBadRecords.COUNTER_GROUP, 
               SkipBadRecords.COUNTER_MAP_PROCESSED_RECORDS, 1);
Index: src/mapred/org/apache/hadoop/mapred/MapTask.java
===================================================================
--- src/mapred/org/apache/hadoop/mapred/MapTask.java	(revision 1596572)
+++ src/mapred/org/apache/hadoop/mapred/MapTask.java	(working copy)
@@ -76,6 +76,9 @@
 import org.apache.hadoop.util.ReflectionUtils;
 import org.apache.hadoop.util.StringUtils;
 
+// Approximate Hadoop extension
+import org.apache.hadoop.mapreduce.SamplingRecordReader;
+
 /** A Map task. */
 class MapTask extends Task {
   /**
@@ -420,7 +423,6 @@
         new TrackedRecordReader<INKEY,INVALUE>(inputSplit, job, reporter);
     job.setBoolean("mapred.skip.on", isSkipping());
 
-
     int numReduceTasks = conf.getNumReduceTasks();
     LOG.info("numReduceTasks: " + numReduceTasks);
     MapOutputCollector collector = null;
@@ -429,9 +431,15 @@
     } else { 
       collector = new DirectMapOutputCollector(umbilical, job, reporter);
     }
+    
+    // Approximate Hadoop: setting for tasks that do it in configure step
+    job.setBoolean("map.approximate", super.isApproximate());
     MapRunnable<INKEY,INVALUE,OUTKEY,OUTVALUE> runner =
       ReflectionUtils.newInstance(job.getMapRunnerClass(), job);
 
+    // Approximate Hadoop: define if we have to run the approximate computation
+    runner.setApproximate(super.isApproximate());
+    
     try {
       runner.run(in, new OldOutputCollector(collector, conf), reporter);
       collector.flush();
@@ -457,7 +465,9 @@
   }
 
   static class NewTrackingRecordReader<K,V> 
-    extends org.apache.hadoop.mapreduce.RecordReader<K,V> {
+    extends org.apache.hadoop.mapreduce.RecordReader<K,V>
+    // Approximate Hadoop
+    implements SamplingRecordReader {
     private final org.apache.hadoop.mapreduce.RecordReader<K,V> real;
     private final org.apache.hadoop.mapreduce.Counter inputRecordCounter;
     private final org.apache.hadoop.mapreduce.Counter fileInputByteCounter;
@@ -469,7 +479,8 @@
     NewTrackingRecordReader(org.apache.hadoop.mapreduce.InputSplit split,
         org.apache.hadoop.mapreduce.InputFormat inputFormat,
         TaskReporter reporter, JobConf job,
-        org.apache.hadoop.mapreduce.TaskAttemptContext taskContext)
+        //org.apache.hadoop.mapreduce.TaskAttemptContext taskContext)
+        org.apache.hadoop.mapreduce.TaskInputOutputContext taskContext)
         throws IOException, InterruptedException {
       this.reporter = reporter;
       this.inputSplit = split;
@@ -553,6 +564,18 @@
     private long getInputBytes(Statistics stats) {
       return stats == null ? 0 : stats.getBytesRead();
     }
+    
+    /**
+     * Approximate Hadoop needs to know the sampling ratio.
+     */
+    public int getSamplingRatio() {
+      if (real instanceof SamplingRecordReader) {
+        return ((SamplingRecordReader) real).getSamplingRatio();
+      } else {
+        // No sampling is 100%
+        return 1;
+      }
+    }
   }
 
   /**
@@ -713,8 +736,12 @@
                     ) throws IOException, ClassNotFoundException,
                              InterruptedException {
     // make a task context so we can get the classes
-    org.apache.hadoop.mapreduce.TaskAttemptContext taskContext =
-      new org.apache.hadoop.mapreduce.TaskAttemptContext(job, getTaskID());
+    //org.apache.hadoop.mapreduce.TaskAttemptContext taskContext =
+    //  new org.apache.hadoop.mapreduce.TaskAttemptContext(job, getTaskID());
+    // Approximate Hadoop: we need the reporter (this is already present on Hadoop 2.0)
+    org.apache.hadoop.mapreduce.MapContext taskContext =
+      new org.apache.hadoop.mapreduce.MapContext(job, getTaskID(), null, null, null, reporter, null);
+
     // make a mapper
     org.apache.hadoop.mapreduce.Mapper<INKEY,INVALUE,OUTKEY,OUTVALUE> mapper =
       (org.apache.hadoop.mapreduce.Mapper<INKEY,INVALUE,OUTKEY,OUTVALUE>)
@@ -761,6 +788,11 @@
                                                      reporter, split);
 
       input.initialize(split, mapperContext);
+      // Approximate Hadoop: define if we have to run the approximate computation
+      if (super.isApproximate() && mapper instanceof org.apache.hadoop.mapreduce.ApproximateMapper) {
+        ((org.apache.hadoop.mapreduce.ApproximateMapper<INKEY,INVALUE,OUTKEY,OUTVALUE>) mapper).setApproximate(true);
+      }
+      
       mapper.run(mapperContext);
       input.close();
       output.close(mapperContext);
Index: src/mapred/org/apache/hadoop/mapred/MapTaskRunner.java
===================================================================
--- src/mapred/org/apache/hadoop/mapred/MapTaskRunner.java	(revision 1596572)
+++ src/mapred/org/apache/hadoop/mapred/MapTaskRunner.java	(working copy)
@@ -23,7 +23,6 @@
 
 /** Runs a map task. */
 class MapTaskRunner extends TaskRunner {
-  
   public MapTaskRunner(TaskInProgress task, TaskTracker tracker, JobConf conf,
                        TaskTracker.RunningJob rjob) 
   throws IOException {
Index: src/mapred/org/apache/hadoop/mapred/Merger.java
===================================================================
--- src/mapred/org/apache/hadoop/mapred/Merger.java	(revision 1596572)
+++ src/mapred/org/apache/hadoop/mapred/Merger.java	(working copy)
@@ -175,6 +175,9 @@
     long segmentOffset = 0;
     long segmentLength = -1;
     
+    // Approximate Hadoop: we add the id for the map
+    ID segmentId = null;
+    
     public Segment(Configuration conf, FileSystem fs, Path file,
                    CompressionCodec codec, boolean preserve) throws IOException {
       this(conf, fs, file, 0, fs.getFileStatus(file).getLen(), codec, preserve);
@@ -200,7 +203,17 @@
       this.segmentLength = reader.getLength();
     }
 
-    private void init(Counters.Counter readsCounter) throws IOException {
+    // Approximate Hadoop: we add the id for the map
+    public void setID(ID segmentId) {
+      this.segmentId = segmentId;
+    }
+    
+    public ID getID() {
+      return this.segmentId;
+    }
+    
+    // Incremental requires this public intead of private
+    public void init(Counters.Counter readsCounter) throws IOException {
       if (reader == null) {
         FSDataInputStream in = fs.open(file);
         in.seek(segmentOffset);
@@ -370,6 +383,16 @@
       int s2 = key2.getPosition();
       int l2 = key2.getLength() - s2;
 
+      // Approximate Hadoop: we sort by source ID first    
+      Segment<K, V> segmentA = (Segment<K, V>)a;
+      Segment<K, V> segmentB = (Segment<K, V>)b;
+      if (segmentA.getID() != null && segmentB.getID() != null) {
+        int compare1 = segmentA.getID().toString().compareTo(segmentB.getID().toString());
+        if (compare1 != 0) {
+          return compare1 < 0;
+        }
+      }
+      
       return comparator.compare(key1.getData(), s1, l1, key2.getData(), s2, l2) < 0;
     }
     
@@ -444,6 +467,7 @@
         initialize(segmentsToMerge.size());
         clear();
         for (Segment<K, V> segment : segmentsToMerge) {
+          // Approximate Hadoop, we have modified the sorting to sort by maps
           put(segment);
         }
         
Index: src/mapred/org/apache/hadoop/mapred/QueueManager.java
===================================================================
--- src/mapred/org/apache/hadoop/mapred/QueueManager.java	(revision 1596572)
+++ src/mapred/org/apache/hadoop/mapred/QueueManager.java	(working copy)
@@ -134,7 +134,7 @@
    * @return Set of queue names.
    */
   public synchronized Set<String> getQueues() {
-    return queues.keySet();
+    return new TreeSet(queues.keySet());
   }
 
   /**
Index: src/mapred/org/apache/hadoop/mapred/ReduceTask.java
===================================================================
--- src/mapred/org/apache/hadoop/mapred/ReduceTask.java	(revision 1596572)
+++ src/mapred/org/apache/hadoop/mapred/ReduceTask.java	(working copy)
@@ -63,6 +63,7 @@
 import org.apache.hadoop.io.IntWritable;
 import org.apache.hadoop.io.RawComparator;
 import org.apache.hadoop.io.SequenceFile;
+import org.apache.hadoop.io.Text;
 import org.apache.hadoop.io.Writable;
 import org.apache.hadoop.io.WritableFactories;
 import org.apache.hadoop.io.WritableFactory;
@@ -129,7 +130,7 @@
     getCounters().findCounter(Counter.REDUCE_OUTPUT_RECORDS);
   private Counters.Counter reduceCombineOutputCounter =
     getCounters().findCounter(Counter.COMBINE_OUTPUT_RECORDS);
-
+    
   // A custom comparator for map output files. Here the ordering is determined
   // by the file's size and path. In case of files with same size and different
   // file paths, the first parameter is considered smaller than the second one.
@@ -343,87 +344,266 @@
        skipWriter.append(key, value);
      }
   }
+  
+	/**
+	 * Incremental. Iterator that blocks.
+	 */
+	public class BlockingIterator implements RawKeyValueIterator {
+		private int curInd = 0;
+		private int size = 0;
+		private Progress prog = new Progress();
+		DataInputBuffer key;
+		DataInputBuffer val;
+		Segment<DataInputBuffer, DataInputBuffer> curSeg;
 
-  @Override
-  @SuppressWarnings("unchecked")
-  public void run(JobConf job, final TaskUmbilicalProtocol umbilical)
-    throws IOException, InterruptedException, ClassNotFoundException {
-    this.umbilical = umbilical;
-    job.setBoolean("mapred.skip.on", isSkipping());
+		public BlockingIterator() {
+			size = reduceCopier.mapsInMemory.size();
+			prog.set(1.0f);
+			curSeg = null;
+		}
+		
+		public DataInputBuffer getKey() throws IOException {
+			return key;
+		}
 
-    if (isMapOrReduce()) {
-      copyPhase = getProgress().addPhase("copy");
-      sortPhase  = getProgress().addPhase("sort");
-      reducePhase = getProgress().addPhase("reduce");
-    }
-    // start thread that will handle communication with parent
-    TaskReporter reporter = new TaskReporter(getProgress(), umbilical,
-        jvmContext);
-    reporter.startCommunicationThread();
-    boolean useNewApi = job.getUseNewReducer();
-    initialize(job, getJobID(), reporter, useNewApi);
+		public DataInputBuffer getValue() throws IOException {
+			return val;
+		}
 
-    // check if it is a cleanupJobTask
-    if (jobCleanup) {
-      runJobCleanupTask(umbilical, reporter);
-      return;
-    }
-    if (jobSetup) {
-      runJobSetupTask(umbilical, reporter);
-      return;
-    }
-    if (taskCleanup) {
-      runTaskCleanupTask(umbilical, reporter);
-      return;
-    }
-    
-    // Initialize the codec
-    codec = initCodec();
+		public boolean next() throws IOException {
+			if (curSeg != null && curSeg.next()) {
+				// get all key/values within this segment
+				// do nothing (next() moves to next k/v within segment)
+			} else {
+				if (curSeg != null)
+					curSeg.close();
 
-    boolean isLocal = "local".equals(job.get("mapred.job.tracker", "local"));
-    if (!isLocal) {
-      reduceCopier = new ReduceCopier(umbilical, job, reporter);
-      if (!reduceCopier.fetchOutputs()) {
-        if(reduceCopier.mergeThrowable instanceof FSError) {
-          throw (FSError)reduceCopier.mergeThrowable;
-        }
-        throw new IOException("Task: " + getTaskID() + 
-            " - The reduce copier failed", reduceCopier.mergeThrowable);
-      }
-    }
-    copyPhase.complete();                         // copy is already complete
-    setPhase(TaskStatus.Phase.SORT);
-    statusUpdate(umbilical);
+				// Get and remove the segment from memory because we won't need it later
+				Segment<DataInputBuffer, DataInputBuffer> seg = null;
+				boolean done = false;
+				while (!done) {
+					seg = reduceCopier.mapsInMemory.getAndRemove();
+					if (seg == null) {
+						LOG.info("INCRED: reached end of iterator stream");
+						done = true;
+						return false;
+					} else {
+						curSeg = seg;
+						// initialize segment
+						LOG.info("INCRED: initializing segment");
+						curSeg.init(new Counters.Counter("fake", "fake", 0));
+						curSeg.next();
+						if (curSeg.getKey().getLength() == 0) {
+							done = false;
+						} else {
+							LOG.info("INCRED: current segment key length = " + curSeg.getKey().getLength() + " value length = " + curSeg.getValue().getLength() + " coming from " + curSeg.getID());
+							done = true;
+						}
+					}
+				}
+			}
+			
+			key = curSeg.getKey();
+			val = curSeg.getValue();
+			
+			return true;
+		}
 
-    final FileSystem rfs = FileSystem.getLocal(job).getRaw();
-    RawKeyValueIterator rIter = isLocal
-      ? Merger.merge(job, rfs, job.getMapOutputKeyClass(),
-          job.getMapOutputValueClass(), codec, getMapFiles(rfs, true),
-          !conf.getKeepFailedTaskFiles(), job.getInt("io.sort.factor", 100),
-          new Path(getTaskID().toString()), job.getOutputKeyComparator(),
-          reporter, spilledRecordsCounter, null)
-      : reduceCopier.createKVIterator(job, rfs, reporter);
-        
-    // free up the data structures
-    mapOutputFilesOnDisk.clear();
-    
-    sortPhase.complete();                         // sort is complete
-    setPhase(TaskStatus.Phase.REDUCE); 
-    statusUpdate(umbilical);
-    Class keyClass = job.getMapOutputKeyClass();
-    Class valueClass = job.getMapOutputValueClass();
-    RawComparator comparator = job.getOutputValueGroupingComparator();
+		String getDataStr(byte data[]) {
+			String str = new String();
+			for (byte b : data) {
+				str += b + '-';
+			}
+			return str;
+		}
 
-    if (useNewApi) {
-      runNewReducer(job, umbilical, reporter, rIter, comparator, 
-                    keyClass, valueClass);
-    } else {
-      runOldReducer(job, umbilical, reporter, rIter, comparator, 
-                    keyClass, valueClass);
-    }
-    done(umbilical, reporter);
-  }
+		/**
+		 * Closes the iterator so that the underlying streams can be closed.
+		 * 
+		 * @throws IOException
+		 */
+		public void close() throws IOException {
 
+		}
+
+		/**
+		 * Gets the Progress object; this has a float (0.0 - 1.0) indicating the
+		 * bytes processed by the iterator so far
+		 */
+		public Progress getProgress() {
+			return prog;
+		}
+	}
+	
+	@Override
+	@SuppressWarnings("unchecked")
+	/**
+	 * Incremental changes added to this method
+	 */
+	public void run(JobConf job, final TaskUmbilicalProtocol umbilical) throws IOException, InterruptedException, ClassNotFoundException {
+		this.umbilical = umbilical;
+		job.setBoolean("mapred.skip.on", isSkipping());
+
+		// Incremental
+		boolean incred = job.getBoolean("mapred.tasks.incremental.reduction", false);
+		
+		if (isMapOrReduce()) {
+			copyPhase = getProgress().addPhase("copy");
+			sortPhase  = getProgress().addPhase("sort");
+			reducePhase = getProgress().addPhase("reduce");
+		}
+		// start thread that will handle communication with parent
+		TaskReporter reporter = new TaskReporter(getProgress(), umbilical, jvmContext);
+		reporter.startCommunicationThread();
+		boolean useNewApi = job.getUseNewReducer();
+		initialize(job, getJobID(), reporter, useNewApi);
+
+		// check if it is a cleanupJobTask
+		if (jobCleanup) {
+			runJobCleanupTask(umbilical, reporter);
+			return;
+		}
+		if (jobSetup) {
+			runJobSetupTask(umbilical, reporter);
+			return;
+		}
+		if (taskCleanup) {
+			runTaskCleanupTask(umbilical, reporter);
+			return;
+		}
+		
+		// Initialize the codec
+		codec = initCodec();
+
+		// Incremental we run everything using incremental
+		if (incred) {
+			// spawn reducer thread
+			Class keyClass = job.getMapOutputKeyClass();
+			Class valueClass = job.getMapOutputValueClass();
+			RawComparator comparator = job.getOutputValueGroupingComparator();
+
+			reduceCopier = new ReduceCopier(umbilical, job, reporter);
+
+			// define iterator using keyclass and valueclass
+			RawKeyValueIterator rIter = new BlockingIterator();
+			if (useNewApi) {
+				runNewReducer(job, umbilical, reporter, rIter, comparator, keyClass, valueClass, true);
+			} else {
+				runOldReducerWrapper(job, umbilical, reporter, rIter, comparator, keyClass, valueClass);
+			}
+			
+			// We finish sort and reduce phase immediately
+			copyPhase.complete(); // copy is already complete
+			setPhase(TaskStatus.Phase.SORT);
+			statusUpdate(umbilical);
+			
+			sortPhase.complete(); // sort is complete
+			setPhase(TaskStatus.Phase.REDUCE);
+			statusUpdate(umbilical);
+		} else {
+			// Incremental this exactly as it was before
+			boolean isLocal = "local".equals(job.get("mapred.job.tracker", "local"));
+			if (!isLocal) {
+				reduceCopier = new ReduceCopier(umbilical, job, reporter);
+				if (!reduceCopier.fetchOutputs()) {
+					if(reduceCopier.mergeThrowable instanceof FSError) {
+						throw (FSError)reduceCopier.mergeThrowable;
+					}
+					throw new IOException("Task: " + getTaskID() + " - The reduce copier failed", reduceCopier.mergeThrowable);
+				}
+			}
+			// Copy phase
+			copyPhase.complete();                         // copy is already complete
+			setPhase(TaskStatus.Phase.SORT);
+			statusUpdate(umbilical);
+			final FileSystem rfs = FileSystem.getLocal(job).getRaw();
+			RawKeyValueIterator rIter = isLocal
+			? Merger.merge(job, rfs, job.getMapOutputKeyClass(),
+				job.getMapOutputValueClass(), codec, getMapFiles(rfs, true),
+				!conf.getKeepFailedTaskFiles(), job.getInt("io.sort.factor", 100),
+				new Path(getTaskID().toString()), job.getOutputKeyComparator(),
+				reporter, spilledRecordsCounter, null)
+			: reduceCopier.createKVIterator(job, rfs, reporter);
+				
+			// free up the data structures
+			mapOutputFilesOnDisk.clear();
+			
+			// Reduce phase
+			sortPhase.complete();                         // sort is complete
+			setPhase(TaskStatus.Phase.REDUCE); 
+			statusUpdate(umbilical);
+			Class keyClass = job.getMapOutputKeyClass();
+			Class valueClass = job.getMapOutputValueClass();
+			RawComparator comparator = job.getOutputValueGroupingComparator();
+
+			if (useNewApi) {
+				runNewReducer(job, umbilical, reporter, rIter, comparator, keyClass, valueClass, false); // Add a flag for not incremental
+			} else {
+				runOldReducer(job, umbilical, reporter, rIter, comparator, keyClass, valueClass);
+			}
+		}
+		done(umbilical, reporter);
+	}
+	
+	/**
+	 * Incremental
+	 */
+	public class OldReducerThreaded<INKEY, INVALUE> implements Runnable {
+		JobConf job;
+		TaskUmbilicalProtocol umbilical;
+		final TaskReporter reporter;
+		RawKeyValueIterator rIter;
+		RawComparator<INKEY> comparator;
+		Class<INKEY> keyClass;
+		Class<INVALUE> valueClass;
+
+		public OldReducerThreaded(JobConf job, TaskUmbilicalProtocol umbilical,
+				final TaskReporter reporter, RawKeyValueIterator rIter,
+				RawComparator<INKEY> comparator, Class<INKEY> keyClass,
+				Class<INVALUE> valueClass) {
+			this.job = job;
+			this.umbilical = umbilical;
+			this.reporter = reporter;
+			this.rIter = rIter;
+			this.comparator = comparator;
+			this.keyClass = keyClass;
+			this.valueClass = valueClass;
+		}
+
+		public void run() {
+			try {
+				runOldReducer(job, umbilical, reporter, rIter, comparator, keyClass,
+						valueClass);
+			} catch (Exception e) {
+				LOG.error("INCRED: oldreducerthread threw exception "
+						+ StringUtils.stringifyException(e));
+			}
+		}
+	}
+
+	/**
+	 * Incremental
+	 */
+	@SuppressWarnings("unchecked")
+	private <INKEY, INVALUE, OUTKEY, OUTVALUE> void runOldReducerWrapper(JobConf job, TaskUmbilicalProtocol umbilical, final TaskReporter reporter, RawKeyValueIterator rIter, RawComparator<INKEY> comparator, Class<INKEY> keyClass, Class<INVALUE> valueClass) {
+		OldReducerThreaded<INKEY, INVALUE> runner = new OldReducerThreaded<INKEY, INVALUE>(job, umbilical, reporter, rIter, comparator, keyClass, valueClass);
+		try {
+			Thread t = new Thread(runner);
+			LOG.info("INCRED: Starting reducerContext runner thread");
+			t.start();
+			LOG.info("INCRED: Starting fetchOutputs");
+			reduceCopier.fetchOutputs();
+			LOG.info("INCRED: Finished fetchOutputs, waiting for runner thread to join");
+			// Notify the reduce that is done
+			t.join();
+			LOG.info("INCRED: Runner thread joined, exiting");
+		} catch (Exception e) {
+			e.printStackTrace();
+			LOG.error("INCRED: runOldReduceWrapper threw exception");
+		}
+	}
+	
+
   private class OldTrackingRecordWriter<K, V> implements RecordWriter<K, V> {
 
     private final RecordWriter<K, V> real;
@@ -483,6 +663,7 @@
                      RawComparator<INKEY> comparator,
                      Class<INKEY> keyClass,
                      Class<INVALUE> valueClass) throws IOException {
+
     Reducer<INKEY,INVALUE,OUTKEY,OUTVALUE> reducer = 
       ReflectionUtils.newInstance(job.getReducerClass(), job);
     // make output collector
@@ -500,7 +681,7 @@
           reporter.progress();
         }
       };
-    
+      
     // apply reduce function
     try {
       //increment processed counter only if skipping feature is enabled
@@ -525,7 +706,7 @@
         values.nextKey();
         values.informReduceProgress();
       }
-
+      
       //Clean up: repeated in catch block below
       reducer.close();
       out.close(reporter);
@@ -596,60 +777,83 @@
     }
   }
 
-  @SuppressWarnings("unchecked")
-  private <INKEY,INVALUE,OUTKEY,OUTVALUE>
-  void runNewReducer(JobConf job,
-                     final TaskUmbilicalProtocol umbilical,
-                     final TaskReporter reporter,
-                     RawKeyValueIterator rIter,
-                     RawComparator<INKEY> comparator,
-                     Class<INKEY> keyClass,
-                     Class<INVALUE> valueClass
-                     ) throws IOException,InterruptedException, 
-                              ClassNotFoundException {
-    // wrap value iterator to report progress.
-    final RawKeyValueIterator rawIter = rIter;
-    rIter = new RawKeyValueIterator() {
-      public void close() throws IOException {
-        rawIter.close();
-      }
-      public DataInputBuffer getKey() throws IOException {
-        return rawIter.getKey();
-      }
-      public Progress getProgress() {
-        return rawIter.getProgress();
-      }
-      public DataInputBuffer getValue() throws IOException {
-        return rawIter.getValue();
-      }
-      public boolean next() throws IOException {
-        boolean ret = rawIter.next();
-        reducePhase.set(rawIter.getProgress().get());
-        reporter.progress();
-        return ret;
-      }
-    };
-    // make a task context so we can get the classes
-    org.apache.hadoop.mapreduce.TaskAttemptContext taskContext =
-      new org.apache.hadoop.mapreduce.TaskAttemptContext(job, getTaskID());
-    // make a reducer
-    org.apache.hadoop.mapreduce.Reducer<INKEY,INVALUE,OUTKEY,OUTVALUE> reducer =
-      (org.apache.hadoop.mapreduce.Reducer<INKEY,INVALUE,OUTKEY,OUTVALUE>)
-        ReflectionUtils.newInstance(taskContext.getReducerClass(), job);
-     org.apache.hadoop.mapreduce.RecordWriter<OUTKEY,OUTVALUE> trackedRW = 
-       new NewTrackingRecordWriter<OUTKEY, OUTVALUE>(reduceOutputCounter,
-         job, reporter, taskContext);
-    job.setBoolean("mapred.skip.on", isSkipping());
-    org.apache.hadoop.mapreduce.Reducer.Context 
-         reducerContext = createReduceContext(reducer, job, getTaskID(),
-                                               rIter, reduceInputKeyCounter,
-                                               reduceInputValueCounter, 
-                                               trackedRW, committer,
-                                               reporter, comparator, keyClass,
-                                               valueClass);
-    reducer.run(reducerContext);
-    trackedRW.close(reducerContext);
-  }
+	@SuppressWarnings("unchecked")
+	/**
+	* Incremental modifies this
+	*/
+	private <INKEY,INVALUE,OUTKEY,OUTVALUE>
+	void runNewReducer(JobConf job,
+			final TaskUmbilicalProtocol umbilical,
+			final TaskReporter reporter,
+			RawKeyValueIterator rIter,
+			RawComparator<INKEY> comparator,
+			Class<INKEY> keyClass,
+			Class<INVALUE> valueClass,
+			boolean incred // Added incremental
+			) throws IOException,InterruptedException, 
+				ClassNotFoundException {
+		// wrap value iterator to report progress.
+		final RawKeyValueIterator rawIter = rIter;
+		rIter = new RawKeyValueIterator() {
+			public void close() throws IOException {
+				rawIter.close();
+			}
+			public DataInputBuffer getKey() throws IOException {
+				return rawIter.getKey();
+			}
+			public Progress getProgress() {
+				return rawIter.getProgress();
+			}
+			public DataInputBuffer getValue() throws IOException {
+				return rawIter.getValue();
+			}
+			public boolean next() throws IOException {
+				boolean ret = rawIter.next();
+				reducePhase.set(rawIter.getProgress().get());
+				reporter.progress();
+				return ret;
+			}
+		};
+		// make a task context so we can get the classes
+		org.apache.hadoop.mapreduce.TaskAttemptContext taskContext =
+		new org.apache.hadoop.mapreduce.TaskAttemptContext(job, getTaskID());
+		// make a reducer
+		org.apache.hadoop.mapreduce.Reducer<INKEY,INVALUE,OUTKEY,OUTVALUE> reducer = (org.apache.hadoop.mapreduce.Reducer<INKEY,INVALUE,OUTKEY,OUTVALUE>) ReflectionUtils.newInstance(taskContext.getReducerClass(), job);
+		org.apache.hadoop.mapreduce.RecordWriter<OUTKEY,OUTVALUE> trackedRW = 
+		new NewTrackingRecordWriter<OUTKEY, OUTVALUE>(reduceOutputCounter, job, reporter, taskContext);
+		job.setBoolean("mapred.skip.on", isSkipping());
+		
+		// Incremental runs the reduce on a thread
+		if (incred) {
+			org.apache.hadoop.mapreduce.Reducer.ContextThreaded reducerContext = createReduceContextThreaded(
+				reducer, job, getTaskID(), rIter, reduceInputKeyCounter, reduceInputValueCounter, trackedRW,
+				committer, reporter, comparator, keyClass, valueClass);
+		
+			LOG.info("INCRED: Starting reducerContext runner thread");
+			Thread t = new Thread(reducerContext);
+			t.start();
+			
+			LOG.info("INCRED: Starting fetchOutputs");
+			reduceCopier.fetchOutputs();
+			
+			LOG.info("INCRED: Finished fetchOutputs, waiting for runner thread to join");
+			t.join();
+			
+			// Close as in the old code
+			trackedRW.close(reducerContext);
+		} else {
+			// This is the old code
+			org.apache.hadoop.mapreduce.Reducer.Context 
+				reducerContext = createReduceContext(reducer, job, getTaskID(),
+								rIter, reduceInputKeyCounter,
+								reduceInputValueCounter, 
+								trackedRW, committer,
+								reporter, comparator, keyClass,
+								valueClass);
+			reducer.run(reducerContext);
+			trackedRW.close(reducerContext);
+		}
+	}
 
   private static enum CopyOutputErrorType {
     NO_ERROR,
@@ -902,7 +1106,12 @@
      */
     private final Map<String, List<MapOutputLocation>> mapLocations = 
       new ConcurrentHashMap<String, List<MapOutputLocation>>();
-
+    
+    // Incremental, keep info about the maps in memory
+    public MapMonitor mapsInMemory = new MapMonitor();
+    private boolean incred = false;
+    
+    
     class ShuffleClientInstrumentation implements MetricsSource {
       final MetricsRegistry registry = new MetricsRegistry("shuffleInput");
       final MetricMutableCounterLong inputBytes =
@@ -1252,6 +1461,10 @@
       
       private final SecretKey jobTokenSecret;
       
+      // Incremental
+      private boolean incred = false;
+      private JobConf job;
+      
       public MapOutputCopier(JobConf job, Reporter reporter, SecretKey jobTokenSecret) {
         setName("MapOutputCopier " + reduceTask.getTaskID() + "." + id);
         LOG.debug(getName() + " created");
@@ -1270,6 +1483,10 @@
           codec = ReflectionUtils.newInstance(codecClass, job);
           decompressor = CodecPool.getDecompressor(codec);
         }
+        
+        // Incremental
+        this.incred = job.getBoolean("mapred.tasks.incremental.reduction", false);
+        this.job = job;
       }
       
       /**
@@ -1379,7 +1596,7 @@
                               ) throws IOException, InterruptedException {
         // check if we still need to copy the output from this location
         if (copiedMapOutputs.contains(loc.getTaskId()) || 
-            obsoleteMapIds.contains(loc.getTaskAttemptId())) {
+            obsoleteMapIds.contains(loc.getTaskAttemptId()) ) {
           return CopyResult.OBSOLETE;
         } 
  
@@ -1425,13 +1642,33 @@
             // Note that we successfully copied the map-output
             noteCopiedMapOutput(loc.getTaskId());
             
+            // Incremental
+            mapsInMemory.put();
+            
             return bytes;
           }
           
-          // Process map-output
-          if (mapOutput.inMemory) {
-            // Save it in the synchronized list of map-outputs
-            mapOutputsFilesInMemory.add(mapOutput);
+	LOG.info("We have a new output from " + mapOutput.mapAttemptId);
+	// Process map-output
+	if (mapOutput.inMemory) {
+		// Incremental
+		if (incred) {
+			// read data from mapoutput into segments
+			// append new segments onto ReduceTask's segment list
+			// long fullSize = mapOutput.data.length;
+			MapOutput mo = mapOutput;
+// 			Reader<K, V> reader = new InMemoryReader<K, V>(getRamManager(), mo.mapAttemptId, mo.data, 0, mo.data.length);
+			Reader<K, V> reader = new InMemoryReader<K, V>(ramManager, mo.mapAttemptId, mo.data, 0, mo.data.length);
+			Segment<K, V> segment = new Segment<K, V>(reader, true);
+
+			// notify threadedReduceContext that new segments are available
+			// Approximate Hadoop needs the id of the map
+			segment.setID(mo.mapAttemptId);
+			mapsInMemory.put(segment);
+		} else {
+			// Save it in the synchronized list of map-outputs
+			mapOutputsFilesInMemory.add(mapOutput);
+		}
           } else {
             // Rename the temporary file to the final file; 
             // ensure it is on the same partition
@@ -1443,12 +1680,21 @@
               throw new IOException("Failed to rename map output " + 
                   tmpMapOutput + " to " + filename);
             }
-
-            synchronized (mapOutputFilesOnDisk) {        
-              addToMapOutputFilesOnDisk(localFileSys.getFileStatus(filename));
-            }
+		// Incremental
+		if (incred) {
+			Segment<K, V> diskSeg = new Segment<K, V>(job, localFileSys, filename, codec, job.getKeepFailedTaskFiles());
+			// Approximate Hadoop needs the id of the map
+			diskSeg.setID(mapOutput.mapAttemptId);
+			mapsInMemory.put(diskSeg);
+		} else {
+			synchronized (mapOutputFilesOnDisk) {        
+				addToMapOutputFilesOnDisk(localFileSys.getFileStatus(filename));
+			}
+		}
           }
 
+          // Approximate Hadoop
+          
           // Note that we successfully copied the map-output
           noteCopiedMapOutput(loc.getTaskId());
         }
@@ -1483,6 +1729,19 @@
       throws IOException, InterruptedException {
         // Connect
         URL url = mapOutputLoc.getOutputLocation();
+        
+        // Approximate Hadoop
+        // Check if there is an output from an old job
+        TaskID taskId = mapOutputLoc.getTaskId();
+        JobID jobId = taskId.getJobID();
+        // The URL is something like http://sol021:50060/mapOutput?job=job_201308221433_0001&map=attempt_201308221433_0001_m_000000_0&reduce=0
+        String previousURLStr = conf.get("mapred.map."+taskId.getId()+".previous.output");
+        if (previousURLStr != null) {
+          LOG.info(taskId + " will use previous output from '" + previousURLStr+"'.");
+          // Replace URL by the old output one
+          url = new URL(previousURLStr);
+        }
+        
         HttpURLConnection connection = (HttpURLConnection)url.openConnection();
         
         InputStream input = setupSecureConnection(mapOutputLoc, connection);
@@ -1504,13 +1763,14 @@
           LOG.warn("Invalid map id ", ia);
           return null;
         }
-        TaskAttemptID expectedMapId = mapOutputLoc.getTaskAttemptId();
+        // Approximate Hadoop. TODO We have to fix this situation.
+        /*TaskAttemptID expectedMapId = mapOutputLoc.getTaskAttemptId();
         if (!mapId.equals(expectedMapId)) {
           LOG.warn("data from wrong map:" + mapId +
               " arrived to reduce task " + reduce +
               ", where as expected map output should be from " + expectedMapId);
           return null;
-        }
+        }*/
         
         long decompressedLength = 
           Long.parseLong(connection.getHeaderField(RAW_MAP_OUTPUT_LENGTH));  
@@ -1568,7 +1828,7 @@
           mapOutput = shuffleToDisk(mapOutputLoc, input, filename, 
               compressedLength);
         }
-            
+        
         return mapOutput;
       }
       
@@ -1597,10 +1857,11 @@
         if (LOG.isDebugEnabled())
           LOG.debug("url="+msgToEncode+";encHash="+encHash+";replyHash="
               +replyHash);
+        // Approximate Hadoop: TODO we don't support verification yet
         // verify that replyHash is HMac of encHash
-        SecureShuffleUtils.verifyReply(replyHash, encHash, jobTokenSecret);
+        //SecureShuffleUtils.verifyReply(replyHash, encHash, jobTokenSecret);
         if (LOG.isDebugEnabled())
-          LOG.debug("for url="+msgToEncode+" sent hash and receievd reply");
+          LOG.debug("for url="+msgToEncode+" sent hash and received reply");
         return input;
       }
 
@@ -1987,6 +2248,9 @@
       this.maxMapRuntime = 0;
       this.reportReadErrorImmediately = 
         conf.getBoolean("mapreduce.reduce.shuffle.notify.readerror", true);
+        
+      // Incremental
+      this.incred = conf.getBoolean("mapred.tasks.incremental.reduction", false);
     }
     
     private boolean busyEnough(int numInFlight) {
@@ -2035,8 +2299,15 @@
       long lastProgressTime = startTime;
       long lastOutputTime = 0;
       
+      // Approximate Hadoop
+      // Here we check when to start dropping maps
+      float approxDropPerc = conf.getFloat("mapred.reduce.approximate.drop.maps.percentage", 1.0f);
+      int   approxDropTime = conf.getInt("mapred.reduce.approximate.drop.maps.extratime", 0);
+      boolean startDropping = false;
+      long startDroppingTime = -1;
+      
         // loop until we get all required outputs
-        while (copiedMapOutputs.size() < numMaps && mergeThrowable == null) {
+        while (copiedMapOutputs.size() < numMaps && mergeThrowable == null && !startDropping) {
           int numEventsAtStartOfScheduling;
           synchronized (copyResultsOrNewEventsLock) {
             numEventsAtStartOfScheduling = numEventsFetched;
@@ -2134,7 +2405,7 @@
               
               if (penalized)
                 continue;
-
+              
               synchronized (knownOutputsByLoc) {
               
                 locItr = knownOutputsByLoc.iterator();
@@ -2144,7 +2415,7 @@
                   MapOutputLocation loc = locItr.next();
               
                   // Do not schedule fetches from OBSOLETE maps
-                  if (obsoleteMapIds.contains(loc.getTaskAttemptId())) {
+                  if (obsoleteMapIds.contains(loc.getTaskAttemptId()) ) {
                     locItr.remove();
                     continue;
                   }
@@ -2164,7 +2435,7 @@
           if (numScheduled > 0 || logNow) {
             LOG.info(reduceTask.getTaskID() + " Scheduled " + numScheduled +
                    " outputs (" + penaltyBox.size() +
-                   " slow hosts and" + numDups + " dup hosts)");
+                   " slow hosts and " + numDups + " dup hosts)");
           }
 
           if (penaltyBox.size() > 0 && logNow) {
@@ -2319,6 +2590,40 @@
             uniqueHosts.remove(cr.getHost());
             numInFlight--;
           }
+          
+          // Approximate Hadoop
+          // Check if we can start dropping map outputs
+          if (copiedMapOutputs.size() < numMaps) {
+            if (startDroppingTime < 0 && copiedMapOutputs.size() >= approxDropPerc*numMaps) {
+              // We have enough mappers, let's wait until we have everybody
+              startDroppingTime = System.currentTimeMillis() + approxDropTime*1000;
+            }
+            if (startDroppingTime > 0 && System.currentTimeMillis() >= startDroppingTime) {
+              LOG.info("We already have " + copiedMapOutputs.size() + " of " + numMaps + " maps. We can drop the rest!");
+              startDropping = true;
+              numMaps = copiedMapOutputs.size();
+            }
+          }
+          
+          // Approximate Hadoop
+          // Learn stuff from the temporary outputs
+          /*if (copiedMapOutputs.size() < numMaps) {
+            // TODO
+            LOG.info("Check if we should start dropping according to our requirements");
+            LOG.info("The list of finished tasks is: " + copiedMapOutputs);
+            LOG.info("Output disk: " + mapOutputFilesOnDisk.size());
+            LOG.info("Output memory: " + mapOutputsFilesInMemory.size());
+            
+            for (MapOutput mapOutput : mapOutputsFilesInMemory) {
+              LOG.info(" Output: " + mapOutput.mapAttemptId + " -> " + mapOutput.path + " size=" + mapOutput.data.length);
+            }
+            
+            // List<MapOutput> mapOutputsFilesInMemory
+            // SortedSet<FileStatus> mapOutputFilesOnDisk
+            // Set<TaskID> copiedMapOutputs
+            //  LOG.info("We already have can start dropping according to our estimations");
+            //  startDropping = true;
+          }*/
         }
         
         // all done, inform the copiers to exit
@@ -2325,9 +2630,9 @@
         exitGetMapEvents= true;
         try {
           getMapEventsThread.join();
-          LOG.info("getMapsEventsThread joined.");
+          LOG.info("GetMapsEventsThread joined.");
         } catch (InterruptedException ie) {
-          LOG.info("getMapsEventsThread threw an exception: " +
+          LOG.info("GetMapsEventsThread threw an exception: " +
               StringUtils.stringifyException(ie));
         }
 
@@ -2340,6 +2645,15 @@
           }
         }
         
+        // Incremental, we return checking if we have all the maps we require
+        if (incred) {
+          // We notify the mapsInMemory to stop waiting
+          synchronized(mapsInMemory) {
+            mapsInMemory.notify();
+          }
+          return copiedMapOutputs.size() == numMaps;
+        }
+        
         // copiers are done, exit and notify the waiting merge threads
         synchronized (mapOutputFilesOnDisk) {
           exitLocalFSMerge = true;
@@ -2414,6 +2728,12 @@
                                      mo.data, 0, mo.data.length);
           Segment<K, V> segment = 
             new Segment<K, V>(reader, true);
+
+          // Approximate Hadoop: we should know where the input comes from sampled data or there is dropping
+          if (getConf().getBoolean("mapred.tasks.clustering", false) == true) {
+            segment.setID(mo.mapAttemptId);
+          }
+
           inMemorySegments.add(segment);
         }
       }
@@ -2447,7 +2767,7 @@
       final Path tmpDir = new Path(getTaskID().toString());
       final RawComparator<K> comparator =
         (RawComparator<K>)job.getOutputKeyComparator();
-
+        
       // segments required to vacate memory
       List<Segment<K,V>> memDiskSegments = new ArrayList<Segment<K,V>>();
       long inMemToDiskBytes = 0;
@@ -2490,7 +2810,7 @@
                    "intermediate, on-disk merge");
         }
       }
-
+      
       // segments on disk
       List<Segment<K,V>> diskSegments = new ArrayList<Segment<K,V>>();
       long onDiskBytes = inMemToDiskBytes;
@@ -2497,7 +2817,8 @@
       Path[] onDisk = getMapFiles(fs, false);
       for (Path file : onDisk) {
         onDiskBytes += fs.getFileStatus(file).getLen();
-        diskSegments.add(new Segment<K, V>(job, fs, file, codec, keepInputs));
+        Segment<K, V> segment = new Segment<K, V>(job, fs, file, codec, keepInputs);
+        diskSegments.add(segment);
       }
       LOG.info("Merging " + onDisk.length + " files, " +
                onDiskBytes + " bytes from disk");
@@ -2509,7 +2830,7 @@
           return o1.getLength() < o2.getLength() ? -1 : 1;
         }
       });
-
+      
       // build final list of segments from merged backed by disk + in-mem
       List<Segment<K,V>> finalSegments = new ArrayList<Segment<K,V>>();
       long inMemBytes = createInMemorySegments(finalSegments, 0);
@@ -2519,6 +2840,7 @@
         final int numInMemSegments = memDiskSegments.size();
         diskSegments.addAll(0, memDiskSegments);
         memDiskSegments.clear();
+        
         RawKeyValueIterator diskMerge = Merger.merge(
             job, fs, keyClass, valueClass, codec, diskSegments,
             ioSortFactor, numInMemSegments, tmpDir, comparator,
@@ -2530,6 +2852,7 @@
         finalSegments.add(new Segment<K,V>(
               new RawKVIteratorReader(diskMerge, onDiskBytes), true));
       }
+      
       return Merger.merge(job, fs, keyClass, valueClass,
                    finalSegments, finalSegments.size(), tmpDir,
                    comparator, reporter, spilledRecordsCounter, null);
@@ -2612,7 +2935,8 @@
     
     
     
-    /** Starts merging the local copy (on disk) of the map's output so that
+    /** 
+     * Starts merging the local copy (on disk) of the map's output so that
      * most of the reducer's input is sorted i.e overlapping shuffle
      * and merge phases.
      */
@@ -2913,9 +3237,9 @@
           switch (event.getTaskStatus()) {
             case SUCCEEDED:
             {
+              TaskAttemptID taskId = event.getTaskAttemptId();
               URI u = URI.create(event.getTaskTrackerHttp());
               String host = u.getHost();
-              TaskAttemptID taskId = event.getTaskAttemptId();
               URL mapOutputLocation = new URL(event.getTaskTrackerHttp() + 
                                       "/mapOutput?job=" + taskId.getJobID() +
                                       "&map=" + taskId + 
@@ -2927,9 +3251,38 @@
                 mapLocations.put(host, loc);
                }
               loc.add(new MapOutputLocation(taskId, host, mapOutputLocation));
-              numNewMaps ++;
+              numNewMaps++;
             }
             break;
+            // Approximate Hadoop
+            case DROPPED:
+            {
+              TaskAttemptID taskId = event.getTaskAttemptId();
+              Configuration jobConf = reduceTask.getConf();
+              // This is a task with output from a previous execution
+              if (jobConf.get("mapred.map."+taskId.getTaskID().getId()+".previous.output") != null) {
+                String previousOutput = jobConf.get("mapred.map."+taskId.getTaskID().getId()+".previous.output");
+                URL mapOutputLocation = new URL(previousOutput);
+                String host = mapOutputLocation.getHost();
+                List<MapOutputLocation> loc = mapLocations.get(host);
+                if (loc == null) {
+                  loc = Collections.synchronizedList(new LinkedList<MapOutputLocation>());
+                  mapLocations.put(host, loc);
+                 }
+                loc.add(new MapOutputLocation(taskId, host, mapOutputLocation));
+                numNewMaps++;
+              // This is just a dropped task, skip it
+              } else {
+                LOG.info("We received a dropped task '" + taskId + "' without previous output. Discard this map.");
+                // We put it as it was an obsolete map and we use that mechanism
+                obsoleteMapIds.add(event.getTaskAttemptId());
+                // We prevent the reduce from fetching these maps.
+                // I believe this is right because this is a TaskEvent and not an AttemptEvent.
+                // Otherwise, we would need to check if all the attempts were dropped.
+                numMaps--;
+              }
+            }
+            break;
             case FAILED:
             case KILLED:
             case OBSOLETE:
@@ -2951,8 +3304,77 @@
         return numNewMaps;
       }
     }
+  
+	/**
+	 * Incremental. Monitor the map status
+	 */
+	public class MapMonitor {
+		private List<Segment<K, V>> processedMapOutputs = Collections.synchronizedList(new LinkedList<Segment<K, V>>());
+
+		private int curInd = -1;
+		private int totalMaps = 0;
+
+		public MapMonitor() {
+			totalMaps = numMaps;
+		}
+
+		// Gets and removes the current object from the list
+		public synchronized Segment<K, V> getAndRemove() {
+			curInd++;
+
+			while (processedMapOutputs.size() == 0) {
+				//if (curInd >= totalMaps) {
+				if (curInd >= totalMaps || curInd >= numMaps) { // With dropping we are able to finish before
+					LOG.info("INCRED: returning null" + curInd + " : " + totalMaps);
+					return null;
+				}
+				try {
+					wait();
+				} catch (Exception e) {
+					LOG.error("INCRED: MapsMonitor threw exception "
+							+ StringUtils.stringifyException(e));
+				}
+			}
+			LOG.info("INCRED: MapMonitor returning item " + curInd + ", current size " + processedMapOutputs.size());
+			Segment<K, V> seg = processedMapOutputs.get(0);
+
+			processedMapOutputs.remove(0);
+			return seg;
+		}
+
+		public synchronized Segment<K, V> get() {
+			curInd++;
+
+			while (processedMapOutputs.size() == 0) {
+				if (curInd >= totalMaps) {
+					LOG.info("INCRED: returning null" + curInd + " : " + totalMaps);
+					return null;
+				}
+				try {
+					wait();
+				} catch (Exception e) {
+					LOG.error("INCRED: MapsMonitor threw exception " + StringUtils.stringifyException(e));
+				}
+			}
+			return processedMapOutputs.get(curInd);
+		}
+
+		public synchronized void put(Segment<K, V> seg) {
+			processedMapOutputs.add(seg);
+			notify();
+		}
+
+		public synchronized void put() {
+			totalMaps--;
+			notify();
+		}
+		
+		public synchronized int size() {
+			return processedMapOutputs.size();
+		}
+	}
   }
-
+  
   /**
    * Return the exponent of the power of two closest to the given
    * positive value, or zero if value leq 0.
Index: src/mapred/org/apache/hadoop/mapred/Reducer.java
===================================================================
--- src/mapred/org/apache/hadoop/mapred/Reducer.java	(revision 1596572)
+++ src/mapred/org/apache/hadoop/mapred/Reducer.java	(working copy)
@@ -195,5 +195,4 @@
   void reduce(K2 key, Iterator<V2> values,
               OutputCollector<K3, V3> output, Reporter reporter)
     throws IOException;
-
 }
Index: src/mapred/org/apache/hadoop/mapred/RunningJob.java
===================================================================
--- src/mapred/org/apache/hadoop/mapred/RunningJob.java	(revision 1596572)
+++ src/mapred/org/apache/hadoop/mapred/RunningJob.java	(working copy)
@@ -174,7 +174,19 @@
   @Deprecated
   public void killTask(String taskId, boolean shouldFail) throws IOException;
   
+  
   /**
+   * Approximate Hadoop.
+   * @param taskId the id of the task to be dropped.
+   * @throws IOException
+   */
+  public void dropTask(TaskAttemptID taskId) throws IOException;
+  
+  /** @deprecated Applications should rather use {@link #dropTask(TaskAttemptID)}*/
+  @Deprecated
+  public void dropTask(String taskId) throws IOException;
+  
+  /**
    * Gets the counters for this job.
    * 
    * @return the counters for this job.
Index: src/mapred/org/apache/hadoop/mapred/TIPStatus.java
===================================================================
--- src/mapred/org/apache/hadoop/mapred/TIPStatus.java	(revision 1596572)
+++ src/mapred/org/apache/hadoop/mapred/TIPStatus.java	(working copy)
@@ -20,5 +20,5 @@
 /** The states of a {@link TaskInProgress} as seen by the JobTracker.
  */
 public enum TIPStatus {
-  PENDING, RUNNING, COMPLETE, KILLED, FAILED;
-}
\ No newline at end of file
+  PENDING, RUNNING, COMPLETE, KILLED, FAILED, DROPPED; // Approximate Hadoop: dropped.
+}
Index: src/mapred/org/apache/hadoop/mapred/Task.java
===================================================================
--- src/mapred/org/apache/hadoop/mapred/Task.java	(revision 1596572)
+++ src/mapred/org/apache/hadoop/mapred/Task.java	(working copy)
@@ -166,6 +166,9 @@
   protected SecretKey tokenSecret;
   protected JvmContext jvmContext;
 
+  // Approximate Hadoop
+  private boolean approximate = false;
+  
   ////////////////////////////////////////////
   // Constructors
   ////////////////////////////////////////////
@@ -194,6 +197,17 @@
     spilledRecordsCounter = counters.findCounter(Counter.SPILLED_RECORDS);
   }
 
+  /**
+   * Approximate Hadoop
+   */
+  public boolean isApproximate() {
+    return this.approximate;
+  }
+  
+  public void setApproximate(boolean approximate) {
+    this.approximate = approximate;
+  }
+  
   ////////////////////////////////////////////
   // Accessors
   ////////////////////////////////////////////
@@ -427,6 +441,8 @@
     out.writeBoolean(writeSkipRecs);
     out.writeBoolean(taskCleanup); 
     Text.writeString(out, user);
+    // Approximate Hadoop
+    out.writeBoolean(approximate);
   }
   
   public void readFields(DataInput in) throws IOException {
@@ -451,6 +467,8 @@
       setPhase(TaskStatus.Phase.CLEANUP);
     }
     user = Text.readString(in);
+    // Approximate Hadoop
+    approximate = in.readBoolean();
   }
 
   @Override
@@ -1266,7 +1284,23 @@
         DataInputBuffer nextKeyBytes = in.getKey();
         keyIn.reset(nextKeyBytes.getData(), nextKeyBytes.getPosition(), nextKeyBytes.getLength());
         nextKey = keyDeserializer.deserialize(nextKey);
+        
+		/*LOG.info("Start checking the key.... ");
+		if (nextKeyBytes.getOriginId() == null) {
+			LOG.info("Nothing....");
+		} else {
+			LOG.info("We have a source!!!!! " + nextKeyBytes.getOriginId());
+		}
+		// TODO Approximate Hadoop
+		if (nextKey != null && nextKey instanceof Text && nextKeyBytes.getOriginId() != null) {
+			LOG.info("The origin of the key is " + nextKeyBytes.getOriginId());
+			Text nextKeyText = (Text) nextKey;
+			nextKeyText.set(nextKeyBytes.getOriginId()+"-"+nextKeyText.toString());
+		}*/
+        
         hasNext = key != null && (comparator.compare(key, nextKey) == 0);
+        
+        
       } else {
         hasNext = false;
       }
@@ -1356,6 +1390,65 @@
       throw new IOException("Can't invoke Context constructor", e);
     }
   }
+  
+	/**
+	 * Incremental
+	 */
+	private static final Constructor<org.apache.hadoop.mapreduce.Reducer.ContextThreaded> contextThreadedConstructor;
+	static {
+		try {
+			contextThreadedConstructor = 
+				org.apache.hadoop.mapreduce.Reducer.ContextThreaded.class.getConstructor(
+					new Class[]{org.apache.hadoop.mapreduce.Reducer.class,
+					Configuration.class,
+					org.apache.hadoop.mapreduce.TaskAttemptID.class,
+					RawKeyValueIterator.class,
+					org.apache.hadoop.mapreduce.Counter.class,
+					org.apache.hadoop.mapreduce.Counter.class,
+					org.apache.hadoop.mapreduce.RecordWriter.class,
+					org.apache.hadoop.mapreduce.OutputCommitter.class,
+					org.apache.hadoop.mapreduce.StatusReporter.class,
+					RawComparator.class,
+					Class.class,
+					Class.class,
+					org.apache.hadoop.mapreduce.Reducer.class}); // Extra arg added by incremental
+		} catch (Exception nme) {
+			throw new IllegalArgumentException("Can't find constructor");
+		}
+	}
+	
+	@SuppressWarnings("unchecked")
+	protected static <INKEY,INVALUE,OUTKEY,OUTVALUE> org.apache.hadoop.mapreduce.Reducer<INKEY,INVALUE,OUTKEY,OUTVALUE>.ContextThreaded createReduceContextThreaded(
+				org.apache.hadoop.mapreduce.Reducer<INKEY,INVALUE,OUTKEY,OUTVALUE> reducer,
+				Configuration job,
+				org.apache.hadoop.mapreduce.TaskAttemptID taskId, 
+				RawKeyValueIterator rIter,
+				org.apache.hadoop.mapreduce.Counter inputKeyCounter,
+				org.apache.hadoop.mapreduce.Counter inputValueCounter,
+				org.apache.hadoop.mapreduce.RecordWriter<OUTKEY,OUTVALUE> output, 
+				org.apache.hadoop.mapreduce.OutputCommitter committer,
+				org.apache.hadoop.mapreduce.StatusReporter reporter,
+				RawComparator<INKEY> comparator,
+				Class<INKEY> keyClass, Class<INVALUE> valueClass
+			) throws IOException, ClassNotFoundException {
+		try {
+			LOG.info("INCRED: Creating new instace of ReduceContextThreaded");
+			return contextThreadedConstructor.newInstance(reducer, job, taskId,
+							rIter, inputKeyCounter, 
+							inputValueCounter, output, 
+							committer, reporter, comparator, 
+							keyClass, valueClass, reducer);
+		} catch (InstantiationException e) {
+			throw new IOException("Can't create Context", e);
+		} catch (InvocationTargetException e) {
+			throw new IOException("Can't invoke Context constructor", e);
+		} catch (IllegalAccessException e) {
+			throw new IOException("Can't invoke Context constructor", e);
+		} catch (Exception e) {
+			throw new IOException("Generic exception");
+		}
+	}
+  
 
   protected static abstract class CombinerRunner<K,V> {
     protected final Counters.Counter inputCounter;
Index: src/mapred/org/apache/hadoop/mapred/TaskCompletionEvent.java
===================================================================
--- src/mapred/org/apache/hadoop/mapred/TaskCompletionEvent.java	(revision 1596572)
+++ src/mapred/org/apache/hadoop/mapred/TaskCompletionEvent.java	(working copy)
@@ -30,7 +30,9 @@
  * job tracker. 
  */
 public class TaskCompletionEvent implements Writable{
-  static public enum Status {FAILED, KILLED, SUCCEEDED, OBSOLETE, TIPFAILED};
+  static public enum Status {FAILED, KILLED, SUCCEEDED, OBSOLETE, TIPFAILED,
+    // Approximate Hadoop
+    DROPPED};
     
   private int eventId; 
   private String taskTrackerHttp;
Index: src/mapred/org/apache/hadoop/mapred/TaskID.java
===================================================================
--- src/mapred/org/apache/hadoop/mapred/TaskID.java	(revision 1596572)
+++ src/mapred/org/apache/hadoop/mapred/TaskID.java	(working copy)
@@ -51,7 +51,7 @@
    * @param isMap whether the tip is a map 
    * @param id the tip number
    */
-  public TaskID(org.apache.hadoop.mapreduce.JobID jobId, boolean isMap,int id) {
+  public TaskID(org.apache.hadoop.mapreduce.JobID jobId, boolean isMap, int id) {
     super(jobId, isMap, id);
   }
   
Index: src/mapred/org/apache/hadoop/mapred/TaskInProgress.java
===================================================================
--- src/mapred/org/apache/hadoop/mapred/TaskInProgress.java	(revision 1596572)
+++ src/mapred/org/apache/hadoop/mapred/TaskInProgress.java	(working copy)
@@ -82,6 +82,7 @@
   private int completes = 0;
   private boolean failed = false;
   private boolean killed = false;
+  private boolean dropped = false;
   private long maxSkipRecords = 0;
   private FailedRanges failedRanges = new FailedRanges();
   private volatile boolean skipping = false;
@@ -102,6 +103,8 @@
   private TreeMap<TaskAttemptID, String> activeTasks = new TreeMap<TaskAttemptID, String>();
   // All attempt Ids of this TIP
   private TreeSet<TaskAttemptID> tasks = new TreeSet<TaskAttemptID>();
+  // Approximate Hadoop
+  private TreeSet<TaskAttemptID> tasksApproximate = new TreeSet<TaskAttemptID>();
   private JobConf conf;
   private Map<TaskAttemptID,List<String>> taskDiagnosticData =
     new TreeMap<TaskAttemptID,List<String>>();
@@ -382,6 +385,14 @@
   }
 
   /**
+   * Approximate Hadoop.
+   * Make a completed task incomplete.
+   */
+  public synchronized void incomplete() {
+    completes = 0;
+  }
+  
+  /**
    * Is the tip a failure?
    * 
    * @return <code>true</code> if tip has failed, else <code>false</code>
@@ -449,7 +460,7 @@
     } else if (isCommitPending(taskid) && !shouldCommit(taskid) &&
                !tasksReportedClosed.contains(taskid)) {
       tasksReportedClosed.add(taskid);
-      close = true; 
+      close = true;
     } else {
       close = tasksToKill.keySet().contains(taskid);
     }   
@@ -485,6 +496,8 @@
     TIPStatus currentStatus = null;
     if (isRunning() && !isComplete()) {
       currentStatus = TIPStatus.RUNNING;
+    } else if (wasDropped()) {
+      currentStatus = TIPStatus.DROPPED;
     } else if (isComplete()) {
       currentStatus = TIPStatus.COMPLETE;
     } else if (wasKilled()) {
@@ -571,7 +584,7 @@
            newState != TaskStatus.State.KILLED_UNCLEAN && 
            newState != TaskStatus.State.UNASSIGNED) && 
           (oldState == newState)) {
-        LOG.warn("Recieved duplicate status update of '" + newState + 
+        LOG.warn("Received duplicate status update of '" + newState + 
                  "' for '" + taskid + "' of TIP '" + getTIPId() + "'" +
                  "oldTT=" + oldStatus.getTaskTracker() + 
                  " while newTT=" + status.getTaskTracker());
@@ -589,6 +602,7 @@
            oldState == TaskStatus.State.FAILED_UNCLEAN || 
            oldState == TaskStatus.State.KILLED_UNCLEAN || 
            oldState == TaskStatus.State.SUCCEEDED ||
+           oldState == TaskStatus.State.DROPPED || // Approximate Hadoop
            oldState == TaskStatus.State.COMMIT_PENDING)) {
         return false;
       }
@@ -767,11 +781,11 @@
    * Indicate that one of the taskids in this TaskInProgress
    * has successfully completed!
    */
-  public void completed(TaskAttemptID taskid) {
+  public void completed(TaskAttemptID taskid, State state) {
     //
     // Record that this taskid is complete
     //
-    completedTask(taskid, TaskStatus.State.SUCCEEDED);
+    completedTask(taskid, state); // TaskStatus.State.SUCCEEDED
         
     // Note the successful taskid
     setSuccessfulTaskid(taskid);
@@ -821,7 +835,7 @@
     return taskStatuses.get(taskid);
   }
   /**
-   * The TIP's been ordered kill()ed.
+   * The TIP's been ordered killed.
    */
   public void kill() {
     if (isComplete() || failed) {
@@ -842,6 +856,50 @@
   }
   
   /**
+   * Approximate Hadoop
+   */
+  public boolean wasDropped() {
+    return dropped;
+  }
+  
+  /**
+   * Approximate Hadoop
+   */
+  public void setDropped() {
+    dropped = true;
+  }
+  
+  /**
+   * Mark the task as approximated.
+   */
+  public void addApproximation(TaskAttemptID attemptId) {
+    tasksApproximate.add(attemptId);
+  }
+  
+  /**
+   * Check if the attempt was approximated.
+   */
+  public boolean isApproximated(TaskAttemptID attemptId) {
+    return tasksApproximate.contains(attemptId);
+  }
+  
+  /**
+   * Check if the task was approximated.
+   */
+  public boolean isApproximated() {
+    /*
+    TaskAttemptID attemptId = getSuccessfulTaskid();
+    return attemptId == null ? false : isApproximated(attemptId);
+    */
+    TaskAttemptID attemptId = getSuccessfulTaskid();
+    if (attemptId != null) {
+      return isApproximated(attemptId);
+    } else {
+      return tasksApproximate.size()>0;
+    }
+  }
+  
+  /**
    * Kill the given task
    */
   boolean killTask(TaskAttemptID taskId, boolean shouldFail) {
@@ -859,6 +917,27 @@
     }
     return false;
   }
+  
+  /**
+   * Approximate Hadoop.
+   * Drop the given task
+   */
+  boolean dropTask(TaskAttemptID attemptId) {
+    TaskStatus st = taskStatuses.get(attemptId);
+    if(st != null && (st.getRunState() == TaskStatus.State.RUNNING
+        || st.getRunState() == TaskStatus.State.COMMIT_PENDING ||
+        st.inTaskCleanupPhase() ||
+        st.getRunState() == TaskStatus.State.UNASSIGNED)
+        && tasksToKill.put(attemptId, false) == null ) {
+      String logStr = "Request received to drop '" + attemptId + "' by user";
+      addDiagnosticInfo(attemptId, logStr);
+      LOG.info(logStr);
+      // We mark this task as dropped
+      dropped = true;
+      return true;
+    }
+    return false;
+  }
 
   /**
    * This method is called whenever there's a status change
@@ -885,7 +964,7 @@
       for (Iterator<TaskAttemptID> it = taskStatuses.keySet().iterator(); it.hasNext();) {
         TaskAttemptID taskid = it.next();
         TaskStatus status = taskStatuses.get(taskid);
-        if (status.getRunState() == TaskStatus.State.SUCCEEDED) {
+        if (status.getRunState() == TaskStatus.State.SUCCEEDED || status.getRunState() == TaskStatus.State.DROPPED) {
           bestProgress = 1;
           bestState = status.getStateString();
           bestCounters = status.getCounters();
@@ -939,7 +1018,8 @@
     //
       
     if (!skipping && activeTasks.size() <= MAX_TASK_EXECS &&
-        (averageProgress - progress >= SPECULATIVE_GAP) &&
+        // Approximate Hadoop: streaming jobs report 100% from second 0, don't consider this as a constraint
+        (averageProgress - progress >= SPECULATIVE_GAP || progress >= 1) &&
         (currentTime - startTime >= SPECULATIVE_LAG) 
         && completes == 0 && !isOnlyCommitPending()) {
       return true;
@@ -1188,7 +1268,7 @@
     
     synchronized void updateState(TaskStatus status){
       if (isTestAttempt() && 
-          (status.getRunState() == TaskStatus.State.SUCCEEDED)) {
+          (status.getRunState() == TaskStatus.State.SUCCEEDED || status.getRunState() == TaskStatus.State.DROPPED)) {
         divide.testPassed = true;
         //since it was the test attempt we need to set it to failed
         //as it worked only on the test range
Index: src/mapred/org/apache/hadoop/mapred/TaskRunner.java
===================================================================
--- src/mapred/org/apache/hadoop/mapred/TaskRunner.java	(revision 1596572)
+++ src/mapred/org/apache/hadoop/mapred/TaskRunner.java	(working copy)
@@ -106,6 +106,11 @@
   protected JobConf conf;
   JvmManager jvmManager;
 
+  /**
+   * Approximate Hadoop
+   */
+  private boolean approximate;
+  
   /** 
    * for cleaning up old map outputs
    */
@@ -129,6 +134,20 @@
   public TaskTracker.TaskInProgress getTaskInProgress() { return tip; }
   public TaskTracker getTracker() { return tracker; }
 
+  /**
+   * Approximate Hadoop
+   */
+  public void setApproximate(boolean approximate) {
+    this.approximate = approximate;
+  }
+  
+  /**
+   * Approximate Hadoop
+   */
+  public boolean isApproximate() {
+    return this.approximate;
+  }
+  
   /** Called to assemble this task's input.  This method is run in the parent
    * process before the child is spawned.  It should not execute user code,
    * only system code. */
@@ -247,7 +266,6 @@
         setupCmds.add(sb.toString());
       }
       setupCmds.add(setup);
-      
       launchJvmAndWait(setupCmds, vargs, stdout, stderr, logSize, workDir);
       tracker.getTaskTrackerInstrumentation().reportTaskEnd(t.getTaskID());
       if (exitCodeSet) {
Index: src/mapred/org/apache/hadoop/mapred/TaskScheduler.java
===================================================================
--- src/mapred/org/apache/hadoop/mapred/TaskScheduler.java	(revision 1596572)
+++ src/mapred/org/apache/hadoop/mapred/TaskScheduler.java	(working copy)
@@ -97,6 +97,19 @@
    */
   public void refresh() throws IOException {}
 
+  /**
+   * Method to verify if we should run a task in a taskTracker.
+   */
+  public synchronized boolean runTaskInTracker(Task task, TaskTracker taskTracker) {
+    return true;
+  }
+  
+  /**
+   * Method to verify if we should run a job in a taskTracker.
+   */
+  public synchronized boolean runJobInTracker(JobInProgress job, String taskTrackerHost) {
+    return true;
+  }
 
   /**
    * Subclasses can override to provide any scheduler-specific checking
Index: src/mapred/org/apache/hadoop/mapred/TaskStatus.java
===================================================================
--- src/mapred/org/apache/hadoop/mapred/TaskStatus.java	(revision 1596572)
+++ src/mapred/org/apache/hadoop/mapred/TaskStatus.java	(working copy)
@@ -42,7 +42,9 @@
 
   // what state is the task in?
   public static enum State {RUNNING, SUCCEEDED, FAILED, UNASSIGNED, KILLED, 
-                            COMMIT_PENDING, FAILED_UNCLEAN, KILLED_UNCLEAN}
+                            COMMIT_PENDING, FAILED_UNCLEAN, KILLED_UNCLEAN,
+                            // Approximate Hadoop
+                            DROPPED}
     
   private final TaskAttemptID taskid;
   private float progress;
Index: src/mapred/org/apache/hadoop/mapred/TaskTracker.java
===================================================================
--- src/mapred/org/apache/hadoop/mapred/TaskTracker.java	(revision 1596572)
+++ src/mapred/org/apache/hadoop/mapred/TaskTracker.java	(working copy)
@@ -280,6 +280,8 @@
    */
   Map<TaskAttemptID, TaskInProgress> runningTasks = null;
   Map<JobID, RunningJob> runningJobs = new TreeMap<JobID, RunningJob>();
+  // Approximate Hadoop
+  Map<JobID, RunningJob> oldJobs = new TreeMap<JobID, RunningJob>();
   private final JobTokenSecretManager jobTokenSecretManager
     = new JobTokenSecretManager();
 
@@ -2248,9 +2250,17 @@
     }
 
     synchronized(runningJobs) {
-      runningJobs.remove(jobId);
+      if (!rjob.keepJobFiles) {
+        runningJobs.remove(jobId);
+      } else {
+        // Approximate Hadoop: we store the jobs that we may reuse
+        oldJobs.put(jobId, runningJobs.remove(jobId));
+      }
     }
-    getJobTokenSecretManager().removeTokenForJob(jobId.toString());  
+    // Approximate Hadoop: we want to access to this token after we kill the job
+    if (!rjob.keepJobFiles) {
+      getJobTokenSecretManager().removeTokenForJob(jobId.toString());
+    }
     distributedCacheManager.removeTaskDistributedCacheManager(jobId);
   }
 
@@ -3989,7 +3999,7 @@
         (TaskTracker) context.getAttribute("task.tracker");
       ShuffleExceptionTracker shuffleExceptionTracking =
         (ShuffleExceptionTracker) context.getAttribute("shuffleExceptionTracking");
-
+      
       verifyRequest(request, response, tracker, jobId);
 
       long startTime = 0;
@@ -4009,7 +4019,11 @@
       synchronized (tracker.runningJobs) {
         RunningJob rjob = tracker.runningJobs.get(JobID.forName(jobId));
         if (rjob == null) {
-          throw new IOException("Unknown job " + jobId + "!!");
+          // Approximate Job: check if the job is in the old jobs
+          rjob = tracker.oldJobs.get(JobID.forName(jobId));
+          if (rjob == null) {
+            throw new IOException("Unknown job " + jobId + "!!");
+          }
         }
         userName = rjob.jobConf.getUser();
         runAsUserName = tracker.getTaskController().getRunAsUser(rjob.jobConf);
@@ -4030,8 +4044,7 @@
         mapOutputFileName = lDirAlloc.getLocalPathToRead(fileKey, conf);
         fileCache.put(fileKey, mapOutputFileName);
       }
-       
-
+      
         /**
          * Read the index file to get the information about where
          * the map-output for the given reducer is available. 
@@ -4061,11 +4074,12 @@
         response.setBufferSize(RESPONSE_BUFFER_SIZE);
 
         /**
-         * Read the data from the sigle map-output file and
+         * Read the data from the single map-output file and
          * send it to the reducer.
          */
         //open the map-output file
         String filePath = mapOutputFileName.toUri().getPath();
+        
         mapOutputIn = SecureIOUtils.openForRead(
             new File(filePath), runAsUserName);
             //new File(mapOutputFileName.toUri().getPath()), runAsUserName);
@@ -4177,13 +4191,25 @@
       LOG.debug("verifying request. enc_str="+enc_str+"; hash=..."+
           urlHashStr.substring(len-len/2, len-1)); // half of the hash for debug
 
+// Approximate Hadoop: we authorize to read the job output
+LOG.error("1 Verify request " + request);
+LOG.error("2 Verify request " + response);
+LOG.error("3 Verify request " + tracker);
+LOG.error("4 Verify request " + jobId);
+LOG.error("5 Verify request " + urlHashStr);
+LOG.error("6 Verify request " + enc_str);
+LOG.error("7 Verify request " + tokenSecret);
+// TODO we have to figure out a way to do this verification for old map outputs
+// TODO if fails when SecureShuffleUtils.verifyReply
+      
+      // Approximate Hadoop: we have removed this check, we have to find a way to put it back
       // verify - throws exception
-      try {
+      /*try {
         SecureShuffleUtils.verifyReply(urlHashStr, enc_str, tokenSecret);
       } catch (IOException ioe) {
         response.sendError(HttpServletResponse.SC_UNAUTHORIZED);
         throw ioe;
-      }
+      }*/
       
       // verification passed - encode the reply
       String reply = SecureShuffleUtils.generateHash(urlHashStr.getBytes(), tokenSecret);
Index: src/mapred/org/apache/hadoop/mapred/TaskTrackerStatus.java
===================================================================
--- src/mapred/org/apache/hadoop/mapred/TaskTrackerStatus.java	(revision 1596572)
+++ src/mapred/org/apache/hadoop/mapred/TaskTrackerStatus.java	(working copy)
@@ -52,6 +52,7 @@
   List<TaskStatus> taskReports;
     
   volatile long lastSeen;
+  private boolean sleep = false; // Agile Hadoop
   private int maxMapTasks;
   private int maxReduceTasks;
   private TaskTrackerHealthStatus healthStatus;
@@ -515,6 +516,18 @@
   }
 
   /**
+   * Agile Hadoop.
+   * 
+   */
+  public boolean isSleep() {
+    return this.sleep && (System.currentTimeMillis() - getLastSeen()) > 5*1000; // 5 seconds
+  }
+  
+  public void setSleep(boolean sleep) {
+    this.sleep = sleep;
+  }
+  
+  /**
    * Get the maximum map slots for this node.
    * @return the maximum map slots for this node
    */
Index: src/mapred/org/apache/hadoop/mapred/lib/MinReducer.java
===================================================================
--- src/mapred/org/apache/hadoop/mapred/lib/MinReducer.java	(revision 0)
+++ src/mapred/org/apache/hadoop/mapred/lib/MinReducer.java	(working copy)
@@ -0,0 +1,239 @@
+package org.apache.hadoop.mapred.lib;
+
+import java.io.IOException;
+
+import java.util.Iterator;
+import java.util.List;
+import java.util.LinkedList;
+import java.util.Collections;
+
+import org.apache.hadoop.io.IntWritable;
+import org.apache.hadoop.io.Text;
+import org.apache.hadoop.mapred.JobConf;
+import org.apache.hadoop.mapred.OutputCollector;
+import org.apache.hadoop.mapred.Reporter;
+import org.apache.hadoop.mapred.Reducer;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+
+/**
+ * This reducer reads values from the maps and selects the minimum.
+ * It uses characteristics from the map output distribution to provide a minimum with a given error requirement.
+ */
+public class MinReducer implements Reducer<Text,Text,Text,Text> {
+	private static final Log LOG = LogFactory.getLog(MinReducer.class);
+
+	//private IntWritable result = new IntWritable();
+	private double minKey = 999999.9;
+	private String minValue = null;
+	private OutputCollector<Text, Text> output = null;
+	
+	// We store the outputs of the maps (original distribution sample)
+	private List<Double> sample = null;
+	
+	// Error requirements
+	private float reqConfidence; // Between 0 and 1
+	private float reqError;      // Between 0 and 100%
+	
+	/**
+	 * Selects the confidence/error requirements.
+	 */
+	public void configure(JobConf conf) {
+		this.sample = new LinkedList<Double>();
+		
+		// Read configuration values for the minimum error required
+		this.reqConfidence = conf.getFloat("approx.minconfidence", 0.95f); // Between 0 and 1
+		this.reqError      = conf.getFloat("approx.minerror", 0.0f);       // In percentage
+	}
+	
+	/**
+	 * Collects the outputs from the maps and selects the one with the minimum cost.
+	 */
+	public void reduce(Text key, Iterator<Text> values, OutputCollector<Text, Text> output, Reporter reporter) throws IOException {
+		this.output = output;
+		if (key.toString().equals("Result")) {
+			while (values.hasNext()) {
+				// Save intermediate result
+				Text val = values.next();
+				output.collect(key, val);
+				
+				// Extract minimum cost
+				double cost = getCostFromValue(val);
+				if (cost < minKey) {
+					minKey = cost;
+					minValue = val.toString();
+					
+				}
+				// Add to the sample
+				this.sample.add(cost);
+				
+				// Calculate the minimum margin 
+				double minError = this.getError();
+				
+				// Calculate the minimum with margin
+				//double minValueWithError  = this.sample.get(0)*(1.0-(minError/100.0));
+				
+				LOG.info(String.format("Current error with a confidence of %.1f is %.2f%% (<= %.2f%%?)", this.reqConfidence*100.0, minError, this.reqError));
+				
+				if (minError <= this.reqError) { // For example 5% error
+					// We mark that we can start dropping
+					LOG.info("Minimum error requirement achieved! We can drop the other maps!");
+					// We set the status to dropping and the JobTracker will take care
+					reporter.setStatus("dropping");
+				}
+			}
+		}
+	}
+	
+	/**
+	 * Get the error when estimating the minimum based on the current sample.
+	 */
+	private double getError() {
+		// Average for the current sample
+		int n = 0;
+		double avg = 0.0;
+		for (double v : this.sample) {
+			avg += v;
+			n++;
+		}
+		avg = avg/n;
+		
+		// Standard deviation for the current sample
+		double stdev = 0.0;
+		for (double v : this.sample) {
+			stdev += Math.pow(v-avg, 2);
+		}
+		stdev = Math.sqrt(stdev/(n-1));
+		
+		// Calculate the error for the standard deviation
+		double maxError = (getZScore(this.reqConfidence)*stdev)/Math.sqrt(n); // Based on CLT
+		
+		// Calculate the standard deviation taking into account the potential sampling error
+		double stdevCorrected = 0.0;
+		for (double v : this.sample) {
+			// We add the maximum error for the estimation
+			stdevCorrected += Math.pow(Math.abs(v-avg)+maxError, 2);
+		}
+		stdevCorrected = Math.sqrt(stdevCorrected/(n-1));
+		
+		// Calculate minimum error end return it
+		return getMinDistError(this.reqConfidence, stdevCorrected, n);
+	}
+	
+	
+	/**
+	 * Get the error to get a minimum with a given confidence after n samples.
+	 * @param confidence Confidence between 0 and 1. 0.95 would be 95% confidence.
+	 * @param stdev Sample standard deviation (S)
+	 * @param n Size of the sample (n)
+	 * @return Maximum estimating error for the defined distribution.
+	 */
+	private double getMinDistError(double confidence, double stdev, int n) {
+		// The regular way would be to use this formula but it gets into imaginary numbers and other weirdnesses
+		// double minerror = Math.sqrt((120*Math.pow(stdev, 3))*(Math.log(-1.0/(0.95-1))/Math.log(n)-1));
+	
+		// We use dicotomic search to calculate the minimum error we can guarrantee
+		double ini = 0.0;
+		double fin = 100.0;
+		while (fin-ini > 0.01) {
+			double mid = (ini+fin)/2.0;
+			double confMid = confidenceMinimum(mid, stdev, n);
+			if (confMid > confidence) {
+				fin = mid;
+			} else {
+				ini = mid;
+			}
+		}
+		return fin;
+		
+		/*
+		// Linear search
+		double minerrorTest = 0.0;
+		for (double error = 0.1; error < 100.0; error += 0.1) {
+			if (confidenceMinimum(error, stdevCorrected, n) > 0.95) {
+				minerrorTest = error;
+				break;
+			}
+		}
+		*/
+	}
+	
+	/**
+	 * Calculate the confidence of the minimum for a distribution with a standard deviation and a number of samples.
+	 * @param error Error (interval) in percentage (er)
+	 * @param stdev Sample standard deviation (S)
+	 * @param n Size of the sample (n)
+	 * @return Confidence of the minimum (a value from 0 to 1 which represents the percentile, eg, 0.95 is 95% confidence)
+	 */
+	private double confidenceMinimum(double error, double stdev, int n) {
+		// We obtain this formula from experiments with multimple minimum distrigutions based on GEV
+		//                               1
+		// confidence =  1 - --------------------------
+		//                    /         1          \ ^n
+		//                   |  1 + --------- er^2  |
+		//                    \     120 x S^2      /
+		return 1.0 - 1.0/Math.pow(1 + (1/(120.0*Math.pow(stdev, 3)))*Math.pow(error, 2), n);
+	}
+	
+	/**
+	 * Parse an output line from the DC placement tool and extract the cost.
+	 */
+	private double getCostFromValue(Text value) {
+		String result = value.toString();
+		result = result.split("f=")[1];
+		result = result.substring(0, result.indexOf("M"));
+		return Double.parseDouble(result);
+	}
+	
+	/**
+	 * Get the Z-score for a normal distribution.
+	 */
+	private static double getZScore(double confidence) {
+		if (confidence == 0)          { return 0.0; } 
+		else if (confidence <= 0.05)  { return 0.0627; } 
+		else if (confidence <= 0.10)  { return 0.1257; } 
+		else if (confidence <= 0.15)  { return 0.1891; } 
+		else if (confidence <= 0.20)  { return 0.2533; } 
+		else if (confidence <= 0.25)  { return 0.3186; } 
+		else if (confidence <= 0.30)  { return 0.3853; } 
+		else if (confidence <= 0.40)  { return 0.5244; } 
+		else if (confidence <= 0.50)  { return 0.67; } 
+		else if (confidence <= 0.60)  { return 0.84; } 
+		else if (confidence <= 0.70)  { return 1.04; } 
+		else if (confidence <= 0.75)  { return 1.15; } 
+		else if (confidence <= 0.80)  { return 1.28; } 
+		else if (confidence <= 0.85)  { return 1.44; }
+		else if (confidence <= 0.90)  { return 1.645; } 
+		else if (confidence <= 0.95)  { return 1.96; }
+		else if (confidence <= 0.98)  { return 2.33; } 
+		else if (confidence <= 0.99)  { return 2.575; }
+		else if (confidence <= 0.995) { return 2.81; } 
+		else if (confidence <= 0.999) { return 3.09; }
+		return 5.0;
+	}
+	
+	/**
+	 * We write the final result. We use the estimation of the error and a confidence interval.
+	 */
+	public void close() {
+		try {
+			LOG.info("Final sample: " + this.sample);
+			
+			// Calculate the minimum with the confidence margin
+			double minCost = Collections.min(this.sample);
+			double minCostWithError = minCost * (1.0 - (this.getError()/100.0));
+			
+			// Save the result into the output
+			if (minValue == null) {
+				LOG.error("No samples");
+			} else {
+				String aux = String.format("(%.2f-%.2f]: ", minCostWithError, minCost)  + " "+ minValue;
+				output.collect(new Text("Final result"), new Text(aux));
+				LOG.info(aux);
+			}
+		} catch (Exception e) {
+			System.out.println("Failure: " + e);
+		}
+	}
+}
Index: src/mapred/org/apache/hadoop/mapred/lib/MultithreadedMapRunner.java
===================================================================
--- src/mapred/org/apache/hadoop/mapred/lib/MultithreadedMapRunner.java	(revision 1596572)
+++ src/mapred/org/apache/hadoop/mapred/lib/MultithreadedMapRunner.java	(working copy)
@@ -191,7 +191,13 @@
     }
   }
 
+  /**
+   * Approximate Hadoop.
+   */
+  public void setApproximate(boolean approximate) {
 
+  }
+
   /**
    * Runnable to execute a single Mapper.map call from a forked thread.
    */
Index: src/mapred/org/apache/hadoop/mapreduce/ApproximateMapper.java
===================================================================
--- src/mapred/org/apache/hadoop/mapreduce/ApproximateMapper.java	(revision 0)
+++ src/mapred/org/apache/hadoop/mapreduce/ApproximateMapper.java	(working copy)
@@ -0,0 +1,69 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.mapreduce;
+
+import java.io.IOException;
+
+/** 
+ * Approximate Hadoop
+ */
+public class ApproximateMapper<KEYIN, VALUEIN, KEYOUT, VALUEOUT> extends Mapper<KEYIN, VALUEIN, KEYOUT, VALUEOUT> {
+  private boolean approximate = false;
+
+  /**
+   * Approximate Hadoop.
+   */
+  @SuppressWarnings("unchecked")
+  protected void mapApproximate(KEYIN key, VALUEIN value, Context context) throws IOException, InterruptedException {
+    super.map(key, value, context);
+  }
+  
+  /**
+   * Approximate Hadoop.
+   * Define if we can approximate this map.
+   */
+  public void setApproximate(boolean approximate) {
+    this.approximate = approximate;
+  }
+  
+  /**
+   * Approximate Hadoop.
+   */
+  public boolean isApproximate() {
+    return this.approximate;
+  }
+  
+  /**
+   * Expert users can override this method for more complete control over the
+   * execution of the Mapper.
+   * @param context
+   * @throws IOException
+   */
+  public void run(Context context) throws IOException, InterruptedException {
+    setup(context);
+    while (context.nextKeyValue()) {
+      if (isApproximate()) {
+        mapApproximate(context.getCurrentKey(), context.getCurrentValue(), context);
+      } else {
+        map(context.getCurrentKey(), context.getCurrentValue(), context);
+      }
+    }
+    cleanup(context);
+  }
+}
\ No newline at end of file
Index: src/mapred/org/apache/hadoop/mapreduce/ApproximateReducer.java
===================================================================
--- src/mapred/org/apache/hadoop/mapreduce/ApproximateReducer.java	(revision 0)
+++ src/mapred/org/apache/hadoop/mapreduce/ApproximateReducer.java	(working copy)
@@ -0,0 +1,66 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.mapreduce;
+
+import java.io.IOException;
+
+/** 
+ * Approximate Hadoop.
+ */
+public class ApproximateReducer<KEYIN,VALUEIN,KEYOUT,VALUEOUT> extends Reducer<KEYIN,VALUEIN,KEYOUT,VALUEOUT> {
+  private boolean approximate = false;
+  
+  /**
+   * Approximate reducer.
+   */
+  @SuppressWarnings("unchecked")
+  protected void reduceApproximate(KEYIN key, Iterable<VALUEIN> values, Context context) throws IOException, InterruptedException {
+    super.reduce(key, values, context);
+  }
+
+  /**
+   * Approximate Hadoop.
+   * Define if we can approximate this map.
+   */
+  public void setApproximate(boolean approximate) {
+    this.approximate = approximate;
+  }
+  
+  /**
+   * Approximate Hadoop.
+   */
+  public boolean isApproximate() {
+    return this.approximate;
+  }
+  
+  /**
+   * Use approximation if it is approximate.
+   */
+  public void run(Context context) throws IOException, InterruptedException {
+    setup(context);
+    while (context.nextKey()) {
+      if (isApproximate()) {
+        reduceApproximate(context.getCurrentKey(), context.getValues(), context);
+      } else {
+        reduce(context.getCurrentKey(), context.getValues(), context);
+      }
+    }
+    cleanup(context);
+  }
+}
Index: src/mapred/org/apache/hadoop/mapreduce/MapContext.java
===================================================================
--- src/mapred/org/apache/hadoop/mapreduce/MapContext.java	(revision 1596572)
+++ src/mapred/org/apache/hadoop/mapreduce/MapContext.java	(working copy)
@@ -67,5 +67,7 @@
     return reader.nextKeyValue();
   }
 
+  public RecordReader<KEYIN,VALUEIN> getReader() {
+    return reader;
+  }
 }
-     
\ No newline at end of file
Index: src/mapred/org/apache/hadoop/mapreduce/ReduceContext.java
===================================================================
--- src/mapred/org/apache/hadoop/mapreduce/ReduceContext.java	(revision 1596572)
+++ src/mapred/org/apache/hadoop/mapreduce/ReduceContext.java	(working copy)
@@ -40,7 +40,8 @@
  */
 public class ReduceContext<KEYIN,VALUEIN,KEYOUT,VALUEOUT>
     extends TaskInputOutputContext<KEYIN,VALUEIN,KEYOUT,VALUEOUT> {
-  private RawKeyValueIterator input;
+  // Incremental needs it protected and not private
+  protected RawKeyValueIterator input; 
   private Counter inputKeyCounter;
   private Counter inputValueCounter;
   private RawComparator<KEYIN> comparator;
@@ -48,7 +49,8 @@
   private VALUEIN value;                              // current value
   private boolean firstValue = false;                 // first value in key
   private boolean nextKeyIsSame = false;              // more w/ this key
-  private boolean hasMore;                            // more in file
+  // Incremental needs it protected and not private
+  protected boolean hasMore;                            // more in file 
   protected Progressable reporter;
   private Deserializer<KEYIN> keyDeserializer;
   private Deserializer<VALUEIN> valueDeserializer;
@@ -55,7 +57,7 @@
   private DataInputBuffer buffer = new DataInputBuffer();
   private BytesWritable currentRawKey = new BytesWritable();
   private ValueIterable iterable = new ValueIterable();
-
+  
   public ReduceContext(Configuration conf, TaskAttemptID taskid,
                        RawKeyValueIterator input, 
                        Counter inputKeyCounter,
@@ -77,7 +79,8 @@
     this.keyDeserializer.open(buffer);
     this.valueDeserializer = serializationFactory.getDeserializer(valueClass);
     this.valueDeserializer.open(buffer);
-    hasMore = input.next();
+    // Incremental doesn't do this here anymore, now we do it in the reducer
+    // hasMore = input.next(); 
   }
 
   /** Start processing next unique key. */
Index: src/mapred/org/apache/hadoop/mapreduce/Reducer.java
===================================================================
--- src/mapred/org/apache/hadoop/mapreduce/Reducer.java	(revision 1596572)
+++ src/mapred/org/apache/hadoop/mapreduce/Reducer.java	(working copy)
@@ -119,6 +119,7 @@
 
   public class Context 
     extends ReduceContext<KEYIN,VALUEIN,KEYOUT,VALUEOUT> {
+    
     public Context(Configuration conf, TaskAttemptID taskid,
                    RawKeyValueIterator input, 
                    Counter inputKeyCounter,
@@ -133,9 +134,51 @@
       super(conf, taskid, input, inputKeyCounter, inputValueCounter,
             output, committer, reporter, 
             comparator, keyClass, valueClass);
+            getfirst();
+       }
+    
+    // Incremental
+    public void getfirst() throws IOException{   
+      hasMore = input.next();
     }
   }
 
+	/**
+	 * Incremental
+	 */
+	public class ContextThreaded extends Context implements Runnable {
+		private Reducer<KEYIN,VALUEIN,KEYOUT,VALUEOUT> reducer;
+
+		public ContextThreaded(Configuration conf, TaskAttemptID taskid,
+				RawKeyValueIterator input, 
+				Counter inputKeyCounter,
+				Counter inputValueCounter,
+				RecordWriter<KEYOUT,VALUEOUT> output,
+				OutputCommitter committer,
+				StatusReporter reporter,
+				RawComparator<KEYIN> comparator,
+				Class<KEYIN> keyClass,
+				Class<VALUEIN> valueClass,
+				Reducer<KEYIN,VALUEIN,KEYOUT,VALUEOUT> reducer) throws IOException, InterruptedException {
+			super(conf, taskid, input, inputKeyCounter, inputValueCounter, output, committer, reporter, comparator, keyClass, valueClass);
+			
+			// To run this as a thread
+			this.reducer = reducer;
+		}
+		
+		public void getfirst() throws IOException{ }
+		
+		public void run(){
+			try{
+				hasMore = input.next();
+				this.reducer.run((Context)this); 
+			} catch (Exception e) {
+				e.printStackTrace();
+			} 
+		}
+	}
+  
+  
   /**
    * Called once at the start of the task.
    */
Index: src/mapred/org/apache/hadoop/mapreduce/SamplingRecordReader.java
===================================================================
--- src/mapred/org/apache/hadoop/mapreduce/SamplingRecordReader.java	(revision 0)
+++ src/mapred/org/apache/hadoop/mapreduce/SamplingRecordReader.java	(working copy)
@@ -0,0 +1,9 @@
+package org.apache.hadoop.mapreduce;
+
+/**
+ * Interface for a RecordReader that allows sampling.
+ * @author Inigo Goiri
+ */
+public interface SamplingRecordReader {
+	public int getSamplingRatio();
+}
\ No newline at end of file
Index: src/mapred/org/apache/hadoop/mapreduce/TaskAttemptContext.java
===================================================================
--- src/mapred/org/apache/hadoop/mapreduce/TaskAttemptContext.java	(revision 1596572)
+++ src/mapred/org/apache/hadoop/mapreduce/TaskAttemptContext.java	(working copy)
@@ -63,4 +63,17 @@
    */
   public void progress() { 
   }
+  
+  /**
+   * Approximate Hadoop needs this. In Hadoop 2 these methods are already here.
+   */
+  public Counter getCounter(Enum<?> counterName) {
+    //return reporter.getCounter(counterName);
+    return null;
+  }
+
+  public Counter getCounter(String groupName, String counterName) {
+    //return reporter.getCounter(groupName, counterName);
+    return null;
+  }
 }
\ No newline at end of file
Index: src/mapred/org/apache/hadoop/mapreduce/lib/input/ApproximateLineRecordReader.java
===================================================================
--- src/mapred/org/apache/hadoop/mapreduce/lib/input/ApproximateLineRecordReader.java	(revision 0)
+++ src/mapred/org/apache/hadoop/mapreduce/lib/input/ApproximateLineRecordReader.java	(working copy)
@@ -0,0 +1,189 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.mapreduce.lib.input;
+
+import java.io.IOException;
+
+import java.util.Random;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.FSDataInputStream;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.fs.Seekable;
+import org.apache.hadoop.io.LongWritable;
+import org.apache.hadoop.io.Text;
+import org.apache.hadoop.io.compress.CodecPool;
+import org.apache.hadoop.io.compress.CompressionCodec;
+import org.apache.hadoop.io.compress.CompressionCodecFactory;
+import org.apache.hadoop.io.compress.Decompressor;
+import org.apache.hadoop.io.compress.SplitCompressionInputStream;
+import org.apache.hadoop.io.compress.SplittableCompressionCodec;
+import org.apache.hadoop.mapreduce.InputSplit;
+import org.apache.hadoop.mapreduce.RecordReader;
+import org.apache.hadoop.mapreduce.SamplingRecordReader;
+import org.apache.hadoop.mapreduce.TaskAttemptContext;
+import org.apache.hadoop.util.LineReader;
+import org.apache.commons.logging.LogFactory;
+import org.apache.commons.logging.Log;
+
+import org.apache.hadoop.mapred.JobClient;
+import org.apache.hadoop.mapred.RunningJob;
+import org.apache.hadoop.mapred.JobConf;
+import org.apache.hadoop.mapred.Counters.Counter;
+
+
+/**
+ * Extension to {@code LineRecordReader} that reads lines using random sampling.
+ * @author Inigo Goiri
+ */
+public class ApproximateLineRecordReader extends LineRecordReader implements SamplingRecordReader {
+  private static final Log LOG = LogFactory.getLog(ApproximateLineRecordReader.class);
+  
+  // Variables for input sampling
+  private int lineNumber = 0; // Counter of the number of lines.
+  private int skipLines = 1; // Sampling ratio
+  private int maxLines = -1;
+  private int prevSkipLines = 0; // How many lines we still have to skip
+
+  // Random number generator for random sampling
+  private Random rnd;
+  
+  /**
+   * This is basically an extension of {@link org.apache.hadoop.mapreduce.LineRecordReader} that add sampling.
+   */
+  public void initialize(InputSplit genericSplit, TaskAttemptContext context) throws IOException {
+    super.initialize(genericSplit, context);
+    
+    // Random number generator to generate a random sample
+    rnd = new Random();
+    
+    // Get the parameters from the user
+    Configuration conf = context.getConfiguration();
+    
+    // Sampling ratio
+    skipLines = conf.getInt("mapred.input.approximate.skip", 1);
+    
+    // Use adaptive sampling ratio
+    if (conf.getBoolean("mapred.input.approximate.skip.adaptive", false)) {
+      try {
+        // Connect to the JobTracker to get the sampling ratio required by the reducers
+        // We use a hybrid between the old and the new APIs, in the newest one we should "Cluster()"
+        JobClient client = new JobClient(new JobConf(conf));
+        RunningJob parentJob = client.getJob(context.getJobID().toString());
+          
+        // Check the values for every reducer
+        int numReducers = conf.getInt("mapred.reduce.tasks", 1);
+        int minSamplingRatio = Integer.MAX_VALUE;
+        org.apache.hadoop.mapred.Counters counters = parentJob.getCounters();
+        // Get the required sampling ratio for each reducer
+        for (int reducer=0; reducer<numReducers; reducer++) {
+          org.apache.hadoop.mapred.Counters.Counter counter = counters.findCounter("SamplingRatio", Integer.toString(reducer));
+          int samplingRatio = skipLines;
+          if (counter.getValue() > 0) {
+            samplingRatio = (int) counter.getValue();
+          }
+          // Check which is the minimum sampling ratio we should follow
+          if (samplingRatio < minSamplingRatio) {
+            minSamplingRatio = samplingRatio;
+          }
+        }
+        skipLines = minSamplingRatio;
+      } catch (Exception e) {
+        System.err.println("Error getting the sampling ratio: " + e);
+      }
+    }
+    
+    // Negative ratios don't mean anything
+    if (skipLines < 1) {
+      skipLines = 1;
+    }
+    
+    // Maximum number of lines
+    maxLines = conf.getInt("mapred.input.approximate.max", -1);
+  }
+  
+  /**
+   * An extension that skips lines from the log.
+   */
+  public boolean nextKeyValue() throws IOException {
+    // Select a random number to skip
+    int randomNumber = rnd.nextInt(this.skipLines);
+    
+    if (key == null) {
+      key = new LongWritable();
+    }
+    key.set(pos);
+    if (value == null) {
+      value = new Text();
+    }
+    int newSize = 0;
+    // We always read one extra line, which lies outside the upper split limit i.e. (end - 1)
+    while (getFilePosition() <= end) {
+      // Read a maximum number of lines
+      if (lineNumber > 0 && lineNumber < maxLines) {
+        break;
+      }
+    
+      newSize = in.readLine(value, maxLineLength, Math.max(maxBytesToConsume(pos), maxLineLength));
+      if (newSize == 0) {
+        break;
+      }
+      
+      // We skip one every "skipLines" lines
+      lineNumber++;
+      
+      // We first skip the number of lines from the previous run
+      if (prevSkipLines > 0) {
+        prevSkipLines--;
+        continue;
+      }
+      
+      // Now we skip the selected random
+      if (randomNumber > 0) {
+        randomNumber--;
+        prevSkipLines--;
+        continue;
+      }
+      
+      pos += newSize;
+      if (newSize < maxLineLength) {
+        break;
+      }
+    }
+    
+    // We now account for the lines we need to skip the next time
+    prevSkipLines = skipLines + prevSkipLines-1;
+    
+    if (newSize == 0) {
+      key = null;
+      value = null;
+      return false;
+    } else {
+      return true;
+    }
+  }
+  
+  /**
+   * Get the sampling ratio.
+   */
+  public int getSamplingRatio() {
+    return skipLines;
+  }
+}
Index: src/mapred/org/apache/hadoop/mapreduce/lib/input/ApproximateTextInputFormat.java
===================================================================
--- src/mapred/org/apache/hadoop/mapreduce/lib/input/ApproximateTextInputFormat.java	(revision 0)
+++ src/mapred/org/apache/hadoop/mapreduce/lib/input/ApproximateTextInputFormat.java	(working copy)
@@ -0,0 +1,36 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.mapreduce.lib.input;
+
+import org.apache.hadoop.io.LongWritable;
+import org.apache.hadoop.io.Text;
+import org.apache.hadoop.mapreduce.InputSplit;
+import org.apache.hadoop.mapreduce.RecordReader;
+import org.apache.hadoop.mapreduce.TaskAttemptContext;
+
+/** 
+ * Extension to {@code TextInputFormat} that reads lines using random sampling.
+ * @author Inigo Goiri
+ */
+public class ApproximateTextInputFormat extends TextInputFormat {
+  @Override
+  public RecordReader<LongWritable, Text> createRecordReader(InputSplit split, TaskAttemptContext context) {
+    return new ApproximateLineRecordReader();
+  }
+}
Index: src/mapred/org/apache/hadoop/mapreduce/lib/input/FileInputFormat.java
===================================================================
--- src/mapred/org/apache/hadoop/mapreduce/lib/input/FileInputFormat.java	(revision 1596572)
+++ src/mapred/org/apache/hadoop/mapreduce/lib/input/FileInputFormat.java	(working copy)
@@ -66,7 +66,7 @@
       }
     }; 
 
-  static final String NUM_INPUT_FILES = "mapreduce.input.num.files";
+  public static final String NUM_INPUT_FILES = "mapreduce.input.num.files";
 
   /**
    * Proxy PathFilter that accepts a path only if all filters given in the
Index: src/mapred/org/apache/hadoop/mapreduce/lib/input/LineRecordReader.java
===================================================================
--- src/mapred/org/apache/hadoop/mapreduce/lib/input/LineRecordReader.java	(revision 1596572)
+++ src/mapred/org/apache/hadoop/mapreduce/lib/input/LineRecordReader.java	(working copy)
@@ -47,13 +47,13 @@
   private static final Log LOG = LogFactory.getLog(LineRecordReader.class);
 
   private CompressionCodecFactory compressionCodecs = null;
-  private long start;
-  private long pos;
-  private long end;
-  private LineReader in;
-  private int maxLineLength;
-  private LongWritable key = null;
-  private Text value = null;
+  protected long start;
+  protected long pos;
+  protected long end;
+  protected LineReader in;
+  protected int maxLineLength;
+  protected LongWritable key = null;
+  protected Text value = null;
   private Seekable filePosition;
   private CompressionCodec codec;
   private Decompressor decompressor;
@@ -108,13 +108,13 @@
     return (codec != null);
   }
 
-  private int maxBytesToConsume(long pos) {
+  protected int maxBytesToConsume(long pos) {
     return isCompressedInput()
       ? Integer.MAX_VALUE
       : (int) Math.min(Integer.MAX_VALUE, end - pos);
   }
 
-  private long getFilePosition() throws IOException {
+  protected long getFilePosition() throws IOException {
     long retVal;
     if (isCompressedInput() && null != filePosition) {
       retVal = filePosition.getPos();
Index: src/webapps/hdfs/dfshealth.jsp
===================================================================
--- src/webapps/hdfs/dfshealth.jsp	(revision 1596572)
+++ src/webapps/hdfs/dfshealth.jsp	(working copy)
@@ -164,9 +164,9 @@
     ArrayList<DatanodeDescriptor> dead = new ArrayList<DatanodeDescriptor>();
     jspHelper.DFSNodesStatus(live, dead);
 
-    ArrayList<DatanodeDescriptor> decommissioning = fsn
-        .getDecommissioningNodes();
-	
+    ArrayList<DatanodeDescriptor> decommissioning = fsn.getDecommissioningNodes();
+    ArrayList<DatanodeDescriptor> sleep = fsn.getSleepingNodes();
+    
     sorterField = request.getParameter("sorter/field");
     sorterOrder = request.getParameter("sorter/order");
     if ( sorterField == null )
@@ -200,32 +200,33 @@
     float percentUsed = fsn.getCapacityUsedPercent();
     float percentRemaining = fsn.getCapacityRemainingPercent();
 
-    out.print( "<div id=\"dfstable\"> <table>\n" +
-	       rowTxt() + colTxt() + "Configured Capacity" + colTxt() + ":" + colTxt() +
-	       StringUtils.byteDesc( total ) +
-	       rowTxt() + colTxt() + "DFS Used" + colTxt() + ":" + colTxt() +
-	       StringUtils.byteDesc( used ) +
-	       rowTxt() + colTxt() + "Non DFS Used" + colTxt() + ":" + colTxt() +
-	       StringUtils.byteDesc( nonDFS ) +
-	       rowTxt() + colTxt() + "DFS Remaining" + colTxt() + ":" + colTxt() +
-	       StringUtils.byteDesc( remaining ) +
-	       rowTxt() + colTxt() + "DFS Used%" + colTxt() + ":" + colTxt() +
-	       StringUtils.limitDecimalTo2(percentUsed) + " %" +
-	       rowTxt() + colTxt() + "DFS Remaining%" + colTxt() + ":" + colTxt() +
-	       StringUtils.limitDecimalTo2(percentRemaining) + " %" +
-	       rowTxt() + colTxt() +
-	       		"<a href=\"dfsnodelist.jsp?whatNodes=LIVE\">Live Nodes</a> " +
-	       		colTxt() + ":" + colTxt() + live.size() +
-	       rowTxt() + colTxt() +
-	       		"<a href=\"dfsnodelist.jsp?whatNodes=DEAD\">Dead Nodes</a> " +
-	       		colTxt() + ":" + colTxt() + dead.size() + rowTxt() + colTxt()
-				+ "<a href=\"dfsnodelist.jsp?whatNodes=DECOMMISSIONING\">"
-				+ "Decommissioning Nodes</a> "
-				+ colTxt() + ":" + colTxt() + decommissioning.size()
-				+ rowTxt() + colTxt()
-				+ "Number of Under-Replicated Blocks" + colTxt() + ":" + colTxt()
-				+ fsn.getUnderReplicatedBlocks()
-                + "</table></div><br>\n" );
+    out.print("<div id=\"dfstable\">\n");
+    out.print("<table>\n");
+    out.print(rowTxt() + colTxt());
+    out.print("Configured Capacity" + colTxt() + ":" + colTxt() + StringUtils.byteDesc( total ));
+    out.print(rowTxt() + colTxt());
+    out.print("DFS Used" + colTxt() + ":" + colTxt() + StringUtils.byteDesc( used ));
+    out.print(rowTxt() + colTxt());
+    out.print("Non DFS Used" + colTxt() + ":" + colTxt() + StringUtils.byteDesc( nonDFS ));
+    out.print(rowTxt() + colTxt());
+    out.print("DFS Remaining" + colTxt() + ":" + colTxt() + StringUtils.byteDesc( remaining ));
+    out.print(rowTxt() + colTxt());
+    out.print("DFS Used%" + colTxt() + ":" + colTxt() + StringUtils.limitDecimalTo2(percentUsed) + " %");
+    out.print(rowTxt() + colTxt());
+    out.print("DFS Remaining%" + colTxt() + ":" + colTxt() + StringUtils.limitDecimalTo2(percentRemaining) + " %");
+    out.print(rowTxt() + colTxt());
+    out.print("<a href=\"dfsnodelist.jsp?whatNodes=LIVE\">Live Nodes</a> " + colTxt() + ":" + colTxt() + (live.size()-sleep.size()));
+    out.print(rowTxt() + colTxt());
+    out.print("<a href=\"dfsnodelist.jsp?whatNodes=SLEEP\">Sleep Nodes</a> " + colTxt() + ":" + colTxt() + sleep.size());
+    out.print(rowTxt() + colTxt());
+    out.print("<a href=\"dfsnodelist.jsp?whatNodes=DEAD\">Dead Nodes</a> " + colTxt() + ":" + colTxt() + dead.size());
+    out.print(rowTxt() + colTxt());
+    out.print("<a href=\"dfsnodelist.jsp?whatNodes=DECOMMISSIONING\">Decommissioning Nodes</a> " + colTxt() + ":" + colTxt() + decommissioning.size());
+    out.print(rowTxt() + colTxt());
+    out.print("Number of Under-Replicated Blocks" + colTxt() + ":" + colTxt() + fsn.getUnderReplicatedBlocks());
+    out.print("</table>\n");
+    out.print("</div>\n");
+    out.print("<br>\n" );
     
     if (live.isEmpty() && dead.isEmpty()) {
         out.print("There are no datanodes in the cluster");
Index: src/webapps/hdfs/dfsnodelist.jsp
===================================================================
--- src/webapps/hdfs/dfsnodelist.jsp	(revision 1596572)
+++ src/webapps/hdfs/dfsnodelist.jsp	(working copy)
@@ -1,6 +1,6 @@
 <%@ page
-    contentType="text/html; charset=UTF-8"
-    isThreadSafe="false"
+	contentType="text/html; charset=UTF-8"
+	isThreadSafe="false"
 	import="javax.servlet.*"
 	import="javax.servlet.http.*"
 	import="java.io.*"
@@ -19,14 +19,22 @@
 <%!
 	JspHelper jspHelper = new JspHelper();
 
+	FSNamesystem fsn;
+	
 	int rowNum = 0;
 	int colNum = 0;
 
-	String rowTxt() { colNum = 0;
-	return "<tr class=\"" + (((rowNum++)%2 == 0)? "rowNormal" : "rowAlt")
-	+ "\"> "; }
-	String colTxt() { return "<td id=\"col" + ++colNum + "\"> "; }
-	void counterReset () { colNum = 0; rowNum = 0 ; }
+	String rowTxt() {
+		colNum = 0;
+		return "<tr class=\"" + (((rowNum++)%2 == 0)? "rowNormal" : "rowAlt") + "\"> ";
+	}
+	String colTxt() {
+		return "<td id=\"col" + ++colNum + "\"> ";
+	}
+	void counterReset () {
+		colNum = 0;
+		rowNum = 0 ;
+	}
 
 	long diskBytes = 1024 * 1024 * 1024;
 	String diskByteStr = "GB";
@@ -40,18 +48,16 @@
 	String order = "ASC";
 	if ( name.equals( sorterField ) ) {
 		ret += sorterOrder;
-		if ( sorterOrder.equals("ASC") )
+		if ( sorterOrder.equals("ASC") ) {
 			order = "DSC";
+		}
 	}
-	ret += " onClick=\"window.document.location=" +
-	"'/dfsnodelist.jsp?whatNodes="+whatNodes+"&sorter/field=" + name + "&sorter/order=" +
-	order + "'\" title=\"sort on this column\"";
+	ret += " onClick=\"window.document.location='/dfsnodelist.jsp?whatNodes="+whatNodes+"&sorter/field=" + name + "&sorter/order=" + order + "'\" title=\"sort on this column\"";
 
 	return ret;
 }
 
-void generateDecommissioningNodeData(JspWriter out, DatanodeDescriptor d,
-      String suffix, boolean alive, int nnHttpPort) throws IOException {
+void generateDecommissioningNodeData(JspWriter out, DatanodeDescriptor d, String suffix, boolean alive, int nnHttpPort) throws IOException {
     String url = "http://" + d.getHostName() + ":" + d.getInfoPort()
       + "/browseDirectory.jsp?namenodeInfoPort=" + nnHttpPort + "&dir="
       + URLEncoder.encode("/", "UTF-8");
@@ -89,92 +95,74 @@
 }
 
 
-public void generateNodeData( JspWriter out, DatanodeDescriptor d,
-		String suffix, boolean alive,
-		int nnHttpPort )
-throws IOException {
+public void generateNodeData(JspWriter out, DatanodeDescriptor d, String suffix, boolean alive, int nnHttpPort) throws IOException {
 
-	/* Say the datanode is dn1.hadoop.apache.org with ip 192.168.0.5
-we use:
-1) d.getHostName():d.getPort() to display.
-Domain and port are stripped if they are common across the nodes.
-i.e. "dn1"
-2) d.getHost():d.Port() for "title".
-i.e. "192.168.0.5:50010"
-3) d.getHostName():d.getInfoPort() for url.
-i.e. "http://dn1.hadoop.apache.org:50075/..."
-Note that "d.getHost():d.getPort()" is what DFS clients use
-to interact with datanodes.
-	 */
+	/* Say the datanode is dn1.hadoop.apache.org with ip 192.168.0.5 we use:
+	1) d.getHostName():d.getPort() to display. Domain and port are stripped if they are common across the nodes. i.e. "dn1"
+	2) d.getHost():d.Port() for "title". i.e. "192.168.0.5:50010"
+	3) d.getHostName():d.getInfoPort() for url. i.e. "http://dn1.hadoop.apache.org:50075/..."
+	Note that "d.getHost():d.getPort()" is what DFS clients use
+	to interact with datanodes.
+	*/
 	// from nn_browsedfscontent.jsp:
-	String url = "http://" + d.getHostName() + ":" + d.getInfoPort() +
-	"/browseDirectory.jsp?namenodeInfoPort=" +
-	nnHttpPort + "&dir=" +
-	URLEncoder.encode("/", "UTF-8");
+	String url = "http://" + d.getHostName() + ":" + d.getInfoPort() + "/browseDirectory.jsp?namenodeInfoPort=" + nnHttpPort + "&dir=" + URLEncoder.encode("/", "UTF-8");
 
 	String name = d.getHostName() + ":" + d.getPort();
 	if ( !name.matches( "\\d+\\.\\d+.\\d+\\.\\d+.*" ) ) 
 		name = name.replaceAll( "\\.[^.:]*", "" );    
 	int idx = (suffix != null && name.endsWith( suffix )) ?
-			name.indexOf( suffix ) : -1;
+	name.indexOf( suffix ) : -1;
 
-			out.print( rowTxt() + "<td class=\"name\"><a title=\""
-					+ d.getHost() + ":" + d.getPort() +
-					"\" href=\"" + url + "\">" +
-					(( idx > 0 ) ? name.substring(0, idx) : name) + "</a>" +
-					(( alive ) ? "" : "\n") );
-			if ( !alive )
-				return;
+	out.print( rowTxt() + "<td class=\"name\"><a title=\""
+			+ d.getHost() + ":" + d.getPort() +
+			"\" href=\"" + url + "\">" +
+			(( idx > 0 ) ? name.substring(0, idx) : name) + "</a>" +
+			(( alive ) ? "" : "\n") );
+	if ( !alive )
+		return;
 
-			long c = d.getCapacity();
-			long u = d.getDfsUsed();
-			long nu = d.getNonDfsUsed();
-			long r = d.getRemaining();
-			String percentUsed = StringUtils.limitDecimalTo2(d.getDfsUsedPercent());    
-			String percentRemaining = StringUtils.limitDecimalTo2(d.getRemainingPercent());    
+	long c = d.getCapacity();
+	long u = d.getDfsUsed();
+	long nu = d.getNonDfsUsed();
+	long r = d.getRemaining();
+	String percentUsed = StringUtils.limitDecimalTo2(d.getDfsUsedPercent());    
+	String percentRemaining = StringUtils.limitDecimalTo2(d.getRemainingPercent());    
 
-			String adminState = (d.isDecommissioned() ? "Decommissioned" :
-				(d.isDecommissionInProgress() ? "Decommission In Progress":
-				"In Service"));
+	String adminState = (d.isDecommissioned() ? "Decommissioned" :
+		(d.isDecommissionInProgress() ? "Decommission In Progress":
+		"In Service"));
+	// Agile Hadoop
+	if (d.isSleep()) {
+		adminState = "Sleeping";
+	}
 
-			long timestamp = d.getLastUpdate();
-			long currentTime = System.currentTimeMillis();
-			out.print("<td class=\"lastcontact\"> " +
-					((currentTime - timestamp)/1000) +
-					"<td class=\"adminstate\">" +
-					adminState +
-					"<td align=\"right\" class=\"capacity\">" +
-					StringUtils.limitDecimalTo2(c*1.0/diskBytes) +
-					"<td align=\"right\" class=\"used\">" +
-					StringUtils.limitDecimalTo2(u*1.0/diskBytes) +      
-					"<td align=\"right\" class=\"nondfsused\">" +
-					StringUtils.limitDecimalTo2(nu*1.0/diskBytes) +      
-					"<td align=\"right\" class=\"remaining\">" +
-					StringUtils.limitDecimalTo2(r*1.0/diskBytes) +      
-					"<td align=\"right\" class=\"pcused\">" + percentUsed +
-					"<td class=\"pcused\">" +
-					ServletUtil.percentageGraph( (int)Double.parseDouble(percentUsed) , 100) +
-					"<td align=\"right\" class=\"pcremaining`\">" + percentRemaining +
-					"<td title=" + "\"blocks scheduled : " + d.getBlocksScheduled() + 
-					"\" class=\"blocks\">" + d.numBlocks() + "\n");
+	long timestamp = d.getLastUpdate();
+	long currentTime = System.currentTimeMillis();
+	out.print("<td class=\"lastcontact\"> " + ((currentTime - timestamp)/1000) + "</td>");
+	out.print("<td class=\"adminstate\">" + adminState + "</td>");
+	out.print("<td align=\"right\" class=\"capacity\">" + StringUtils.limitDecimalTo2(c*1.0/diskBytes) + "</td>");
+	out.print("<td align=\"right\" class=\"used\">" + StringUtils.limitDecimalTo2(u*1.0/diskBytes) + "</td>"); 
+	out.print("<td align=\"right\" class=\"nondfsused\">" + StringUtils.limitDecimalTo2(nu*1.0/diskBytes) + "</td>");    
+	out.print("<td align=\"right\" class=\"remaining\">" + StringUtils.limitDecimalTo2(r*1.0/diskBytes) + "</td>"); 
+	out.print("<td align=\"right\" class=\"pcused\">" + percentUsed + "</td>");
+	out.print("<td class=\"pcused\">" +ServletUtil.percentageGraph( (int)Double.parseDouble(percentUsed) , 100) + "</td>");
+	out.print("<td align=\"right\" class=\"pcremaining`\">" + percentRemaining + "</td>");
+	out.print("<td title=" + "\"blocks scheduled : " + d.getBlocksScheduled() + "\" class=\"blocks\">" + d.numBlocks() + "</td>\n");
 }
 
-
-
-public void generateDFSNodesList(JspWriter out, 
-		NameNode nn,
-		HttpServletRequest request)
-throws IOException {
+public void generateDFSNodesList(JspWriter out, NameNode nn, HttpServletRequest request) throws IOException {
 	ArrayList<DatanodeDescriptor> live = new ArrayList<DatanodeDescriptor>();    
 	ArrayList<DatanodeDescriptor> dead = new ArrayList<DatanodeDescriptor>();
 	jspHelper.DFSNodesStatus(live, dead);
+	
+	ArrayList<DatanodeDescriptor> sleep = fsn.getSleepingNodes();
 
-       //verify input for correctness 
-       String whatNodes = request.getParameter("whatNodes");// show only live or only dead nodes
-       if (whatNodes == null || whatNodes.length() == 0) {
-         out.print("Invalid input");
-         return;
-       }
+	//verify input for correctness 
+	String whatNodes = request.getParameter("whatNodes");// show only live or only dead nodes
+	if (whatNodes == null || whatNodes.length() == 0) {
+		out.print("Invalid input");
+		return;
+	}
 
 	sorterField = request.getParameter("sorter/field");
 	sorterOrder = request.getParameter("sorter/order");
@@ -211,58 +199,57 @@
 
 	if (live.isEmpty() && dead.isEmpty()) {
 		out.print("There are no datanodes in the cluster");
-	}
-	else {
+	} else {
 
 		int nnHttpPort = nn.getHttpAddress().getPort();
 		out.print( "<div id=\"dfsnodetable\"> ");
-		if(whatNodes.equals("LIVE")) {
+		if (whatNodes.equals("LIVE") || whatNodes.equals("SLEEP")) {
+			ArrayList<DatanodeDescriptor> nodes = new ArrayList<DatanodeDescriptor>();
+			if (whatNodes.equals("LIVE")) {
+				nodes = live;
+				out.print("<a name=\"LiveNodes\" id=\"title\">Live Datanodes: " + nodes.size() + "</a>");
+			} else if (whatNodes.equals("SLEEP")) {
+				nodes = sleep;
+				out.print("<a name=\"SleepNodes\" id=\"title\">Sleep Datanodes: " + nodes.size() + "</a>");
+			}
+		
+			out.print("<br><br>\n");
+			out.print("<table border=1 cellspacing=0>\n" );
 
-			out.print( 
-					"<a name=\"LiveNodes\" id=\"title\">" +
-					"Live Datanodes : " + live.size() + "</a>" +
-			"<br><br>\n<table border=1 cellspacing=0>\n" );
-
 			counterReset();
 
-			if ( live.size() > 0 ) {
-
-				if ( live.get(0).getCapacity() > 1024 * diskBytes ) {
+			if ( nodes.size() > 0 ) {
+				if ( nodes.get(0).getCapacity() > 1024 * diskBytes ) {
 					diskBytes *= 1024;
 					diskByteStr = "TB";
 				}
+				out.print("<tr class=\"headerRow\">");
+				out.print("<th " + NodeHeaderStr("name") + "> Node</th>");
+				out.print("<th " + NodeHeaderStr("lastcontact") + "> Last <br>Contact</th>\n");
+				out.print("<th " + NodeHeaderStr("adminstate") + "> Admin State</th>\n");
+				out.print("<th " + NodeHeaderStr("capacity") + "> Configured <br>Capacity (" + diskByteStr + ")</th>\n");
+				out.print("<th " + NodeHeaderStr("used") + "> Used <br>(" + diskByteStr + ")</th>\n");
+				out.print("<th " + NodeHeaderStr("nondfsused") + "> Non DFS <br>Used (" + diskByteStr + ")</th>\n");
+				out.print("<th " + NodeHeaderStr("remaining") + "> Remaining <br>(" + diskByteStr + ")</th>\n");
+				out.print("<th " + NodeHeaderStr("pcused") + "> Used <br>(%)</th>\n");
+				out.print("<th " + NodeHeaderStr("pcused") + "> Used <br>(%)</th>\n");
+				out.print("<th " + NodeHeaderStr("pcremaining") + "> Remaining <br>(%)</th>\n");
+				out.print("<th " + NodeHeaderStr("blocks") + "> Blocks</th>\n" );
+				out.print("<tr>\n");
 
-				out.print( "<tr class=\"headerRow\"> <th " +
-						NodeHeaderStr("name") + "> Node <th " +
-						NodeHeaderStr("lastcontact") + "> Last <br>Contact <th " +
-						NodeHeaderStr("adminstate") + "> Admin State <th " +
-						NodeHeaderStr("capacity") + "> Configured <br>Capacity (" + 
-						diskByteStr + ") <th " + 
-						NodeHeaderStr("used") + "> Used <br>(" + 
-						diskByteStr + ") <th " + 
-						NodeHeaderStr("nondfsused") + "> Non DFS <br>Used (" + 
-						diskByteStr + ") <th " + 
-						NodeHeaderStr("remaining") + "> Remaining <br>(" + 
-						diskByteStr + ") <th " + 
-						NodeHeaderStr("pcused") + "> Used <br>(%) <th " + 
-						NodeHeaderStr("pcused") + "> Used <br>(%) <th " +
-						NodeHeaderStr("pcremaining") + "> Remaining <br>(%) <th " +
-						NodeHeaderStr("blocks") + "> Blocks\n" );
-
-				jspHelper.sortNodeList(live, sorterField, sorterOrder);
-				for ( int i=0; i < live.size(); i++ ) {
-					generateNodeData(out, live.get(i), port_suffix, true, nnHttpPort);
+				jspHelper.sortNodeList(nodes, sorterField, sorterOrder);
+				for (int i=0; i < nodes.size(); i++) {
+					generateNodeData(out, nodes.get(i), port_suffix, true, nnHttpPort);
 				}
 			}
 			out.print("</table>\n");
 		} else if (whatNodes.equals("DEAD")) {
-
-			out.print("<br> <a name=\"DeadNodes\" id=\"title\"> " +
-					" Dead Datanodes : " +dead.size() + "</a><br><br>\n");
-
+			out.print("<br> <a name=\"DeadNodes\" id=\"title\">Dead Datanodes : " +dead.size() + "</a>");
+			out.print("<br><br>\n");
 			if ( dead.size() > 0 ) {
-				out.print( "<table border=1 cellspacing=0> <tr id=\"row1\"> " +
-				"<td> Node \n" );
+				out.print("<table border=1 cellspacing=0>");
+				out.print("<tr id=\"row1\">");
+				out.print("<td> Node \n" );
 
 				jspHelper.sortNodeList(dead, "name", "ASC");
 				for ( int i=0; i < dead.size() ; i++ ) {
@@ -279,18 +266,14 @@
 			    + " Decommissioning Datanodes : " + decommissioning.size()
                             + "</a><br><br>\n");
 	                if (decommissioning.size() > 0) {
-                          out.print("<table border=1 cellspacing=0> <tr class=\"headRow\"> "
-                              + "<th " + NodeHeaderStr("name")
-                              + "> Node <th " + NodeHeaderStr("lastcontact")
-                              + "> Last <br>Contact <th "
-                              + NodeHeaderStr("underreplicatedblocks")
-                              + "> Under Replicated Blocks <th "
-                              + NodeHeaderStr("blockswithonlydecommissioningreplicas")
-		              + "> Blocks With No <br> Live Replicas <th "
-	                      + NodeHeaderStr("underrepblocksinfilesunderconstruction") 
-	                      + "> Under Replicated Blocks <br> In Files Under Construction"
-	                      + " <th " + NodeHeaderStr("timesincedecommissionrequest")
-	                      + "> Time Since Decommissioning Started"
+                          out.print("<table border=1 cellspacing=0>"+
+                              "<tr class=\"headRow\"> " + 
+                              "<th " + NodeHeaderStr("name") + "> Node" + 
+                              "<th " + NodeHeaderStr("lastcontact") + "> Last <br>Contact" + 
+                              "<th " + NodeHeaderStr("underreplicatedblocks") + "> Under Replicated Blocks" + 
+                              "<th " + NodeHeaderStr("blockswithonlydecommissioningreplicas") + "> Blocks With No <br> Live Replicas" + 
+                              "<th " + NodeHeaderStr("underrepblocksinfilesunderconstruction") + "> Under Replicated Blocks <br> In Files Under Construction" + 
+                              "<th " + NodeHeaderStr("timesincedecommissionrequest") + "> Time Since Decommissioning Started"
                           );
                           jspHelper.sortNodeList(decommissioning, "name", "ASC");
                           for (int i = 0; i < decommissioning.size(); i++) {
@@ -308,26 +291,29 @@
 }%>
 
 <%
-NameNode nn = (NameNode)application.getAttribute("name.node");
-FSNamesystem fsn = nn.getNamesystem();
+NameNode nn = (NameNode) application.getAttribute("name.node");
+fsn = nn.getNamesystem();
 String namenodeLabel = nn.getNameNodeAddress().getHostName() + ":" + nn.getNameNodeAddress().getPort();
 %>
 
 <html>
-
+<head>
 <link rel="stylesheet" type="text/css" href="/static/hadoop.css">
 <title>Hadoop NameNode <%=namenodeLabel%></title>
-  
+</head>
+
 <body>
-<h1>NameNode '<%=namenodeLabel%>'</h1>
+<h1>NameNode '<a href="dfshealth.jsp"><%=namenodeLabel%></a>' Machine List</h1>
 
-
-<div id="dfstable"> <table>	  
+<div id="dfstable">
+<table>	  
 <tr> <td id="col1"> Started: <td> <%= fsn.getStartTime()%>
 <tr> <td id="col1"> Version: <td> <%= VersionInfo.getVersion()%>, r<%= VersionInfo.getRevision()%>
 <tr> <td id="col1"> Compiled: <td> <%= VersionInfo.getDate()%> by <%= VersionInfo.getUser()%>
 <tr> <td id="col1"> Upgrades: <td> <%= jspHelper.getUpgradeStatusText()%>
-</table></div><br>				      
+</table>
+</div>
+<br>
 
 <b><a href="/nn_browsedfscontent.jsp">Browse the filesystem</a></b><br>
 <b><a href="/logs/">Namenode Logs</a></b><br>
Index: src/webapps/history/analysejobhistory.jsp
===================================================================
--- src/webapps/history/analysejobhistory.jsp	(revision 1596572)
+++ src/webapps/history/analysejobhistory.jsp	(working copy)
@@ -11,12 +11,19 @@
   import="org.apache.hadoop.mapred.JobHistory.*"
 %>
 
-<%!	private static SimpleDateFormat dateFormat 
+<%!
+	private static SimpleDateFormat dateFormat 
                               = new SimpleDateFormat("d/MM HH:mm:ss") ; 
 %>
-<%!	private static final long serialVersionUID = 1L;
+<%!
+	private static final long serialVersionUID = 1L;
 %>
-<html><body>
+<html>
+<head>
+<link rel="stylesheet" type="text/css" href="/static/hadoop.css">
+<title>Job</title>
+</head>
+<body>
 <%
   String logFile = request.getParameter("logFile");
   if (logFile == null) {
@@ -23,6 +30,10 @@
     out.println("Missing job!!");
     return;
   }
+  String viewType = request.getParameter("view");
+  boolean showIds = false;
+  if ("true".equalsIgnoreCase(request.getParameter("ids"))) 
+    showIds = true;
   String encodedLogFileName = JobHistory.JobInfo.encodeJobHistoryFilePath(logFile);
   String jobid = JSPUtil.getJobID(new Path(encodedLogFileName).getName());
   String numTasks = request.getParameter("numTasks");
@@ -39,25 +50,28 @@
     return;
   }%>
 <h2>Hadoop Job <a href="jobdetailshistory.jsp?logFile=<%=encodedLogFileName%>"><%=jobid %> </a></h2>
-<b>User : </b> <%=HtmlQuoting.quoteHtmlChars(job.get(Keys.USER)) %><br/> 
-<b>JobName : </b> <%=HtmlQuoting.quoteHtmlChars(job.get(Keys.JOBNAME)) %><br/> 
-<b>JobConf : </b> <%=job.get(Keys.JOBCONF) %><br/> 
-<b>Submitted At : </b> <%=StringUtils.getFormattedTimeWithDiff(dateFormat, job.getLong(Keys.SUBMIT_TIME), 0 ) %><br/> 
-<b>Launched At : </b> <%=StringUtils.getFormattedTimeWithDiff(dateFormat, job.getLong(Keys.LAUNCH_TIME), job.getLong(Keys.SUBMIT_TIME)) %><br/>
-<b>Finished At : </b>  <%=StringUtils.getFormattedTimeWithDiff(dateFormat, job.getLong(Keys.FINISH_TIME), job.getLong(Keys.LAUNCH_TIME)) %><br/>
-<b>Status : </b> <%= ((job.get(Keys.JOB_STATUS) == null)?"Incomplete" :job.get(Keys.JOB_STATUS)) %><br/> 
+<b>User: </b> <%=HtmlQuoting.quoteHtmlChars(job.get(Keys.USER)) %><br/> 
+<b>JobName: </b> <%=HtmlQuoting.quoteHtmlChars(job.get(Keys.JOBNAME)) %><br/> 
+<b>JobConf: </b> <%=job.get(Keys.JOBCONF) %><br/> 
+<b>Submitted At: </b> <%=StringUtils.getFormattedTimeWithDiff(dateFormat, job.getLong(Keys.SUBMIT_TIME), 0 ) %><br/> 
+<b>Launched At: </b> <%=StringUtils.getFormattedTimeWithDiff(dateFormat, job.getLong(Keys.LAUNCH_TIME), job.getLong(Keys.SUBMIT_TIME)) %><br/>
+<b>Finished At: </b>  <%=StringUtils.getFormattedTimeWithDiff(dateFormat, job.getLong(Keys.FINISH_TIME), job.getLong(Keys.LAUNCH_TIME)) %><br/>
+<b>Status: </b> <%= ((job.get(Keys.JOB_STATUS) == null)?"Incomplete" :job.get(Keys.JOB_STATUS)) %><br/> 
 <hr/>
 <center>
 <%
   if (!Values.SUCCESS.name().equals(job.get(Keys.JOB_STATUS))) {
     out.print("<h3>No Analysis available as job did not finish</h3>");
-    return;
+    //return;
   }
   Map<String, JobHistory.Task> tasks = job.getAllTasks();
   int finishedMaps = job.getInt(Keys.FINISHED_MAPS)  ;
   int finishedReduces = job.getInt(Keys.FINISHED_REDUCES) ;
-  JobHistory.Task [] mapTasks = new JobHistory.Task[finishedMaps]; 
-  JobHistory.Task [] reduceTasks = new JobHistory.Task[finishedReduces]; 
+  //JobHistory.Task [] mapTasks = new JobHistory.Task[finishedMaps]; 
+  //JobHistory.Task [] reduceTasks = new JobHistory.Task[finishedReduces]; 
+  List<JobHistory.Task> mapTasks =    new LinkedList<JobHistory.Task>(); 
+  List<JobHistory.Task> reduceTasks = new LinkedList<JobHistory.Task>(); 
+  List<JobHistory.Task> otherTasks =  new LinkedList<JobHistory.Task>(); 
   int mapIndex = 0 , reduceIndex=0; 
   long avgMapTime = 0;
   long avgReduceTime = 0;
@@ -66,24 +80,25 @@
   for (JobHistory.Task task : tasks.values()) {
     Map<String, TaskAttempt> attempts = task.getTaskAttempts();
     for (JobHistory.TaskAttempt attempt : attempts.values()) {
-      if (attempt.get(Keys.TASK_STATUS).equals(Values.SUCCESS.name())) {
-        long avgFinishTime = (attempt.getLong(Keys.FINISH_TIME) -
-      		                attempt.getLong(Keys.START_TIME));
+      //if (attempt.get(Keys.TASK_STATUS).equals(Values.SUCCESS.name())) {
+        long avgFinishTime = (attempt.getLong(Keys.FINISH_TIME) - attempt.getLong(Keys.START_TIME));
         if (Values.MAP.name().equals(task.get(Keys.TASK_TYPE))) {
-          mapTasks[mapIndex++] = attempt ; 
+          //mapTasks[mapIndex++] = attempt; 
+          mapTasks.add(attempt); 
           avgMapTime += avgFinishTime;
         } else if (Values.REDUCE.name().equals(task.get(Keys.TASK_TYPE))) { 
-          reduceTasks[reduceIndex++] = attempt;
-          avgShuffleTime += (attempt.getLong(Keys.SHUFFLE_FINISHED) - 
-                             attempt.getLong(Keys.START_TIME));
-          avgReduceTime += (attempt.getLong(Keys.FINISH_TIME) -
-                            attempt.getLong(Keys.SHUFFLE_FINISHED));
+          //reduceTasks[reduceIndex++] = attempt;
+          reduceTasks.add(attempt);
+          avgShuffleTime += (attempt.getLong(Keys.SHUFFLE_FINISHED) - attempt.getLong(Keys.START_TIME));
+          avgReduceTime += (attempt.getLong(Keys.FINISH_TIME) - attempt.getLong(Keys.SHUFFLE_FINISHED));
+        } else {
+          otherTasks.add(attempt);
         }
-        break;
-      }
+      //  break;
+      //}
     }
   }
-	 
+  
   if (finishedMaps > 0) {
     avgMapTime /= finishedMaps;
   }
@@ -116,11 +131,211 @@
     }
   };
 
-  if (mapTasks.length > 0) {
-    Arrays.sort(mapTasks, cMap);
-    JobHistory.Task minMap = mapTasks[mapTasks.length-1] ;
+  Comparator<JobHistory.Task> cIdOrder = new Comparator<JobHistory.Task>(){
+    public int compare(JobHistory.Task t1, JobHistory.Task t2){
+      TaskAttemptID attemptId1 = TaskAttemptID.forName(t1.get(Keys.TASK_ATTEMPT_ID));
+      TaskAttemptID attemptId2 = TaskAttemptID.forName(t2.get(Keys.TASK_ATTEMPT_ID));
+      return attemptId1.toString().compareTo(attemptId2.toString());
+    }
+  };
+  Comparator<JobHistory.Task> cStart = new Comparator<JobHistory.Task>(){
+    public int compare(JobHistory.Task t1, JobHistory.Task t2){
+      return (int) (t1.getLong(Keys.START_TIME) - t2.getLong(Keys.START_TIME));
+    }
+  }; 
+  
+  
+  if (mapTasks.size() > 0) {
+    //Arrays.sort(mapTasks, cMap);
+    //JobHistory.Task minMap = mapTasks[mapTasks.length-1] ;
+    Collections.sort(mapTasks,    cIdOrder);
+    Collections.sort(reduceTasks, cIdOrder);
+    Collections.sort(otherTasks,  cIdOrder);
 %>
 
+
+<%-- Viewer --%>
+<%
+	int width = 1000;
+	long t0 = job.getLong(Keys.SUBMIT_TIME);
+	long tf = job.getLong(Keys.FINISH_TIME);
+	
+	// This was needed because server clocks were out of sync
+	/*for (JobHistory.Task mapTask : mapTasks) {
+		long ts = mapTask.getLong(Keys.START_TIME);
+		if (ts < t0) {
+			t0 = ts;
+		}
+	}*/
+	
+	// Merge all tasks
+	List<JobHistory.Task> allTasks = new LinkedList<JobHistory.Task>(); 
+	allTasks.addAll(mapTasks);
+	allTasks.addAll(reduceTasks);
+	allTasks.addAll(otherTasks);
+	
+	// If we show the nodes, sort it by start time
+	if ("nodes".equalsIgnoreCase(viewType) || "nodetasks".equalsIgnoreCase(viewType)) {
+		Collections.sort(allTasks, cStart);
+	}
+	
+	// Start drawing execution
+	out.println("<table width=\""+width+"px\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\">");
+	// Nodes - > Tasks
+	if ("nodetasks".equalsIgnoreCase(viewType)) {
+		// Classify: we put the tasks in the nodes following their id order
+		HashMap<String, List<JobHistory.Task>> nodeTasks = new HashMap<String, List<JobHistory.Task>>();
+		for (JobHistory.Task task : allTasks) {
+			String nodeId = task.get(Keys.HOSTNAME);
+			if (nodeId.lastIndexOf("/") >= 0) {
+				nodeId = nodeId.substring(nodeId.lastIndexOf("/")+1);
+			}
+			if (!nodeTasks.containsKey(nodeId)) {
+				nodeTasks.put(nodeId, new LinkedList<JobHistory.Task>());
+			}
+			nodeTasks.get(nodeId).add(task);
+		}
+		// Output
+		List<String> nodes = new LinkedList<String>();
+		nodes.addAll(nodeTasks.keySet());
+		nodes.remove("dropped");
+		Collections.sort(nodes);
+		for (String nodeId : nodes) {
+			// Node
+			out.println("<tr>");
+			out.println("<td valign=\"top\" height=\"20px\">"+nodeId+"</td>");
+			// Tasks
+			out.println("<td>");
+			out.println("<table border=\"0\" cellspacing=\"0\" cellpadding=\"0\">");
+			for (JobHistory.Task task : nodeTasks.get(nodeId)) {
+				// Read task properties
+				long ts = task.getLong(Keys.START_TIME);
+				long te = task.getLong(Keys.FINISH_TIME);
+				TaskID taskId = TaskID.forName(task.get(Keys.TASKID));
+				
+				// Task progress
+				out.println("<tr><td>");
+				out.println(JSPUtil.getExecutionGraph((ts-t0)/1000, (te-t0)/1000, JSPUtil.getTaskColor(task), showIds ? ""+taskId.getId() : ""));
+				out.println("</td></tr>");
+			}
+			out.println("</table>");
+			out.println("</td>");
+			out.println("</tr>");
+		}
+	// Nodes
+	} else if ("nodes".equalsIgnoreCase(viewType)) {
+		// Classify: we put the tasks in the execution slots of the node
+		HashMap<String, LinkedList<LinkedList<JSPUtil.TaskExecution>>> nodeExecutions = new HashMap<String, LinkedList<LinkedList<JSPUtil.TaskExecution>>>();
+		for (JobHistory.Task task : allTasks) {
+			// Read task properties
+			long ts = task.getLong(Keys.START_TIME);
+			long te = task.getLong(Keys.FINISH_TIME);
+			TaskID taskId = TaskID.forName(task.get(Keys.TASKID));
+			JSPUtil.TaskExecution newTask = new JSPUtil.TaskExecution((ts-t0)/1000, (te-t0)/1000, JSPUtil.getTaskColor(task), showIds ? ""+taskId.getId() : "");
+		
+			String nodeId = task.get(Keys.HOSTNAME);
+			if (nodeId.lastIndexOf("/") >= 0) {
+				nodeId = nodeId.substring(nodeId.lastIndexOf("/")+1);
+			}
+			if (!nodeExecutions.containsKey(nodeId)) {
+				nodeExecutions.put(nodeId, new LinkedList<LinkedList<JSPUtil.TaskExecution>>());
+			}
+			LinkedList<LinkedList<JSPUtil.TaskExecution>> nodeExecution = nodeExecutions.get(nodeId);
+			boolean added = false;
+			for (LinkedList<JSPUtil.TaskExecution> nodeSlot : nodeExecution) {
+				// Check if the new task overlaps with the previous one
+				boolean overlap = false;
+				JSPUtil.TaskExecution prevTask = nodeSlot.getLast();
+				if(prevTask.end > newTask.start) {
+					overlap = true;
+				}
+				if (!overlap) {
+					nodeSlot.addLast(newTask);
+					added = true;
+					break;
+				}
+			}
+			if (!added) {
+				// If we couldn't add it, we create a new slot and put it there
+				nodeExecution.add(new LinkedList<JSPUtil.TaskExecution>());
+				nodeExecution.getLast().add(newTask);
+			}
+		}
+		
+		// Output
+		LinkedList<String> nodes = new LinkedList<String>();
+		nodes.addAll(nodeExecutions.keySet());
+		nodes.remove("dropped");
+		Collections.sort(nodes);
+		for (String nodeId : nodes) {
+			// Node
+			out.println("<tr>");
+			out.println("<td valign=\"top\" height=\"20px\">"+nodeId+"</td>");
+			// Slots
+			out.println("<td>");
+			out.println("<table border=\"0\" cellspacing=\"0\" cellpadding=\"0\">");
+			LinkedList<LinkedList<JSPUtil.TaskExecution>> nodeExecution = nodeExecutions.get(nodeId);
+			for (LinkedList<JSPUtil.TaskExecution> nodeSlot : nodeExecution) {
+				out.println("<tr><td>");
+				out.println(JSPUtil.getExecutionGraph(nodeSlot));
+				out.println("</td></tr>");
+			}
+			out.println("</table>");
+			out.println("</td>");
+			out.println("</tr>");
+		}
+	// All tasks
+	} else {
+		// We just plot all the tasks using the id order
+		for (JobHistory.Task task : allTasks) {
+			// Read task properties
+			long ts = task.getLong(Keys.START_TIME);
+			long te = task.getLong(Keys.FINISH_TIME);
+			TaskID taskId = TaskID.forName(task.get(Keys.TASKID));
+			
+			// Task progress
+			out.println("<tr><td/><td>"); // Empty node
+			out.println(JSPUtil.getExecutionGraph((ts-t0)/1000, (te-t0)/1000, JSPUtil.getTaskColor(task), showIds ? ""+taskId.getId() : ""));
+			out.println("</td></tr>");
+		}
+	}
+	// Draw job
+	out.println("<tr>");
+	out.println("<td/>"); // Empty column for "node id"
+	out.println("<td>" + JSPUtil.getExecutionGraph((job.getLong(Keys.LAUNCH_TIME)-t0)/1000, (tf-t0)/1000, "#00FF00") + "</td>");
+	out.println("</tr>");
+	out.println("</table>");
+	
+	// Menu to switch between views
+	if ("nodes".equalsIgnoreCase(viewType)) {
+		out.println("<a href=\"analysejobhistory.jsp?logFile="+encodedLogFileName+"&view=all&ids="+showIds+"\">Tasks</a>&nbsp;&nbsp;\n");
+		out.println("<a href=\"analysejobhistory.jsp?logFile="+encodedLogFileName+"&view=nodetasks&ids="+showIds+"\">Nodes&rarr;Tasks</a>&nbsp;&nbsp;\n");
+		out.println("Nodes&nbsp;&nbsp;\n");
+	} else if ("nodetasks".equalsIgnoreCase(viewType)) {
+		out.println("<a href=\"analysejobhistory.jsp?logFile="+encodedLogFileName+"&view=all&ids="+showIds+"\">Tasks</a>&nbsp;&nbsp;\n");
+		out.println("Nodes&rarr;Tasks&nbsp;&nbsp;\n");
+		out.println("<a href=\"analysejobhistory.jsp?logFile="+encodedLogFileName+"&view=nodes&ids="+showIds+"\">Nodes</a>&nbsp;&nbsp;\n");
+	} else {
+		out.println("Tasks&nbsp;&nbsp;\n");
+		out.println("<a href=\"analysejobhistory.jsp?logFile="+encodedLogFileName+"&view=nodetasks&ids="+showIds+"\">Nodes&rarr;Tasks</a>&nbsp;&nbsp;\n");
+		out.println("<a href=\"analysejobhistory.jsp?logFile="+encodedLogFileName+"&view=nodes&ids="+showIds+"\">Nodes</a>&nbsp;&nbsp;\n");
+	}
+	if (showIds) {
+		out.println("<a href=\"analysejobhistory.jsp?logFile="+encodedLogFileName+"&view="+viewType+"&ids=false\">Hide ids</a>&nbsp;&nbsp;\n");
+	} else {
+		out.println("<a href=\"analysejobhistory.jsp?logFile="+encodedLogFileName+"&view="+viewType+"&ids=true\">Show ids</a>&nbsp;&nbsp;\n");
+	}
+	out.println("<hr/>");
+	
+%>
+<%-- Viewer --%>
+
+<%
+	Collections.sort(mapTasks, cMap);
+	JobHistory.Task minMap = mapTasks.get(mapTasks.size()-1);
+%>
+
+
 <h3>Time taken by best performing Map task 
 <a href="taskdetailshistory.jsp?logFile=<%=encodedLogFileName%>&tipid=<%=minMap.get(Keys.TASKID)%>">
 <%=minMap.get(Keys.TASKID) %></a> : <%=StringUtils.formatTimeDiff(minMap.getLong(Keys.FINISH_TIME), minMap.getLong(Keys.START_TIME) ) %></h3>
@@ -130,12 +345,12 @@
 <table border="2" cellpadding="5" cellspacing="2">
 <tr><td>Task Id</td><td>Time taken</td></tr>
 <%
-  for (int i=0;i<showTasks && i<mapTasks.length; i++) {
+  for (int i=0;i<showTasks && i<mapTasks.size(); i++) {
 %>
     <tr>
-    <td><a href="taskdetailshistory.jsp?logFile=<%=encodedLogFileName%>&tipid=<%=mapTasks[i].get(Keys.TASKID)%>">
-        <%=mapTasks[i].get(Keys.TASKID) %></a></td>
-    <td><%=StringUtils.formatTimeDiff(mapTasks[i].getLong(Keys.FINISH_TIME), mapTasks[i].getLong(Keys.START_TIME)) %></td>
+    <td><a href="taskdetailshistory.jsp?logFile=<%=encodedLogFileName%>&tipid=<%=mapTasks.get(i).get(Keys.TASKID)%>">
+        <%=mapTasks.get(i).get(Keys.TASKID) %></a></td>
+    <td><%=StringUtils.formatTimeDiff(mapTasks.get(i).getLong(Keys.FINISH_TIME), mapTasks.get(i).getLong(Keys.START_TIME)) %></td>
     </tr>
 <%
   }
@@ -143,8 +358,10 @@
 </table>
 <%  
 
-    Arrays.sort(mapTasks, cFinishMapRed);
-    JobHistory.Task lastMap = mapTasks[0] ;
+    //Arrays.sort(mapTasks, cFinishMapRed);
+    //JobHistory.Task lastMap = mapTasks[0];
+    Collections.sort(mapTasks, cFinishMapRed);
+    JobHistory.Task lastMap = mapTasks.get(0);
 %>
 
 <h3>The last Map task 
@@ -159,9 +376,11 @@
 <%
   }//end if(mapTasks.length > 0)
 
-  if (reduceTasks.length <= 0) return;
-  Arrays.sort(reduceTasks, cShuffle); 
-  JobHistory.Task minShuffle = reduceTasks[reduceTasks.length-1] ;
+  if (reduceTasks.size() <= 0) return;
+  //Arrays.sort(reduceTasks, cShuffle); 
+  //JobHistory.Task minShuffle = reduceTasks[reduceTasks.length-1] ;
+  Collections.sort(reduceTasks, cShuffle); 
+  JobHistory.Task minShuffle = reduceTasks.get(reduceTasks.size()-1);
 %>
 <h3>Time taken by best performing shufflejobId
 <a href="taskdetailshistory.jsp?logFile=<%=encodedLogFileName%>
@@ -172,18 +391,20 @@
 <%=StringUtils.formatTimeDiff(avgShuffleTime, 0) %></h3>
 <h3>Worse performing Shuffle(s)</h3>
 <table border="2" cellpadding="5" cellspacing="2">
-<tr><td>Task Id</td><td>Time taken</td></tr>
+<tr>
+<td>Task Id</td><td>Time taken</td>
+</tr>
 <%
-  for (int i=0;i<showTasks && i<reduceTasks.length; i++) {
+  for (int i=0;i<showTasks && i<reduceTasks.size(); i++) {
 %>
     <tr>
     <td><a href="taskdetailshistory.jsp?logFile=
-<%=encodedLogFileName%>&tipid=<%=reduceTasks[i].get(Keys.TASKID)%>">
-<%=reduceTasks[i].get(Keys.TASKID) %></a></td>
+<%=encodedLogFileName%>&tipid=<%=reduceTasks.get(i).get(Keys.TASKID)%>">
+<%=reduceTasks.get(i).get(Keys.TASKID) %></a></td>
     <td><%=
            StringUtils.formatTimeDiff(
-                       reduceTasks[i].getLong(Keys.SHUFFLE_FINISHED),
-                       reduceTasks[i].getLong(Keys.START_TIME)) %>
+                       reduceTasks.get(i).getLong(Keys.SHUFFLE_FINISHED),
+                       reduceTasks.get(i).getLong(Keys.START_TIME)) %>
     </td>
     </tr>
 <%
@@ -199,8 +420,10 @@
       return (l2<l1 ? -1 : (l2==l1 ? 0 : 1));
     }
   };
-  Arrays.sort(reduceTasks, cFinishShuffle);
-  JobHistory.Task lastShuffle = reduceTasks[0] ;
+  //Arrays.sort(reduceTasks, cFinishShuffle);
+  //JobHistory.Task lastShuffle = reduceTasks[0] ;
+  Collections.sort(reduceTasks, cFinishShuffle);
+  JobHistory.Task lastShuffle = reduceTasks.get(0);
 %>
 
 <h3>The last Shuffle  
@@ -221,8 +444,10 @@
       return (l2<l1 ? -1 : (l2==l1 ? 0 : 1));
     }
   }; 
-  Arrays.sort(reduceTasks, cReduce); 
-  JobHistory.Task minReduce = reduceTasks[reduceTasks.length-1] ;
+  //Arrays.sort(reduceTasks, cReduce); 
+  //JobHistory.Task minReduce = reduceTasks[reduceTasks.length-1] ;
+  Collections.sort(reduceTasks, cReduce); 
+  JobHistory.Task minReduce = reduceTasks.get(reduceTasks.size()-1);
 %>
 <hr/>
 <h3>Time taken by best performing Reduce task : 
@@ -237,14 +462,14 @@
 <table border="2" cellpadding="5" cellspacing="2">
 <tr><td>Task Id</td><td>Time taken</td></tr>
 <%
-  for (int i=0;i<showTasks && i<reduceTasks.length; i++) {
+  for (int i=0;i<showTasks && i<reduceTasks.size(); i++) {
 %>
     <tr>
-    <td><a href="taskdetailshistory.jsp?logFile=<%=encodedLogFileName%>&tipid=<%=reduceTasks[i].get(Keys.TASKID)%>">
-        <%=reduceTasks[i].get(Keys.TASKID) %></a></td>
+    <td><a href="taskdetailshistory.jsp?logFile=<%=encodedLogFileName%>&tipid=<%=reduceTasks.get(i).get(Keys.TASKID)%>">
+        <%=reduceTasks.get(i).get(Keys.TASKID) %></a></td>
     <td><%=StringUtils.formatTimeDiff(
-             reduceTasks[i].getLong(Keys.FINISH_TIME), 
-             reduceTasks[i].getLong(Keys.SHUFFLE_FINISHED)) %></td>
+             reduceTasks.get(i).getLong(Keys.FINISH_TIME), 
+             reduceTasks.get(i).getLong(Keys.SHUFFLE_FINISHED)) %></td>
     </tr>
 <%
   }
@@ -251,8 +476,10 @@
 %>
 </table>
 <%  
-  Arrays.sort(reduceTasks, cFinishMapRed);
-  JobHistory.Task lastReduce = reduceTasks[0] ;
+  //Arrays.sort(reduceTasks, cFinishMapRed);
+  //JobHistory.Task lastReduce = reduceTasks[0] ;
+  Collections.sort(reduceTasks, cFinishMapRed);
+  JobHistory.Task lastReduce = reduceTasks.get(0);
 %>
 
 <h3>The last Reduce task 
@@ -263,4 +490,5 @@
                               lastReduce.getLong(Keys.FINISH_TIME), 
                               job.getLong(Keys.LAUNCH_TIME) ) %></h3>
 </center>
-</body></html>
+</body>
+</html>
Index: src/webapps/history/jobhistoryhome.jsp
===================================================================
--- src/webapps/history/jobhistoryhome.jsp	(revision 1596572)
+++ src/webapps/history/jobhistoryhome.jsp	(working copy)
@@ -13,8 +13,12 @@
   import="javax.servlet.jsp.*"
   import="java.text.SimpleDateFormat"
   import="org.apache.hadoop.http.HtmlQuoting"
+  import="org.apache.hadoop.mapred.JobHistory.*"
 %>
 <%	
+  // We will need the ACL Manager too
+  ACLsManager aclsManager = (ACLsManager) application.getAttribute("aclManager");
+
   JobConf jobConf = (JobConf) application.getAttribute("jobConf");
   String trackerUrl;
   String trackerName;
@@ -429,6 +433,10 @@
               "<td>Job Id</td><td>Name</td><td>User</td>") ; 
     out.print("</tr>"); 
     
+    // Viewer
+    List<JobHistory.Task> allAttempts = new LinkedList<JobHistory.Task>();
+    List<JobHistory.JobInfo> allJobs =  new LinkedList<JobHistory.JobInfo>();
+    
     Set<String> displayedJobs = new HashSet<String>();
     for (int i = start - 1; i < start + length - 1; ++i) {
       Path jobFile = jobFiles[i];
@@ -475,11 +483,129 @@
 %>
 </center> 
 <%
+	// Viewer
+	// Get the data from the job
+	JobHistory.JobInfo job = JSPUtil.checkAccessAndGetJobInfo(request, response, jobConf, aclsManager, fs, jobFile);
+	allJobs.add(job);
+	Map<String, JobHistory.Task> tasks = job.getAllTasks();
+	// Add all the attempts
+	for (JobHistory.Task task : tasks.values()) {
+		Map<String, TaskAttempt> attempts = task.getTaskAttempts();
+		for (JobHistory.TaskAttempt attempt : attempts.values()) {
+			allAttempts.add(attempt); 
+		}
+	}
+
     } // end while trackers 
     out.print("</table>");
-
+    
     // show the navigation info (bottom)
     printNavigationTool(pageno, size, maxPageNo, searchPlusScan, out);
+
+	// Viewer
+	// Get parameters
+	boolean showIds = false;
+	if ("true".equalsIgnoreCase(request.getParameter("ids"))) {
+		showIds = true;
+	}
+	// Get figure zoom
+	float zoom = 0.1f;
+	try {
+		zoom = Float.parseFloat(request.getParameter("zoom"));
+	} catch (Exception e) {
+		zoom = 0.1f;
+	}
+	
+	// Sort all the attempts
+	Comparator<JobHistory.Task> cStart = new Comparator<JobHistory.Task>(){
+		public int compare(JobHistory.Task t1, JobHistory.Task t2){
+			return (int) (t1.getLong(Keys.START_TIME) - t2.getLong(Keys.START_TIME));
+		}
+	}; 
+	Collections.sort(allAttempts, cStart);
+	
+	
+	long t0 = allAttempts.get(0).getLong(Keys.START_TIME);
+	
+	// Classify: we put the tasks in the execution slots of the node
+	HashMap<String, LinkedList<LinkedList<JSPUtil.TaskExecution>>> nodeExecutions = new HashMap<String, LinkedList<LinkedList<JSPUtil.TaskExecution>>>();
+	for (JobHistory.Task task : allAttempts) {
+		// Read task properties
+		long ts = task.getLong(Keys.START_TIME);
+		long te = task.getLong(Keys.FINISH_TIME);
+		TaskID taskId = TaskID.forName(task.get(Keys.TASKID));
+		JSPUtil.TaskExecution newTask = new JSPUtil.TaskExecution((ts-t0)/1000, (te-t0)/1000, JSPUtil.getTaskColor(task), showIds ? ""+taskId.getId() : "");
+	
+		String nodeId = task.get(Keys.HOSTNAME);
+		if (nodeId.lastIndexOf("/") >= 0) {
+			nodeId = nodeId.substring(nodeId.lastIndexOf("/")+1);
+		}
+		if (!nodeExecutions.containsKey(nodeId)) {
+			nodeExecutions.put(nodeId, new LinkedList<LinkedList<JSPUtil.TaskExecution>>());
+		}
+		LinkedList<LinkedList<JSPUtil.TaskExecution>> nodeExecution = nodeExecutions.get(nodeId);
+		boolean added = false;
+		for (LinkedList<JSPUtil.TaskExecution> nodeSlot : nodeExecution) {
+			// Check if the new task overlaps with the previous one
+			boolean overlap = false;
+			JSPUtil.TaskExecution prevTask = nodeSlot.getLast();
+			if(prevTask.end > newTask.start) {
+				overlap = true;
+			}
+			if (!overlap) {
+				nodeSlot.addLast(newTask);
+				added = true;
+				break;
+			}
+		}
+		if (!added) {
+			// If we couldn't add it, we create a new slot and put it there
+			nodeExecution.add(new LinkedList<JSPUtil.TaskExecution>());
+			nodeExecution.getLast().add(newTask);
+		}
+	}
+	
+	// Output
+	out.println("<h2>Job traces</h2>");
+	out.println("<table width=\"1800px\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\">");
+	LinkedList<String> nodes = new LinkedList<String>(nodeExecutions.keySet());
+	nodes.remove("dropped");
+	Collections.sort(nodes);
+	for (String nodeId : nodes) {
+		// Node
+		out.println("<tr>");
+		out.println("<td valign=\"top\" height=\"20px\">"+nodeId+"</td>");
+		// Slots
+		out.println("<td>");
+		out.println("<table border=\"0\" cellspacing=\"0\" cellpadding=\"0\">");
+		LinkedList<LinkedList<JSPUtil.TaskExecution>> nodeExecution = nodeExecutions.get(nodeId);
+		for (LinkedList<JSPUtil.TaskExecution> nodeSlot : nodeExecution) {
+			out.println("<tr><td>");
+			out.println(JSPUtil.getExecutionGraph(nodeSlot, zoom));
+			out.println("</td></tr>");
+		}
+		out.println("</table>");
+		out.println("</td>");
+		out.println("</tr>");
+	}
+	out.println("</tr>");
+	// Draw job
+	for (JobHistory.JobInfo job : allJobs) {
+		out.println("<tr>");
+		out.println("<td/>"); // Empty column for "node id"
+		out.println("<td>" + JSPUtil.getExecutionGraph((job.getLong(Keys.LAUNCH_TIME)-t0)/1000, (job.getLong(Keys.FINISH_TIME)-t0)/1000, "#00FF00", "", zoom) + "</td>");
+		out.println("</tr>");
+	}
+	out.println("</table>");
+	// Zoom menu
+	if (zoom != 1)     out.println("<a href=\"jobhistoryhome.jsp?zoom=1\">1:1</a>");
+	if (zoom != 0.5)   out.println("<a href=\"jobhistoryhome.jsp?zoom=0.5\">1:2</a>");
+	if (zoom != 0.2)   out.println("<a href=\"jobhistoryhome.jsp?zoom=0.2\">1:5</a>");
+	if (zoom != 0.1)   out.println("<a href=\"jobhistoryhome.jsp?zoom=0.1\">1:10</a>");
+	if (zoom != 0.05)  out.println("<a href=\"jobhistoryhome.jsp?zoom=0.05\">1:20</a>");
+	if (zoom != 0.02)  out.println("<a href=\"jobhistoryhome.jsp?zoom=0.02\">1:50</a>");
+	if (zoom != 0.01)  out.println("<a href=\"jobhistoryhome.jsp?zoom=0.01\">1:100</a>");
+	if (zoom != 0.001) out.println("<a href=\"jobhistoryhome.jsp?zoom=0.001\">1:1000</a>");
 %>
 <%!
     private void printJob(String timestamp,
Index: src/webapps/job/jobdetails.jsp
===================================================================
--- src/webapps/job/jobdetails.jsp	(revision 1596572)
+++ src/webapps/job/jobdetails.jsp	(working copy)
@@ -39,11 +39,15 @@
     int runningTasks = 0;
     int finishedTasks = 0;
     int killedTasks = 0;
+    int droppedTasks = 0;
+    int approxTasks = 0;
     int failedTaskAttempts = 0;
     int killedTaskAttempts = 0;
     for(int i=0; i < totalTasks; ++i) {
       TaskInProgress task = tasks[i];
-      if (task.isComplete()) {
+      if (task.wasDropped()) {
+        droppedTasks += 1;
+      } else if (task.isComplete()) {
         finishedTasks += 1;
       } else if (task.isRunning()) {
         runningTasks += 1;
@@ -50,10 +54,13 @@
       } else if (task.wasKilled()) {
         killedTasks += 1;
       }
+      if (task.isApproximated()) {
+        approxTasks += 1;
+      }
       failedTaskAttempts += task.numTaskFailures();
       killedTaskAttempts += task.numKilledTasks();
     }
-    int pendingTasks = totalTasks - runningTasks - killedTasks - finishedTasks; 
+    int pendingTasks = totalTasks - runningTasks - killedTasks - finishedTasks - droppedTasks; 
     out.print("<tr><th><a href=\"jobtasks.jsp?jobid=" + jobId + 
               "&type="+ kind + "&pagenum=1\">" + kind + 
               "</a></th><td align=\"right\">" + 
@@ -77,6 +84,16 @@
                 "&pagenum=1" + "&state=completed\">" + finishedTasks + "</a>" 
                : "0") + 
               "</td><td align=\"right\">" + 
+              ((approxTasks > 0) 
+               ?"<a href=\"jobtasks.jsp?jobid=" + jobId + "&type="+ kind +
+                "&pagenum=1" + "&state=approximated\">" + approxTasks + "</a>"
+               : "0") + 
+              "</td><td align=\"right\">" + 
+              ((droppedTasks > 0) 
+               ?"<a href=\"jobtasks.jsp?jobid=" + jobId + "&type="+ kind +
+                "&pagenum=1" + "&state=dropped\">" + droppedTasks + "</a>"
+               : "0") + 
+              "</td><td align=\"right\">" + 
               ((killedTasks > 0) 
                ?"<a href=\"jobtasks.jsp?jobid=" + jobId + "&type="+ kind +
                 "&pagenum=1" + "&state=killed\">" + killedTasks + "</a>"
@@ -107,9 +124,13 @@
     int runningTasks = 0;
     int finishedTasks = 0;
     int killedTasks = 0;
+    int droppedTasks = 0;
+    int approxTasks = 0;
     for(int i=0; i < totalTasks; ++i) {
       TaskInProgress task = tasks[i];
-      if (task.isComplete()) {
+      if (task.wasDropped()) {
+        droppedTasks += 1;
+      } else if (task.isComplete()) {
         finishedTasks += 1;
       } else if (task.isRunning()) {
         runningTasks += 1;
@@ -116,8 +137,11 @@
       } else if (task.isFailed()) {
         killedTasks += 1;
       }
+      if (task.isApproximated()) {
+        approxTasks += 1;
+      }
     }
-    int pendingTasks = totalTasks - runningTasks - killedTasks - finishedTasks; 
+    int pendingTasks = totalTasks - runningTasks - killedTasks - finishedTasks - droppedTasks; 
     out.print(((runningTasks > 0)  
                ? "<a href=\"jobtasks.jsp?jobid=" + jobId + "&type="+ kind + 
                  "&pagenum=1" + "&state=running\">" + " Running" + 
@@ -343,8 +367,15 @@
     }
     out.print("<hr>\n");
     out.print("<table border=2 cellpadding=\"5\" cellspacing=\"2\">");
-    out.print("<tr><th>Kind</th><th>% Complete</th><th>Num Tasks</th>" +
-              "<th>Pending</th><th>Running</th><th>Complete</th>" +
+    out.print("<tr>"+
+              "<th>Kind</th>"+
+              "<th>% Complete</th>"+
+              "<th>Num Tasks</th>" +
+              "<th>Pending</th>"+
+              "<th>Running</th>"+
+              "<th>Complete</th>"+
+              "<th>Approx</th>" +
+              "<th>Dropped</th>" +
               "<th>Killed</th>" +
               "<th><a href=\"jobfailures.jsp?jobid=" + jobId + 
               "\">Failed/Killed<br>Task Attempts</a></th></tr>\n");
Index: src/webapps/job/jobtasks.jsp
===================================================================
--- src/webapps/job/jobtasks.jsp	(revision 1596572)
+++ src/webapps/job/jobtasks.jsp	(working copy)
@@ -76,6 +76,8 @@
       if (("completed".equals(state) && reports[i].getCurrentStatus() == TIPStatus.COMPLETE) 
           || ("running".equals(state) && reports[i].getCurrentStatus() == TIPStatus.RUNNING) 
           || ("killed".equals(state) && reports[i].getCurrentStatus() == TIPStatus.KILLED) 
+          || ("dropped".equals(state) && reports[i].getCurrentStatus() == TIPStatus.DROPPED) 
+          || ("approximated".equals(state) && reports[i].getCurrentStatus() == TIPStatus.COMPLETE) 
           || ("pending".equals(state)  && reports[i].getCurrentStatus() == TIPStatus.PENDING)) {
         filteredReports.add(reports[i]);
       }
@@ -100,11 +102,11 @@
         end_index = report_len;
     }
     for (int i = start_index ; i < end_index; i++) {
-          TaskReport report = reports[i];
-          out.print("<tr><td><a href=\"taskdetails.jsp?tipid=" +
+         TaskReport report = reports[i];
+         out.print("<tr><td><a href=\"taskdetails.jsp?tipid=" +
             report.getTaskID() + "\">"  + report.getTaskID() + "</a></td>");
          out.print("<td>" + StringUtils.formatPercent(report.getProgress(),2) +
-        		   ServletUtil.percentageGraph(report.getProgress() * 100f, 80) + "</td>");
+                   ServletUtil.percentageGraph(report.getProgress() * 100f, 80) + "</td>");
          out.print("<td>"  + HtmlQuoting.quoteHtmlChars(report.getState()) + "<br/></td>");
          out.println("<td>" + StringUtils.getFormattedTimeWithDiff(dateFormat, report.getStartTime(),0) + "<br/></td>");
          out.println("<td>" + StringUtils.getFormattedTimeWithDiff(dateFormat, 
Index: src/webapps/job/machines.jsp
===================================================================
--- src/webapps/job/machines.jsp	(revision 1596572)
+++ src/webapps/job/machines.jsp	(working copy)
@@ -1,71 +1,100 @@
+<%!
+/**
+ *  Licensed under the Apache License, Version 2.0 (the "License");
+ *  you may not use this file except in compliance with the License.
+ *  You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ *  Unless required by applicable law or agreed to in writing, software
+ *  distributed under the License is distributed on an "AS IS" BASIS,
+ *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ *  See the License for the specific language governing permissions and
+ *  limitations under the License.
+ */
+%>
 <%@ page
-  contentType="text/html; charset=UTF-8"
-  import="javax.servlet.*"
-  import="javax.servlet.http.*"
-  import="java.io.*"
-  import="java.util.*"
-  import="java.text.DecimalFormat"
-  import="org.apache.hadoop.mapred.*"
-  import="org.apache.hadoop.util.*"
+	contentType="text/html; charset=UTF-8"
+	import="javax.servlet.*"
+	import="javax.servlet.http.*"
+	import="java.io.*"
+	import="java.util.*"
+	import="java.text.DecimalFormat"
+	import="org.apache.hadoop.mapred.*"
+	import="org.apache.hadoop.util.*"
 %>
-<%!	private static final long serialVersionUID = 1L;
+<%!	
+	private static final long serialVersionUID = 1L;
 %>
 <%
-  JobTracker tracker = (JobTracker) application.getAttribute("job.tracker");
-  String trackerName = 
-           StringUtils.simpleHostname(tracker.getJobTrackerMachine());
-  String type = request.getParameter("type");
+	JobTracker tracker = (JobTracker) application.getAttribute("job.tracker");
+	String trackerName = StringUtils.simpleHostname(tracker.getJobTrackerMachine());
+	String type = request.getParameter("type");
 %>
 <%!
-  public void generateTaskTrackerTable(JspWriter out,
-                                       String type,
-                                       JobTracker tracker) throws IOException {
+  public void generateTaskTrackerTable(JspWriter out, String type, JobTracker tracker) throws IOException {
     Collection c;
+    out.print("<div id=\"dfsnodetable\">\n");
     if (("blacklisted").equals(type)) {
-      out.println("<h2>Blacklisted Task Trackers</h2>");
       c = tracker.blacklistedTaskTrackers();
+      out.print("<a name=\"BlackNodes\" id=\"title\">Blacklisted Task Trackers: " + c.size() + "</a>");
+      out.print("<br><br>\n");
     } else if (("graylisted").equals(type)) {
-      out.println("<h2>Graylisted Task Trackers</h2>");
       c = tracker.graylistedTaskTrackers();
+      out.print("<a name=\"GrayNodes\" id=\"title\">Graylisted Task Trackers: " + c.size() + "</a>");
+      out.print("<br><br>\n");
     } else if (("active").equals(type)) {
-      out.println("<h2>Active Task Trackers</h2>");
       c = tracker.activeTaskTrackers();
+      out.print("<a name=\"LiveNodes\" id=\"title\">Live Task Trackers: " + c.size() + "</a>");
+      out.print("<br><br>\n");
     } else {
-      out.println("<h2>Task Trackers</h2>");
       c = tracker.taskTrackers();
+      out.print("<a name=\"Nodes\" id=\"title\">Task Trackers: " + c.size() + "</a>");
+      out.print("<br><br>\n");
     }
-    int noCols = 9 + 
-      (2 * tracker.getStatistics().collector.DEFAULT_COLLECT_WINDOWS.length);
-    if (type.equals("blacklisted") || type.equals("graylisted")) {
-      noCols = noCols + 1;
+    int noCols = 9 + (2 * tracker.getStatistics().collector.DEFAULT_COLLECT_WINDOWS.length);
+    if (type != null) {
+      if (type.equals("blacklisted") || type.equals("graylisted")) {
+        noCols = noCols + 1;
+      }
     }
     if (c.size() == 0) {
       out.print("There are currently no known " + type + " Task Trackers.");
     } else {
-      out.print("<center>\n");
-      out.print("<table border=\"2\" cellpadding=\"5\" cellspacing=\"2\">\n");
-      out.print("<tr><td align=\"center\" colspan=\""+ noCols +"\"><b>Task Trackers</b></td></tr>\n");
-      out.print("<tr><td><b>Name</b></td><td><b>Host</b></td>" +
-                "<td><b># running tasks</b></td>" +
-                "<td><b>Max Map Tasks</b></td>" +
-                "<td><b>Max Reduce Tasks</b></td>" +
-                "<td><b>Task Failures</b></td>" +
-                "<td><b>Directory Failures</b></td>" +
-                "<td><b>Node Health Status</b></td>" +
-                "<td><b>Seconds Since Node Last Healthy</b></td>");
+      out.print("<table border=1 cellspacing=0>\n");
+      //out.print("<table border=\"2\" cellpadding=\"5\" cellspacing=\"2\">\n");
+      //out.print("<tr><td align=\"center\" colspan=\""+ noCols +"\"><b>Task Trackers</b></td></tr>\n");
+      out.print("<tr class=\"headerRow\">\n");
+      out.print("<th>Node</th>\n");
+      out.print("<th>Last <br>Contact</th>\n");
+      out.print("<th>Admin State</th>\n");
+      out.print("<th># running tasks</th>\n");
+      out.print("<th>Max Map</th>\n");
+      out.print("<th>Max Reduce</th>\n");
+      out.print("<th>Task Failures</th>\n");
+      out.print("<th>Directory Failures</th>\n");
+      out.print("<th>Last Healthy</th>\n");
       if (type.equals("blacklisted")) {
-        out.print("<td><b>Reason for Blacklisting</b></td>");
+        out.print("<th>Reason for Blacklisting</th>\n");
       } else if (type.equals("graylisted")) {
-        out.print("<td><b>Reason for Graylisting</b></td>");
+        out.print("<th>Reason for Graylisting</th>\n");
       }
-      for (StatisticsCollector.TimeWindow window : tracker.getStatistics().
-           collector.DEFAULT_COLLECT_WINDOWS) {
-         out.println("<td><b>Total Tasks "+window.name+"</b></td>");
-         out.println("<td><b>Succeeded Tasks "+window.name+"</b></td>");
-       }
+      for (StatisticsCollector.TimeWindow window : tracker.getStatistics().collector.DEFAULT_COLLECT_WINDOWS) {
+         out.print("<th>Total Tasks <br/>"+window.name+"</th>\n");
+         out.print("<th>Succes Tasks <br/>"+window.name+"</th>\n");
+      }
+      out.print("</tr>\n");
+
+      // Sorting
+      if (c instanceof List) {
+        Comparator<TaskTrackerStatus> comparator = new Comparator<TaskTrackerStatus>() {
+          public int compare(TaskTrackerStatus c1, TaskTrackerStatus c2) {
+            return c1.getTrackerName().compareTo(c2.getTrackerName());
+          }
+        };
+        Collections.sort((List)c, comparator);
+      }
       
-      out.print("<td><b>Seconds since heartbeat</b></td></tr>\n");
-
       int maxFailures = 0;
       String failureKing = null;
       for (Iterator it = c.iterator(); it.hasNext(); ) {
@@ -77,10 +106,14 @@
         if(sinceHealthCheck == 0) {
           healthString = "N/A";
         } else {
-          healthString = (isHealthy?"Healthy":"Unhealthy");
+          healthString = (isHealthy? "Healthy" : "Unhealthy");
           sinceHealthCheck = System.currentTimeMillis() - sinceHealthCheck;
           sinceHealthCheck =  sinceHealthCheck/1000;
         }
+        // Agile Hadoop
+        if (tt.isSleep()) {
+          healthString = "Sleeping";
+        }
         if (sinceHeartbeat > 0) {
           sinceHeartbeat = sinceHeartbeat / 1000;
         }
@@ -95,44 +128,46 @@
           failureKing = tt.getTrackerName();
         }
         int numDirFailures = tt.getDirFailures();
-        out.print("<tr><td><a href=\"http://");
-        out.print(tt.getHost() + ":" + tt.getHttpPort() + "/\">");
-        out.print(tt.getTrackerName() + "</a></td><td>");
-        out.print(tt.getHost() + "</td><td>" + numCurTasks +
-                  "</td><td>" + tt.getMaxMapSlots() +
-                  "</td><td>" + tt.getMaxReduceSlots() + 
-                  "</td><td>" + numTaskFailures +
-                  "</td><td>" + numDirFailures +
-                  "</td><td>" + healthString +
-                  "</td><td>" + sinceHealthCheck); 
+        out.print("<tr>\n");
+        out.print("<td><a href=\"http://" + tt.getHost() + ":" + tt.getHttpPort() + "/\">" + tt.getHost() + "</a></td>");
+        // Added a cleaner tracker name
+        /*String trackerName = tt.getTrackerName();
+        if (trackerName.startsWith("tracker_"))
+          trackerName = trackerName.substring("tracker_".length());
+        if (trackerName.indexOf(":") >= 0) {
+          trackerName = trackerName.substring(0, trackerName.indexOf(":"));
+        }
+        out.print("<td>" + trackerName + "</a></td>");
+        out.print("<td>" + tt.getHost() + "</td>\n");*/
+        out.print("<td>" + sinceHeartbeat + "</td>\n");
+        out.print("<td>" + healthString + "</td>\n");
+        out.print("<td>" + numCurTasks + "</td>\n");
+        out.print("<td>" + tt.getMaxMapSlots() + "</td>\n");
+        out.print("<td>" + tt.getMaxReduceSlots() + "</td>\n");
+        out.print("<td>" + numTaskFailures + "</td>\n");
+        out.print("<td>" + numDirFailures + "</td>\n");
+        out.print("<td>" + sinceHealthCheck + "</td>\n"); 
         if (type.equals("blacklisted")) {
-          out.print("</td><td>" + tracker.getReasonsForBlacklisting(tt.getHost()));
+          out.print("<td>" + tracker.getReasonsForBlacklisting(tt.getHost()) + "</td>\n");
         } else if (type.equals("graylisted")) {
-          out.print("</td><td>" + tracker.getReasonsForGraylisting(tt.getHost()));
+          out.print("<td>" + tracker.getReasonsForGraylisting(tt.getHost()) + "</td>\n");
         }
-        for (StatisticsCollector.TimeWindow window : tracker.getStatistics().
-          collector.DEFAULT_COLLECT_WINDOWS) {
-          JobTrackerStatistics.TaskTrackerStat ttStat = tracker.getStatistics().
-             getTaskTrackerStat(tt.getTrackerName());
-          out.println("</td><td>" + ttStat.totalTasksStat.getValues().
-                                get(window).getValue());
-          out.println("</td><td>" + ttStat.succeededTasksStat.getValues().
-                                get(window).getValue());
+        for (StatisticsCollector.TimeWindow window : tracker.getStatistics().collector.DEFAULT_COLLECT_WINDOWS) {
+          JobTrackerStatistics.TaskTrackerStat ttStat = tracker.getStatistics().getTaskTrackerStat(tt.getTrackerName());
+          out.print("<td>" + ttStat.totalTasksStat.getValues().get(window).getValue() + "</td>\n");
+          out.print("<td>" + ttStat.succeededTasksStat.getValues().get(window).getValue() + "</td>\n");
         }
-        
-        out.print("</td><td>" + sinceHeartbeat + "</td></tr>\n");
+        out.print("</tr>\n");
       }
       out.print("</table>\n");
-      out.print("</center>\n");
       if (maxFailures > 0) {
-        out.print("Highest Failures: " + failureKing + " with " + maxFailures + 
-                  " failures<br>\n");
+        out.print("Highest Failures: " + failureKing + " with " + maxFailures + " failures<br>\n");
       }
     }
+    out.print("</div>\n");
   }
 
-  public void generateTableForExcludedNodes(JspWriter out, JobTracker tracker) 
-  throws IOException {
+  public void generateTableForExcludedNodes(JspWriter out, JobTracker tracker) throws IOException {
     // excluded nodes
     out.println("<h2>Excluded Nodes</h2>");
     Collection<String> d = tracker.getExcludedNodes();
@@ -154,20 +189,20 @@
 %>
 
 <html>
+<head>
+<link rel="stylesheet" type="text/css" href="/static/hadoop.css">
+<title>Hadoop JobTracker <%=trackerName%></title>
+</head>
 
-<title><%=trackerName%> Hadoop Machine List</title>
-
 <body>
-<h1><a href="jobtracker.jsp"><%=trackerName%></a> Hadoop Machine List</h1>
+<h1>JobTracker '<a href="jobtracker.jsp"><%=trackerName%></a>' Machine List</h1>
 
 <%
-  if (("excluded").equals(type)) {
-    generateTableForExcludedNodes(out, tracker);
-  } else {
-    generateTaskTrackerTable(out, type, tracker);
-  }
+	if (("excluded").equals(type)) {
+		generateTableForExcludedNodes(out, tracker);
+	} else {
+		generateTaskTrackerTable(out, type, tracker);
+	}
+	
+	out.println(ServletUtil.htmlFooter());
 %>
-
-<%
-out.println(ServletUtil.htmlFooter());
-%>
